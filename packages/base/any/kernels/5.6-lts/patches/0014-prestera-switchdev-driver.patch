From 5a0f05150788b99cdcd7384723c4ad011b7dda03 Mon Sep 17 00:00:00 2001
From: Taras Chornyi <taras.chornyi@plvision.eu>
Date: Mon, 16 Nov 2020 18:47:04 +0200
Subject: [PATCH] Prestera switchdev driver

Marvell Prestera 98DX326x integrates up to 24 ports of 1GbE with 8
ports of 10GbE uplinks or 2 ports of 40Gbps stacking for a largely
wireless SMB deployment.

Prestera Switchdev is a firmware based driver that operates via PCI bus.  The
current implementation supports only boards designed for the Marvell Switchdev
solution and requires special firmware.

This driver implementation includes only L1, basic L2 support, and RX/TX.

The core Prestera switching logic is implemented in prestera_main.c, there is
an intermediate hw layer between core logic and firmware. It is
implemented in prestera_hw.c, the purpose of it is to encapsulate hw
related logic, in future there is a plan to support more devices with
different HW related configurations.

Signed-off-by: Taras Chornyi <taras.chornyi@plvision.eu>
---
 drivers/net/ethernet/marvell/Kconfig          |    1 +
 drivers/net/ethernet/marvell/Makefile         |    1 +
 .../net/ethernet/marvell/prestera_sw/Kconfig  |   27 +
 .../net/ethernet/marvell/prestera_sw/Makefile |   17 +
 .../ethernet/marvell/prestera_sw/prestera.c   | 2379 +++++++++++++
 .../ethernet/marvell/prestera_sw/prestera.h   |  587 ++++
 .../marvell/prestera_sw/prestera_acl.c        |  400 +++
 .../marvell/prestera_sw/prestera_debugfs.c    |  160 +
 .../marvell/prestera_sw/prestera_debugfs.h    |   14 +
 .../marvell/prestera_sw/prestera_devlink.c    |  108 +
 .../marvell/prestera_sw/prestera_devlink.h    |   23 +
 .../marvell/prestera_sw/prestera_drv_ver.h    |   23 +
 .../marvell/prestera_sw/prestera_dsa.c        |  316 ++
 .../marvell/prestera_sw/prestera_dsa.h        |   67 +
 .../marvell/prestera_sw/prestera_flower.c     |  429 +++
 .../marvell/prestera_sw/prestera_fw_log.c     |  422 +++
 .../marvell/prestera_sw/prestera_fw_log.h     |   15 +
 .../marvell/prestera_sw/prestera_hw.c         | 2246 ++++++++++++
 .../marvell/prestera_sw/prestera_hw.h         |  324 ++
 .../marvell/prestera_sw/prestera_log.c        |  203 ++
 .../marvell/prestera_sw/prestera_log.h        |   58 +
 .../marvell/prestera_sw/prestera_pci.c        |  917 +++++
 .../marvell/prestera_sw/prestera_router.c     | 3060 +++++++++++++++++
 .../marvell/prestera_sw/prestera_rxtx.c       |  222 ++
 .../marvell/prestera_sw/prestera_rxtx.h       |   32 +
 .../marvell/prestera_sw/prestera_rxtx_priv.h  |   61 +
 .../marvell/prestera_sw/prestera_rxtx_sdma.c  |  769 +++++
 .../marvell/prestera_sw/prestera_switchdev.c  | 1647 +++++++++
 28 files changed, 14528 insertions(+)
 create mode 100644 drivers/net/ethernet/marvell/prestera_sw/Kconfig
 create mode 100644 drivers/net/ethernet/marvell/prestera_sw/Makefile
 create mode 100644 drivers/net/ethernet/marvell/prestera_sw/prestera.c
 create mode 100644 drivers/net/ethernet/marvell/prestera_sw/prestera.h
 create mode 100644 drivers/net/ethernet/marvell/prestera_sw/prestera_acl.c
 create mode 100644 drivers/net/ethernet/marvell/prestera_sw/prestera_debugfs.c
 create mode 100644 drivers/net/ethernet/marvell/prestera_sw/prestera_debugfs.h
 create mode 100644 drivers/net/ethernet/marvell/prestera_sw/prestera_devlink.c
 create mode 100644 drivers/net/ethernet/marvell/prestera_sw/prestera_devlink.h
 create mode 100644 drivers/net/ethernet/marvell/prestera_sw/prestera_drv_ver.h
 create mode 100644 drivers/net/ethernet/marvell/prestera_sw/prestera_dsa.c
 create mode 100644 drivers/net/ethernet/marvell/prestera_sw/prestera_dsa.h
 create mode 100644 drivers/net/ethernet/marvell/prestera_sw/prestera_flower.c
 create mode 100644 drivers/net/ethernet/marvell/prestera_sw/prestera_fw_log.c
 create mode 100644 drivers/net/ethernet/marvell/prestera_sw/prestera_fw_log.h
 create mode 100644 drivers/net/ethernet/marvell/prestera_sw/prestera_hw.c
 create mode 100644 drivers/net/ethernet/marvell/prestera_sw/prestera_hw.h
 create mode 100644 drivers/net/ethernet/marvell/prestera_sw/prestera_log.c
 create mode 100644 drivers/net/ethernet/marvell/prestera_sw/prestera_log.h
 create mode 100644 drivers/net/ethernet/marvell/prestera_sw/prestera_pci.c
 create mode 100644 drivers/net/ethernet/marvell/prestera_sw/prestera_router.c
 create mode 100644 drivers/net/ethernet/marvell/prestera_sw/prestera_rxtx.c
 create mode 100644 drivers/net/ethernet/marvell/prestera_sw/prestera_rxtx.h
 create mode 100644 drivers/net/ethernet/marvell/prestera_sw/prestera_rxtx_priv.h
 create mode 100644 drivers/net/ethernet/marvell/prestera_sw/prestera_rxtx_sdma.c
 create mode 100644 drivers/net/ethernet/marvell/prestera_sw/prestera_switchdev.c

diff --git a/drivers/net/ethernet/marvell/Kconfig b/drivers/net/ethernet/marvell/Kconfig
index 3d5caea09..4e48aad97 100644
--- a/drivers/net/ethernet/marvell/Kconfig
+++ b/drivers/net/ethernet/marvell/Kconfig
@@ -171,5 +171,6 @@ config SKY2_DEBUG
 
 
 source "drivers/net/ethernet/marvell/octeontx2/Kconfig"
+source "drivers/net/ethernet/marvell/prestera_sw/Kconfig"
 
 endif # NET_VENDOR_MARVELL
diff --git a/drivers/net/ethernet/marvell/Makefile b/drivers/net/ethernet/marvell/Makefile
index 89dea7284..b7b5a8156 100644
--- a/drivers/net/ethernet/marvell/Makefile
+++ b/drivers/net/ethernet/marvell/Makefile
@@ -11,4 +11,5 @@ obj-$(CONFIG_MVPP2) += mvpp2/
 obj-$(CONFIG_PXA168_ETH) += pxa168_eth.o
 obj-$(CONFIG_SKGE) += skge.o
 obj-$(CONFIG_SKY2) += sky2.o
+obj-$(CONFIG_MRVL_PRESTERA_SW) += prestera_sw/
 obj-y		+= octeontx2/
diff --git a/drivers/net/ethernet/marvell/prestera_sw/Kconfig b/drivers/net/ethernet/marvell/prestera_sw/Kconfig
new file mode 100644
index 000000000..a19e3604d
--- /dev/null
+++ b/drivers/net/ethernet/marvell/prestera_sw/Kconfig
@@ -0,0 +1,27 @@
+# SPDX-License-Identifier: GPL-2.0-only
+#
+# Marvell switch drivers configuration
+#
+
+config MRVL_PRESTERA_SW
+	tristate "Marvell Technologies Prestera switchdev support"
+	depends on NET_SWITCHDEV
+	depends on BRIDGE
+	default m
+	---help---
+	  This driver supports Marvell Technologies Prestera Switchdev
+	  Ethernet Switch ASICs.
+
+	  To compile this driver as a module, choose M here: the
+	  module will be called prestera_sw.
+
+config MRVL_PRESTERA_PCI
+	tristate "Marvell's Prestera Switchdev IPC PCI kernel module"
+	depends on MRVL_PRESTERA_SW
+	default m
+	---help---
+	  This driver supports Marvell's Prestera Switchdev IPC
+          PCI kernel module
+
+	  To compile this driver as a module, choose M here: the
+	  module will be called prestera_nl.
diff --git a/drivers/net/ethernet/marvell/prestera_sw/Makefile b/drivers/net/ethernet/marvell/prestera_sw/Makefile
new file mode 100644
index 000000000..73d3c668d
--- /dev/null
+++ b/drivers/net/ethernet/marvell/prestera_sw/Makefile
@@ -0,0 +1,17 @@
+# SPDX-License-Identifier: GPL-2.0
+#
+# Makefile for the Marvell Switch driver.
+#
+
+ccflags-$(CONFIG_MRVL_PRESTERA_USE_INTR_DRIVER)    += -DMRVL_PRESTERA_USE_INTR_DRIVER
+
+obj-$(CONFIG_MRVL_PRESTERA_SW) += prestera_sw.o
+prestera_sw-objs := prestera.o \
+	prestera_hw.o prestera_switchdev.o prestera_devlink.o prestera_fw_log.o \
+	prestera_rxtx.o prestera_rxtx_sdma.o prestera_dsa.o prestera_router.o \
+	prestera_acl.o prestera_flower.o prestera_debugfs.o
+
+prestera_sw-$(CONFIG_MRVL_PRESTERA_DEBUG) += prestera_log.o
+ccflags-$(CONFIG_MRVL_PRESTERA_DEBUG) += -DCONFIG_MRVL_PRESTERA_DEBUG
+
+obj-$(CONFIG_MRVL_PRESTERA_PCI) += prestera_pci.o
diff --git a/drivers/net/ethernet/marvell/prestera_sw/prestera.c b/drivers/net/ethernet/marvell/prestera_sw/prestera.c
new file mode 100644
index 000000000..2f7a75821
--- /dev/null
+++ b/drivers/net/ethernet/marvell/prestera_sw/prestera.c
@@ -0,0 +1,2379 @@
+/* SPDX-License-Identifier: BSD-3-Clause OR GPL-2.0
+ *
+ * Copyright (c) 2019-2020 Marvell International Ltd. All rights reserved.
+ *
+ */
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/list.h>
+#include <linux/netdevice.h>
+#include <linux/netdev_features.h>
+#include <linux/inetdevice.h>
+#include <linux/etherdevice.h>
+#include <linux/ethtool.h>
+#include <linux/jiffies.h>
+#include <linux/if_bridge.h>
+#include <linux/phylink.h>
+#include <linux/of.h>
+#include <net/switchdev.h>
+#include <linux/of.h>
+#include <linux/of_net.h>
+
+#include "prestera.h"
+#include "prestera_hw.h"
+#include "prestera_debugfs.h"
+#include "prestera_devlink.h"
+#include "prestera_dsa.h"
+#include "prestera_rxtx.h"
+#include "prestera_drv_ver.h"
+
+static u8 trap_policer_profile = 1;
+
+#define MVSW_PR_MTU_DEFAULT 1536
+#define MVSW_PR_MAC_ADDR_OFFSET 4
+
+#define PORT_STATS_CACHE_TIMEOUT_MS	(msecs_to_jiffies(1000))
+#define PORT_STATS_CNT	(sizeof(struct mvsw_pr_port_stats) / sizeof(u64))
+#define PORT_STATS_IDX(name) \
+	(offsetof(struct mvsw_pr_port_stats, name) / sizeof(u64))
+#define PORT_STATS_FIELD(name)	\
+	[PORT_STATS_IDX(name)] = __stringify(name)
+
+static struct list_head switches_registered;
+
+static const char mvsw_driver_kind[] = "prestera_sw";
+static const char mvsw_driver_name[] = "mvsw_switchdev";
+static const char mvsw_driver_version[] = PRESTERA_DRV_VER;
+
+#define mvsw_dev(sw)		((sw)->dev->dev)
+#define mvsw_dev_name(sw)	dev_name((sw)->dev->dev)
+
+static struct workqueue_struct *mvsw_pr_wq;
+
+struct mvsw_pr_link_mode {
+	enum ethtool_link_mode_bit_indices eth_mode;
+	u32 speed;
+	u64 pr_mask;
+	u8 duplex;
+	u8 port_type;
+};
+
+static const struct mvsw_pr_link_mode
+mvsw_pr_link_modes[MVSW_LINK_MODE_MAX] = {
+	[MVSW_LINK_MODE_10baseT_Half_BIT] = {
+		.eth_mode =  ETHTOOL_LINK_MODE_10baseT_Half_BIT,
+		.speed = 10,
+		.pr_mask = 1 << MVSW_LINK_MODE_10baseT_Half_BIT,
+		.duplex = MVSW_PORT_DUPLEX_HALF,
+		.port_type = MVSW_PORT_TYPE_TP,
+	},
+	[MVSW_LINK_MODE_10baseT_Full_BIT] = {
+		.eth_mode =  ETHTOOL_LINK_MODE_10baseT_Full_BIT,
+		.speed = 10,
+		.pr_mask = 1 << MVSW_LINK_MODE_10baseT_Full_BIT,
+		.duplex = MVSW_PORT_DUPLEX_FULL,
+		.port_type = MVSW_PORT_TYPE_TP,
+	},
+	[MVSW_LINK_MODE_100baseT_Half_BIT] = {
+		.eth_mode =  ETHTOOL_LINK_MODE_100baseT_Half_BIT,
+		.speed = 100,
+		.pr_mask = 1 << MVSW_LINK_MODE_100baseT_Half_BIT,
+		.duplex = MVSW_PORT_DUPLEX_HALF,
+		.port_type = MVSW_PORT_TYPE_TP,
+	},
+	[MVSW_LINK_MODE_100baseT_Full_BIT] = {
+		.eth_mode =  ETHTOOL_LINK_MODE_100baseT_Full_BIT,
+		.speed = 100,
+		.pr_mask = 1 << MVSW_LINK_MODE_100baseT_Full_BIT,
+		.duplex = MVSW_PORT_DUPLEX_FULL,
+		.port_type = MVSW_PORT_TYPE_TP,
+	},
+	[MVSW_LINK_MODE_1000baseT_Half_BIT] = {
+		.eth_mode =  ETHTOOL_LINK_MODE_1000baseT_Half_BIT,
+		.speed = 1000,
+		.pr_mask = 1 << MVSW_LINK_MODE_1000baseT_Half_BIT,
+		.duplex = MVSW_PORT_DUPLEX_HALF,
+		.port_type = MVSW_PORT_TYPE_TP,
+	},
+	[MVSW_LINK_MODE_1000baseT_Full_BIT] = {
+		.eth_mode =  ETHTOOL_LINK_MODE_1000baseT_Full_BIT,
+		.speed = 1000,
+		.pr_mask = 1 << MVSW_LINK_MODE_1000baseT_Full_BIT,
+		.duplex = MVSW_PORT_DUPLEX_FULL,
+		.port_type = MVSW_PORT_TYPE_TP,
+	},
+	[MVSW_LINK_MODE_1000baseX_Full_BIT] = {
+		.eth_mode = ETHTOOL_LINK_MODE_1000baseX_Full_BIT,
+		.speed = 1000,
+		.pr_mask = 1 << MVSW_LINK_MODE_1000baseX_Full_BIT,
+		.duplex = MVSW_PORT_DUPLEX_FULL,
+		.port_type = MVSW_PORT_TYPE_FIBRE,
+	},
+	[MVSW_LINK_MODE_1000baseKX_Full_BIT] = {
+		.eth_mode = ETHTOOL_LINK_MODE_1000baseKX_Full_BIT,
+		.speed = 1000,
+		.pr_mask = 1 << MVSW_LINK_MODE_1000baseKX_Full_BIT,
+		.duplex = MVSW_PORT_DUPLEX_FULL,
+		.port_type = MVSW_PORT_TYPE_TP,
+	},
+	[MVSW_LINK_MODE_2500baseX_Full_BIT] = {
+		.eth_mode =  ETHTOOL_LINK_MODE_2500baseX_Full_BIT,
+		.speed = 2500,
+		.pr_mask = 1 << MVSW_LINK_MODE_2500baseX_Full_BIT,
+		.duplex = MVSW_PORT_DUPLEX_FULL,
+	},
+	[MVSW_LINK_MODE_10GbaseKR_Full_BIT] = {
+		.eth_mode = ETHTOOL_LINK_MODE_10000baseKR_Full_BIT,
+		.speed = 10000,
+		.pr_mask = 1 << MVSW_LINK_MODE_10GbaseKR_Full_BIT,
+		.duplex = MVSW_PORT_DUPLEX_FULL,
+		.port_type = MVSW_PORT_TYPE_TP,
+	},
+	[MVSW_LINK_MODE_10GbaseSR_Full_BIT] = {
+		.eth_mode = ETHTOOL_LINK_MODE_10000baseSR_Full_BIT,
+		.speed = 10000,
+		.pr_mask = 1 << MVSW_LINK_MODE_10GbaseSR_Full_BIT,
+		.duplex = MVSW_PORT_DUPLEX_FULL,
+		.port_type = MVSW_PORT_TYPE_FIBRE,
+	},
+	[MVSW_LINK_MODE_10GbaseLR_Full_BIT] = {
+		.eth_mode = ETHTOOL_LINK_MODE_10000baseLR_Full_BIT,
+		.speed = 10000,
+		.pr_mask = 1 << MVSW_LINK_MODE_10GbaseLR_Full_BIT,
+		.duplex = MVSW_PORT_DUPLEX_FULL,
+		.port_type = MVSW_PORT_TYPE_FIBRE,
+	},
+	[MVSW_LINK_MODE_20GbaseKR2_Full_BIT] = {
+		.eth_mode = ETHTOOL_LINK_MODE_20000baseKR2_Full_BIT,
+		.speed = 20000,
+		.pr_mask = 1 << MVSW_LINK_MODE_20GbaseKR2_Full_BIT,
+		.duplex = MVSW_PORT_DUPLEX_FULL,
+		.port_type = MVSW_PORT_TYPE_TP,
+	},
+	[MVSW_LINK_MODE_25GbaseCR_Full_BIT] = {
+		.eth_mode = ETHTOOL_LINK_MODE_25000baseCR_Full_BIT,
+		.speed = 25000,
+		.pr_mask = 1 << MVSW_LINK_MODE_25GbaseCR_Full_BIT,
+		.duplex = MVSW_PORT_DUPLEX_FULL,
+		.port_type = MVSW_PORT_TYPE_DA,
+	},
+	[MVSW_LINK_MODE_25GbaseKR_Full_BIT] = {
+		.eth_mode = ETHTOOL_LINK_MODE_25000baseKR_Full_BIT,
+		.speed = 25000,
+		.pr_mask = 1 << MVSW_LINK_MODE_25GbaseKR_Full_BIT,
+		.duplex = MVSW_PORT_DUPLEX_FULL,
+		.port_type = MVSW_PORT_TYPE_TP,
+	},
+	[MVSW_LINK_MODE_25GbaseSR_Full_BIT] = {
+		.eth_mode = ETHTOOL_LINK_MODE_25000baseSR_Full_BIT,
+		.speed = 25000,
+		.pr_mask = 1 << MVSW_LINK_MODE_25GbaseSR_Full_BIT,
+		.duplex = MVSW_PORT_DUPLEX_FULL,
+		.port_type = MVSW_PORT_TYPE_FIBRE,
+	},
+	[MVSW_LINK_MODE_40GbaseKR4_Full_BIT] = {
+		.eth_mode = ETHTOOL_LINK_MODE_40000baseKR4_Full_BIT,
+		.speed = 40000,
+		.pr_mask = 1 << MVSW_LINK_MODE_40GbaseKR4_Full_BIT,
+		.duplex = MVSW_PORT_DUPLEX_FULL,
+		.port_type = MVSW_PORT_TYPE_TP,
+	},
+	[MVSW_LINK_MODE_40GbaseCR4_Full_BIT] = {
+		.eth_mode = ETHTOOL_LINK_MODE_40000baseCR4_Full_BIT,
+		.speed = 40000,
+		.pr_mask = 1 << MVSW_LINK_MODE_40GbaseCR4_Full_BIT,
+		.duplex = MVSW_PORT_DUPLEX_FULL,
+		.port_type = MVSW_PORT_TYPE_DA,
+	},
+	[MVSW_LINK_MODE_40GbaseSR4_Full_BIT] = {
+		.eth_mode = ETHTOOL_LINK_MODE_40000baseSR4_Full_BIT,
+		.speed = 40000,
+		.pr_mask = 1 << MVSW_LINK_MODE_40GbaseSR4_Full_BIT,
+		.duplex = MVSW_PORT_DUPLEX_FULL,
+		.port_type = MVSW_PORT_TYPE_FIBRE,
+	},
+	[MVSW_LINK_MODE_50GbaseCR2_Full_BIT] = {
+		.eth_mode = ETHTOOL_LINK_MODE_50000baseCR2_Full_BIT,
+		.speed = 50000,
+		.pr_mask = 1 << MVSW_LINK_MODE_50GbaseCR2_Full_BIT,
+		.duplex = MVSW_PORT_DUPLEX_FULL,
+		.port_type = MVSW_PORT_TYPE_DA,
+	},
+	[MVSW_LINK_MODE_50GbaseKR2_Full_BIT] = {
+		.eth_mode = ETHTOOL_LINK_MODE_50000baseKR2_Full_BIT,
+		.speed = 50000,
+		.pr_mask = 1 << MVSW_LINK_MODE_50GbaseKR2_Full_BIT,
+		.duplex = MVSW_PORT_DUPLEX_FULL,
+		.port_type = MVSW_PORT_TYPE_TP,
+	},
+	[MVSW_LINK_MODE_50GbaseSR2_Full_BIT] = {
+		.eth_mode = ETHTOOL_LINK_MODE_50000baseSR2_Full_BIT,
+		.speed = 50000,
+		.pr_mask = 1 << MVSW_LINK_MODE_50GbaseSR2_Full_BIT,
+		.duplex = MVSW_PORT_DUPLEX_FULL,
+		.port_type = MVSW_PORT_TYPE_FIBRE,
+	},
+	[MVSW_LINK_MODE_100GbaseKR4_Full_BIT] = {
+		.eth_mode = ETHTOOL_LINK_MODE_100000baseKR4_Full_BIT,
+		.speed = 100000,
+		.pr_mask = 1 << MVSW_LINK_MODE_100GbaseKR4_Full_BIT,
+		.duplex = MVSW_PORT_DUPLEX_FULL,
+		.port_type = MVSW_PORT_TYPE_TP,
+	},
+	[MVSW_LINK_MODE_100GbaseSR4_Full_BIT] = {
+		.eth_mode = ETHTOOL_LINK_MODE_100000baseSR4_Full_BIT,
+		.speed = 100000,
+		.pr_mask = 1 << MVSW_LINK_MODE_100GbaseSR4_Full_BIT,
+		.duplex = MVSW_PORT_DUPLEX_FULL,
+		.port_type = MVSW_PORT_TYPE_FIBRE,
+	},
+	[MVSW_LINK_MODE_100GbaseCR4_Full_BIT] = {
+		.eth_mode = ETHTOOL_LINK_MODE_100000baseCR4_Full_BIT,
+		.speed = 100000,
+		.pr_mask = 1 << MVSW_LINK_MODE_100GbaseCR4_Full_BIT,
+		.duplex = MVSW_PORT_DUPLEX_FULL,
+		.port_type = MVSW_PORT_TYPE_DA,
+	}
+};
+
+struct mvsw_pr_fec {
+	u32 eth_fec;
+	enum ethtool_link_mode_bit_indices eth_mode;
+	u8 pr_fec;
+};
+
+static const struct mvsw_pr_fec mvsw_pr_fec_caps[MVSW_PORT_FEC_MAX] = {
+	[MVSW_PORT_FEC_OFF_BIT] = {
+		.eth_fec = ETHTOOL_FEC_OFF,
+		.eth_mode = ETHTOOL_LINK_MODE_FEC_NONE_BIT,
+		.pr_fec = 1 << MVSW_PORT_FEC_OFF_BIT,
+	},
+	[MVSW_PORT_FEC_BASER_BIT] = {
+		.eth_fec = ETHTOOL_FEC_BASER,
+		.eth_mode = ETHTOOL_LINK_MODE_FEC_BASER_BIT,
+		.pr_fec = 1 << MVSW_PORT_FEC_BASER_BIT,
+	},
+	[MVSW_PORT_FEC_RS_BIT] = {
+		.eth_fec = ETHTOOL_FEC_RS,
+		.eth_mode = ETHTOOL_LINK_MODE_FEC_RS_BIT,
+		.pr_fec = 1 << MVSW_PORT_FEC_RS_BIT,
+	}
+};
+
+struct mvsw_pr_port_type {
+	enum ethtool_link_mode_bit_indices eth_mode;
+	u8 eth_type;
+};
+
+static const struct mvsw_pr_port_type
+mvsw_pr_port_types[MVSW_PORT_TYPE_MAX] = {
+	[MVSW_PORT_TYPE_NONE] = {
+		.eth_mode = __ETHTOOL_LINK_MODE_MASK_NBITS,
+		.eth_type = PORT_NONE,
+	},
+	[MVSW_PORT_TYPE_TP] = {
+		.eth_mode = ETHTOOL_LINK_MODE_TP_BIT,
+		.eth_type = PORT_TP,
+	},
+	[MVSW_PORT_TYPE_AUI] = {
+		.eth_mode = ETHTOOL_LINK_MODE_AUI_BIT,
+		.eth_type = PORT_AUI,
+	},
+	[MVSW_PORT_TYPE_MII] = {
+		.eth_mode = ETHTOOL_LINK_MODE_MII_BIT,
+		.eth_type = PORT_MII,
+	},
+	[MVSW_PORT_TYPE_FIBRE] = {
+		.eth_mode = ETHTOOL_LINK_MODE_FIBRE_BIT,
+		.eth_type = PORT_FIBRE,
+	},
+	[MVSW_PORT_TYPE_BNC] = {
+		.eth_mode = ETHTOOL_LINK_MODE_BNC_BIT,
+		.eth_type = PORT_BNC,
+	},
+	[MVSW_PORT_TYPE_DA] = {
+		.eth_mode = ETHTOOL_LINK_MODE_TP_BIT,
+		.eth_type = PORT_TP,
+	},
+	[MVSW_PORT_TYPE_OTHER] = {
+		.eth_mode = __ETHTOOL_LINK_MODE_MASK_NBITS,
+		.eth_type = PORT_OTHER,
+	}
+};
+
+static const char mvsw_pr_port_cnt_name[PORT_STATS_CNT][ETH_GSTRING_LEN] = {
+	PORT_STATS_FIELD(good_octets_received),
+	PORT_STATS_FIELD(bad_octets_received),
+	PORT_STATS_FIELD(mac_trans_error),
+	PORT_STATS_FIELD(broadcast_frames_received),
+	PORT_STATS_FIELD(multicast_frames_received),
+	PORT_STATS_FIELD(frames_64_octets),
+	PORT_STATS_FIELD(frames_65_to_127_octets),
+	PORT_STATS_FIELD(frames_128_to_255_octets),
+	PORT_STATS_FIELD(frames_256_to_511_octets),
+	PORT_STATS_FIELD(frames_512_to_1023_octets),
+	PORT_STATS_FIELD(frames_1024_to_max_octets),
+	PORT_STATS_FIELD(excessive_collision),
+	PORT_STATS_FIELD(multicast_frames_sent),
+	PORT_STATS_FIELD(broadcast_frames_sent),
+	PORT_STATS_FIELD(fc_sent),
+	PORT_STATS_FIELD(fc_received),
+	PORT_STATS_FIELD(buffer_overrun),
+	PORT_STATS_FIELD(undersize),
+	PORT_STATS_FIELD(fragments),
+	PORT_STATS_FIELD(oversize),
+	PORT_STATS_FIELD(jabber),
+	PORT_STATS_FIELD(rx_error_frame_received),
+	PORT_STATS_FIELD(bad_crc),
+	PORT_STATS_FIELD(collisions),
+	PORT_STATS_FIELD(late_collision),
+	PORT_STATS_FIELD(unicast_frames_received),
+	PORT_STATS_FIELD(unicast_frames_sent),
+	PORT_STATS_FIELD(sent_multiple),
+	PORT_STATS_FIELD(sent_deferred),
+	PORT_STATS_FIELD(good_octets_sent),
+};
+
+static LIST_HEAD(mvsw_pr_block_cb_list);
+
+static struct mvsw_pr_port *__find_pr_port(const struct mvsw_pr_switch *sw,
+					   u32 port_id)
+{
+	struct mvsw_pr_port *port;
+
+	list_for_each_entry(port, &sw->port_list, list) {
+		if (port->id == port_id)
+			return port;
+	}
+
+	return NULL;
+}
+
+static int mvsw_pr_port_state_set(struct net_device *dev, bool is_up)
+{
+	struct mvsw_pr_port *port = netdev_priv(dev);
+	int err;
+
+	if (!is_up)
+		netif_stop_queue(dev);
+
+	err = mvsw_pr_hw_port_state_set(port, is_up);
+
+	if (is_up && !err)
+		netif_start_queue(dev);
+
+	return err;
+}
+
+static int mvsw_pr_port_open(struct net_device *dev)
+{
+	return mvsw_pr_port_state_set(dev, true);
+}
+
+static int mvsw_pr_port_close(struct net_device *dev)
+{
+	struct mvsw_pr_port *port = netdev_priv(dev);
+
+	mvsw_pr_fdb_flush_port(port, MVSW_PR_FDB_FLUSH_MODE_DYNAMIC);
+
+	return mvsw_pr_port_state_set(dev, false);
+}
+
+static netdev_tx_t mvsw_pr_port_xmit(struct sk_buff *skb,
+				     struct net_device *dev)
+{
+	struct mvsw_pr_port *port = netdev_priv(dev);
+	struct mvsw_pr_rxtx_info rxtx_info = {
+		.port_id = port->id
+	};
+
+	return mvsw_pr_rxtx_xmit(skb, &rxtx_info);
+}
+
+/* TC flower */
+static int
+mvsw_pr_setup_tc_cls_flower(struct prestera_acl_block *acl_block,
+			    struct flow_cls_offload *f)
+{
+	struct mvsw_pr_switch *sw = prestera_acl_block_sw(acl_block);
+
+	if (f->common.chain_index != 0)
+		return -EOPNOTSUPP;
+
+	switch (f->command) {
+	case FLOW_CLS_REPLACE:
+		return mvsw_pr_flower_replace(sw, acl_block, f);
+	case FLOW_CLS_DESTROY:
+		mvsw_pr_flower_destroy(sw, acl_block, f);
+		return 0;
+	case FLOW_CLS_STATS:
+		return mvsw_pr_flower_stats(sw, acl_block, f);
+	default:
+		return -EOPNOTSUPP;
+	}
+}
+
+static int mvsw_pr_setup_tc_block_cb_flower(enum tc_setup_type type,
+					    void *type_data, void *cb_priv)
+{
+	struct prestera_acl_block *acl_block = cb_priv;
+
+	switch (type) {
+	case TC_SETUP_CLSFLOWER:
+		if (prestera_acl_block_disabled(acl_block))
+			return -EOPNOTSUPP;
+		return mvsw_pr_setup_tc_cls_flower(acl_block, type_data);
+	default:
+		return -EOPNOTSUPP;
+	}
+}
+
+static void mvsw_pr_tc_block_flower_release(void *cb_priv)
+{
+	struct prestera_acl_block *acl_block = cb_priv;
+
+	prestera_acl_block_destroy(acl_block);
+}
+
+static int
+mvsw_pr_setup_tc_block_flower_bind(struct mvsw_pr_port *port,
+				   struct flow_block_offload *f)
+{
+	struct mvsw_pr_switch *sw = port->sw;
+	struct prestera_acl_block *acl_block;
+	struct flow_block_cb *block_cb;
+	bool register_block = false;
+	bool disable_block = false;
+	int err;
+
+	block_cb = flow_block_cb_lookup(f->block,
+					mvsw_pr_setup_tc_block_cb_flower, sw);
+	if (!block_cb) {
+		acl_block = prestera_acl_block_create(sw, f->net);
+		if (!acl_block)
+			return -ENOMEM;
+		block_cb = flow_block_cb_alloc(mvsw_pr_setup_tc_block_cb_flower,
+					       sw, acl_block,
+					       mvsw_pr_tc_block_flower_release);
+		if (IS_ERR(block_cb)) {
+			prestera_acl_block_destroy(acl_block);
+			err = PTR_ERR(block_cb);
+			goto err_cb_register;
+		}
+		register_block = true;
+	} else {
+		acl_block = flow_block_cb_priv(block_cb);
+	}
+	flow_block_cb_incref(block_cb);
+
+	if (!tc_can_offload(port->net_dev)) {
+		if (prestera_acl_block_rule_count(acl_block)) {
+			err = -EOPNOTSUPP;
+			goto err_block_bind;
+		}
+
+		disable_block = true;
+	}
+
+	err = prestera_acl_block_bind(sw, acl_block, port);
+	if (err)
+		goto err_block_bind;
+
+	if (register_block) {
+		flow_block_cb_add(block_cb, f);
+		list_add_tail(&block_cb->driver_list, &mvsw_pr_block_cb_list);
+	}
+
+	if (disable_block)
+		prestera_acl_block_disable_inc(acl_block);
+
+	port->acl_block = acl_block;
+	return 0;
+
+err_block_bind:
+	if (!flow_block_cb_decref(block_cb))
+		flow_block_cb_free(block_cb);
+err_cb_register:
+	return err;
+}
+
+static void
+mvsw_pr_setup_tc_block_flower_unbind(struct mvsw_pr_port *port,
+				     struct flow_block_offload *f)
+{
+	struct mvsw_pr_switch *sw = port->sw;
+	struct prestera_acl_block *acl_block;
+	struct flow_block_cb *block_cb;
+	int err;
+
+	block_cb = flow_block_cb_lookup(f->block,
+					mvsw_pr_setup_tc_block_cb_flower, sw);
+	if (!block_cb)
+		return;
+
+	acl_block = flow_block_cb_priv(block_cb);
+
+	if (!tc_can_offload(port->net_dev))
+		prestera_acl_block_disable_dec(acl_block);
+
+	err = prestera_acl_block_unbind(sw, acl_block, port);
+	if (!err && !flow_block_cb_decref(block_cb)) {
+		flow_block_cb_remove(block_cb, f);
+		list_del(&block_cb->driver_list);
+	}
+	port->acl_block = NULL;
+}
+
+static int mvsw_sp_setup_tc_block(struct mvsw_pr_port *port,
+				  struct flow_block_offload *f)
+{
+	if (f->binder_type != FLOW_BLOCK_BINDER_TYPE_CLSACT_INGRESS)
+		return -EOPNOTSUPP;
+
+	f->driver_block_list = &mvsw_pr_block_cb_list;
+
+	switch (f->command) {
+	case FLOW_BLOCK_BIND:
+		return mvsw_pr_setup_tc_block_flower_bind(port, f);
+	case FLOW_BLOCK_UNBIND:
+		mvsw_pr_setup_tc_block_flower_unbind(port, f);
+		return 0;
+	default:
+		return -EOPNOTSUPP;
+	}
+}
+
+static int mvsw_pr_setup_tc(struct net_device *dev, enum tc_setup_type type,
+			    void *type_data)
+{
+	struct mvsw_pr_port *port = netdev_priv(dev);
+
+	switch (type) {
+	case TC_SETUP_BLOCK:
+		return mvsw_sp_setup_tc_block(port, type_data);
+	default:
+		return -EOPNOTSUPP;
+	}
+}
+
+static void mvsw_pr_set_rx_mode(struct net_device *dev)
+{
+	/* TO DO: add implementation */
+}
+
+static int mvsw_is_valid_mac_addr(struct mvsw_pr_port *port, u8 *addr)
+{
+	int err;
+
+	if (!is_valid_ether_addr(addr))
+		return -EADDRNOTAVAIL;
+
+	err = memcmp(port->sw->base_mac, addr, ETH_ALEN - 1);
+	if (err)
+		return -EINVAL;
+
+	return 0;
+}
+
+static int mvsw_pr_port_set_mac_address(struct net_device *dev, void *p)
+{
+	struct mvsw_pr_port *port = netdev_priv(dev);
+	struct sockaddr *addr = p;
+	int err;
+
+	err = mvsw_is_valid_mac_addr(port, addr->sa_data);
+	if (err)
+		return err;
+
+	err = mvsw_pr_hw_port_mac_set(port, addr->sa_data);
+	if (!err)
+		memcpy(dev->dev_addr, addr->sa_data, dev->addr_len);
+
+	return err;
+}
+
+static int mvsw_pr_port_change_mtu(struct net_device *dev, int mtu)
+{
+	struct mvsw_pr_port *port = netdev_priv(dev);
+	int err;
+
+	if (port->sw->mtu_min <= mtu && mtu <= port->sw->mtu_max)
+		err = mvsw_pr_hw_port_mtu_set(port, mtu);
+	else
+		err = -EINVAL;
+
+	if (!err)
+		dev->mtu = mtu;
+
+	return err;
+}
+
+static void mvsw_pr_port_get_stats64(struct net_device *dev,
+				     struct rtnl_link_stats64 *stats)
+{
+	struct mvsw_pr_port *port = netdev_priv(dev);
+	struct mvsw_pr_port_stats *port_stats = &port->cached_hw_stats.stats;
+
+	stats->rx_packets =	port_stats->broadcast_frames_received +
+				port_stats->multicast_frames_received +
+				port_stats->unicast_frames_received;
+
+	stats->tx_packets =	port_stats->broadcast_frames_sent +
+				port_stats->multicast_frames_sent +
+				port_stats->unicast_frames_sent;
+
+	stats->rx_bytes = port_stats->good_octets_received;
+
+	stats->tx_bytes = port_stats->good_octets_sent;
+
+	stats->rx_errors = port_stats->rx_error_frame_received;
+	stats->tx_errors = port_stats->mac_trans_error;
+
+	stats->rx_dropped = port_stats->buffer_overrun;
+	stats->tx_dropped = 0;
+
+	stats->multicast = port_stats->multicast_frames_received;
+	stats->collisions = port_stats->excessive_collision;
+
+	stats->rx_crc_errors = port_stats->bad_crc;
+}
+
+static void mvsw_pr_port_get_hw_stats(struct mvsw_pr_port *port)
+{
+	mvsw_pr_hw_port_stats_get(port, &port->cached_hw_stats.stats);
+}
+
+static void update_stats_cache(struct work_struct *work)
+{
+	struct mvsw_pr_port *port =
+		container_of(work, struct mvsw_pr_port,
+			     cached_hw_stats.caching_dw.work);
+
+	rtnl_lock();
+	mvsw_pr_port_get_hw_stats(port);
+	rtnl_unlock();
+
+	queue_delayed_work(mvsw_pr_wq, &port->cached_hw_stats.caching_dw,
+			   PORT_STATS_CACHE_TIMEOUT_MS);
+}
+
+static bool mvsw_pr_port_has_offload_stats(const struct net_device *dev,
+					   int attr_id)
+{
+	/* TO DO: add implementation */
+	return false;
+}
+
+static int mvsw_pr_port_get_offload_stats(int attr_id,
+					  const struct net_device *dev,
+					  void *sp)
+{
+	/* TO DO: add implementation */
+	return 0;
+}
+
+static int mvsw_pr_feature_hw_tc(struct net_device *dev, bool enable)
+{
+	struct mvsw_pr_port *port = netdev_priv(dev);
+
+	if (!enable) {
+		if (prestera_acl_block_rule_count(port->acl_block)) {
+			netdev_err(dev, "Active offloaded tc filters, can't turn hw_tc_offload off\n");
+			return -EINVAL;
+		}
+		prestera_acl_block_disable_inc(port->acl_block);
+	} else {
+		prestera_acl_block_disable_dec(port->acl_block);
+	}
+	return 0;
+}
+
+static int
+mvsw_pr_handle_feature(struct net_device *dev,
+		       netdev_features_t wanted_features,
+		       netdev_features_t feature,
+		       int (*feature_handler)(struct net_device *dev,
+					      bool enable))
+{
+	netdev_features_t changes = wanted_features ^ dev->features;
+	bool enable = !!(wanted_features & feature);
+	int err;
+
+	if (!(changes & feature))
+		return 0;
+
+	err = feature_handler(dev, enable);
+	if (err) {
+		netdev_err(dev, "%s feature %pNF failed, err %d\n",
+			   enable ? "Enable" : "Disable", &feature, err);
+		return err;
+	}
+
+	if (enable)
+		dev->features |= feature;
+	else
+		dev->features &= ~feature;
+
+	return 0;
+}
+
+static int mvsw_pr_set_features(struct net_device *dev,
+				netdev_features_t features)
+{
+	netdev_features_t oper_features = dev->features;
+	int err = 0;
+
+	err |= mvsw_pr_handle_feature(dev, features, NETIF_F_HW_TC,
+				       mvsw_pr_feature_hw_tc);
+
+	if (err) {
+		dev->features = oper_features;
+		return -EINVAL;
+	}
+
+	return 0;
+}
+
+static void mvsw_pr_port_get_drvinfo(struct net_device *dev,
+				     struct ethtool_drvinfo *drvinfo)
+{
+	struct mvsw_pr_port *port = netdev_priv(dev);
+	struct mvsw_pr_switch *sw = port->sw;
+
+	strlcpy(drvinfo->driver, mvsw_driver_kind, sizeof(drvinfo->driver));
+	strlcpy(drvinfo->bus_info, mvsw_dev_name(sw), sizeof(drvinfo->bus_info));
+	snprintf(drvinfo->fw_version, sizeof(drvinfo->fw_version),
+		 "%d.%d.%d",
+		 sw->dev->fw_rev.maj,
+		 sw->dev->fw_rev.min,
+		 sw->dev->fw_rev.sub);
+}
+
+static const struct net_device_ops mvsw_pr_netdev_ops = {
+	.ndo_open = mvsw_pr_port_open,
+	.ndo_stop = mvsw_pr_port_close,
+	.ndo_start_xmit = mvsw_pr_port_xmit,
+	.ndo_setup_tc = mvsw_pr_setup_tc,
+	.ndo_change_mtu = mvsw_pr_port_change_mtu,
+	.ndo_set_rx_mode = mvsw_pr_set_rx_mode,
+	.ndo_get_stats64 = mvsw_pr_port_get_stats64,
+	.ndo_set_features = mvsw_pr_set_features,
+	.ndo_set_mac_address = mvsw_pr_port_set_mac_address,
+	.ndo_has_offload_stats = mvsw_pr_port_has_offload_stats,
+	.ndo_get_offload_stats = mvsw_pr_port_get_offload_stats,
+	.ndo_get_devlink_port = prestera_devlink_get_port,
+};
+
+bool mvsw_pr_netdev_check(const struct net_device *dev)
+{
+	return dev->netdev_ops == &mvsw_pr_netdev_ops;
+}
+
+static int mvsw_pr_lower_dev_walk(struct net_device *lower_dev, void *data)
+{
+	struct mvsw_pr_port **pport = data;
+
+	if (mvsw_pr_netdev_check(lower_dev)) {
+		*pport = netdev_priv(lower_dev);
+		return 1;
+	}
+
+	return 0;
+}
+
+struct mvsw_pr_port *mvsw_pr_port_dev_lower_find(struct net_device *dev)
+{
+	struct mvsw_pr_port *port;
+
+	if (!dev)
+		return NULL;
+
+	if (mvsw_pr_netdev_check(dev))
+		return netdev_priv(dev);
+
+	port = NULL;
+	netdev_walk_all_lower_dev(dev, mvsw_pr_lower_dev_walk, &port);
+
+	return port;
+}
+
+struct mvsw_pr_switch *mvsw_pr_switch_get(struct net_device *dev)
+{
+	struct mvsw_pr_port *port;
+
+	port = mvsw_pr_port_dev_lower_find(dev);
+	return port ? port->sw : NULL;
+}
+
+static void mvsw_modes_to_eth(unsigned long *eth_modes, u64 link_modes, u8 fec,
+			      u8 type)
+{
+	u32 mode;
+
+	for (mode = 0; mode < MVSW_LINK_MODE_MAX; mode++) {
+		if ((mvsw_pr_link_modes[mode].pr_mask & link_modes) == 0)
+			continue;
+		if (type != MVSW_PORT_TYPE_NONE &&
+		    mvsw_pr_link_modes[mode].port_type != type)
+			continue;
+		__set_bit(mvsw_pr_link_modes[mode].eth_mode, eth_modes);
+	}
+
+	for (mode = 0; mode < MVSW_PORT_FEC_MAX; mode++) {
+		if ((mvsw_pr_fec_caps[mode].pr_fec & fec) == 0)
+			continue;
+		__set_bit(mvsw_pr_fec_caps[mode].eth_mode, eth_modes);
+	}
+}
+
+static void mvsw_pr_port_autoneg_get(struct ethtool_link_ksettings *ecmd,
+				     struct mvsw_pr_port *port)
+{
+	ecmd->base.autoneg = port->autoneg ? AUTONEG_ENABLE : AUTONEG_DISABLE;
+
+	mvsw_modes_to_eth(ecmd->link_modes.supported,
+			  port->caps.supp_link_modes,
+			  port->caps.supp_fec,
+			  port->caps.type);
+
+	if (port->caps.type != MVSW_PORT_TYPE_TP)
+		return;
+
+	ethtool_link_ksettings_add_link_mode(ecmd, supported, Autoneg);
+
+	if (!netif_running(port->net_dev))
+		return;
+
+	if (port->autoneg) {
+		mvsw_modes_to_eth(ecmd->link_modes.advertising,
+				  port->adver_link_modes,
+				  port->adver_fec,
+				  port->caps.type);
+		ethtool_link_ksettings_add_link_mode(ecmd, advertising,
+						     Autoneg);
+	} else if (port->caps.transceiver == MVSW_PORT_TRANSCEIVER_COPPER)
+		ethtool_link_ksettings_add_link_mode(ecmd, advertising,
+						     Autoneg);
+}
+
+static int mvsw_modes_from_eth(struct mvsw_pr_port *port,
+			       const struct ethtool_link_ksettings *ecmd,
+			       u64 *link_modes, u8 *fec)
+{
+	struct ethtool_link_ksettings curr = {};
+	u32 mode;
+
+	ethtool_link_ksettings_zero_link_mode(&curr, supported);
+	ethtool_link_ksettings_zero_link_mode(&curr, advertising);
+
+	mvsw_pr_port_autoneg_get(&curr, port);
+
+	if (linkmode_equal(ecmd->link_modes.advertising,
+			   curr.link_modes.advertising)) {
+		*link_modes = port->adver_link_modes;
+		*fec = port->adver_fec;
+		return 0;
+	}
+
+	if (!linkmode_subset(ecmd->link_modes.advertising,
+			     ecmd->link_modes.supported)) {
+		netdev_err(port->net_dev, "Unsupported link mode requested");
+		return -EINVAL;
+	}
+
+	*link_modes  = 0;
+	*fec = 0;
+	for (mode = 0; mode < MVSW_LINK_MODE_MAX; mode++) {
+		if (!test_bit(mvsw_pr_link_modes[mode].eth_mode,
+			      ecmd->link_modes.advertising))
+			continue;
+		if (mvsw_pr_link_modes[mode].port_type != port->caps.type)
+			continue;
+		*link_modes |= mvsw_pr_link_modes[mode].pr_mask;
+	}
+
+	for (mode = 0; mode < MVSW_PORT_FEC_MAX; mode++) {
+		if (!test_bit(mvsw_pr_fec_caps[mode].eth_mode,
+			      ecmd->link_modes.advertising))
+			continue;
+		*fec |= mvsw_pr_fec_caps[mode].pr_fec;
+	}
+
+	if (*link_modes == 0 && *fec == 0) {
+		netdev_err(port->net_dev, "No link modes requested");
+		return -EINVAL;
+	}
+	if (*link_modes == 0)
+		*link_modes = port->adver_link_modes;
+	if (*fec == 0)
+		*fec = port->adver_fec ? port->adver_fec :
+					 BIT(MVSW_PORT_FEC_OFF_BIT);
+
+	return 0;
+}
+
+static void mvsw_pr_port_supp_types_get(struct ethtool_link_ksettings *ecmd,
+					struct mvsw_pr_port *port)
+{
+	u32 mode;
+	u8 ptype;
+
+	for (mode = 0; mode < MVSW_LINK_MODE_MAX; mode++) {
+		if ((mvsw_pr_link_modes[mode].pr_mask &
+		    port->caps.supp_link_modes) == 0)
+			continue;
+		ptype = mvsw_pr_link_modes[mode].port_type;
+		__set_bit(mvsw_pr_port_types[ptype].eth_mode,
+			  ecmd->link_modes.supported);
+	}
+}
+
+static void mvsw_pr_port_speed_get(struct ethtool_link_ksettings *ecmd,
+				   struct mvsw_pr_port *port)
+{
+	u32 speed;
+	int err;
+
+	err = mvsw_pr_hw_port_speed_get(port, &speed);
+	ecmd->base.speed = !err ? speed : SPEED_UNKNOWN;
+}
+
+static int mvsw_pr_port_link_mode_set(struct mvsw_pr_port *port,
+				      u32 speed, u8 duplex, u8 type)
+{
+	u32 new_mode = MVSW_LINK_MODE_MAX;
+	u32 mode;
+
+	for (mode = 0; mode < MVSW_LINK_MODE_MAX; mode++) {
+		if (speed != mvsw_pr_link_modes[mode].speed)
+			continue;
+		if (duplex != mvsw_pr_link_modes[mode].duplex)
+			continue;
+		if (!(mvsw_pr_link_modes[mode].pr_mask &
+		    port->caps.supp_link_modes))
+			continue;
+		if (type != mvsw_pr_link_modes[mode].port_type)
+			continue;
+
+		new_mode = mode;
+		break;
+	}
+
+	if (new_mode == MVSW_LINK_MODE_MAX) {
+		netdev_err(port->net_dev, "Unsupported speed/duplex requested");
+		return -EINVAL;
+	}
+
+	return mvsw_pr_hw_port_link_mode_set(port, new_mode);
+}
+
+static int mvsw_pr_port_speed_duplex_set(const struct ethtool_link_ksettings
+					 *ecmd, struct mvsw_pr_port *port)
+{
+	int err;
+	u8 duplex;
+	u32 speed;
+	u32 curr_mode;
+
+	err = mvsw_pr_hw_port_link_mode_get(port, &curr_mode);
+	if (err || curr_mode >= MVSW_LINK_MODE_MAX)
+		return -EINVAL;
+
+	if (ecmd->base.duplex != DUPLEX_UNKNOWN)
+		duplex = ecmd->base.duplex == DUPLEX_FULL ?
+			 MVSW_PORT_DUPLEX_FULL : MVSW_PORT_DUPLEX_HALF;
+	else
+		duplex = mvsw_pr_link_modes[curr_mode].duplex;
+
+	if (ecmd->base.speed != SPEED_UNKNOWN)
+		speed = ecmd->base.speed;
+	else
+		speed = mvsw_pr_link_modes[curr_mode].speed;
+
+	return mvsw_pr_port_link_mode_set(port, speed, duplex, port->caps.type);
+}
+
+static u8 mvsw_pr_port_type_get(struct mvsw_pr_port *port)
+{
+	if (port->caps.type < MVSW_PORT_TYPE_MAX)
+		return mvsw_pr_port_types[port->caps.type].eth_type;
+	return PORT_OTHER;
+}
+
+static int mvsw_pr_port_type_set(const struct ethtool_link_ksettings *ecmd,
+				 struct mvsw_pr_port *port)
+{
+	int err;
+	u32 type, mode;
+	u32 new_mode = MVSW_LINK_MODE_MAX;
+
+	for (type = 0; type < MVSW_PORT_TYPE_MAX; type++) {
+		if (mvsw_pr_port_types[type].eth_type == ecmd->base.port &&
+		    test_bit(mvsw_pr_port_types[type].eth_mode,
+			     ecmd->link_modes.supported)) {
+			break;
+		}
+	}
+
+	if (type == port->caps.type)
+		return 0;
+
+	if (type != port->caps.type && ecmd->base.autoneg == AUTONEG_ENABLE)
+		return -EINVAL;
+
+	if (type == MVSW_PORT_TYPE_MAX) {
+		pr_err("Unsupported port type requested\n");
+		return -EINVAL;
+	}
+
+	for (mode = 0; mode < MVSW_LINK_MODE_MAX; mode++) {
+		if ((mvsw_pr_link_modes[mode].pr_mask &
+		    port->caps.supp_link_modes) &&
+		    type == mvsw_pr_link_modes[mode].port_type) {
+			new_mode = mode;
+		}
+	}
+
+	if (new_mode < MVSW_LINK_MODE_MAX)
+		err = mvsw_pr_hw_port_link_mode_set(port, new_mode);
+	else
+		err = -EINVAL;
+
+	if (!err) {
+		port->caps.type = type;
+		port->autoneg = false;
+	}
+
+	return err;
+}
+
+static void mvsw_pr_port_remote_cap_get(struct ethtool_link_ksettings *ecmd,
+					struct mvsw_pr_port *port)
+{
+	u64 bitmap;
+	bool pause;
+	bool asym_pause;
+
+	if (!mvsw_pr_hw_port_remote_cap_get(port, &bitmap)) {
+		mvsw_modes_to_eth(ecmd->link_modes.lp_advertising,
+				  bitmap, 0, MVSW_PORT_TYPE_NONE);
+
+		if (!bitmap_empty(ecmd->link_modes.lp_advertising,
+				  __ETHTOOL_LINK_MODE_MASK_NBITS)) {
+			ethtool_link_ksettings_add_link_mode(ecmd,
+							     lp_advertising,
+							     Autoneg);
+		}
+	}
+
+	if (mvsw_pr_hw_port_remote_fc_get(port, &pause, &asym_pause))
+		return;
+	if (pause)
+		ethtool_link_ksettings_add_link_mode(ecmd,
+						     lp_advertising,
+						     Pause);
+	if (asym_pause)
+		ethtool_link_ksettings_add_link_mode(ecmd,
+						     lp_advertising,
+						     Asym_Pause);
+}
+
+static void mvsw_pr_port_duplex_get(struct ethtool_link_ksettings *ecmd,
+				    struct mvsw_pr_port *port)
+{
+	u8 duplex;
+
+	if (!mvsw_pr_hw_port_duplex_get(port, &duplex)) {
+		ecmd->base.duplex = duplex == MVSW_PORT_DUPLEX_FULL ?
+				    DUPLEX_FULL : DUPLEX_HALF;
+	} else {
+		ecmd->base.duplex = DUPLEX_UNKNOWN;
+	}
+}
+
+static int mvsw_pr_port_autoneg_set(struct mvsw_pr_port *port, bool enable,
+				    u64 link_modes, u8 fec)
+{
+	if (port->caps.type != MVSW_PORT_TYPE_TP)
+		return enable ? -EINVAL : 0;
+
+	if (port->autoneg == enable && port->adver_link_modes == link_modes &&
+	    port->adver_fec == fec)
+		return 0;
+
+	if (mvsw_pr_hw_port_autoneg_set(port, enable, link_modes, fec))
+		return -EINVAL;
+
+	port->autoneg = enable;
+	port->adver_link_modes = link_modes;
+	port->adver_fec = fec;
+	return 0;
+}
+
+static int mvsw_pr_port_nway_reset(struct net_device *dev)
+{
+	struct mvsw_pr_port *port = netdev_priv(dev);
+
+	if (netif_running(dev) &&
+	    port->caps.transceiver == MVSW_PORT_TRANSCEIVER_COPPER &&
+	    port->caps.type == MVSW_PORT_TYPE_TP)
+		return mvsw_pr_hw_port_autoneg_restart(port);
+
+	return -EINVAL;
+}
+
+static int mvsw_pr_port_mdix_set(const struct ethtool_link_ksettings *ecmd,
+				 struct mvsw_pr_port *port)
+{
+	if (ecmd->base.eth_tp_mdix_ctrl != ETH_TP_MDI_INVALID &&
+	    port->caps.transceiver == MVSW_PORT_TRANSCEIVER_COPPER &&
+	    port->caps.type == MVSW_PORT_TYPE_TP)
+		return mvsw_pr_hw_port_mdix_set(port,
+						ecmd->base.eth_tp_mdix_ctrl);
+	return 0;
+}
+
+static int mvsw_pr_port_get_link_ksettings(struct net_device *dev,
+					   struct ethtool_link_ksettings *ecmd)
+{
+	struct mvsw_pr_port *port = netdev_priv(dev);
+
+	ethtool_link_ksettings_zero_link_mode(ecmd, supported);
+	ethtool_link_ksettings_zero_link_mode(ecmd, advertising);
+	ethtool_link_ksettings_zero_link_mode(ecmd, lp_advertising);
+
+	mvsw_pr_port_supp_types_get(ecmd, port);
+
+	mvsw_pr_port_autoneg_get(ecmd, port);
+
+	if (port->autoneg && netif_carrier_ok(dev) &&
+	    port->caps.transceiver == MVSW_PORT_TRANSCEIVER_COPPER)
+		mvsw_pr_port_remote_cap_get(ecmd, port);
+
+	if (netif_carrier_ok(dev)) {
+		mvsw_pr_port_speed_get(ecmd, port);
+		mvsw_pr_port_duplex_get(ecmd, port);
+	} else {
+		ecmd->base.speed = SPEED_UNKNOWN;
+		ecmd->base.duplex = DUPLEX_UNKNOWN;
+	}
+
+	ecmd->base.port = mvsw_pr_port_type_get(port);
+
+	if (port->caps.type == MVSW_PORT_TYPE_TP &&
+	    port->caps.transceiver == MVSW_PORT_TRANSCEIVER_COPPER)
+		mvsw_pr_hw_port_mdix_get(port, &ecmd->base.eth_tp_mdix,
+					 &ecmd->base.eth_tp_mdix_ctrl);
+
+	return 0;
+}
+
+static int mvsw_pr_port_set_link_ksettings(struct net_device *dev,
+					   const struct ethtool_link_ksettings
+					   *ecmd)
+{
+	struct mvsw_pr_port *port = netdev_priv(dev);
+	u64 adver_modes = 0;
+	u8 adver_fec = 0;
+	int err;
+
+	err = mvsw_pr_port_type_set(ecmd, port);
+	if (err)
+		return err;
+
+	if (port->caps.transceiver == MVSW_PORT_TRANSCEIVER_COPPER) {
+		err = mvsw_pr_port_mdix_set(ecmd, port);
+		if (err)
+			return err;
+	}
+
+	if (ecmd->base.autoneg == AUTONEG_ENABLE) {
+		if (mvsw_modes_from_eth(port, ecmd, &adver_modes, &adver_fec))
+			return -EINVAL;
+		if (!port->autoneg && !adver_modes)
+			adver_modes = port->caps.supp_link_modes;
+	} else {
+		adver_modes = port->adver_link_modes;
+		adver_fec = port->adver_fec;
+	}
+
+	err = mvsw_pr_port_autoneg_set(port,
+				       ecmd->base.autoneg == AUTONEG_ENABLE,
+				       adver_modes, adver_fec);
+	if (err)
+		return err;
+
+	if (ecmd->base.autoneg == AUTONEG_DISABLE) {
+		err = mvsw_pr_port_speed_duplex_set(ecmd, port);
+		if (err)
+			return err;
+	}
+
+	return 0;
+}
+
+static int mvsw_pr_port_get_fecparam(struct net_device *dev,
+				     struct ethtool_fecparam *fecparam)
+{
+	struct mvsw_pr_port *port = netdev_priv(dev);
+	u32 mode;
+	u8 active;
+	int err;
+
+	err = mvsw_pr_hw_port_fec_get(port, &active);
+	if (err)
+		return err;
+
+	fecparam->fec = 0;
+	for (mode = 0; mode < MVSW_PORT_FEC_MAX; mode++) {
+		if ((mvsw_pr_fec_caps[mode].pr_fec & port->caps.supp_fec) == 0)
+			continue;
+		fecparam->fec |= mvsw_pr_fec_caps[mode].eth_fec;
+	}
+
+	if (active < MVSW_PORT_FEC_MAX)
+		fecparam->active_fec = mvsw_pr_fec_caps[active].eth_fec;
+	else
+		fecparam->active_fec = ETHTOOL_FEC_AUTO;
+
+	return 0;
+}
+
+static int mvsw_pr_port_set_fecparam(struct net_device *dev,
+				     struct ethtool_fecparam *fecparam)
+{
+	struct mvsw_pr_port *port = netdev_priv(dev);
+	u8 fec, active;
+	u32 mode;
+	int err;
+
+	if (port->autoneg) {
+		netdev_err(dev, "FEC set is not allowed while autoneg is on\n");
+		return -EINVAL;
+	}
+
+	err = mvsw_pr_hw_port_fec_get(port, &active);
+	if (err)
+		return err;
+
+	fec = MVSW_PORT_FEC_MAX;
+	for (mode = 0; mode < MVSW_PORT_FEC_MAX; mode++) {
+		if ((mvsw_pr_fec_caps[mode].eth_fec & fecparam->fec) &&
+		    (mvsw_pr_fec_caps[mode].pr_fec & port->caps.supp_fec)) {
+			fec = mode;
+			break;
+		}
+	}
+
+	if (fec == active)
+		return 0;
+
+	if (fec == MVSW_PORT_FEC_MAX) {
+		netdev_err(dev, "Unsupported FEC requested");
+		return -EINVAL;
+	}
+
+	return mvsw_pr_hw_port_fec_set(port, fec);
+}
+
+static void mvsw_pr_port_get_ethtool_stats(struct net_device *dev,
+					   struct ethtool_stats *stats,
+					   u64 *data)
+{
+	struct mvsw_pr_port *port = netdev_priv(dev);
+	struct mvsw_pr_port_stats *port_stats = &port->cached_hw_stats.stats;
+
+	memcpy((u8 *)data, port_stats, sizeof(*port_stats));
+}
+
+static void mvsw_pr_port_get_strings(struct net_device *dev,
+				     u32 stringset, u8 *data)
+{
+	if (stringset != ETH_SS_STATS)
+		return;
+
+	memcpy(data, *mvsw_pr_port_cnt_name, sizeof(mvsw_pr_port_cnt_name));
+}
+
+static int mvsw_pr_port_get_sset_count(struct net_device *dev, int sset)
+{
+	switch (sset) {
+	case ETH_SS_STATS:
+		return PORT_STATS_CNT;
+	default:
+		return -EOPNOTSUPP;
+	}
+}
+
+static const struct ethtool_ops mvsw_pr_ethtool_ops = {
+	.get_drvinfo = mvsw_pr_port_get_drvinfo,
+	.get_link_ksettings = mvsw_pr_port_get_link_ksettings,
+	.set_link_ksettings = mvsw_pr_port_set_link_ksettings,
+	.get_fecparam = mvsw_pr_port_get_fecparam,
+	.set_fecparam = mvsw_pr_port_set_fecparam,
+	.get_sset_count = mvsw_pr_port_get_sset_count,
+	.get_strings = mvsw_pr_port_get_strings,
+	.get_ethtool_stats = mvsw_pr_port_get_ethtool_stats,
+	.get_link = ethtool_op_get_link,
+	.nway_reset = mvsw_pr_port_nway_reset
+};
+
+int mvsw_pr_port_learning_set(struct mvsw_pr_port *port, bool learn)
+{
+	return mvsw_pr_hw_port_learning_set(port, learn);
+}
+
+int mvsw_pr_port_uc_flood_set(struct mvsw_pr_port *port, bool flood)
+{
+	return mvsw_pr_hw_port_uc_flood_set(port, flood);
+}
+
+int mvsw_pr_port_mc_flood_set(struct mvsw_pr_port *port, bool flood)
+{
+	return mvsw_pr_hw_port_mc_flood_set(port, flood);
+}
+
+int mvsw_pr_port_pvid_set(struct mvsw_pr_port *port, u16 vid)
+{
+	int err;
+
+	if (!vid) {
+		err = mvsw_pr_hw_port_accept_frame_type_set
+		    (port, MVSW_ACCEPT_FRAME_TYPE_TAGGED);
+		if (err)
+			return err;
+	} else {
+		err = mvsw_pr_hw_vlan_port_vid_set(port, vid);
+		if (err)
+			return err;
+		err = mvsw_pr_hw_port_accept_frame_type_set
+		    (port, MVSW_ACCEPT_FRAME_TYPE_ALL);
+		if (err)
+			goto err_port_allow_untagged_set;
+	}
+
+	port->pvid = vid;
+	return 0;
+
+err_port_allow_untagged_set:
+	mvsw_pr_hw_vlan_port_vid_set(port, port->pvid);
+	return err;
+}
+
+int mvsw_pr_port_vid_stp_set(struct mvsw_pr_port *port, u16 vid, u8 state)
+{
+	u8 hw_state = state;
+
+	switch (state) {
+	case BR_STATE_DISABLED:
+		hw_state = MVSW_STP_DISABLED;
+		break;
+
+	case BR_STATE_BLOCKING:
+	case BR_STATE_LISTENING:
+		hw_state = MVSW_STP_BLOCK_LISTEN;
+		break;
+
+	case BR_STATE_LEARNING:
+		hw_state = MVSW_STP_LEARN;
+		break;
+
+	case BR_STATE_FORWARDING:
+		hw_state = MVSW_STP_FORWARD;
+		break;
+
+	default:
+		return -EINVAL;
+	}
+
+	return mvsw_pr_hw_port_vid_stp_set(port, vid, hw_state);
+}
+
+struct mvsw_pr_port_vlan*
+mvsw_pr_port_vlan_find_by_vid(const struct mvsw_pr_port *port, u16 vid)
+{
+	struct mvsw_pr_port_vlan *port_vlan;
+
+	list_for_each_entry(port_vlan, &port->vlans_list, list) {
+		if (port_vlan->vid == vid)
+			return port_vlan;
+	}
+
+	return NULL;
+}
+
+struct mvsw_pr_port_vlan*
+mvsw_pr_port_vlan_create(struct mvsw_pr_port *port, u16 vid, bool untagged)
+{
+	struct mvsw_pr_port_vlan *port_vlan;
+	int err;
+
+	port_vlan = mvsw_pr_port_vlan_find_by_vid(port, vid);
+	if (port_vlan)
+		return ERR_PTR(-EEXIST);
+
+	err = mvsw_pr_port_vlan_set(port, vid, true, untagged);
+	if (err)
+		return ERR_PTR(err);
+
+	port_vlan = kzalloc(sizeof(*port_vlan), GFP_KERNEL);
+	if (!port_vlan) {
+		err = -ENOMEM;
+		goto err_port_vlan_alloc;
+	}
+
+	port_vlan->mvsw_pr_port = port;
+	port_vlan->vid = vid;
+
+	list_add(&port_vlan->list, &port->vlans_list);
+
+	return port_vlan;
+
+err_port_vlan_alloc:
+	mvsw_pr_port_vlan_set(port, vid, false, false);
+	return ERR_PTR(err);
+}
+
+static void
+mvsw_pr_port_vlan_cleanup(struct mvsw_pr_port_vlan *port_vlan)
+{
+	if (port_vlan->bridge_port)
+		mvsw_pr_port_vlan_bridge_leave(port_vlan);
+}
+
+void mvsw_pr_port_vlan_destroy(struct mvsw_pr_port_vlan *port_vlan)
+{
+	struct mvsw_pr_port *port = port_vlan->mvsw_pr_port;
+	u16 vid = port_vlan->vid;
+
+	mvsw_pr_port_vlan_cleanup(port_vlan);
+	list_del(&port_vlan->list);
+	kfree(port_vlan);
+	mvsw_pr_hw_vlan_port_set(port, vid, false, false);
+}
+
+int mvsw_pr_port_vlan_set(struct mvsw_pr_port *port, u16 vid,
+			  bool is_member, bool untagged)
+{
+	return mvsw_pr_hw_vlan_port_set(port, vid, is_member, untagged);
+}
+
+#ifdef CONFIG_PHYLINK
+static void mvsw_pr_link_validate(struct phylink_config *config,
+				  unsigned long *supp,
+				  struct phylink_link_state *state)
+{
+}
+
+static void mvsw_pr_mac_pcs_get_state(struct phylink_config *config,
+				      struct phylink_link_state *state)
+{
+}
+
+static void mvsw_pr_mac_config(struct phylink_config *config, unsigned int mode,
+			       const struct phylink_link_state *state)
+{
+}
+
+static void mvsw_pr_mac_an_restart(struct phylink_config *config)
+{
+}
+
+static void mvsw_pr_mac_link_down(struct phylink_config *config,
+				  unsigned int mode, phy_interface_t interface)
+{
+}
+
+static void mvsw_pr_mac_link_up(struct phylink_config *config,
+				unsigned int mode, phy_interface_t interface,
+				struct phy_device *phy)
+{
+}
+
+static const struct phylink_mac_ops mvsw_pr_mac_ops = {
+	.validate = mvsw_pr_link_validate,
+	.mac_pcs_get_state = mvsw_pr_mac_pcs_get_state,
+	.mac_config = mvsw_pr_mac_config,
+	.mac_an_restart = mvsw_pr_mac_an_restart,
+	.mac_link_down = mvsw_pr_mac_link_down,
+	.mac_link_up = mvsw_pr_mac_link_up,
+};
+
+static int mvsw_pr_port_sfp_bind(struct mvsw_pr_port *port)
+{
+	struct mvsw_pr_switch *sw = port->sw;
+	struct device_node *ports, *node;
+	struct fwnode_handle *fwnode;
+	struct phylink *phy_link;
+	int err;
+
+	if (!sw->np)
+		return 0;
+
+	of_node_get(sw->np);
+
+	ports = of_find_node_by_name(sw->np, "ports");
+
+	for_each_child_of_node(ports, node) {
+		int num;
+
+		err = of_property_read_u32(node, "prestera,port-num", &num);
+		if (err) {
+			dev_err(sw->dev->dev,
+				"device node %pOF has no valid reg property: %d\n",
+				node, err);
+			return err;
+		}
+
+		if (port->fp_id != num)
+			continue;
+
+		port->phy_config.dev = &port->net_dev->dev;
+		port->phy_config.type = PHYLINK_NETDEV;
+		port->phy_config.pcs_poll = true;
+
+		fwnode = of_fwnode_handle(node);
+
+		phy_link = phylink_create(&port->phy_config, fwnode,
+					  PHY_INTERFACE_MODE_INTERNAL,
+					  &mvsw_pr_mac_ops);
+		if (IS_ERR(phy_link)) {
+			netdev_err(port->net_dev, "failed to create phylink\n");
+			return PTR_ERR(phy_link);
+		}
+
+		port->phy_link = phy_link;
+		break;
+	}
+
+	return 0;
+}
+#else
+static int mvsw_pr_port_sfp_bind(struct mvsw_pr_port *port)
+{
+	return 0;
+}
+#endif
+
+static int mvsw_pr_port_create(struct mvsw_pr_switch *sw, u32 id)
+{
+	struct net_device *net_dev;
+	struct mvsw_pr_port *port;
+	char *mac;
+	int err;
+
+	net_dev = alloc_etherdev(sizeof(*port));
+	if (!net_dev)
+		return -ENOMEM;
+
+	port = netdev_priv(net_dev);
+
+	INIT_LIST_HEAD(&port->vlans_list);
+	port->pvid = MVSW_PR_DEFAULT_VID;
+	port->net_dev = net_dev;
+	port->id = id;
+	port->sw = sw;
+	port->lag_id = sw->lag_max;
+
+	err = mvsw_pr_hw_port_info_get(port, &port->fp_id,
+				       &port->hw_id, &port->dev_id);
+	if (err) {
+		dev_err(mvsw_dev(sw), "Failed to get port(%u) info\n", id);
+		goto err_free_netdev;
+	}
+
+	err = prestera_devlink_port_register(port);
+	if (err)
+		goto err_devl_port_reg;
+
+	net_dev->needed_headroom = MVSW_PR_DSA_HLEN + 4;
+
+	net_dev->netdev_ops = &mvsw_pr_netdev_ops;
+	net_dev->ethtool_ops = &mvsw_pr_ethtool_ops;
+	net_dev->features |= NETIF_F_NETNS_LOCAL | NETIF_F_HW_TC;
+	net_dev->hw_features |= NETIF_F_HW_TC;
+	net_dev->ethtool_ops = &mvsw_pr_ethtool_ops;
+	net_dev->netdev_ops = &mvsw_pr_netdev_ops;
+
+	netif_carrier_off(net_dev);
+
+	net_dev->mtu = min_t(unsigned int, sw->mtu_max, MVSW_PR_MTU_DEFAULT);
+	net_dev->min_mtu = sw->mtu_min;
+	net_dev->max_mtu = sw->mtu_max;
+
+	err = mvsw_pr_hw_port_mtu_set(port, net_dev->mtu);
+	if (err) {
+		dev_err(mvsw_dev(sw), "Failed to set port(%u) mtu\n", id);
+		goto err_port_init;
+	}
+
+	/* Only 0xFF mac addrs are supported */
+	if (port->fp_id >= 0xFF)
+		goto err_port_init;
+
+	mac = net_dev->dev_addr;
+	memcpy(mac, sw->base_mac, net_dev->addr_len);
+	mac[net_dev->addr_len - 1] += port->fp_id + MVSW_PR_MAC_ADDR_OFFSET;
+
+	err = mvsw_pr_hw_port_mac_set(port, mac);
+	if (err) {
+		dev_err(mvsw_dev(sw), "Failed to set port(%u) mac addr\n", id);
+		goto err_port_init;
+	}
+
+	err = mvsw_pr_hw_port_cap_get(port, &port->caps);
+	if (err) {
+		dev_err(mvsw_dev(sw), "Failed to get port(%u) caps\n", id);
+		goto err_port_init;
+	}
+
+	port->adver_link_modes = 0;
+	port->adver_fec = 0;
+	port->autoneg = false;
+	mvsw_pr_port_autoneg_set(port, true, port->caps.supp_link_modes,
+				 BIT(MVSW_PORT_FEC_OFF_BIT));
+
+	err = mvsw_pr_hw_port_state_set(port, false);
+	if (err) {
+		dev_err(mvsw_dev(sw), "Failed to set port(%u) down\n", id);
+		goto err_port_init;
+	}
+
+	INIT_DELAYED_WORK(&port->cached_hw_stats.caching_dw,
+			  &update_stats_cache);
+
+	err = register_netdev(net_dev);
+	if (err)
+		goto err_port_init;
+
+	if (port->caps.transceiver == MVSW_PORT_TRANSCEIVER_SFP) {
+		err = mvsw_pr_port_sfp_bind(port);
+		if (err)
+			goto err_sfp_bind;
+	}
+
+	list_add(&port->list, &sw->port_list);
+
+	mvsw_pr_port_uc_flood_set(port, false);
+	mvsw_pr_port_mc_flood_set(port, false);
+
+	prestera_devlink_port_set(port);
+
+	return 0;
+
+err_sfp_bind:
+	unregister_netdev(net_dev);
+	prestera_devlink_port_unregister(port);
+err_port_init:
+err_devl_port_reg:
+err_free_netdev:
+	free_netdev(net_dev);
+	return err;
+}
+
+static void mvsw_pr_port_vlan_flush(struct mvsw_pr_port *port,
+				    bool flush_default)
+{
+	struct mvsw_pr_port_vlan *port_vlan, *tmp;
+
+	list_for_each_entry_safe(port_vlan, tmp, &port->vlans_list, list) {
+		if (!flush_default && port_vlan->vid == MVSW_PR_DEFAULT_VID)
+			continue;
+
+		mvsw_pr_port_vlan_destroy(port_vlan);
+	}
+}
+
+int mvsw_pr_8021d_bridge_create(struct mvsw_pr_switch *sw, u16 *bridge_id)
+{
+	return mvsw_pr_hw_bridge_create(sw, bridge_id);
+}
+
+int mvsw_pr_8021d_bridge_delete(struct mvsw_pr_switch *sw, u16 bridge_id)
+{
+	return mvsw_pr_hw_bridge_delete(sw, bridge_id);
+}
+
+int mvsw_pr_8021d_bridge_port_add(struct mvsw_pr_port *port, u16 bridge_id)
+{
+	return mvsw_pr_hw_bridge_port_add(port, bridge_id);
+}
+
+int mvsw_pr_8021d_bridge_port_delete(struct mvsw_pr_port *port, u16 bridge_id)
+{
+	return mvsw_pr_hw_bridge_port_delete(port, bridge_id);
+}
+
+int mvsw_pr_switch_ageing_set(struct mvsw_pr_switch *sw, u32 ageing_time)
+{
+	return mvsw_pr_hw_switch_ageing_set(sw, ageing_time / 1000);
+}
+
+int mvsw_pr_dev_if_type(const struct net_device *dev)
+{
+	struct macvlan_dev *vlan;
+
+	if (is_vlan_dev(dev) && netif_is_bridge_master(vlan_dev_real_dev(dev)))
+		return MVSW_IF_VID_E;
+	else if (netif_is_bridge_master(dev))
+		return MVSW_IF_VID_E;
+	else if (netif_is_lag_master(dev))
+		return MVSW_IF_LAG_E;
+	else if (netif_is_macvlan(dev)) {
+		vlan = netdev_priv(dev);
+		return mvsw_pr_dev_if_type(vlan->lowerdev);
+	}
+	else
+		return MVSW_IF_PORT_E;
+}
+
+int mvsw_pr_lpm_add(struct mvsw_pr_switch *sw, u16 hw_vr_id,
+		    struct mvsw_pr_ip_addr *addr, u32 prefix_len, u32 grp_id)
+{
+	/* Dont waste time on hw requests,
+	 * if router (and probably vr) aborted
+	 */
+	if (sw->router->aborted)
+		return -ENOENT;
+
+	/* TODO: ipv6 key type check before call designated hw cb */
+	return mvsw_pr_hw_lpm_add(sw, hw_vr_id, addr->u.ipv4,
+				  prefix_len, grp_id);
+}
+
+int mvsw_pr_lpm_del(struct mvsw_pr_switch *sw, u16 hw_vr_id,
+		    struct mvsw_pr_ip_addr *addr, u32 prefix_len)
+{
+	/* Dont waste time on hw requests,
+	 * if router (and probably vr) aborted
+	 */
+	if (sw->router->aborted)
+		return -ENOENT;
+
+	/* TODO: ipv6 key type check before call designated hw cb */
+	return mvsw_pr_hw_lpm_del(sw, hw_vr_id, addr->u.ipv4,
+				  prefix_len);
+}
+
+int mvsw_pr_nh_entries_set(const struct mvsw_pr_switch *sw, int count,
+			   struct mvsw_pr_neigh_info *nhs, u32 grp_id)
+{
+	/* Dont waste time on hw requests,
+	 * if router (and probably vr) aborted
+	 */
+	if (sw->router->aborted)
+		return -ENOENT;
+
+	return mvsw_pr_hw_nh_entries_set(sw, count, nhs, grp_id);
+}
+
+int mvsw_pr_nh_entries_get(const struct mvsw_pr_switch *sw, int count,
+			   struct mvsw_pr_neigh_info *nhs, u32 grp_id)
+{
+	/* Dont waste time on hw requests,
+	 * if router (and probably vr) aborted
+	 */
+	if (sw->router->aborted)
+		return -ENOENT;
+
+	return mvsw_pr_hw_nh_entries_get(sw, count, nhs, grp_id);
+}
+
+int mvsw_pr_nh_group_create(const struct mvsw_pr_switch *sw, u16 nh_count,
+			    u32 *grp_id)
+{
+	/* Dont waste time on hw requests,
+	 * if router (and probably vr) aborted
+	 */
+	if (sw->router->aborted)
+		return -ENOENT;
+
+	return mvsw_pr_hw_nh_group_create(sw, nh_count, grp_id);
+}
+
+int mvsw_pr_nh_group_delete(const struct mvsw_pr_switch *sw, u16 nh_count,
+			    u32 grp_id)
+{
+	/* Dont waste time on hw requests,
+	 * if router (and probably vr) aborted
+	 */
+	if (sw->router->aborted)
+		return -ENOENT;
+
+	return mvsw_pr_hw_nh_group_delete(sw, nh_count, grp_id);
+}
+
+int mvsw_pr_mp4_hash_set(const struct mvsw_pr_switch *sw, u8 hash_policy)
+{
+	return mvsw_pr_hw_mp4_hash_set(sw, hash_policy);
+}
+
+int mvsw_pr_fdb_flush_vlan(struct mvsw_pr_switch *sw, u16 vid,
+			   enum mvsw_pr_fdb_flush_mode mode)
+{
+	return mvsw_pr_hw_fdb_flush_vlan(sw, vid, mode);
+}
+
+int mvsw_pr_fdb_flush_port_vlan(struct mvsw_pr_port *port, u16 vid,
+				enum mvsw_pr_fdb_flush_mode mode)
+{
+	if (mvsw_pr_port_is_lag_member(port))
+		return mvsw_pr_hw_fdb_flush_lag_vlan(port->sw, port->lag_id,
+						     vid, mode);
+	else
+		return mvsw_pr_hw_fdb_flush_port_vlan(port, vid, mode);
+}
+
+int mvsw_pr_fdb_flush_port(struct mvsw_pr_port *port,
+			   enum mvsw_pr_fdb_flush_mode mode)
+{
+	if (mvsw_pr_port_is_lag_member(port))
+		return mvsw_pr_hw_fdb_flush_lag(port->sw, port->lag_id, mode);
+	else
+		return mvsw_pr_hw_fdb_flush_port(port, mode);
+}
+
+int mvsw_pr_macvlan_add(const struct mvsw_pr_switch *sw, u16 vr_id,
+			const u8 *mac, u16 vid)
+{
+	return mvsw_pr_hw_macvlan_add(sw, vr_id, mac,  vid);
+}
+
+int mvsw_pr_macvlan_del(const struct mvsw_pr_switch *sw, u16 vr_id,
+			const u8 *mac, u16 vid)
+{
+	return mvsw_pr_hw_macvlan_del(sw, vr_id, mac,  vid);
+}
+
+static struct prestera_lag *
+prestera_lag_get(struct mvsw_pr_switch *sw, u8 id)
+{
+	return id < sw->lag_max ? &sw->lags[id] : NULL;
+}
+
+static void mvsw_pr_port_lag_create(struct mvsw_pr_switch *sw, u16 lag_id,
+				    struct net_device *lag_dev)
+{
+	INIT_LIST_HEAD(&sw->lags[lag_id].members);
+	sw->lags[lag_id].dev = lag_dev;
+}
+
+static void mvsw_pr_port_lag_destroy(struct mvsw_pr_switch *sw, u16 lag_id)
+{
+	WARN_ON(!list_empty(&sw->lags[lag_id].members));
+	sw->lags[lag_id].dev = NULL;
+	sw->lags[lag_id].member_count = 0;
+}
+
+int prestera_lag_member_add(struct mvsw_pr_port *port,
+			    struct net_device *lag_dev, u16 lag_id)
+{
+	struct mvsw_pr_switch *sw = port->sw;
+	struct prestera_lag_member *member;
+	struct prestera_lag *lag;
+
+	lag = prestera_lag_get(sw, lag_id);
+
+	if (lag->member_count >= sw->lag_member_max)
+		return -ENOSPC;
+	else if (!lag->member_count)
+		mvsw_pr_port_lag_create(sw, lag_id, lag_dev);
+
+	member = kzalloc(sizeof(*member), GFP_KERNEL);
+	if (!member)
+		return -ENOMEM;
+
+	if (mvsw_pr_hw_lag_member_add(port, lag_id)) {
+		kfree(member);
+		if (!lag->member_count)
+			mvsw_pr_port_lag_destroy(sw, lag_id);
+		return -EBUSY;
+	}
+
+	member->port = port;
+	list_add(&member->list, &lag->members);
+	lag->member_count++;
+	port->lag_id = lag_id;
+	return 0;
+}
+
+int prestera_lag_member_del(struct mvsw_pr_port *port)
+{
+	struct mvsw_pr_switch *sw = port->sw;
+	struct prestera_lag_member *member;
+	struct list_head *pos, *n;
+	u16 lag_id = port->lag_id;
+	struct prestera_lag *lag;
+	int err;
+
+	lag = prestera_lag_get(sw, lag_id);
+	if (!lag || !lag->member_count)
+		return -EINVAL;
+
+	err = mvsw_pr_hw_lag_member_del(port, lag_id);
+	if (err)
+		return err;
+
+	lag->member_count--;
+	port->lag_id = sw->lag_max;
+
+	list_for_each_safe(pos, n, &lag->members) {
+		member = list_entry(pos, typeof(*member), list);
+		if (member->port->id == port->id) {
+			list_del(&member->list);
+			kfree(member);
+			break;
+		}
+	}
+
+	if (!lag->member_count) {
+		prestera_lag_router_leave(sw, lag->dev);
+		mvsw_pr_port_lag_destroy(sw, lag_id);
+	}
+
+	return 0;
+}
+
+int prestera_lag_member_enable(struct mvsw_pr_port *port, bool enable)
+{
+	return mvsw_pr_hw_lag_member_enable(port, port->lag_id, enable);
+}
+
+bool mvsw_pr_port_is_lag_member(const struct mvsw_pr_port *port)
+{
+	return port->lag_id < port->sw->lag_max;
+}
+
+int prestera_lag_id_find(struct mvsw_pr_switch *sw, struct net_device *lag_dev,
+			 u16 *lag_id)
+{
+	struct prestera_lag *lag;
+	int free_id = -1;
+	int id;
+
+	for (id = 0; id < sw->lag_max; id++) {
+		lag = prestera_lag_get(sw, id);
+		if (lag->member_count) {
+			if (lag->dev == lag_dev) {
+				*lag_id = id;
+				return 0;
+			}
+		} else if (free_id < 0) {
+			free_id = id;
+		}
+	}
+	if (free_id < 0)
+		return -ENOSPC;
+	*lag_id = free_id;
+	return 0;
+}
+
+void prestera_lag_member_rif_leave(const struct mvsw_pr_port *port,
+				   u16 lag_id, u16 vr_id)
+{
+	mvsw_pr_hw_lag_member_rif_leave(port, lag_id, vr_id);
+}
+
+static int prestera_lag_init(struct mvsw_pr_switch *sw)
+{
+	sw->lags = kcalloc(sw->lag_max, sizeof(*sw->lags), GFP_KERNEL);
+	return sw->lags ? 0 : -ENOMEM;
+}
+
+static void prestera_lag_fini(struct mvsw_pr_switch *sw)
+{
+	u8 idx;
+
+	for (idx = 0; idx < sw->lag_max; idx++)
+		WARN_ON(sw->lags[idx].member_count);
+
+	kfree(sw->lags);
+}
+
+static int mvsw_pr_clear_ports(struct mvsw_pr_switch *sw)
+{
+	struct net_device *net_dev;
+	struct list_head *pos, *n;
+	struct mvsw_pr_port *port;
+
+	list_for_each_safe(pos, n, &sw->port_list) {
+		port = list_entry(pos, typeof(*port), list);
+		net_dev = port->net_dev;
+
+		cancel_delayed_work_sync(&port->cached_hw_stats.caching_dw);
+		prestera_devlink_port_clear(port);
+#ifdef CONFIG_PHYLINK
+		if (port->phy_link)
+			phylink_destroy(port->phy_link);
+#endif
+		unregister_netdev(net_dev);
+		mvsw_pr_port_vlan_flush(port, true);
+		WARN_ON_ONCE(!list_empty(&port->vlans_list));
+		mvsw_pr_port_router_leave(port);
+		prestera_devlink_port_unregister(port);
+		free_netdev(net_dev);
+		list_del(pos);
+	}
+	return (!list_empty(&sw->port_list));
+}
+
+static void mvsw_pr_port_handle_event(struct mvsw_pr_switch *sw,
+				      struct mvsw_pr_event *evt, void *arg)
+{
+	struct mvsw_pr_port *port;
+	struct delayed_work *caching_dw;
+
+	port = __find_pr_port(sw, evt->port_evt.port_id);
+	if (!port)
+		return;
+
+	caching_dw = &port->cached_hw_stats.caching_dw;
+
+	switch (evt->id) {
+	case MVSW_PORT_EVENT_STATE_CHANGED:
+		if (evt->port_evt.data.oper_state) {
+			netif_carrier_on(port->net_dev);
+			if (!delayed_work_pending(caching_dw))
+				queue_delayed_work(mvsw_pr_wq, caching_dw, 0);
+		} else {
+			netif_carrier_off(port->net_dev);
+			if (delayed_work_pending(caching_dw))
+				cancel_delayed_work(caching_dw);
+		}
+		break;
+	}
+}
+
+static bool prestera_lag_exists(const struct mvsw_pr_switch *sw, u16 lag_id)
+{
+	return lag_id < sw->lag_max &&
+	       sw->lags[lag_id].member_count != 0;
+}
+
+static void mvsw_pr_fdb_handle_event(struct mvsw_pr_switch *sw,
+				     struct mvsw_pr_event *evt, void *arg)
+{
+	struct switchdev_notifier_fdb_info info;
+	struct net_device *dev = NULL;
+	struct mvsw_pr_port *port;
+	u16 lag_id;
+
+	switch (evt->fdb_evt.type) {
+	case MVSW_PR_FDB_ENTRY_TYPE_REG_PORT:
+		port = __find_pr_port(sw, evt->fdb_evt.dest.port_id);
+		if (port)
+			dev = port->net_dev;
+		break;
+	case MVSW_PR_FDB_ENTRY_TYPE_LAG:
+		lag_id = evt->fdb_evt.dest.lag_id;
+		if (prestera_lag_exists(sw, lag_id))
+			dev = sw->lags[lag_id].dev;
+		break;
+	default:
+		return;
+	}
+
+	if (!dev)
+		return;
+
+	info.addr = evt->fdb_evt.data.mac;
+	info.vid = evt->fdb_evt.vid;
+	info.offloaded = true;
+
+	rtnl_lock();
+	switch (evt->id) {
+	case MVSW_FDB_EVENT_LEARNED:
+		call_switchdev_notifiers(SWITCHDEV_FDB_ADD_TO_BRIDGE,
+					 dev, &info.info, NULL);
+		break;
+	case MVSW_FDB_EVENT_AGED:
+		call_switchdev_notifiers(SWITCHDEV_FDB_DEL_TO_BRIDGE,
+					 dev, &info.info, NULL);
+		break;
+	}
+	rtnl_unlock();
+}
+
+int mvsw_pr_fdb_add(struct mvsw_pr_port *port, const unsigned char *mac,
+		    u16 vid, bool dynamic)
+{
+	if (mvsw_pr_port_is_lag_member(port))
+		return mvsw_pr_hw_lag_fdb_add(port->sw, port->lag_id,
+					      mac, vid, dynamic);
+	else
+		return mvsw_pr_hw_fdb_add(port, mac, vid, dynamic);
+}
+
+int mvsw_pr_fdb_del(struct mvsw_pr_port *port, const unsigned char *mac,
+		    u16 vid)
+{
+	if (mvsw_pr_port_is_lag_member(port))
+		return mvsw_pr_hw_lag_fdb_del(port->sw, port->lag_id,
+					      mac, vid);
+	else
+		return mvsw_pr_hw_fdb_del(port, mac, vid);
+}
+
+static void mvsw_pr_fdb_event_handler_unregister(struct mvsw_pr_switch *sw)
+{
+	mvsw_pr_hw_event_handler_unregister(sw, MVSW_EVENT_TYPE_FDB);
+}
+
+static void mvsw_pr_port_event_handler_unregister(struct mvsw_pr_switch *sw)
+{
+	mvsw_pr_hw_event_handler_unregister(sw, MVSW_EVENT_TYPE_PORT);
+}
+
+static void mvsw_pr_event_handlers_unregister(struct mvsw_pr_switch *sw)
+{
+	mvsw_pr_fdb_event_handler_unregister(sw);
+	mvsw_pr_port_event_handler_unregister(sw);
+}
+
+static int mvsw_pr_fdb_event_handler_register(struct mvsw_pr_switch *sw)
+{
+	return mvsw_pr_hw_event_handler_register(sw, MVSW_EVENT_TYPE_FDB,
+						 mvsw_pr_fdb_handle_event,
+						 NULL);
+}
+
+static int mvsw_pr_port_event_handler_register(struct mvsw_pr_switch *sw)
+{
+	return mvsw_pr_hw_event_handler_register(sw, MVSW_EVENT_TYPE_PORT,
+						 mvsw_pr_port_handle_event,
+						 NULL);
+}
+
+static int mvsw_pr_event_handlers_register(struct mvsw_pr_switch *sw)
+{
+	int err;
+
+	err = mvsw_pr_port_event_handler_register(sw);
+	if (err)
+		return err;
+
+	err = mvsw_pr_fdb_event_handler_register(sw);
+	if (err)
+		goto err_fdb_handler_register;
+
+	return 0;
+
+err_fdb_handler_register:
+	mvsw_pr_port_event_handler_unregister(sw);
+	return err;
+}
+
+int mvsw_pr_schedule_dw(struct delayed_work *dwork, unsigned long delay)
+{
+	return queue_delayed_work(mvsw_pr_wq, dwork, delay);
+}
+
+const struct mvsw_pr_port *mvsw_pr_port_find(u32 dev_hw_id, u32 port_hw_id)
+{
+	struct mvsw_pr_port *port = NULL;
+	struct mvsw_pr_switch *sw;
+
+	list_for_each_entry(sw, &switches_registered, list) {
+		list_for_each_entry(port, &sw->port_list, list) {
+			if (port->hw_id == port_hw_id &&
+			    port->dev_id == dev_hw_id)
+				return port;
+		}
+	}
+	return NULL;
+}
+
+static int mvsw_pr_sw_init_base_mac(struct mvsw_pr_switch *sw)
+{
+	struct device_node *mac_dev_np;
+	u32 lsb;
+	int err;
+
+	if (sw->np) {
+		mac_dev_np = of_parse_phandle(sw->np, "base-mac-provider", 0);
+		if (mac_dev_np) {
+			const char *base_mac;
+
+			base_mac = of_get_mac_address(mac_dev_np);
+			if (!IS_ERR(base_mac))
+				ether_addr_copy(sw->base_mac, base_mac);
+		}
+	}
+
+	if (!is_valid_ether_addr(sw->base_mac))
+		eth_random_addr(sw->base_mac);
+
+	lsb = sw->base_mac[ETH_ALEN - 1];
+	if (lsb + sw->port_count + MVSW_PR_MAC_ADDR_OFFSET > 0xFF)
+		sw->base_mac[ETH_ALEN - 1] = 0;
+
+	err = mvsw_pr_hw_switch_mac_set(sw, sw->base_mac);
+	if (err)
+		return err;
+
+	return 0;
+}
+
+static int mvsw_pr_init(struct mvsw_pr_switch *sw)
+{
+	u32 port;
+	int err;
+
+	sw->np = of_find_compatible_node(NULL, NULL, "marvell,prestera");
+
+	err = mvsw_pr_hw_switch_init(sw);
+	if (err) {
+		dev_err(mvsw_dev(sw), "Failed to init Switch device\n");
+		return err;
+	}
+
+	err = mvsw_pr_hw_switch_trap_policer_set(sw, trap_policer_profile);
+	if (err) {
+		dev_err(mvsw_dev(sw), "Failed to set trap policer profile\n");
+		return err;
+	}
+
+	err = mvsw_pr_sw_init_base_mac(sw);
+	if (err)
+		return err;
+
+	dev_info(mvsw_dev(sw), "Initialized Switch device\n");
+
+	err = prestera_lag_init(sw);
+	if (err)
+		return err;
+
+	err = prestera_switchdev_register(sw);
+	if (err)
+		return err;
+
+	err = prestera_devlink_register(sw);
+	if (err)
+		goto err_devl_reg;
+
+	INIT_LIST_HEAD(&sw->port_list);
+
+	for (port = 0; port < sw->port_count; port++) {
+		err = mvsw_pr_port_create(sw, port);
+		if (err)
+			goto err_ports_init;
+	}
+
+	err = mvsw_pr_rxtx_switch_init(sw);
+	if (err)
+		goto err_rxtx_init;
+
+	err = mvsw_pr_event_handlers_register(sw);
+	if (err)
+		goto err_event_handlers;
+
+	err = mvsw_pr_debugfs_init(sw);
+	if (err)
+		goto err_debugfs_init;
+
+	err = prestera_acl_init(sw);
+	if (err)
+		goto err_acl_init;
+
+	return 0;
+
+err_acl_init:
+err_debugfs_init:
+	mvsw_pr_event_handlers_unregister(sw);
+err_event_handlers:
+	mvsw_pr_rxtx_switch_fini(sw);
+err_rxtx_init:
+err_ports_init:
+	mvsw_pr_clear_ports(sw);
+err_devl_reg:
+	prestera_switchdev_unregister(sw);
+	return err;
+}
+
+static void mvsw_pr_fini(struct mvsw_pr_switch *sw)
+{
+	mvsw_pr_debugfs_fini(sw);
+
+	mvsw_pr_event_handlers_unregister(sw);
+
+	mvsw_pr_clear_ports(sw);
+	mvsw_pr_rxtx_switch_fini(sw);
+	prestera_devlink_unregister(sw);
+	prestera_switchdev_unregister(sw);
+	prestera_acl_fini(sw);
+	prestera_lag_fini(sw);
+	of_node_put(sw->np);
+}
+
+int prestera_device_register(struct prestera_device *dev)
+{
+	struct mvsw_pr_switch *sw;
+	int err;
+
+	sw = prestera_devlink_alloc();
+	if (!sw)
+		return -ENOMEM;
+
+	dev->priv = sw;
+	sw->dev = dev;
+
+	err = mvsw_pr_init(sw);
+	if (err) {
+		prestera_devlink_free(sw);
+		return err;
+	}
+
+	list_add(&sw->list, &switches_registered);
+
+	return 0;
+}
+EXPORT_SYMBOL(prestera_device_register);
+
+void prestera_device_unregister(struct prestera_device *dev)
+{
+	struct mvsw_pr_switch *sw = dev->priv;
+
+	list_del(&sw->list);
+	mvsw_pr_fini(sw);
+	prestera_devlink_free(sw);
+}
+EXPORT_SYMBOL(prestera_device_unregister);
+
+static int __init mvsw_pr_module_init(void)
+{
+	int err;
+
+	INIT_LIST_HEAD(&switches_registered);
+
+	mvsw_pr_wq = alloc_workqueue(mvsw_driver_name, 0, 0);
+	if (!mvsw_pr_wq)
+		return -ENOMEM;
+
+	err = mvsw_pr_rxtx_init();
+	if (err) {
+		pr_err("failed to initialize prestera rxtx\n");
+		destroy_workqueue(mvsw_pr_wq);
+		return err;
+	}
+
+	pr_info("Loading Marvell Prestera Switch Driver\n");
+	return 0;
+}
+
+static void __exit mvsw_pr_module_exit(void)
+{
+	destroy_workqueue(mvsw_pr_wq);
+	mvsw_pr_rxtx_fini();
+
+	pr_info("Unloading Marvell Prestera Switch Driver\n");
+}
+
+module_init(mvsw_pr_module_init);
+module_exit(mvsw_pr_module_exit);
+
+MODULE_AUTHOR("Marvell Semi.");
+MODULE_LICENSE("GPL");
+MODULE_DESCRIPTION("Marvell Prestera switch driver");
+MODULE_VERSION(PRESTERA_DRV_VER);
+
+module_param(trap_policer_profile, byte, 0444);
diff --git a/drivers/net/ethernet/marvell/prestera_sw/prestera.h b/drivers/net/ethernet/marvell/prestera_sw/prestera.h
new file mode 100644
index 000000000..594743a0a
--- /dev/null
+++ b/drivers/net/ethernet/marvell/prestera_sw/prestera.h
@@ -0,0 +1,587 @@
+/* SPDX-License-Identifier: BSD-3-Clause OR GPL-2.0
+ *
+ * Copyright (c) 2019-2020 Marvell International Ltd. All rights reserved.
+ *
+ */
+
+#ifndef _MVSW_PRESTERA_H_
+#define _MVSW_PRESTERA_H_
+
+#include <linux/skbuff.h>
+#include <linux/notifier.h>
+#include <uapi/linux/if_ether.h>
+#include <linux/if_macvlan.h>
+#include <linux/workqueue.h>
+#include <linux/phylink.h>
+#include <net/pkt_cls.h>
+#include <net/devlink.h>
+
+#define PRESTERA_DRV_NAME       "prestera"
+
+#define MVSW_MSG_MAX_SIZE 1500
+
+#define MVSW_PR_DEFAULT_VID 1
+
+#define MVSW_PR_MIN_AGEING_TIME 32000
+#define MVSW_PR_MAX_AGEING_TIME 1000000000
+#define MVSW_PR_DEFAULT_AGEING_TIME 300000
+
+#define MVSW_PR_NHGR_SIZE_MAX 4
+
+struct prestera_fw_rev {
+	u16 maj;
+	u16 min;
+	u16 sub;
+};
+
+struct mvsw_pr_bridge_port;
+struct prestera_acl;
+struct prestera_acl_block;
+struct prestera_acl_rule;
+struct prestera_acl_ruleset;
+
+struct mvsw_pr_port_vlan {
+	struct list_head list;
+	struct mvsw_pr_port *mvsw_pr_port;
+	u16 vid;
+	struct mvsw_pr_bridge_port *bridge_port;
+	struct list_head bridge_vlan_node;
+};
+
+struct mvsw_pr_port_stats {
+	u64 good_octets_received;
+	u64 bad_octets_received;
+	u64 mac_trans_error;
+	u64 broadcast_frames_received;
+	u64 multicast_frames_received;
+	u64 frames_64_octets;
+	u64 frames_65_to_127_octets;
+	u64 frames_128_to_255_octets;
+	u64 frames_256_to_511_octets;
+	u64 frames_512_to_1023_octets;
+	u64 frames_1024_to_max_octets;
+	u64 excessive_collision;
+	u64 multicast_frames_sent;
+	u64 broadcast_frames_sent;
+	u64 fc_sent;
+	u64 fc_received;
+	u64 buffer_overrun;
+	u64 undersize;
+	u64 fragments;
+	u64 oversize;
+	u64 jabber;
+	u64 rx_error_frame_received;
+	u64 bad_crc;
+	u64 collisions;
+	u64 late_collision;
+	u64 unicast_frames_received;
+	u64 unicast_frames_sent;
+	u64 sent_multiple;
+	u64 sent_deferred;
+	u64 good_octets_sent;
+};
+
+struct mvsw_pr_port_caps {
+	u64 supp_link_modes;
+	u8 supp_fec;
+	u8 type;
+	u8 transceiver;
+};
+
+struct mvsw_pr_port {
+	struct devlink_port dl_port;
+	struct net_device *net_dev;
+	struct mvsw_pr_switch *sw;
+	u32 id;
+	u32 hw_id;
+	u32 dev_id;
+	u16 fp_id;
+	u16 pvid;
+	bool autoneg;
+	u64 adver_link_modes;
+	u8 adver_fec;
+	u16 lag_id;
+	struct mvsw_pr_port_caps caps;
+	struct list_head list;
+	struct list_head vlans_list;
+	struct {
+		struct mvsw_pr_port_stats stats;
+		struct delayed_work caching_dw;
+	} cached_hw_stats;
+	struct prestera_acl_block *acl_block;
+
+	struct phylink_config phy_config;
+	struct phylink *phy_link;
+};
+
+struct prestera_switchdev {
+	struct mvsw_pr_switch *sw;
+	struct notifier_block swdev_n;
+	struct notifier_block swdev_blocking_n;
+};
+
+struct mvsw_pr_fib {
+	struct mvsw_pr_switch *sw;
+	struct notifier_block fib_nb;
+	struct notifier_block netevent_nb;
+};
+
+struct prestera_device {
+	struct device *dev;
+	struct prestera_fw_rev fw_rev;
+	struct workqueue_struct *dev_wq;
+	u8 __iomem *pp_regs;
+	void *priv;
+
+	/* called by device driver to handle received packets */
+	void (*recv_pkt)(struct prestera_device *dev);
+
+	/* called by device driver to pass event up to the higher layer */
+	int (*recv_msg)(struct prestera_device *dev, u8 *msg, size_t size);
+
+	/* called by higher layer to send request to the firmware */
+	int (*send_req)(struct prestera_device *dev, u8 *in_msg,
+			size_t in_size, u8 *out_msg, size_t out_size,
+			unsigned int wait);
+};
+
+enum mvsw_pr_event_type {
+	MVSW_EVENT_TYPE_UNSPEC,
+	MVSW_EVENT_TYPE_PORT,
+	MVSW_EVENT_TYPE_FDB,
+	MVSW_EVENT_TYPE_RXTX,
+	MVSW_EVENT_TYPE_FW_LOG,
+
+	MVSW_EVENT_TYPE_MAX,
+};
+
+enum mvsw_pr_rxtx_event_id {
+	MVSW_RXTX_EVENT_UNSPEC,
+
+	MVSW_RXTX_EVENT_RCV_PKT,
+
+	MVSW_RXTX_EVENT_MAX,
+};
+
+enum mvsw_pr_port_event_id {
+	MVSW_PORT_EVENT_UNSPEC,
+	MVSW_PORT_EVENT_STATE_CHANGED,
+
+	MVSW_PORT_EVENT_MAX,
+};
+
+enum mvsw_pr_fdb_event_id {
+	MVSW_FDB_EVENT_UNSPEC,
+	MVSW_FDB_EVENT_LEARNED,
+	MVSW_FDB_EVENT_AGED,
+
+	MVSW_FDB_EVENT_MAX,
+};
+
+enum mvsw_pr_fdb_entry_type {
+	MVSW_PR_FDB_ENTRY_TYPE_REG_PORT,
+	MVSW_PR_FDB_ENTRY_TYPE_LAG,
+	MVSW_PR_FDB_ENTRY_TYPE_MAX
+};
+
+struct mvsw_pr_fdb_event {
+	enum mvsw_pr_fdb_entry_type type;
+	union {
+		u32 port_id;
+		u16 lag_id;
+	} dest;
+	u32 vid;
+	union {
+		u8 mac[ETH_ALEN];
+	} data;
+};
+
+struct mvsw_pr_port_event {
+	u32 port_id;
+	union {
+		u32 oper_state;
+	} data;
+};
+
+struct mvsw_pr_fw_log_event {
+	u32 log_len;
+	u8 *data;
+};
+
+struct mvsw_pr_event {
+	u16 id;
+	union {
+		struct mvsw_pr_port_event port_evt;
+		struct mvsw_pr_fdb_event fdb_evt;
+		struct mvsw_pr_fw_log_event fw_log_evt;
+	};
+};
+
+struct prestera_lag_member {
+	struct list_head list;
+	struct mvsw_pr_port *port;
+};
+
+struct prestera_lag {
+	struct net_device *dev;
+	u16 member_count;
+	struct list_head members;
+};
+
+enum mvsw_pr_if_type {
+	/* the interface is of port type (dev,port) */
+	MVSW_IF_PORT_E = 0,
+
+	/* the interface is of lag type (lag-id) */
+	MVSW_IF_LAG_E = 1,
+
+	/* the interface is of Vid type (vlan-id) */
+	MVSW_IF_VID_E = 3,
+};
+
+struct mvsw_pr_iface {
+	enum mvsw_pr_if_type type;
+	struct {
+		u32 hw_dev_num;
+		u32 port_num;
+	} dev_port;
+	u16 vr_id;
+	u16 lag_id;
+	u16 vlan_id;
+	u32 hw_dev_num;
+};
+
+struct mvsw_pr_bridge;
+struct mvsw_pr_router;
+struct mvsw_pr_rif;
+
+struct mvsw_pr_switch {
+	struct list_head list;
+	struct prestera_device *dev;
+	struct list_head event_handlers;
+	char base_mac[ETH_ALEN];
+	struct list_head port_list;
+	u32 port_count;
+	u32 mtu_min;
+	u32 mtu_max;
+	u8 id;
+	u8 lag_max;
+	u8 lag_member_max;
+	struct prestera_acl *acl;
+	struct mvsw_pr_bridge *bridge;
+	struct prestera_switchdev *switchdev;
+	struct mvsw_pr_router *router;
+	struct prestera_lag *lags;
+	struct notifier_block netdevice_nb;
+	struct device_node *np;
+};
+
+struct mvsw_pr_router {
+	struct mvsw_pr_switch *sw;
+	struct list_head rif_list;	/* list of mvsw_pr_rif */
+	struct list_head vr_list;	/* list of mvsw_pr_vr */
+	struct rhashtable nh_neigh_ht;
+	struct rhashtable nexthop_group_ht;
+	struct rhashtable fib_ht;
+	struct rhashtable kern_fib_cache_ht;
+	struct rhashtable kern_neigh_cache_ht;
+	struct {
+		struct delayed_work dw;
+		unsigned int interval;	/* ms */
+	} neighs_update;
+	struct notifier_block netevent_nb;
+	struct notifier_block inetaddr_nb;
+	struct notifier_block fib_nb;
+	bool aborted;
+};
+
+enum mvsw_pr_fdb_flush_mode {
+	MVSW_PR_FDB_FLUSH_MODE_DYNAMIC = BIT(0),
+	MVSW_PR_FDB_FLUSH_MODE_STATIC = BIT(1),
+	MVSW_PR_FDB_FLUSH_MODE_ALL = MVSW_PR_FDB_FLUSH_MODE_DYNAMIC
+				   | MVSW_PR_FDB_FLUSH_MODE_STATIC,
+};
+
+enum prestera_acl_rule_match_entry_type {
+	MVSW_ACL_RULE_MATCH_ENTRY_TYPE_ETH_TYPE = 1,
+	MVSW_ACL_RULE_MATCH_ENTRY_TYPE_ETH_DMAC,
+	MVSW_ACL_RULE_MATCH_ENTRY_TYPE_ETH_SMAC,
+	MVSW_ACL_RULE_MATCH_ENTRY_TYPE_IP_PROTO,
+	MVSW_ACL_RULE_MATCH_ENTRY_TYPE_PORT,
+	MVSW_ACL_RULE_MATCH_ENTRY_TYPE_IP_SRC,
+	MVSW_ACL_RULE_MATCH_ENTRY_TYPE_IP_DST,
+	MVSW_ACL_RULE_MATCH_ENTRY_TYPE_L4_PORT_SRC,
+	MVSW_ACL_RULE_MATCH_ENTRY_TYPE_L4_PORT_DST,
+	MVSW_ACL_RULE_MATCH_ENTRY_TYPE_L4_PORT_RANGE_SRC,
+	MVSW_ACL_RULE_MATCH_ENTRY_TYPE_L4_PORT_RANGE_DST,
+	MVSW_ACL_RULE_MATCH_ENTRY_TYPE_VLAN_ID,
+	MVSW_ACL_RULE_MATCH_ENTRY_TYPE_VLAN_TPID,
+	MVSW_ACL_RULE_MATCH_ENTRY_TYPE_ICMP_TYPE,
+	MVSW_ACL_RULE_MATCH_ENTRY_TYPE_ICMP_CODE
+};
+
+enum prestera_acl_rule_action {
+	MVSW_ACL_RULE_ACTION_ACCEPT,
+	MVSW_ACL_RULE_ACTION_DROP,
+	MVSW_ACL_RULE_ACTION_TRAP,
+	MVSW_ACL_RULE_ACTION_POLICE
+};
+
+struct prestera_acl_rule_match_entry {
+	struct list_head list;
+	enum prestera_acl_rule_match_entry_type type;
+	union {
+		struct {
+			u8 key, mask;
+		} u8;
+		struct {
+			u16 key, mask;
+		} u16;
+		struct {
+			u32 key, mask;
+		} u32;
+		struct {
+			u64 key, mask;
+		} u64;
+		struct {
+			u8 key[ETH_ALEN];
+			u8 mask[ETH_ALEN];
+		} mac;
+	} keymask;
+};
+
+struct prestera_acl_rule_action_entry {
+	struct list_head list;
+	enum prestera_acl_rule_action id;
+	union {
+		struct {
+			u64 rate, burst;
+		} police;
+	};
+};
+
+struct mvsw_pr_ip_addr {
+	enum {
+		MVSW_PR_IPV4 = 0,
+		MVSW_PR_IPV6
+	} v;
+	union {
+		__be32 ipv4;
+		struct in6_addr ipv6;
+	} u;
+};
+
+struct mvsw_pr_fib_key {
+	struct mvsw_pr_ip_addr addr;
+	u32 prefix_len;
+	u32 tb_id;
+};
+
+struct mvsw_pr_fib_info {
+	struct mvsw_pr_vr *vr;
+	struct list_head vr_node;
+	enum mvsw_pr_fib_type {
+		MVSW_PR_FIB_TYPE_INVALID = 0,
+		/* must be pointer to nh_grp id */
+		MVSW_PR_FIB_TYPE_UC_NH,
+		/* It can be connected route
+		 * and will be overlapped with neighbours
+		 */
+		MVSW_PR_FIB_TYPE_TRAP,
+		MVSW_PR_FIB_TYPE_DROP
+	} type;
+	/* Valid only if type = UC_NH*/
+	struct mvsw_pr_nexthop_group *nh_grp;
+};
+
+/* Used for hw call */
+struct mvsw_pr_neigh_info {
+	struct mvsw_pr_iface iface;
+	unsigned char ha[ETH_ALEN];
+	bool connected; /* indicate, if mac/oif valid */
+};
+
+struct mvsw_pr_nh_neigh_key {
+	struct mvsw_pr_ip_addr addr;
+	struct mvsw_pr_rif *rif;
+};
+
+/* Used to notify nh about neigh change */
+struct mvsw_pr_nh_neigh {
+	struct mvsw_pr_nh_neigh_key key;
+	struct mvsw_pr_neigh_info info;
+	struct rhash_head ht_node; /* node of mvsw_pr_vr */
+	struct list_head nexthop_group_list;
+};
+
+int mvsw_pr_switch_ageing_set(struct mvsw_pr_switch *sw, u32 ageing_time);
+
+int mvsw_pr_port_learning_set(struct mvsw_pr_port *mvsw_pr_port,
+			      bool learn_enable);
+int mvsw_pr_port_uc_flood_set(struct mvsw_pr_port *mvsw_pr_port, bool flood);
+int mvsw_pr_port_mc_flood_set(struct mvsw_pr_port *mvsw_pr_port, bool flood);
+int mvsw_pr_port_pvid_set(struct mvsw_pr_port *mvsw_pr_port, u16 vid);
+int mvsw_pr_port_vid_stp_set(struct mvsw_pr_port *port, u16 vid, u8 state);
+struct mvsw_pr_port_vlan *
+mvsw_pr_port_vlan_create(struct mvsw_pr_port *mvsw_pr_port, u16 vid,
+			 bool untagged);
+void mvsw_pr_port_vlan_destroy(struct mvsw_pr_port_vlan *mvsw_pr_port_vlan);
+int mvsw_pr_port_vlan_set(struct mvsw_pr_port *mvsw_pr_port, u16 vid,
+			  bool is_member, bool untagged);
+
+struct mvsw_pr_bridge_device *
+mvsw_pr_bridge_device_find(const struct mvsw_pr_bridge *bridge,
+			   const struct net_device *br_dev);
+u16 mvsw_pr_vlan_dev_vlan_id(struct mvsw_pr_bridge *bridge,
+			     struct net_device *dev);
+int mvsw_pr_8021d_bridge_create(struct mvsw_pr_switch *sw, u16 *bridge_id);
+int mvsw_pr_8021d_bridge_delete(struct mvsw_pr_switch *sw, u16 bridge_id);
+int mvsw_pr_8021d_bridge_port_add(struct mvsw_pr_port *mvsw_pr_port,
+				  u16 bridge_id);
+int mvsw_pr_8021d_bridge_port_delete(struct mvsw_pr_port *mvsw_pr_port,
+				     u16 bridge_id);
+
+int mvsw_pr_fdb_add(struct mvsw_pr_port *mvsw_pr_port, const unsigned char *mac,
+		    u16 vid, bool dynamic);
+int mvsw_pr_fdb_del(struct mvsw_pr_port *mvsw_pr_port, const unsigned char *mac,
+		    u16 vid);
+int mvsw_pr_fdb_flush_vlan(struct mvsw_pr_switch *sw, u16 vid,
+			   enum mvsw_pr_fdb_flush_mode mode);
+int mvsw_pr_fdb_flush_port_vlan(struct mvsw_pr_port *port, u16 vid,
+				enum mvsw_pr_fdb_flush_mode mode);
+int mvsw_pr_fdb_flush_port(struct mvsw_pr_port *port,
+			   enum mvsw_pr_fdb_flush_mode mode);
+int mvsw_pr_macvlan_add(const struct mvsw_pr_switch *sw, u16 vr_id,
+			const u8 *mac, u16 vid);
+int mvsw_pr_macvlan_del(const struct mvsw_pr_switch *sw, u16 vr_id,
+			const u8 *mac, u16 vid);
+
+int prestera_lag_member_add(struct mvsw_pr_port *port,
+			    struct net_device *lag_dev, u16 lag_id);
+int prestera_lag_member_del(struct mvsw_pr_port *port);
+int prestera_lag_member_enable(struct mvsw_pr_port *port, bool enable);
+bool mvsw_pr_port_is_lag_member(const struct mvsw_pr_port *port);
+int prestera_lag_id_find(struct mvsw_pr_switch *sw, struct net_device *lag_dev,
+			 u16 *lag_id);
+void prestera_lag_member_rif_leave(const struct mvsw_pr_port *port,
+				   u16 lag_id, u16 vr_id);
+
+int mvsw_pr_dev_if_type(const struct net_device *dev);
+
+/* prestera_flower.c */
+int mvsw_pr_flower_replace(struct mvsw_pr_switch *sw,
+			   struct prestera_acl_block *block,
+			   struct flow_cls_offload *f);
+void mvsw_pr_flower_destroy(struct mvsw_pr_switch *sw,
+			    struct prestera_acl_block *block,
+			    struct flow_cls_offload *f);
+int mvsw_pr_flower_stats(struct mvsw_pr_switch *sw,
+			 struct prestera_acl_block *block,
+			 struct flow_cls_offload *f);
+
+/* prestera_acl.c */
+int prestera_acl_init(struct mvsw_pr_switch *sw);
+void prestera_acl_fini(struct mvsw_pr_switch *sw);
+struct prestera_acl_block *
+prestera_acl_block_create(struct mvsw_pr_switch *sw, struct net *net);
+void prestera_acl_block_destroy(struct prestera_acl_block *block);
+struct net *prestera_acl_block_net(struct prestera_acl_block *block);
+struct mvsw_pr_switch *prestera_acl_block_sw(struct prestera_acl_block *block);
+unsigned int prestera_acl_block_rule_count(struct prestera_acl_block *block);
+void prestera_acl_block_disable_inc(struct prestera_acl_block *block);
+void prestera_acl_block_disable_dec(struct prestera_acl_block *block);
+bool prestera_acl_block_disabled(const struct prestera_acl_block *block);
+int prestera_acl_block_bind(struct mvsw_pr_switch *sw,
+			    struct prestera_acl_block *block,
+			    struct mvsw_pr_port *port);
+int prestera_acl_block_unbind(struct mvsw_pr_switch *sw,
+			      struct prestera_acl_block *block,
+			      struct mvsw_pr_port *port);
+struct prestera_acl_ruleset *
+prestera_acl_block_ruleset_get(struct prestera_acl_block *block);
+struct prestera_acl_rule *
+prestera_acl_rule_create(struct prestera_acl_block *block,
+			 unsigned long cookie);
+u32 prestera_acl_rule_priority_get(struct prestera_acl_rule *rule);
+void prestera_acl_rule_priority_set(struct prestera_acl_rule *rule,
+				    u32 priority);
+u8 prestera_acl_rule_hw_tc_get(struct prestera_acl_rule *rule);
+void prestera_acl_rule_hw_tc_set(struct prestera_acl_rule *rule, u8 hw_tc);
+u16 prestera_acl_rule_ruleset_id_get(const struct prestera_acl_rule *rule);
+struct list_head *
+prestera_acl_rule_action_list_get(struct prestera_acl_rule *rule);
+u8 prestera_acl_rule_action_len(struct prestera_acl_rule *rule);
+void prestera_acl_rule_action_add(struct prestera_acl_rule *rule,
+				  struct prestera_acl_rule_action_entry *entry);
+struct list_head *
+prestera_acl_rule_match_list_get(struct prestera_acl_rule *rule);
+void prestera_acl_rule_match_add(struct prestera_acl_rule *rule,
+				 struct prestera_acl_rule_match_entry *entry);
+void prestera_acl_rule_destroy(struct prestera_acl_rule *rule);
+struct prestera_acl_rule *
+prestera_acl_rule_lookup(struct prestera_acl_ruleset *ruleset,
+			 unsigned long cookie);
+int prestera_acl_rule_add(struct mvsw_pr_switch *sw,
+			  struct prestera_acl_rule *rule);
+void prestera_acl_rule_del(struct mvsw_pr_switch *sw,
+			   struct prestera_acl_rule *rule);
+int prestera_acl_rule_get_stats(struct mvsw_pr_switch *sw,
+				struct prestera_acl_rule *rule,
+				u64 *packets, u64 *bytes, u64 *last_use);
+
+/* VLAN API */
+struct mvsw_pr_port_vlan *
+mvsw_pr_port_vlan_find_by_vid(const struct mvsw_pr_port *mvsw_pr_port, u16 vid);
+void
+mvsw_pr_port_vlan_bridge_leave(struct mvsw_pr_port_vlan *mvsw_pr_port_vlan);
+
+int prestera_switchdev_register(struct mvsw_pr_switch *sw);
+void prestera_switchdev_unregister(struct mvsw_pr_switch *sw);
+
+int prestera_device_register(struct prestera_device *dev);
+void prestera_device_unregister(struct prestera_device *dev);
+
+bool mvsw_pr_netdev_check(const struct net_device *dev);
+struct mvsw_pr_switch *mvsw_pr_switch_get(struct net_device *dev);
+struct mvsw_pr_port *mvsw_pr_port_dev_lower_find(struct net_device *dev);
+
+const struct mvsw_pr_port *mvsw_pr_port_find(u32 dev_hw_id, u32 port_hw_id);
+int mvsw_pr_schedule_dw(struct delayed_work *dwork, unsigned long delay);
+
+/* prestera_router.c */
+int mvsw_pr_router_init(struct mvsw_pr_switch *sw);
+void mvsw_pr_router_fini(struct mvsw_pr_switch *sw);
+int mvsw_pr_netdevice_router_port_event(struct net_device *dev,
+					unsigned long event, void *ptr);
+int mvsw_pr_inetaddr_valid_event(struct notifier_block *unused,
+				 unsigned long event, void *ptr);
+int mvsw_pr_netdevice_vrf_event(struct net_device *dev, unsigned long event,
+				struct netdev_notifier_changeupper_info *info);
+void mvsw_pr_port_router_leave(struct mvsw_pr_port *mvsw_pr_port);
+int mvsw_pr_lpm_add(struct mvsw_pr_switch *sw, u16 hw_vr_id,
+		    struct mvsw_pr_ip_addr *addr, u32 prefix_len, u32 grp_id);
+int mvsw_pr_lpm_del(struct mvsw_pr_switch *sw, u16 hw_vr_id,
+		    struct mvsw_pr_ip_addr *addr, u32 prefix_len);
+int mvsw_pr_nh_entries_set(const struct mvsw_pr_switch *sw, int count,
+			   struct mvsw_pr_neigh_info *nhs, u32 grp_id);
+int mvsw_pr_nh_entries_get(const struct mvsw_pr_switch *sw, int count,
+			   struct mvsw_pr_neigh_info *nhs, u32 grp_id);
+int mvsw_pr_nh_group_create(const struct mvsw_pr_switch *sw, u16 nh_count,
+			    u32 *grp_id);
+int mvsw_pr_nh_group_delete(const struct mvsw_pr_switch *sw, u16 nh_count,
+			    u32 grp_id);
+int mvsw_pr_mp4_hash_set(const struct mvsw_pr_switch *sw, u8 hash_policy);
+
+void mvsw_pr_rif_enable(struct mvsw_pr_switch *sw, struct net_device *dev,
+			bool enable);
+bool mvsw_pr_rif_exists(const struct mvsw_pr_switch *sw,
+			const struct net_device *dev);
+void mvsw_pr_router_lag_member_leave(const struct mvsw_pr_port *port,
+				     const struct net_device *dev);
+void prestera_lag_router_leave(struct mvsw_pr_switch *sw,
+			       struct net_device *lag_dev);
+
+void mvsw_pr_bridge_device_rifs_destroy(struct mvsw_pr_switch *sw,
+					struct net_device *bridge_dev);
+
+#endif /* _MVSW_PRESTERA_H_ */
diff --git a/drivers/net/ethernet/marvell/prestera_sw/prestera_acl.c b/drivers/net/ethernet/marvell/prestera_sw/prestera_acl.c
new file mode 100644
index 000000000..05317aa3d
--- /dev/null
+++ b/drivers/net/ethernet/marvell/prestera_sw/prestera_acl.c
@@ -0,0 +1,400 @@
+// SPDX-License-Identifier: BSD-3-Clause OR GPL-2.0
+//
+// Copyright (c) 2020 Marvell International Ltd. All rights reserved.
+//
+
+#include "prestera.h"
+#include "prestera_hw.h"
+
+#define MVSW_ACL_RULE_DEF_HW_TC		3
+
+struct prestera_acl {
+	struct mvsw_pr_switch *sw;
+	struct list_head rules;
+};
+
+struct prestera_acl_block_binding {
+	struct list_head list;
+	struct mvsw_pr_port *port;
+};
+
+struct prestera_acl_ruleset {
+	struct rhashtable rule_ht;
+	struct mvsw_pr_switch *sw;
+	u16 id;
+};
+
+struct prestera_acl_block {
+	struct list_head binding_list;
+	struct mvsw_pr_switch *sw;
+	unsigned int rule_count;
+	unsigned int disable_count;
+	struct net *net;
+	struct prestera_acl_ruleset *ruleset;
+};
+
+struct prestera_acl_rule {
+	struct rhash_head ht_node; /* Member of acl HT */
+	struct list_head list;
+	struct list_head match_list;
+	struct list_head action_list;
+	struct prestera_acl_block *block;
+	unsigned long cookie;
+	u32 priority;
+	u8 n_actions;
+	u8 hw_tc;
+	u32 id;
+};
+
+static const struct rhashtable_params prestera_acl_rule_ht_params = {
+	.key_len = sizeof(unsigned long),
+	.key_offset = offsetof(struct prestera_acl_rule, cookie),
+	.head_offset = offsetof(struct prestera_acl_rule, ht_node),
+	.automatic_shrinking = true,
+};
+
+static struct prestera_acl_ruleset *
+prestera_acl_ruleset_create(struct mvsw_pr_switch *sw)
+{
+	int err;
+	struct prestera_acl_ruleset *ruleset;
+
+	ruleset = kzalloc(sizeof(*ruleset), GFP_KERNEL);
+	if (!ruleset)
+		return ERR_PTR(-ENOMEM);
+
+	err = rhashtable_init(&ruleset->rule_ht, &prestera_acl_rule_ht_params);
+	if (err)
+		goto err_rhashtable_init;
+
+	err = mvsw_pr_hw_acl_ruleset_create(sw, &ruleset->id);
+	if (err)
+		goto err_ruleset_create;
+
+	ruleset->sw = sw;
+
+	return ruleset;
+
+err_ruleset_create:
+	rhashtable_destroy(&ruleset->rule_ht);
+err_rhashtable_init:
+	kfree(ruleset);
+	return ERR_PTR(err);
+}
+
+static void prestera_acl_ruleset_destroy(struct prestera_acl_ruleset *ruleset)
+{
+	mvsw_pr_hw_acl_ruleset_del(ruleset->sw, ruleset->id);
+	rhashtable_destroy(&ruleset->rule_ht);
+	kfree(ruleset);
+}
+
+struct prestera_acl_block *
+prestera_acl_block_create(struct mvsw_pr_switch *sw, struct net *net)
+{
+	struct prestera_acl_block *block;
+
+	block = kzalloc(sizeof(*block), GFP_KERNEL);
+	if (!block)
+		return NULL;
+	INIT_LIST_HEAD(&block->binding_list);
+	block->net = net;
+	block->sw = sw;
+
+	block->ruleset = prestera_acl_ruleset_create(sw);
+	if (IS_ERR(block->ruleset)) {
+		kfree(block);
+		return NULL;
+	}
+
+	return block;
+}
+
+void prestera_acl_block_destroy(struct prestera_acl_block *block)
+{
+	prestera_acl_ruleset_destroy(block->ruleset);
+	WARN_ON(!list_empty(&block->binding_list));
+	kfree(block);
+}
+
+static struct prestera_acl_block_binding *
+prestera_acl_block_lookup(struct prestera_acl_block *block,
+			  struct mvsw_pr_port *port)
+{
+	struct prestera_acl_block_binding *binding;
+
+	list_for_each_entry(binding, &block->binding_list, list)
+		if (binding->port == port)
+			return binding;
+
+	return NULL;
+}
+
+unsigned int prestera_acl_block_rule_count(struct prestera_acl_block *block)
+{
+	return block ? block->rule_count : 0;
+}
+
+void prestera_acl_block_disable_inc(struct prestera_acl_block *block)
+{
+	if (block)
+		block->disable_count++;
+}
+
+void prestera_acl_block_disable_dec(struct prestera_acl_block *block)
+{
+	if (block)
+		block->disable_count--;
+}
+
+bool prestera_acl_block_disabled(const struct prestera_acl_block *block)
+{
+	return block->disable_count;
+}
+
+int prestera_acl_block_bind(struct mvsw_pr_switch *sw,
+			    struct prestera_acl_block *block,
+			    struct mvsw_pr_port *port)
+{
+	struct prestera_acl_block_binding *binding;
+	int err;
+
+	if (WARN_ON(prestera_acl_block_lookup(block, port)))
+		return -EEXIST;
+
+	binding = kzalloc(sizeof(*binding), GFP_KERNEL);
+	if (!binding)
+		return -ENOMEM;
+	binding->port = port;
+
+	err = mvsw_pr_hw_acl_port_bind(port, block->ruleset->id);
+	if (err)
+		goto err_rules_bind;
+
+	list_add(&binding->list, &block->binding_list);
+	return 0;
+
+err_rules_bind:
+	kfree(binding);
+	return err;
+}
+
+int prestera_acl_block_unbind(struct mvsw_pr_switch *sw,
+			      struct prestera_acl_block *block,
+			      struct mvsw_pr_port *port)
+{
+	struct prestera_acl_block_binding *binding;
+
+	binding = prestera_acl_block_lookup(block, port);
+	if (!binding)
+		return -ENOENT;
+
+	list_del(&binding->list);
+
+	mvsw_pr_hw_acl_port_unbind(port, block->ruleset->id);
+
+	kfree(binding);
+	return 0;
+}
+
+struct prestera_acl_ruleset *
+prestera_acl_block_ruleset_get(struct prestera_acl_block *block)
+{
+	return block->ruleset;
+}
+
+u16 prestera_acl_rule_ruleset_id_get(const struct prestera_acl_rule *rule)
+{
+	return rule->block->ruleset->id;
+}
+
+struct net *prestera_acl_block_net(struct prestera_acl_block *block)
+{
+	return block->net;
+}
+
+struct mvsw_pr_switch *prestera_acl_block_sw(struct prestera_acl_block *block)
+{
+	return block->sw;
+}
+
+struct prestera_acl_rule *
+prestera_acl_rule_lookup(struct prestera_acl_ruleset *ruleset,
+			 unsigned long cookie)
+{
+	return rhashtable_lookup_fast(&ruleset->rule_ht, &cookie,
+				      prestera_acl_rule_ht_params);
+}
+
+struct prestera_acl_rule *
+prestera_acl_rule_create(struct prestera_acl_block *block,
+			 unsigned long cookie)
+{
+	struct prestera_acl_rule *rule;
+
+	rule = kzalloc(sizeof(*rule), GFP_KERNEL);
+	if (!rule)
+		return ERR_PTR(-ENOMEM);
+
+	INIT_LIST_HEAD(&rule->match_list);
+	INIT_LIST_HEAD(&rule->action_list);
+	rule->cookie = cookie;
+	rule->block = block;
+	rule->hw_tc = MVSW_ACL_RULE_DEF_HW_TC;
+
+	return rule;
+}
+
+struct list_head *
+prestera_acl_rule_match_list_get(struct prestera_acl_rule *rule)
+{
+	return &rule->match_list;
+}
+
+struct list_head *
+prestera_acl_rule_action_list_get(struct prestera_acl_rule *rule)
+{
+	return &rule->action_list;
+}
+
+void prestera_acl_rule_action_add(struct prestera_acl_rule *rule,
+				  struct prestera_acl_rule_action_entry *entry)
+{
+	list_add(&entry->list, &rule->action_list);
+	rule->n_actions++;
+}
+
+u8 prestera_acl_rule_action_len(struct prestera_acl_rule *rule)
+{
+	return rule->n_actions;
+}
+
+u32 prestera_acl_rule_priority_get(struct prestera_acl_rule *rule)
+{
+	return rule->priority;
+}
+
+void prestera_acl_rule_priority_set(struct prestera_acl_rule *rule,
+				    u32 priority)
+{
+	rule->priority = priority;
+}
+
+u8 prestera_acl_rule_hw_tc_get(struct prestera_acl_rule *rule)
+{
+	return rule->hw_tc;
+}
+
+void prestera_acl_rule_hw_tc_set(struct prestera_acl_rule *rule, u8 hw_tc)
+{
+	rule->hw_tc = hw_tc;
+}
+
+void prestera_acl_rule_match_add(struct prestera_acl_rule *rule,
+				 struct prestera_acl_rule_match_entry *entry)
+{
+	list_add(&entry->list, &rule->match_list);
+}
+
+void prestera_acl_rule_destroy(struct prestera_acl_rule *rule)
+{
+	struct prestera_acl_rule_action_entry *a_entry;
+	struct prestera_acl_rule_match_entry *m_entry;
+	struct list_head *pos, *n;
+
+	list_for_each_safe(pos, n, &rule->match_list) {
+		m_entry = list_entry(pos, typeof(*m_entry), list);
+		list_del(pos);
+		kfree(m_entry);
+	}
+	list_for_each_safe(pos, n, &rule->action_list) {
+		a_entry = list_entry(pos, typeof(*a_entry), list);
+		list_del(pos);
+		kfree(a_entry);
+	}
+	kfree(rule);
+}
+
+int prestera_acl_rule_add(struct mvsw_pr_switch *sw,
+			  struct prestera_acl_rule *rule)
+{
+	int err;
+	u32 rule_id;
+
+	/* try to add rule to hash table first */
+	err = rhashtable_insert_fast(&rule->block->ruleset->rule_ht,
+				     &rule->ht_node,
+				     prestera_acl_rule_ht_params);
+	if (err)
+		return err;
+
+	/* add rule to hw */
+	err = mvsw_pr_hw_acl_rule_add(sw, rule, &rule_id);
+	if (err)
+		goto err_rule_add;
+
+	rule->id = rule_id;
+
+	list_add_tail(&rule->list, &sw->acl->rules);
+	rule->block->rule_count++;
+
+	return 0;
+
+err_rule_add:
+	rhashtable_remove_fast(&rule->block->ruleset->rule_ht, &rule->ht_node,
+			       prestera_acl_rule_ht_params);
+	return err;
+}
+
+void prestera_acl_rule_del(struct mvsw_pr_switch *sw,
+			   struct prestera_acl_rule *rule)
+{
+	rhashtable_remove_fast(&rule->block->ruleset->rule_ht, &rule->ht_node,
+			       prestera_acl_rule_ht_params);
+	rule->block->rule_count--;
+	list_del(&rule->list);
+	mvsw_pr_hw_acl_rule_del(sw, rule->id);
+}
+
+int prestera_acl_rule_get_stats(struct mvsw_pr_switch *sw,
+				struct prestera_acl_rule *rule,
+				u64 *packets, u64 *bytes, u64 *last_use)
+{
+	u64 current_packets;
+	u64 current_bytes;
+	int err;
+
+	err = mvsw_pr_hw_acl_rule_stats_get(sw, rule->id, &current_packets,
+					    &current_bytes);
+	if (err)
+		return err;
+
+	*packets = current_packets;
+	*bytes = current_bytes;
+	*last_use = jiffies;
+
+	return 0;
+}
+
+int prestera_acl_init(struct mvsw_pr_switch *sw)
+{
+	struct prestera_acl *acl;
+
+	acl = kzalloc(sizeof(*acl), GFP_KERNEL);
+	if (!acl)
+		return -ENOMEM;
+
+	INIT_LIST_HEAD(&acl->rules);
+	sw->acl = acl;
+	acl->sw = sw;
+
+	return 0;
+}
+
+void prestera_acl_fini(struct mvsw_pr_switch *sw)
+{
+	struct prestera_acl *acl = sw->acl;
+
+	WARN_ON(!list_empty(&acl->rules));
+	kfree(acl);
+}
diff --git a/drivers/net/ethernet/marvell/prestera_sw/prestera_debugfs.c b/drivers/net/ethernet/marvell/prestera_sw/prestera_debugfs.c
new file mode 100644
index 000000000..1b4b8557e
--- /dev/null
+++ b/drivers/net/ethernet/marvell/prestera_sw/prestera_debugfs.c
@@ -0,0 +1,160 @@
+// SPDX-License-Identifier: BSD-3-Clause OR GPL-2.0
+/*
+ * Copyright (c) 2020 Marvell International Ltd. All rights reserved.
+ *
+ */
+#include <linux/kernel.h>
+#include <linux/fs.h>
+#include <linux/device.h>
+#include <linux/debugfs.h>
+
+#include "prestera_debugfs.h"
+#include "prestera.h"
+#include "prestera_log.h"
+#include "prestera_fw_log.h"
+#include "prestera_rxtx.h"
+
+#define DEBUGFS_ROOTDIR	"prestera"
+
+#define CPU_CODE_SUBDIR_NAME	"traps"
+#define CPU_CODE_MAX_BUF_SIZE	(MVSW_PR_RXTX_CPU_CODE_MAX_NUM * 32)
+
+static ssize_t cpu_code_stats_read(struct file *file,
+				   char __user *ubuf,
+				   size_t count, loff_t *ppos);
+
+struct mvsw_pr_debugfs {
+	struct dentry *root_dir;
+	struct dentry *cpu_code_subdir;
+	const struct file_operations cpu_code_stats_fops;
+	char *cpu_code_stats_buf;
+	/* serialize access to cpu_code_stats_buf */
+	struct mutex cpu_code_stats_mtx;
+};
+
+static struct mvsw_pr_debugfs prestera_debugfs = {
+	.cpu_code_stats_fops = {
+		.read = cpu_code_stats_read,
+		.open = simple_open,
+		.llseek = default_llseek,
+	},
+};
+
+int mvsw_pr_debugfs_init(struct mvsw_pr_switch *sw)
+{
+	const struct file_operations *fops =
+		&prestera_debugfs.cpu_code_stats_fops;
+	char file_name[] = "cpu_code_XXX_stats";
+	int err;
+	int i;
+
+	mutex_init(&prestera_debugfs.cpu_code_stats_mtx);
+
+	prestera_debugfs.cpu_code_stats_buf =
+		kzalloc(CPU_CODE_MAX_BUF_SIZE, GFP_KERNEL);
+
+	if (!prestera_debugfs.cpu_code_stats_buf)
+		return -ENOMEM;
+
+	err = mvsw_pr_fw_log_init(sw);
+	if (err)
+		return err;
+
+	prestera_debugfs.root_dir = debugfs_create_dir(DEBUGFS_ROOTDIR, NULL);
+	if (!prestera_debugfs.root_dir) {
+		err = -ENOMEM;
+		goto root_dir_alloc_failed;
+	}
+
+	prestera_debugfs.cpu_code_subdir =
+		debugfs_create_dir(CPU_CODE_SUBDIR_NAME,
+				   prestera_debugfs.root_dir);
+	if (!prestera_debugfs.cpu_code_subdir) {
+		err = -ENOMEM;
+		goto cpu_code_subdir_alloc_failed;
+	}
+
+	for (i = 0; i < MVSW_PR_RXTX_CPU_CODE_MAX_NUM; ++i) {
+		snprintf(file_name, sizeof(file_name), "cpu_code_%d_stats", i);
+		if (!debugfs_create_file(file_name, 0644,
+					 prestera_debugfs.cpu_code_subdir,
+					 (void *)(long)i, fops)) {
+			err = -ENOMEM;
+			goto cpu_code_single_file_creation_failed;
+		}
+	}
+
+	strncpy(file_name, "cpu_code_stats", sizeof(file_name));
+
+	if (!debugfs_create_file(file_name, 0644,
+				 prestera_debugfs.cpu_code_subdir,
+				 (void *)(long)MVSW_PR_RXTX_CPU_CODE_MAX_NUM,
+				 fops)) {
+		err = -ENOMEM;
+		goto cpu_code_single_file_creation_failed;
+	}
+
+	return 0;
+
+cpu_code_single_file_creation_failed:
+	debugfs_remove(prestera_debugfs.cpu_code_subdir);
+cpu_code_subdir_alloc_failed:
+	debugfs_remove(prestera_debugfs.root_dir);
+root_dir_alloc_failed:
+	mvsw_pr_fw_log_fini(sw);
+
+	return err;
+}
+
+void mvsw_pr_debugfs_fini(struct mvsw_pr_switch *sw)
+{
+	mvsw_pr_fw_log_fini(sw);
+
+	debugfs_remove(prestera_debugfs.cpu_code_subdir);
+	debugfs_remove(prestera_debugfs.root_dir);
+
+	mutex_destroy(&prestera_debugfs.cpu_code_stats_mtx);
+
+	kfree(prestera_debugfs.cpu_code_stats_buf);
+}
+
+static ssize_t cpu_code_stats_read(struct file *file,
+				   char __user *ubuf,
+				   size_t count, loff_t *ppos)
+{
+	char *buf = prestera_debugfs.cpu_code_stats_buf;
+	u16 cpu_code = (u16)(long)file->private_data;
+	u64 cpu_code_stats;
+	/* as the snprintf doesn't count for \0, start with 1 */
+	int buf_len = 1;
+	int ret;
+
+	mutex_lock(&prestera_debugfs.cpu_code_stats_mtx);
+
+	if (cpu_code == MVSW_PR_RXTX_CPU_CODE_MAX_NUM) {
+		int i;
+
+		memset(buf, 0, CPU_CODE_MAX_BUF_SIZE);
+
+		for (i = 0; i < MVSW_PR_RXTX_CPU_CODE_MAX_NUM; ++i) {
+			cpu_code_stats = mvsw_pr_rxtx_get_cpu_code_stats(i);
+
+			if (!cpu_code_stats)
+				continue;
+
+			buf_len += snprintf(buf + buf_len,
+					    CPU_CODE_MAX_BUF_SIZE - buf_len,
+					    "%u:%llu\n", i, cpu_code_stats);
+		}
+
+	} else {
+		cpu_code_stats = mvsw_pr_rxtx_get_cpu_code_stats((u8)cpu_code);
+
+		buf_len += sprintf(buf, "%llu\n", cpu_code_stats);
+	}
+
+	ret = simple_read_from_buffer(ubuf, count, ppos, buf, buf_len);
+	mutex_unlock(&prestera_debugfs.cpu_code_stats_mtx);
+
+	return ret;
+}
diff --git a/drivers/net/ethernet/marvell/prestera_sw/prestera_debugfs.h b/drivers/net/ethernet/marvell/prestera_sw/prestera_debugfs.h
new file mode 100644
index 000000000..b3ce1a41b
--- /dev/null
+++ b/drivers/net/ethernet/marvell/prestera_sw/prestera_debugfs.h
@@ -0,0 +1,14 @@
+/* SPDX-License-Identifier: BSD-3-Clause OR GPL-2.0
+ *
+ * Copyright (c) 2020 Marvell International Ltd. All rights reserved.
+ *
+ */
+#ifndef _MVSW_PRESTERA_DEBUGFS_H_
+#define _MVSW_PRESTERA_DEBUGFS_H_
+
+struct mvsw_pr_switch;
+
+int mvsw_pr_debugfs_init(struct mvsw_pr_switch *sw);
+void mvsw_pr_debugfs_fini(struct mvsw_pr_switch *sw);
+
+#endif /* _MVSW_PRESTERA_DEBUGFS_H_ */
diff --git a/drivers/net/ethernet/marvell/prestera_sw/prestera_devlink.c b/drivers/net/ethernet/marvell/prestera_sw/prestera_devlink.c
new file mode 100644
index 000000000..8770b5f7e
--- /dev/null
+++ b/drivers/net/ethernet/marvell/prestera_sw/prestera_devlink.c
@@ -0,0 +1,108 @@
+// SPDX-License-Identifier: BSD-3-Clause OR GPL-2.0
+/* Copyright (c) 2020 Marvell International Ltd. All rights reserved */
+
+#include <net/devlink.h>
+
+#include "prestera_devlink.h"
+
+static int prestera_dl_info_get(struct devlink *dl,
+				struct devlink_info_req *req,
+				struct netlink_ext_ack *extack)
+{
+	struct mvsw_pr_switch *sw = devlink_priv(dl);
+	char buf[16];
+	int err;
+
+	err = devlink_info_driver_name_put(req, PRESTERA_DRV_NAME);
+	if (err)
+		return err;
+
+	snprintf(buf, sizeof(buf), "%d.%d.%d",
+		 sw->dev->fw_rev.maj,
+		 sw->dev->fw_rev.min,
+		 sw->dev->fw_rev.sub);
+
+	return devlink_info_version_running_put(req,
+					       DEVLINK_INFO_VERSION_GENERIC_FW,
+					       buf);
+}
+
+static const struct devlink_ops prestera_dl_ops = {
+	.info_get = prestera_dl_info_get,
+};
+
+struct mvsw_pr_switch *prestera_devlink_alloc(void)
+{
+	struct devlink *dl;
+
+	dl = devlink_alloc(&prestera_dl_ops, sizeof(struct mvsw_pr_switch));
+
+	return devlink_priv(dl);
+}
+
+void prestera_devlink_free(struct mvsw_pr_switch *sw)
+{
+	struct devlink *dl = priv_to_devlink(sw);
+
+	devlink_free(dl);
+}
+
+int prestera_devlink_register(struct mvsw_pr_switch *sw)
+{
+	struct devlink *dl = priv_to_devlink(sw);
+	int err;
+
+	err = devlink_register(dl, sw->dev->dev);
+	if (err)
+		dev_err(sw->dev->dev, "devlink_register failed: %d\n", err);
+
+	return err;
+}
+
+void prestera_devlink_unregister(struct mvsw_pr_switch *sw)
+{
+	struct devlink *dl = priv_to_devlink(sw);
+
+	devlink_unregister(dl);
+}
+
+int prestera_devlink_port_register(struct mvsw_pr_port *port)
+{
+	struct mvsw_pr_switch *sw = port->sw;
+	struct devlink *dl = priv_to_devlink(sw);
+	int err;
+
+	devlink_port_attrs_set(&port->dl_port, DEVLINK_PORT_FLAVOUR_PHYSICAL,
+			       port->fp_id, false, 0, &port->sw->id,
+			       sizeof(port->sw->id));
+
+	err = devlink_port_register(dl, &port->dl_port, port->fp_id);
+	if (err) {
+		dev_err(sw->dev->dev, "devlink_port_register failed\n");
+		return err;
+	}
+
+	return 0;
+}
+
+void prestera_devlink_port_unregister(struct mvsw_pr_port *port)
+{
+	devlink_port_unregister(&port->dl_port);
+}
+
+void prestera_devlink_port_set(struct mvsw_pr_port *port)
+{
+	devlink_port_type_eth_set(&port->dl_port, port->net_dev);
+}
+
+void prestera_devlink_port_clear(struct mvsw_pr_port *port)
+{
+	devlink_port_type_clear(&port->dl_port);
+}
+
+struct devlink_port *prestera_devlink_get_port(struct net_device *dev)
+{
+	struct mvsw_pr_port *port = netdev_priv(dev);
+
+	return &port->dl_port;
+}
diff --git a/drivers/net/ethernet/marvell/prestera_sw/prestera_devlink.h b/drivers/net/ethernet/marvell/prestera_sw/prestera_devlink.h
new file mode 100644
index 000000000..4d51c4287
--- /dev/null
+++ b/drivers/net/ethernet/marvell/prestera_sw/prestera_devlink.h
@@ -0,0 +1,23 @@
+/* SPDX-License-Identifier: BSD-3-Clause OR GPL-2.0 */
+/* Copyright (c) 2020 Marvell International Ltd. All rights reserved. */
+
+#ifndef _PRESTERA_DEVLINK_H_
+#define _PRESTERA_DEVLINK_H_
+
+#include "prestera.h"
+
+struct mvsw_pr_switch *prestera_devlink_alloc(void);
+void prestera_devlink_free(struct mvsw_pr_switch *sw);
+
+int prestera_devlink_register(struct mvsw_pr_switch *sw);
+void prestera_devlink_unregister(struct mvsw_pr_switch *sw);
+
+int prestera_devlink_port_register(struct mvsw_pr_port *port);
+void prestera_devlink_port_unregister(struct mvsw_pr_port *port);
+
+void prestera_devlink_port_set(struct mvsw_pr_port *port);
+void prestera_devlink_port_clear(struct mvsw_pr_port *port);
+
+struct devlink_port *prestera_devlink_get_port(struct net_device *dev);
+
+#endif /* _PRESTERA_DEVLINK_H_ */
diff --git a/drivers/net/ethernet/marvell/prestera_sw/prestera_drv_ver.h b/drivers/net/ethernet/marvell/prestera_sw/prestera_drv_ver.h
new file mode 100644
index 000000000..29b337586
--- /dev/null
+++ b/drivers/net/ethernet/marvell/prestera_sw/prestera_drv_ver.h
@@ -0,0 +1,23 @@
+/* SPDX-License-Identifier: BSD-3-Clause OR GPL-2.0
+ *
+ * Copyright (c) 2019-2020 Marvell International Ltd. All rights reserved.
+ *
+ */
+#ifndef _PRESTERA_DRV_VER_H_
+#define _PRESTERA_DRV_VER_H_
+
+#include <linux/stringify.h>
+
+/* Prestera driver version */
+#define PRESTERA_DRV_VER_MAJOR	2
+#define PRESTERA_DRV_VER_MINOR	0
+#define PRESTERA_DRV_VER_PATCH	0
+#define PRESTERA_DRV_VER_EXTRA
+
+#define PRESTERA_DRV_VER \
+		__stringify(PRESTERA_DRV_VER_MAJOR)  "." \
+		__stringify(PRESTERA_DRV_VER_MINOR)  "." \
+		__stringify(PRESTERA_DRV_VER_PATCH)  \
+		__stringify(PRESTERA_DRV_VER_EXTRA)
+
+#endif  /* _PRESTERA_DRV_VER_H_ */
diff --git a/drivers/net/ethernet/marvell/prestera_sw/prestera_dsa.c b/drivers/net/ethernet/marvell/prestera_sw/prestera_dsa.c
new file mode 100644
index 000000000..3a1b81979
--- /dev/null
+++ b/drivers/net/ethernet/marvell/prestera_sw/prestera_dsa.c
@@ -0,0 +1,316 @@
+// SPDX-License-Identifier: BSD-3-Clause OR GPL-2.0
+/*
+ * Copyright (c) 2020 Marvell International Ltd. All rights reserved.
+ *
+ */
+#include "prestera.h"
+#include "prestera_dsa.h"
+
+#include <linux/string.h>
+#include <linux/bitops.h>
+#include <linux/bitfield.h>
+#include <linux/errno.h>
+
+#define W0_MASK_IS_TAGGED	BIT(29)
+
+/* TrgDev[4:0] = {Word0[28:24]} */
+#define W0_MASK_HW_DEV_NUM	GENMASK(28, 24)
+
+/* SrcPort/TrgPort extended to 8b
+ * SrcPort/TrgPort[7:0] = {Word2[20], Word1[11:10], Word0[23:19]}
+ */
+#define W0_MASK_IFACE_PORT_NUM	GENMASK(23, 19)
+
+/* bits 30:31 - TagCommand 1 = FROM_CPU */
+#define W0_MASK_DSA_CMD		GENMASK(31, 30)
+
+/* bits 13:15 -- UP */
+#define W0_MASK_VPT		GENMASK(15, 13)
+
+#define W0_MASK_EXT_BIT		BIT(12)
+#define W0_MASK_OPCODE		GENMASK(18, 16)
+
+/* bit 16 - CFI */
+#define W0_MASK_CFI_BIT		BIT(16)
+
+/* bits 0:11 -- VID */
+#define W0_MASK_VID		GENMASK(11, 0)
+
+#define W1_MASK_SRC_IS_TARNK	BIT(27)
+
+/* SrcPort/TrgPort extended to 8b
+ * SrcPort/TrgPort[7:0] = {Word2[20], Word1[11:10], Word0[23:19]}
+ */
+#define W1_MASK_IFACE_PORT_NUM	GENMASK(11, 10)
+
+#define W1_MASK_EXT_BIT		BIT(31)
+#define W1_MASK_CFI_BIT		BIT(30)
+
+/* bit 30 -- EgressFilterEn */
+#define W1_MASK_EGR_FILTER_EN	BIT(30)
+
+/* bit 28 -- egrFilterRegistered */
+#define W1_MASK_EGR_FILTER_REG	BIT(28)
+
+/* bits 20-24 -- Src-ID */
+#define W1_MASK_SRC_ID		GENMASK(24, 20)
+
+/* bits 15-19 -- SrcDev */
+#define W1_MASK_SRC_DEV		GENMASK(19, 15)
+
+/* SrcTrunk is extended to 12b
+ * SrcTrunk[11:0] = {Word2[14:3]
+ */
+#define W2_MASK_SRC_TRANK_ID	GENMASK(14, 3)
+
+/* SRCePort[16:0]/TRGePort[16:0]/ = {Word2[19:3]} */
+#define W2_MASK_IFACE_EPORT	GENMASK(19, 3)
+
+/* SrcPort/TrgPort extended to 8b
+ * SrcPort/TrgPort[7:0] = {Word2[20], Word1[11:10], Word0[23:19]}
+ */
+#define W2_MASK_IFACE_PORT_NUM	BIT(20)
+
+#define W2_MASK_EXT_BIT		BIT(31)
+
+/* 5b SrcID is extended to 12 bits
+ * SrcID[11:0] = {Word2[27:21], Word1[24:20]}
+ */
+#define W2_MASK_SRC_ID		GENMASK(27, 21)
+
+/* 5b SrcDev is extended to 12b
+ * SrcDev[11:0] = {Word2[20:14], Word1[19:15]}
+ */
+#define W2_MASK_SRC_DEV		GENMASK(20, 14)
+
+/* trgHwDev and trgPort
+ * TrgDev[11:5] = {Word3[6:0]}
+ */
+#define W3_MASK_HW_DEV_NUM	GENMASK(6, 0)
+
+/* bits 0-7 -- CpuCode */
+#define W1_MASK_CPU_CODE	GENMASK(7, 0)
+
+/* VID becomes 16b eVLAN. eVLAN[15:0] = {Word3[30:27], Word0[11:0]} */
+#define W3_MASK_VID		GENMASK(30, 27)
+
+/* TRGePort[16:0] = {Word3[23:7]} */
+#define W3_MASK_DST_EPORT	GENMASK(23, 7)
+
+#define DEV_NUM_MASK		GENMASK(11, 5)
+#define VID_MASK		GENMASK(15, 12)
+
+static int net_if_dsa_to_cpu_parse(const u32 *words_ptr,
+				   struct mvsw_pr_dsa *dsa_info_ptr)
+{
+	u32 get_value;	/* used to get needed bits from the DSA */
+	struct mvsw_pr_dsa_to_cpu *to_cpu_ptr;
+
+	to_cpu_ptr = &dsa_info_ptr->dsa_info.to_cpu;
+	to_cpu_ptr->is_tagged =
+	    (bool)FIELD_GET(W0_MASK_IS_TAGGED, words_ptr[0]);
+	to_cpu_ptr->hw_dev_num = FIELD_GET(W0_MASK_HW_DEV_NUM, words_ptr[0]);
+	to_cpu_ptr->src_is_trunk =
+	    (bool)FIELD_GET(W1_MASK_SRC_IS_TARNK, words_ptr[1]);
+
+	/* set hw dev num */
+	get_value = FIELD_GET(W3_MASK_HW_DEV_NUM, words_ptr[3]);
+	to_cpu_ptr->hw_dev_num &= W3_MASK_HW_DEV_NUM;
+	to_cpu_ptr->hw_dev_num |= FIELD_PREP(DEV_NUM_MASK, get_value);
+
+	get_value = FIELD_GET(W1_MASK_CPU_CODE, words_ptr[1]);
+	to_cpu_ptr->cpu_code = (u8)get_value;
+
+	if (to_cpu_ptr->src_is_trunk) {
+		to_cpu_ptr->iface.src_trunk_id =
+		    (u16)FIELD_GET(W2_MASK_SRC_TRANK_ID, words_ptr[2]);
+	} else {
+		/* When to_cpu_ptr->is_egress_pipe = false:
+		 *   this field indicates the source ePort number assigned by
+		 *   the ingress device.
+		 * When to_cpu_ptr->is_egress_pipe = true:
+		 *   this field indicates the target ePort number assigned by
+		 *   the ingress device.
+		 */
+		to_cpu_ptr->iface.eport =
+		    FIELD_GET(W2_MASK_IFACE_EPORT, words_ptr[2]);
+	}
+	to_cpu_ptr->iface.port_num =
+	    (FIELD_GET(W0_MASK_IFACE_PORT_NUM, words_ptr[0]) << 0) |
+	    (FIELD_GET(W1_MASK_IFACE_PORT_NUM, words_ptr[1]) << 5) |
+	    (FIELD_GET(W2_MASK_IFACE_PORT_NUM, words_ptr[2]) << 7);
+
+	return 0;
+}
+
+int mvsw_pr_dsa_parse(const u8 *dsa_bytes_ptr, struct mvsw_pr_dsa *dsa_info_ptr)
+{
+	u32 get_value;		/* used to get needed bits from the DSA */
+	u32 words_ptr[4] = { 0 };	/* DSA tag can be up to 4 words */
+	u32 *dsa_words_ptr = (u32 *)dsa_bytes_ptr;
+
+	/* sanity */
+	if (unlikely(!dsa_info_ptr || !dsa_bytes_ptr))
+		return -EINVAL;
+
+	/* zero results */
+	memset(dsa_info_ptr, 0, sizeof(struct mvsw_pr_dsa));
+
+	/* copy the data of the first word */
+	words_ptr[0] = ntohl((__force __be32)dsa_words_ptr[0]);
+
+	/* set the common parameters */
+	dsa_info_ptr->dsa_cmd =
+	    (enum mvsw_pr_dsa_cmd)FIELD_GET(W0_MASK_DSA_CMD, words_ptr[0]);
+
+	/* vid & vlan prio */
+	dsa_info_ptr->common_params.vid =
+	    (u16)FIELD_GET(W0_MASK_VID, words_ptr[0]);
+	dsa_info_ptr->common_params.vpt =
+	    (u8)FIELD_GET(W0_MASK_VPT, words_ptr[0]);
+
+	/* only to CPU is supported */
+	if (unlikely(dsa_info_ptr->dsa_cmd != MVSW_NET_DSA_CMD_TO_CPU_E))
+		return -EINVAL;
+
+	/* check extended bit */
+	if (FIELD_GET(W0_MASK_EXT_BIT, words_ptr[0]) == 0)
+		/* 1 words DSA tag is not supported */
+		return -EINVAL;
+
+	/* check that the "old" cpu opcode is set the 0xF
+	 * (with the extended bit)
+	 */
+	if (FIELD_GET(W0_MASK_OPCODE, words_ptr[0]) != 0x07)
+		return -EINVAL;
+
+	/* copy the data of the second word */
+	words_ptr[1] = ntohl((__force __be32)dsa_words_ptr[1]);
+
+	/* check the extended bit */
+	if (FIELD_GET(W1_MASK_EXT_BIT, words_ptr[1]) == 0)
+		/* 2 words DSA tag is not supported */
+		return -EINVAL;
+
+	/* copy the data of the third word */
+	words_ptr[2] = ntohl((__force __be32)dsa_words_ptr[2]);
+
+	/* check the extended bit */
+	if (FIELD_GET(W2_MASK_EXT_BIT, words_ptr[1]) == 0)
+		/* 3 words DSA tag is not supported */
+		return -EINVAL;
+
+	/* copy the data of the forth word */
+	words_ptr[3] = ntohl((__force __be32)dsa_words_ptr[3]);
+
+	/* VID */
+	get_value = FIELD_GET(W3_MASK_VID, words_ptr[3]);
+	dsa_info_ptr->common_params.vid &= ~VID_MASK;
+	dsa_info_ptr->common_params.vid |= FIELD_PREP(VID_MASK, get_value);
+
+	dsa_info_ptr->common_params.cfi_bit =
+	    (u8)FIELD_GET(W1_MASK_CFI_BIT, words_ptr[1]);
+
+	return net_if_dsa_to_cpu_parse(words_ptr, dsa_info_ptr);
+}
+
+static int net_if_dsa_tag_from_cpu_build(const struct mvsw_pr_dsa *dsa_info_ptr,
+					 u32 *words_ptr)
+{
+	u32 trg_hw_dev = 0;
+	u32 trg_port = 0;
+	const struct mvsw_pr_dsa_from_cpu *from_cpu_ptr =
+	    &dsa_info_ptr->dsa_info.from_cpu;
+
+	if (unlikely(from_cpu_ptr->dst_iface.type != MVSW_IF_PORT_E))
+		/* only sending to port interface is supported */
+		return -EINVAL;
+
+	words_ptr[0] |=
+	    FIELD_PREP(W0_MASK_DSA_CMD, MVSW_NET_DSA_CMD_FROM_CPU_E);
+
+	trg_hw_dev = from_cpu_ptr->dst_iface.dev_port.hw_dev_num;
+	trg_port = from_cpu_ptr->dst_iface.dev_port.port_num;
+
+	if (trg_hw_dev >= BIT(12))
+		return -EINVAL;
+
+	if (trg_port >= BIT(8) || trg_port >= BIT(10))
+		return -EINVAL;
+
+	words_ptr[0] |= FIELD_PREP(W0_MASK_HW_DEV_NUM, trg_hw_dev);
+	words_ptr[3] |= FIELD_PREP(W3_MASK_HW_DEV_NUM, (trg_hw_dev >> 5));
+
+	if (dsa_info_ptr->common_params.cfi_bit == 1)
+		words_ptr[0] |= FIELD_PREP(W0_MASK_CFI_BIT, 1);
+
+	words_ptr[0] |= FIELD_PREP(W0_MASK_VPT,
+				   dsa_info_ptr->common_params.vpt);
+	words_ptr[0] |= FIELD_PREP(W0_MASK_VID,
+				   dsa_info_ptr->common_params.vid);
+
+	/* set extended bits */
+	words_ptr[0] |= FIELD_PREP(W0_MASK_EXT_BIT, 1);
+	words_ptr[1] |= FIELD_PREP(W1_MASK_EXT_BIT, 1);
+	words_ptr[2] |= FIELD_PREP(W2_MASK_EXT_BIT, 1);
+
+	if (from_cpu_ptr->egr_filter_en)
+		words_ptr[1] |= FIELD_PREP(W1_MASK_EGR_FILTER_EN, 1);
+
+	if (from_cpu_ptr->egr_filter_registered)
+		words_ptr[1] |= FIELD_PREP(W1_MASK_EGR_FILTER_REG, 1);
+
+	/* check src_id & src_hw_dev */
+	if (from_cpu_ptr->src_id >= BIT(12) ||
+	    from_cpu_ptr->src_hw_dev >= BIT(12)) {
+		return -EINVAL;
+	}
+
+	words_ptr[1] |= FIELD_PREP(W1_MASK_SRC_ID, from_cpu_ptr->src_id);
+	words_ptr[1] |= FIELD_PREP(W1_MASK_SRC_DEV, from_cpu_ptr->src_hw_dev);
+
+	words_ptr[2] |= FIELD_PREP(W2_MASK_SRC_ID, from_cpu_ptr->src_id >> 5);
+	words_ptr[2] |= FIELD_PREP(W2_MASK_SRC_DEV,
+				   from_cpu_ptr->src_hw_dev >> 5);
+
+	/* bits 0:9 -- reserved with value 0 */
+	if (from_cpu_ptr->dst_eport >= BIT(17))
+		return -EINVAL;
+
+	words_ptr[3] |= FIELD_PREP(W3_MASK_DST_EPORT, from_cpu_ptr->dst_eport);
+	words_ptr[3] |= FIELD_PREP(W3_MASK_VID,
+				   (dsa_info_ptr->common_params.vid >> 12));
+
+	return 0;
+}
+
+int mvsw_pr_dsa_build(const struct mvsw_pr_dsa *dsa_info_ptr,
+		      u8 *dsa_bytes_ptr)
+{
+	int rc;
+	u32 words_ptr[4] = { 0 };	/* 4 words of DSA tag */
+	__be32 *dsa_words_ptr = (__be32 *)dsa_bytes_ptr;
+
+	if (unlikely(!dsa_info_ptr || !dsa_bytes_ptr))
+		return -EINVAL;
+
+	if (dsa_info_ptr->common_params.cfi_bit >= BIT(1) ||
+	    dsa_info_ptr->common_params.vpt >= BIT(3)) {
+		return -EINVAL;
+	}
+
+	if (unlikely(dsa_info_ptr->dsa_cmd != MVSW_NET_DSA_CMD_FROM_CPU_E))
+		return -EINVAL;
+
+	/* build form CPU DSA tag */
+	rc = net_if_dsa_tag_from_cpu_build(dsa_info_ptr, words_ptr);
+	if (rc != 0)
+		return rc;
+
+	dsa_words_ptr[0] = htonl(words_ptr[0]);
+	dsa_words_ptr[1] = htonl(words_ptr[1]);
+	dsa_words_ptr[2] = htonl(words_ptr[2]);
+	dsa_words_ptr[3] = htonl(words_ptr[3]);
+
+	return 0;
+}
diff --git a/drivers/net/ethernet/marvell/prestera_sw/prestera_dsa.h b/drivers/net/ethernet/marvell/prestera_sw/prestera_dsa.h
new file mode 100644
index 000000000..a118a4859
--- /dev/null
+++ b/drivers/net/ethernet/marvell/prestera_sw/prestera_dsa.h
@@ -0,0 +1,67 @@
+/* SPDX-License-Identifier: BSD-3-Clause OR GPL-2.0
+ *
+ * Copyright (c) 2020 Marvell International Ltd. All rights reserved.
+ *
+ */
+#ifndef _MVSW_PRESTERA_DSA_H_
+#define _MVSW_PRESTERA_DSA_H_
+
+#include <linux/types.h>
+
+#define MVSW_PR_DSA_HLEN	16
+
+enum mvsw_pr_dsa_cmd {
+	/* DSA command is "To CPU" */
+	MVSW_NET_DSA_CMD_TO_CPU_E = 0,
+
+	/* DSA command is "FROM CPU" */
+	MVSW_NET_DSA_CMD_FROM_CPU_E,
+};
+
+struct mvsw_pr_dsa_common {
+	/* the value vlan priority tag (APPLICABLE RANGES: 0..7) */
+	u8 vpt;
+
+	/* CFI bit of the vlan tag (APPLICABLE RANGES: 0..1) */
+	u8 cfi_bit;
+
+	/* Vlan id */
+	u16 vid;
+};
+
+struct mvsw_pr_dsa_to_cpu {
+	bool is_tagged;
+	u32 hw_dev_num;
+	bool src_is_trunk;
+	u8 cpu_code;
+	struct {
+		u16 src_trunk_id;
+		u32 port_num;
+		u32 eport;
+	} iface;
+};
+
+struct mvsw_pr_dsa_from_cpu {
+	struct mvsw_pr_iface dst_iface;	/* vid/port */
+	bool egr_filter_en;
+	bool egr_filter_registered;
+	u32 src_id;
+	u32 src_hw_dev;
+	u32 dst_eport;	/* for port but not for vid */
+};
+
+struct mvsw_pr_dsa {
+	struct mvsw_pr_dsa_common common_params;
+	enum mvsw_pr_dsa_cmd dsa_cmd;
+	union {
+		struct mvsw_pr_dsa_to_cpu to_cpu;
+		struct mvsw_pr_dsa_from_cpu from_cpu;
+	} dsa_info;
+};
+
+int mvsw_pr_dsa_parse(const u8 *dsa_bytes_ptr,
+		      struct mvsw_pr_dsa *dsa_info_ptr);
+int mvsw_pr_dsa_build(const struct mvsw_pr_dsa *dsa_info_ptr,
+		      u8 *dsa_bytes_ptr);
+
+#endif /* _MVSW_PRESTERA_DSA_H_ */
diff --git a/drivers/net/ethernet/marvell/prestera_sw/prestera_flower.c b/drivers/net/ethernet/marvell/prestera_sw/prestera_flower.c
new file mode 100644
index 000000000..1817601f8
--- /dev/null
+++ b/drivers/net/ethernet/marvell/prestera_sw/prestera_flower.c
@@ -0,0 +1,429 @@
+// SPDX-License-Identifier: BSD-3-Clause OR GPL-2.0
+//
+// Copyright (c) 2020 Marvell International Ltd. All rights reserved.
+//
+
+#include "prestera.h"
+#include "prestera_hw.h"
+
+#define PRESTERA_DEFAULT_TC_NUM	8
+
+static int mvsw_pr_flower_parse_actions(struct mvsw_pr_switch *sw,
+					struct prestera_acl_block *block,
+					struct prestera_acl_rule *rule,
+					struct flow_action *flow_action,
+					struct netlink_ext_ack *extack)
+{
+	const struct flow_action_entry *act;
+	struct prestera_acl_rule_action_entry *a_entry;
+	int i;
+
+	if (!flow_action_has_entries(flow_action))
+		return 0;
+
+	flow_action_for_each(i, act, flow_action) {
+		/* allocate action entry */
+		a_entry = kmalloc(sizeof(*a_entry), GFP_KERNEL);
+		if (!a_entry)
+			return -ENOMEM;
+
+		switch (act->id) {
+		case FLOW_ACTION_ACCEPT:
+			a_entry->id = MVSW_ACL_RULE_ACTION_ACCEPT;
+			prestera_acl_rule_action_add(rule, a_entry);
+			break;
+		case FLOW_ACTION_DROP:
+			a_entry->id = MVSW_ACL_RULE_ACTION_DROP;
+			prestera_acl_rule_action_add(rule, a_entry);
+			break;
+		case FLOW_ACTION_TRAP:
+			a_entry->id = MVSW_ACL_RULE_ACTION_TRAP;
+			prestera_acl_rule_action_add(rule, a_entry);
+			break;
+		case FLOW_ACTION_POLICE:
+			a_entry->id = MVSW_ACL_RULE_ACTION_POLICE;
+			a_entry->police.rate = act->police.rate_bytes_ps;
+			a_entry->police.burst =
+				div_u64(a_entry->police.rate *
+					PSCHED_NS2TICKS(act->police.burst),
+					PSCHED_TICKS_PER_SEC);
+			prestera_acl_rule_action_add(rule, a_entry);
+			break;
+		default:
+			kfree(a_entry);
+			NL_SET_ERR_MSG_MOD(extack, "Unsupported action");
+			pr_err("Unsupported action\n");
+			return -EOPNOTSUPP;
+		}
+	}
+
+	return 0;
+}
+
+static int mvsw_pr_flower_parse_meta(struct prestera_acl_rule *rule,
+				     struct flow_cls_offload *f,
+				     struct prestera_acl_block *block)
+{
+	struct flow_rule *f_rule = flow_cls_offload_flow_rule(f);
+	struct prestera_acl_rule_match_entry *m_entry;
+	struct mvsw_pr_port *port;
+	struct net_device *ingress_dev;
+	struct flow_match_meta match;
+
+	flow_rule_match_meta(f_rule, &match);
+	if (match.mask->ingress_ifindex != 0xFFFFFFFF) {
+		NL_SET_ERR_MSG_MOD(f->common.extack,
+				   "Unsupported ingress ifindex mask");
+		return -EINVAL;
+	}
+
+	ingress_dev = __dev_get_by_index(prestera_acl_block_net(block),
+					 match.key->ingress_ifindex);
+	if (!ingress_dev) {
+		NL_SET_ERR_MSG_MOD(f->common.extack,
+				   "Can't find specified ingress port to match on");
+		return -EINVAL;
+	}
+
+	if (!mvsw_pr_netdev_check(ingress_dev)) {
+		NL_SET_ERR_MSG_MOD(f->common.extack,
+				   "Can't match on switchdev ingress port");
+		return -EINVAL;
+	}
+	port = netdev_priv(ingress_dev);
+
+	/* add port key,mask */
+	m_entry = kmalloc(sizeof(*m_entry), GFP_KERNEL);
+	if (!m_entry)
+		return -ENOMEM;
+
+	m_entry->type = MVSW_ACL_RULE_MATCH_ENTRY_TYPE_PORT;
+	m_entry->keymask.u64.key = port->hw_id | ((u64)port->dev_id << 32);
+	m_entry->keymask.u64.mask = ~(u64)0;
+	prestera_acl_rule_match_add(rule, m_entry);
+
+	return 0;
+}
+
+static int mvsw_pr_flower_parse(struct mvsw_pr_switch *sw,
+				struct prestera_acl_block *block,
+				struct prestera_acl_rule *rule,
+				struct flow_cls_offload *f)
+{
+	struct flow_rule *f_rule = flow_cls_offload_flow_rule(f);
+	struct flow_dissector *dissector = f_rule->match.dissector;
+	struct prestera_acl_rule_match_entry *m_entry;
+	u16 n_proto_mask = 0;
+	u16 n_proto_key = 0;
+	u16 addr_type = 0;
+	u8 ip_proto = 0;
+	u32 hwtc = 0;
+	int err;
+
+	if (dissector->used_keys &
+	    ~(BIT(FLOW_DISSECTOR_KEY_META) |
+	      BIT(FLOW_DISSECTOR_KEY_CONTROL) |
+	      BIT(FLOW_DISSECTOR_KEY_BASIC) |
+	      BIT(FLOW_DISSECTOR_KEY_ETH_ADDRS) |
+	      BIT(FLOW_DISSECTOR_KEY_IPV4_ADDRS) |
+	      BIT(FLOW_DISSECTOR_KEY_IPV6_ADDRS) |
+	      BIT(FLOW_DISSECTOR_KEY_ICMP) |
+	      BIT(FLOW_DISSECTOR_KEY_PORTS) |
+	      BIT(FLOW_DISSECTOR_KEY_PORTS_RANGE) |
+	      BIT(FLOW_DISSECTOR_KEY_VLAN))) {
+		NL_SET_ERR_MSG_MOD(f->common.extack, "Unsupported key");
+		return -EOPNOTSUPP;
+	}
+
+	if (f->classid) {
+		/* The classid values of TC_H_MIN_PRIORITY through
+		 * TC_H_MIN_PRIORITY + PRESTERA_DEFAULT_TC_NUM - 1 represents
+		 * the hardware traffic classes.
+		 */
+		hwtc = TC_H_MIN(f->classid) - TC_H_MIN_PRIORITY;
+		if (hwtc >= PRESTERA_DEFAULT_TC_NUM) {
+			NL_SET_ERR_MSG_MOD(f->common.extack,
+					   "Unsupported HW TC");
+			return -EINVAL;
+		}
+		prestera_acl_rule_hw_tc_set(rule, hwtc);
+	}
+
+	prestera_acl_rule_priority_set(rule, f->common.prio);
+
+	if (flow_rule_match_key(f_rule, FLOW_DISSECTOR_KEY_META)) {
+		err = mvsw_pr_flower_parse_meta(rule, f, block);
+		if (err)
+			return err;
+	}
+
+	if (flow_rule_match_key(f_rule, FLOW_DISSECTOR_KEY_CONTROL)) {
+		struct flow_match_control match;
+
+		flow_rule_match_control(f_rule, &match);
+		addr_type = match.key->addr_type;
+	}
+
+	if (flow_rule_match_key(f_rule, FLOW_DISSECTOR_KEY_BASIC)) {
+		struct flow_match_basic match;
+
+		flow_rule_match_basic(f_rule, &match);
+		n_proto_key = ntohs(match.key->n_proto);
+		n_proto_mask = ntohs(match.mask->n_proto);
+
+		if (n_proto_key == ETH_P_ALL) {
+			n_proto_key = 0;
+			n_proto_mask = 0;
+		}
+
+		/* add eth type key,mask */
+		m_entry = kmalloc(sizeof(*m_entry), GFP_KERNEL);
+		if (!m_entry)
+			return -ENOMEM;
+
+		m_entry->type = MVSW_ACL_RULE_MATCH_ENTRY_TYPE_ETH_TYPE;
+		m_entry->keymask.u16.key = n_proto_key;
+		m_entry->keymask.u16.mask = n_proto_mask;
+		prestera_acl_rule_match_add(rule, m_entry);
+
+		/* add ip proto key,mask */
+		m_entry = kmalloc(sizeof(*m_entry), GFP_KERNEL);
+		if (!m_entry)
+			return -ENOMEM;
+
+		m_entry->type = MVSW_ACL_RULE_MATCH_ENTRY_TYPE_IP_PROTO;
+		m_entry->keymask.u8.key = match.key->ip_proto;
+		m_entry->keymask.u8.mask = match.mask->ip_proto;
+		prestera_acl_rule_match_add(rule, m_entry);
+		ip_proto = match.key->ip_proto;
+	}
+
+	if (flow_rule_match_key(f_rule, FLOW_DISSECTOR_KEY_ETH_ADDRS)) {
+		struct flow_match_eth_addrs match;
+
+		flow_rule_match_eth_addrs(f_rule, &match);
+
+		/* add ethernet dst key,mask */
+		m_entry = kmalloc(sizeof(*m_entry), GFP_KERNEL);
+		if (!m_entry)
+			return -ENOMEM;
+
+		m_entry->type = MVSW_ACL_RULE_MATCH_ENTRY_TYPE_ETH_DMAC;
+		memcpy(&m_entry->keymask.mac.key,
+		       &match.key->dst, sizeof(match.key->dst));
+		memcpy(&m_entry->keymask.mac.mask,
+		       &match.mask->dst, sizeof(match.mask->dst));
+		prestera_acl_rule_match_add(rule, m_entry);
+
+		/* add ethernet src key,mask */
+		m_entry = kmalloc(sizeof(*m_entry), GFP_KERNEL);
+		if (!m_entry)
+			return -ENOMEM;
+
+		m_entry->type = MVSW_ACL_RULE_MATCH_ENTRY_TYPE_ETH_SMAC;
+		memcpy(&m_entry->keymask.mac.key,
+		       &match.key->src, sizeof(match.key->src));
+		memcpy(&m_entry->keymask.mac.mask,
+		       &match.mask->src, sizeof(match.mask->src));
+		prestera_acl_rule_match_add(rule, m_entry);
+	}
+
+	if (addr_type == FLOW_DISSECTOR_KEY_IPV4_ADDRS) {
+		struct flow_match_ipv4_addrs match;
+
+		flow_rule_match_ipv4_addrs(f_rule, &match);
+
+		m_entry = kmalloc(sizeof(*m_entry), GFP_KERNEL);
+		if (!m_entry)
+			return -ENOMEM;
+
+		m_entry->type = MVSW_ACL_RULE_MATCH_ENTRY_TYPE_IP_SRC;
+		memcpy(&m_entry->keymask.u32.key,
+		       &match.key->src, sizeof(match.key->src));
+		memcpy(&m_entry->keymask.u32.mask,
+		       &match.mask->src, sizeof(match.mask->src));
+		prestera_acl_rule_match_add(rule, m_entry);
+
+		m_entry = kmalloc(sizeof(*m_entry), GFP_KERNEL);
+		if (!m_entry)
+			return -ENOMEM;
+
+		m_entry->type = MVSW_ACL_RULE_MATCH_ENTRY_TYPE_IP_DST;
+		memcpy(&m_entry->keymask.u32.key,
+		       &match.key->dst, sizeof(match.key->dst));
+		memcpy(&m_entry->keymask.u32.mask,
+		       &match.mask->dst, sizeof(match.mask->dst));
+		prestera_acl_rule_match_add(rule, m_entry);
+	}
+
+	if (flow_rule_match_key(f_rule, FLOW_DISSECTOR_KEY_PORTS)) {
+		struct flow_match_ports match;
+
+		if (ip_proto != IPPROTO_TCP && ip_proto != IPPROTO_UDP) {
+			NL_SET_ERR_MSG_MOD
+			    (f->common.extack,
+			     "Only UDP and TCP keys are supported");
+			return -EINVAL;
+		}
+
+		flow_rule_match_ports(f_rule, &match);
+
+		m_entry = kmalloc(sizeof(*m_entry), GFP_KERNEL);
+		if (!m_entry)
+			return -ENOMEM;
+		m_entry->type = MVSW_ACL_RULE_MATCH_ENTRY_TYPE_L4_PORT_SRC;
+		m_entry->keymask.u16.key = ntohs(match.key->src);
+		m_entry->keymask.u16.mask = ntohs(match.mask->src);
+		prestera_acl_rule_match_add(rule, m_entry);
+
+		m_entry = kmalloc(sizeof(*m_entry), GFP_KERNEL);
+		if (!m_entry)
+			return -ENOMEM;
+		m_entry->type = MVSW_ACL_RULE_MATCH_ENTRY_TYPE_L4_PORT_DST;
+		m_entry->keymask.u16.key = ntohs(match.key->dst);
+		m_entry->keymask.u16.mask = ntohs(match.mask->dst);
+		prestera_acl_rule_match_add(rule, m_entry);
+	}
+
+	if (flow_rule_match_key(f_rule, FLOW_DISSECTOR_KEY_PORTS_RANGE)) {
+		struct flow_match_ports_range match;
+
+		flow_rule_match_ports_range(f_rule, &match);
+
+		m_entry = kmalloc(sizeof(*m_entry), GFP_KERNEL);
+		if (!m_entry)
+			return -ENOMEM;
+		m_entry->type =
+			MVSW_ACL_RULE_MATCH_ENTRY_TYPE_L4_PORT_RANGE_SRC;
+		m_entry->keymask.u32.key = ntohs(match.key->tp_min.src) |
+				(u32)ntohs(match.key->tp_max.src) << 16;
+		m_entry->keymask.u32.mask = ntohs(match.mask->tp_min.src) |
+				(u32)ntohs(match.mask->tp_max.src) << 16;
+		prestera_acl_rule_match_add(rule, m_entry);
+
+		m_entry = kmalloc(sizeof(*m_entry), GFP_KERNEL);
+		if (!m_entry)
+			return -ENOMEM;
+		m_entry->type =
+			MVSW_ACL_RULE_MATCH_ENTRY_TYPE_L4_PORT_RANGE_DST;
+		m_entry->keymask.u32.key = ntohs(match.key->tp_min.dst) |
+				(u32)ntohs(match.key->tp_max.dst) << 16;
+		m_entry->keymask.u32.mask = ntohs(match.mask->tp_min.dst) |
+				(u32)ntohs(match.mask->tp_max.dst) << 16;
+		prestera_acl_rule_match_add(rule, m_entry);
+	}
+
+	if (flow_rule_match_key(f_rule, FLOW_DISSECTOR_KEY_VLAN)) {
+		struct flow_match_vlan match;
+
+		flow_rule_match_vlan(f_rule, &match);
+
+		if (match.mask->vlan_id != 0) {
+			m_entry = kmalloc(sizeof(*m_entry), GFP_KERNEL);
+			if (!m_entry)
+				return -ENOMEM;
+			m_entry->type = MVSW_ACL_RULE_MATCH_ENTRY_TYPE_VLAN_ID;
+			m_entry->keymask.u16.key = match.key->vlan_id;
+			m_entry->keymask.u16.mask = match.mask->vlan_id;
+			prestera_acl_rule_match_add(rule, m_entry);
+		}
+
+		m_entry = kmalloc(sizeof(*m_entry), GFP_KERNEL);
+		if (!m_entry)
+			return -ENOMEM;
+		m_entry->type = MVSW_ACL_RULE_MATCH_ENTRY_TYPE_VLAN_TPID;
+		m_entry->keymask.u16.key = ntohs(match.key->vlan_tpid);
+		m_entry->keymask.u16.mask = ntohs(match.mask->vlan_tpid);
+		prestera_acl_rule_match_add(rule, m_entry);
+	}
+
+	if (flow_rule_match_key(f_rule, FLOW_DISSECTOR_KEY_ICMP)) {
+		struct flow_match_icmp match;
+
+		flow_rule_match_icmp(f_rule, &match);
+
+		m_entry = kmalloc(sizeof(*m_entry), GFP_KERNEL);
+		if (!m_entry)
+			return -ENOMEM;
+		m_entry->type = MVSW_ACL_RULE_MATCH_ENTRY_TYPE_ICMP_TYPE;
+		m_entry->keymask.u8.key = match.key->type;
+		m_entry->keymask.u8.mask = match.mask->type;
+		prestera_acl_rule_match_add(rule, m_entry);
+
+		m_entry = kmalloc(sizeof(*m_entry), GFP_KERNEL);
+		if (!m_entry)
+			return -ENOMEM;
+		m_entry->type = MVSW_ACL_RULE_MATCH_ENTRY_TYPE_ICMP_CODE;
+		m_entry->keymask.u8.key = match.key->code;
+		m_entry->keymask.u8.mask = match.mask->code;
+		prestera_acl_rule_match_add(rule, m_entry);
+	}
+
+	return mvsw_pr_flower_parse_actions(sw, block, rule,
+					    &f->rule->action,
+					    f->common.extack);
+}
+
+int mvsw_pr_flower_replace(struct mvsw_pr_switch *sw,
+			   struct prestera_acl_block *block,
+			   struct flow_cls_offload *f)
+{
+	struct prestera_acl_rule *rule;
+	int err;
+
+	rule = prestera_acl_rule_create(block, f->cookie);
+	if (IS_ERR(rule))
+		return PTR_ERR(rule);
+
+	err = mvsw_pr_flower_parse(sw, block, rule, f);
+	if (err)
+		goto err_flower_parse;
+
+	err = prestera_acl_rule_add(sw, rule);
+	if (err)
+		goto err_rule_add;
+
+	return 0;
+
+err_rule_add:
+err_flower_parse:
+	prestera_acl_rule_destroy(rule);
+	return err;
+}
+
+void mvsw_pr_flower_destroy(struct mvsw_pr_switch *sw,
+			    struct prestera_acl_block *block,
+			    struct flow_cls_offload *f)
+{
+	struct prestera_acl_rule *rule;
+
+	rule = prestera_acl_rule_lookup(prestera_acl_block_ruleset_get(block),
+					f->cookie);
+	if (rule) {
+		prestera_acl_rule_del(sw, rule);
+		prestera_acl_rule_destroy(rule);
+	}
+}
+
+int mvsw_pr_flower_stats(struct mvsw_pr_switch *sw,
+			 struct prestera_acl_block *block,
+			 struct flow_cls_offload *f)
+{
+	struct prestera_acl_rule *rule;
+	u64 packets;
+	u64 lastuse;
+	u64 bytes;
+	int err;
+
+	rule = prestera_acl_rule_lookup(prestera_acl_block_ruleset_get(block),
+					f->cookie);
+	if (!rule)
+		return -EINVAL;
+
+	err = prestera_acl_rule_get_stats(sw, rule, &packets, &bytes, &lastuse);
+	if (err)
+		return err;
+
+	flow_stats_update(&f->stats, bytes, packets, lastuse);
+	return 0;
+}
diff --git a/drivers/net/ethernet/marvell/prestera_sw/prestera_fw_log.c b/drivers/net/ethernet/marvell/prestera_sw/prestera_fw_log.c
new file mode 100644
index 000000000..e1c9f6ae3
--- /dev/null
+++ b/drivers/net/ethernet/marvell/prestera_sw/prestera_fw_log.c
@@ -0,0 +1,422 @@
+/* SPDX-License-Identifier: BSD-3-Clause OR GPL-2.0
+ *
+ * Copyright (c) 2019-2020 Marvell International Ltd. All rights reserved.
+ *
+ */
+#include <linux/sysfs.h>
+#include <linux/fs.h>
+#include <linux/etherdevice.h>
+#include <linux/string.h>
+#include <linux/ctype.h>
+#include <linux/debugfs.h>
+
+#include "prestera.h"
+#include "prestera_hw.h"
+#include "prestera_log.h"
+#include "prestera_fw_log.h"
+
+#define FW_LOG_DBGFS_CFG_DIR	"mvsw_pr_fw_log"
+#define FW_LOG_DBGFS_CFG_NAME	"cfg"
+#define FW_LOG_DBGFS_MAX_STR_LEN	64
+#define FW_LOG_PR_LOG_PREFIX	"[mvsw_pr_fw_log]"
+#define FW_LOG_PR_LIB_SIZE	32
+#define FW_LOG_PR_READ_BUF_SIZE	8192
+#define MVSW_FW_LOG_INFO(fmt, ...)	\
+	pr_info(fmt, ##__VA_ARGS__)
+
+#define FW_LOG_READ_TABLE_FMT	"%-23s"
+
+#define mvsw_dev(sw)		((sw)->dev->dev)
+
+static void mvsw_pr_fw_log_evt_handler(struct mvsw_pr_switch *,
+				       struct mvsw_pr_event *,
+				       void *);
+static ssize_t mvsw_pr_fw_log_debugfs_read(struct file *file,
+					   char __user *ubuf,
+					   size_t count, loff_t *ppos);
+static ssize_t mvsw_pr_fw_log_debugfs_write(struct file *file,
+					    const char __user *ubuf,
+					    size_t count, loff_t *ppos);
+static inline int mvsw_pr_fw_log_get_type_from_str(const char *str);
+static inline int mvsw_pr_fw_log_get_lib_from_str(const char *str);
+
+static int mvsw_pr_fw_log_event_handler_register(struct mvsw_pr_switch *sw);
+static void mvsw_pr_fw_log_event_handler_unregister(struct mvsw_pr_switch *sw);
+
+struct mvsw_pr_fw_log_prv_debugfs {
+	struct dentry *cfg_dir;
+	struct dentry *cfg;
+	const struct file_operations cfg_fops;
+	char *read_buf;
+};
+
+static u8 fw_log_lib_type_config[MVSW_FW_LOG_LIB_MAX] = { 0 };
+
+static struct mvsw_pr_fw_log_prv_debugfs fw_log_debugfs_handle = {
+	.cfg_dir = NULL,
+	.cfg_fops = {
+		.read = mvsw_pr_fw_log_debugfs_read,
+		.write = mvsw_pr_fw_log_debugfs_write,
+		.open = simple_open,
+		.llseek = default_llseek,
+	}
+};
+
+static const char *mvsw_pr_fw_log_lib_id2name[MVSW_FW_LOG_LIB_MAX] = {
+	[MVSW_FW_LOG_LIB_ALL] =  "all",
+	[MVSW_FW_LOG_LIB_BRIDGE] =  "bridge",
+	[MVSW_FW_LOG_LIB_CNC] =  "cnc",
+	[MVSW_FW_LOG_LIB_CONFIG] =  "config",
+	[MVSW_FW_LOG_LIB_COS] =  "cos",
+	[MVSW_FW_LOG_LIB_CSCD] =  "cscd",
+	[MVSW_FW_LOG_LIB_CUT_THROUGH] =  "cut-through",
+	[MVSW_FW_LOG_LIB_DIAG] =  "diag",
+	[MVSW_FW_LOG_LIB_DRAGONITE] =  "dragonite",
+	[MVSW_FW_LOG_LIB_EGRESS] =  "egress",
+	[MVSW_FW_LOG_LIB_EXACT_MATCH] =  "exact-match",
+	[MVSW_FW_LOG_LIB_FABRIC] =  "fabric",
+	[MVSW_FW_LOG_LIB_BRIDGE_FDB_MANAGER] =  "fdb-manager",
+	[MVSW_FW_LOG_LIB_FLOW_MANAGER] =  "flow-manager",
+	[MVSW_FW_LOG_LIB_HW_INIT] =  "hw-init",
+	[MVSW_FW_LOG_LIB_I2C] =  "i2c",
+	[MVSW_FW_LOG_LIB_INGRESS] =  "ingress",
+	[MVSW_FW_LOG_LIB_INIT] =  "init",
+	[MVSW_FW_LOG_LIB_IPFIX] =  "ipfix",
+	[MVSW_FW_LOG_LIB_IP] =  "ip",
+	[MVSW_FW_LOG_LIB_IP_LPM] =  "ip-lpm",
+	[MVSW_FW_LOG_LIB_L2_MLL] =  "l2-mll",
+	[MVSW_FW_LOG_LIB_LATENCY_MONITORING] =  "latency-monitoring",
+	[MVSW_FW_LOG_LIB_LOGICAL_TARGET] =  "logical-target",
+	[MVSW_FW_LOG_LIB_LPM] =  "lpm",
+	[MVSW_FW_LOG_LIB_MIRROR] =  "mirror",
+	[MVSW_FW_LOG_LIB_MULTI_PORT_GROUP] =  "multi-port-group",
+	[MVSW_FW_LOG_LIB_NETWORK_IF] =  "network-if",
+	[MVSW_FW_LOG_LIB_NST] =  "nst",
+	[MVSW_FW_LOG_LIB_OAM] =  "oam",
+	[MVSW_FW_LOG_LIB_PACKET_ANALYZER] =  "packet-analyzer",
+	[MVSW_FW_LOG_LIB_PCL] =  "pcl",
+	[MVSW_FW_LOG_LIB_PHA] =  "pha",
+	[MVSW_FW_LOG_LIB_PHY] =  "phy",
+	[MVSW_FW_LOG_LIB_POLICER] =  "policer",
+	[MVSW_FW_LOG_LIB_PROTECTION] =  "protection",
+	[MVSW_FW_LOG_LIB_PTP] =  "ptp",
+	[MVSW_FW_LOG_LIB_RESOURCE_MANAGER] =  "resource-manager",
+	[MVSW_FW_LOG_LIB_SMI] =  "smi",
+	[MVSW_FW_LOG_LIB_SYSTEM_RECOVERY] =  "system-recovery",
+	[MVSW_FW_LOG_LIB_TAM] =  "tam",
+	[MVSW_FW_LOG_LIB_TCAM] =  "tcam",
+	[MVSW_FW_LOG_LIB_TM] =  "tm",
+	[MVSW_FW_LOG_LIB_TM_GLUE] =  "tm-glue",
+	[MVSW_FW_LOG_LIB_TRUNK] =  "trunk",
+	[MVSW_FW_LOG_LIB_TTI] =  "tti",
+	[MVSW_FW_LOG_LIB_TUNNEL] =  "tunnel",
+	[MVSW_FW_LOG_LIB_VERSION] =  "version",
+	[MVSW_FW_LOG_LIB_VIRTUAL_TCAM] =  "virtual-tcam",
+	[MVSW_FW_LOG_LIB_VNT] =  "vnt",
+	[MVSW_FW_LOG_LIB_PPU] = "ppu",
+	[MVSW_FW_LOG_LIB_EXACT_MATCH_MANAGER] = "exact-match-manager",
+	[MVSW_FW_LOG_LIB_MAC_SEC] = "mac-sec",
+};
+
+static const char *mvsw_pr_fw_log_prv_type_id2name[MVSW_FW_LOG_TYPE_MAX] = {
+	[MVSW_FW_LOG_TYPE_INFO] = "info",
+	[MVSW_FW_LOG_TYPE_ENTRY_LEVEL_FUNCTION] = "entry-level-function",
+	[MVSW_FW_LOG_TYPE_ERROR] = "error",
+	[MVSW_FW_LOG_TYPE_ALL] = "all",
+	[MVSW_FW_LOG_TYPE_NONE]  = "none",
+};
+
+static void mvsw_pr_fw_log_evt_handler(struct mvsw_pr_switch *sw,
+				       struct mvsw_pr_event *evt, void *arg)
+{
+	u32 log_len = evt->fw_log_evt.log_len;
+	u8 *buf = evt->fw_log_evt.data;
+
+	buf[log_len] = '\0';
+
+	MVSW_FW_LOG_INFO(FW_LOG_PR_LOG_PREFIX "%s\n", buf);
+}
+
+static ssize_t mvsw_pr_fw_log_format_str(void)
+{
+	char *buf = fw_log_debugfs_handle.read_buf;
+	int chars_written = 0;
+	int lib, type;
+	int ret;
+
+	memset(buf, 0, FW_LOG_PR_READ_BUF_SIZE);
+
+	ret = snprintf(buf, FW_LOG_PR_READ_BUF_SIZE, FW_LOG_READ_TABLE_FMT,
+		       " ");
+	if (ret < 0)
+		return ret;
+
+	chars_written += ret;
+
+	for (type = 0; type < MVSW_FW_LOG_TYPE_MAX; ++type) {
+		if (type == MVSW_FW_LOG_TYPE_NONE ||
+		    type == MVSW_FW_LOG_TYPE_ALL)
+			continue;
+
+		ret = snprintf(buf + chars_written,
+			       FW_LOG_PR_READ_BUF_SIZE - chars_written,
+			       FW_LOG_READ_TABLE_FMT,
+			       mvsw_pr_fw_log_prv_type_id2name[type]);
+		if (ret < 0)
+			return ret;
+
+		chars_written += ret;
+	}
+
+	strcat(buf, "\n");
+	++chars_written;
+
+	for (lib = 0; lib < MVSW_FW_LOG_LIB_MAX; ++lib) {
+		if (lib == MVSW_FW_LOG_LIB_ALL ||
+		    !mvsw_pr_fw_log_lib_id2name[lib])
+			continue;
+
+		ret = snprintf(buf + chars_written,
+			       FW_LOG_PR_READ_BUF_SIZE - chars_written,
+			       FW_LOG_READ_TABLE_FMT,
+			       mvsw_pr_fw_log_lib_id2name[lib]);
+		if (ret < 0)
+			return ret;
+
+		chars_written += ret;
+
+		for (type = 0; type < MVSW_FW_LOG_TYPE_MAX; ++type) {
+			if (type == MVSW_FW_LOG_TYPE_NONE ||
+			    type == MVSW_FW_LOG_TYPE_ALL)
+				continue;
+
+			ret = snprintf(buf + chars_written,
+				       FW_LOG_PR_READ_BUF_SIZE - chars_written,
+				       FW_LOG_READ_TABLE_FMT,
+				       fw_log_lib_type_config[lib] & BIT(type)
+						? "+" : "-");
+			if (ret < 0)
+				return ret;
+
+			chars_written += ret;
+		}
+		strlcat(buf, "\n", FW_LOG_PR_READ_BUF_SIZE);
+		++chars_written;
+	}
+
+	return chars_written;
+}
+
+static ssize_t mvsw_pr_fw_log_debugfs_read(struct file *file,
+					   char __user *ubuf,
+					   size_t count, loff_t *ppos)
+{
+	char *buf = fw_log_debugfs_handle.read_buf;
+
+	return simple_read_from_buffer(ubuf, count, ppos, buf,
+				       FW_LOG_PR_READ_BUF_SIZE);
+}
+
+static int mvsw_pr_fw_log_parse_usr_input(int *name, int *type,
+					  const char __user *ubuf, size_t count)
+{
+	u8 tmp_buf[FW_LOG_DBGFS_MAX_STR_LEN] = { 0 };
+	u8 lib_str[FW_LOG_PR_LIB_SIZE] = { 0 };
+	u8 type_str[FW_LOG_PR_LIB_SIZE] = { 0 };
+	ssize_t len_to_copy = count - 1;
+	u8 *ppos_lib, *ppos_type;
+	char *end = tmp_buf;
+	int err;
+
+	if (len_to_copy > FW_LOG_DBGFS_MAX_STR_LEN) {
+		MVSW_LOG_ERROR("Len is > than max(%zu vs max possible %d)\n",
+			       count, FW_LOG_DBGFS_MAX_STR_LEN);
+		return -EMSGSIZE;
+	}
+
+	err = copy_from_user(tmp_buf, ubuf, len_to_copy);
+	if (err)
+		return -EINVAL;
+
+	ppos_lib  = strsep(&end, " \t");
+	ppos_type = strsep(&end, " \t\0");
+
+	if (!ppos_lib || !ppos_type)
+		return -EINVAL;
+
+	strcpy(lib_str, ppos_lib);
+
+	strcpy(type_str, ppos_type);
+
+	if (iscntrl(lib_str[0]) || isspace(lib_str[0]) || lib_str[0] == '\0' ||
+	    iscntrl(type_str[0]) || isspace(type_str[0]) ||
+	    type_str[0] == '\0') {
+		return -EINVAL;
+	}
+
+	*name = mvsw_pr_fw_log_get_lib_from_str(lib_str);
+	*type = mvsw_pr_fw_log_get_type_from_str(type_str);
+
+	if (*name >= MVSW_FW_LOG_LIB_MAX ||
+	    *type >= MVSW_FW_LOG_TYPE_MAX ||
+	    (*name != MVSW_FW_LOG_LIB_ALL && *type == MVSW_FW_LOG_TYPE_NONE))
+		return -EINVAL;
+
+	return 0;
+}
+
+static ssize_t mvsw_pr_fw_log_debugfs_write(struct file *file,
+					    const char __user *ubuf,
+					    size_t count, loff_t *ppos)
+{
+	struct mvsw_pr_switch *sw = file->private_data;
+	int lib, type;
+	int i, j;
+	int err;
+
+	err = mvsw_pr_fw_log_parse_usr_input(&lib, &type, ubuf, count);
+	if (err)
+		goto error;
+
+	err = mvsw_pr_hw_fw_log_level_set(sw, lib, type);
+	if (err) {
+		dev_err(mvsw_dev(sw), "Failed to send request to firmware\n");
+		return err;
+	}
+
+	/* specific lib and specific type */
+	if (lib != MVSW_FW_LOG_LIB_ALL && type != MVSW_FW_LOG_TYPE_ALL) {
+		/* special type 'NONE' to disable feature */
+		if (type == MVSW_FW_LOG_TYPE_NONE)
+			memset(fw_log_lib_type_config, 0,
+			       sizeof(fw_log_lib_type_config));
+		/* Actual type should be switched */
+		else
+			fw_log_lib_type_config[lib] ^= (1 << type);
+	/* specific lib but all types */
+	} else if (lib != MVSW_FW_LOG_LIB_ALL && type == MVSW_FW_LOG_TYPE_ALL) {
+		for (j = 0; j < MVSW_FW_LOG_TYPE_ALL; ++j)
+			fw_log_lib_type_config[lib] ^= (1 << j);
+	/* specific type but all libs */
+	} else if (lib == MVSW_FW_LOG_LIB_ALL && type != MVSW_FW_LOG_TYPE_ALL) {
+		for (i = 0; i < MVSW_FW_LOG_LIB_ALL; ++i)
+			fw_log_lib_type_config[i] |= (1 << type);
+	/* all libs and all types */
+	} else {
+		for (i = 0; i < MVSW_FW_LOG_LIB_ALL; ++i) {
+			for (j = 0; j < MVSW_FW_LOG_TYPE_ALL; ++j)
+				fw_log_lib_type_config[i] |= (1 << j);
+		}
+	}
+
+	err = mvsw_pr_fw_log_format_str();
+	if (err <= 0) {
+		dev_err(mvsw_dev(sw), "Failed to form output string\n");
+		return err;
+	}
+
+	return count;
+
+error:
+	dev_warn(mvsw_dev(sw),
+		 "Invalid str received, make sure request is valid\n");
+	dev_warn(mvsw_dev(sw),
+		 "Valid fmt consists of: \"lib type\" string, e.g:\n");
+	dev_warn(mvsw_dev(sw),
+		 "\"phy error\" for 'phy' lib 'error' logs enabled\n");
+
+	return err;
+}
+
+static inline int mvsw_pr_fw_log_get_type_from_str(const char *str)
+{
+	int i;
+
+	for (i = 0; i < MVSW_FW_LOG_TYPE_MAX; ++i) {
+		if (!mvsw_pr_fw_log_prv_type_id2name[i])
+			continue;
+
+		if (strcmp(mvsw_pr_fw_log_prv_type_id2name[i], str) == 0)
+			return i;
+	}
+
+	return MVSW_FW_LOG_TYPE_MAX;
+}
+
+static inline int mvsw_pr_fw_log_get_lib_from_str(const char *str)
+{
+	int i;
+
+	for (i = 0; i < MVSW_FW_LOG_LIB_MAX; ++i) {
+		if (!mvsw_pr_fw_log_lib_id2name[i])
+			continue;
+
+		if (strcmp(mvsw_pr_fw_log_lib_id2name[i], str) == 0)
+			return i;
+	}
+
+	return MVSW_FW_LOG_LIB_MAX;
+}
+
+static int mvsw_pr_fw_log_event_handler_register(struct mvsw_pr_switch *sw)
+{
+	return mvsw_pr_hw_event_handler_register(sw, MVSW_EVENT_TYPE_FW_LOG,
+						 mvsw_pr_fw_log_evt_handler,
+						 NULL);
+}
+
+static void mvsw_pr_fw_log_event_handler_unregister(struct mvsw_pr_switch *sw)
+{
+	mvsw_pr_hw_event_handler_unregister(sw, MVSW_EVENT_TYPE_FW_LOG);
+}
+
+int mvsw_pr_fw_log_init(struct mvsw_pr_switch *sw)
+{
+	fw_log_debugfs_handle.cfg_dir =
+		debugfs_create_dir(FW_LOG_DBGFS_CFG_DIR, NULL);
+
+	if (!fw_log_debugfs_handle.cfg_dir) {
+		MVSW_LOG_ERROR("Failed to create debugfs dir entry");
+		return -1;
+	}
+
+	fw_log_debugfs_handle.cfg =
+		debugfs_create_file(FW_LOG_DBGFS_CFG_NAME, 0644,
+				    fw_log_debugfs_handle.cfg_dir, sw,
+				    &fw_log_debugfs_handle.cfg_fops);
+
+	if (!fw_log_debugfs_handle.cfg) {
+		MVSW_LOG_ERROR("Failed to create debugfs dir entry");
+		debugfs_remove(fw_log_debugfs_handle.cfg_dir);
+		return -1;
+	}
+
+	if (mvsw_pr_fw_log_event_handler_register(sw))
+		goto error;
+
+	fw_log_debugfs_handle.read_buf =
+		kzalloc(FW_LOG_PR_READ_BUF_SIZE, GFP_KERNEL);
+
+	if (!fw_log_debugfs_handle.read_buf)
+		goto error;
+
+	mvsw_pr_hw_fw_log_level_set(sw, MVSW_FW_LOG_LIB_ALL,
+				    MVSW_FW_LOG_TYPE_NONE);
+	mvsw_pr_fw_log_format_str();
+
+	return 0;
+error:
+	debugfs_remove(fw_log_debugfs_handle.cfg);
+	debugfs_remove(fw_log_debugfs_handle.cfg_dir);
+	return -1;
+}
+
+void mvsw_pr_fw_log_fini(struct mvsw_pr_switch *sw)
+{
+	mvsw_pr_fw_log_event_handler_unregister(sw);
+
+	kfree(fw_log_debugfs_handle.read_buf);
+
+	debugfs_remove(fw_log_debugfs_handle.cfg);
+	debugfs_remove(fw_log_debugfs_handle.cfg_dir);
+}
diff --git a/drivers/net/ethernet/marvell/prestera_sw/prestera_fw_log.h b/drivers/net/ethernet/marvell/prestera_sw/prestera_fw_log.h
new file mode 100644
index 000000000..ccd551439
--- /dev/null
+++ b/drivers/net/ethernet/marvell/prestera_sw/prestera_fw_log.h
@@ -0,0 +1,15 @@
+/* SPDX-License-Identifier: BSD-3-Clause OR GPL-2.0
+ *
+ * Copyright (c) 2019-2020 Marvell International Ltd. All rights reserved.
+ *
+ */
+
+#ifndef _MVSW_PRESTERA_FW_LOG_H_
+#define _MVSW_PRESTERA_FW_LOG_H_
+
+#include "prestera.h"
+
+int  mvsw_pr_fw_log_init(struct mvsw_pr_switch *sw);
+void mvsw_pr_fw_log_fini(struct mvsw_pr_switch *sw);
+
+#endif /* _MVSW_PRESTERA_FW_LOG_H_ */
diff --git a/drivers/net/ethernet/marvell/prestera_sw/prestera_hw.c b/drivers/net/ethernet/marvell/prestera_sw/prestera_hw.c
new file mode 100644
index 000000000..f81be6b99
--- /dev/null
+++ b/drivers/net/ethernet/marvell/prestera_sw/prestera_hw.c
@@ -0,0 +1,2246 @@
+/* SPDX-License-Identifier: BSD-3-Clause OR GPL-2.0
+ *
+ * Copyright (c) 2019-2020 Marvell International Ltd. All rights reserved.
+ *
+ */
+#include <linux/etherdevice.h>
+#include <linux/ethtool.h>
+#include <linux/netdevice.h>
+#include <linux/list.h>
+
+#include "prestera_hw.h"
+#include "prestera.h"
+#include "prestera_log.h"
+#include "prestera_fw_log.h"
+#include "prestera_rxtx.h"
+
+#define MVSW_PR_INIT_TIMEOUT 30000000	/* 30sec */
+#define MVSW_PR_MIN_MTU 64
+#define MVSW_PR_MSG_BUFF_CHUNK_SIZE	32	/* bytes */
+
+enum mvsw_msg_type {
+	MVSW_MSG_TYPE_SWITCH_INIT = 0x1,
+	MVSW_MSG_TYPE_SWITCH_ATTR_SET = 0x2,
+
+	MVSW_MSG_TYPE_PORT_ATTR_SET = 0x100,
+	MVSW_MSG_TYPE_PORT_ATTR_GET = 0x101,
+	MVSW_MSG_TYPE_PORT_INFO_GET = 0x110,
+
+	MVSW_MSG_TYPE_VLAN_CREATE = 0x200,
+	MVSW_MSG_TYPE_VLAN_DELETE = 0x201,
+	MVSW_MSG_TYPE_VLAN_PORT_SET = 0x202,
+	MVSW_MSG_TYPE_VLAN_PVID_SET = 0x203,
+
+	MVSW_MSG_TYPE_FDB_ADD = 0x300,
+	MVSW_MSG_TYPE_FDB_DELETE = 0x301,
+	MVSW_MSG_TYPE_FDB_FLUSH_PORT = 0x310,
+	MVSW_MSG_TYPE_FDB_FLUSH_VLAN = 0x311,
+	MVSW_MSG_TYPE_FDB_FLUSH_PORT_VLAN = 0x312,
+	MVSW_MSG_TYPE_FDB_MACVLAN_ADD = 0x320,
+	MVSW_MSG_TYPE_FDB_MACVLAN_DEL = 0x321,
+
+	MVSW_MSG_TYPE_LOG_LEVEL_SET,
+
+	MVSW_MSG_TYPE_BRIDGE_CREATE = 0x400,
+	MVSW_MSG_TYPE_BRIDGE_DELETE = 0x401,
+	MVSW_MSG_TYPE_BRIDGE_PORT_ADD = 0x402,
+	MVSW_MSG_TYPE_BRIDGE_PORT_DELETE = 0x403,
+
+	MVSW_MSG_TYPE_ACL_RULE_ADD = 0x500,
+	MVSW_MSG_TYPE_ACL_RULE_DELETE = 0x501,
+	MVSW_MSG_TYPE_ACL_RULE_STATS_GET = 0x510,
+	MVSW_MSG_TYPE_ACL_RULESET_CREATE = 0x520,
+	MVSW_MSG_TYPE_ACL_RULESET_DELETE = 0x521,
+	MVSW_MSG_TYPE_ACL_PORT_BIND = 0x530,
+	MVSW_MSG_TYPE_ACL_PORT_UNBIND = 0x531,
+
+	MVSW_MSG_TYPE_ROUTER_RIF_CREATE = 0x600,
+	MVSW_MSG_TYPE_ROUTER_RIF_DELETE = 0x601,
+	MVSW_MSG_TYPE_ROUTER_RIF_SET = 0x602,
+	MVSW_MSG_TYPE_ROUTER_LPM_ADD = 0x610,
+	MVSW_MSG_TYPE_ROUTER_LPM_DELETE = 0x611,
+	MVSW_MSG_TYPE_ROUTER_NH_GRP_SET = 0x622,
+	MVSW_MSG_TYPE_ROUTER_NH_GRP_GET = 0x644,
+	MVSW_MSG_TYPE_ROUTER_NH_GRP_ADD = 0x623,
+	MVSW_MSG_TYPE_ROUTER_NH_GRP_DELETE = 0x624,
+	MVSW_MSG_TYPE_ROUTER_VR_CREATE = 0x630,
+	MVSW_MSG_TYPE_ROUTER_VR_DELETE = 0x631,
+	MVSW_MSG_TYPE_ROUTER_VR_ABORT = 0x632,
+	MVSW_MSG_TYPE_ROUTER_MP_HASH_SET = 0x650,
+
+	MVSW_MSG_TYPE_RXTX_INIT = 0x800,
+
+	MVSW_MSG_TYPE_LAG_ADD = 0x900,
+	MVSW_MSG_TYPE_LAG_DELETE = 0x901,
+	MVSW_MSG_TYPE_LAG_ENABLE = 0x902,
+	MVSW_MSG_TYPE_LAG_DISABLE = 0x903,
+	MVSW_MSG_TYPE_LAG_ROUTER_LEAVE = 0x904,
+
+	MVSW_MSG_TYPE_STP_PORT_SET = 0x1000,
+
+	MVSW_MSG_TYPE_ACK = 0x10000,
+	MVSW_MSG_TYPE_MAX
+};
+
+enum mvsw_msg_port_attr {
+	MVSW_MSG_PORT_ATTR_ADMIN_STATE = 1,
+	MVSW_MSG_PORT_ATTR_OPER_STATE = 2,
+	MVSW_MSG_PORT_ATTR_MTU = 3,
+	MVSW_MSG_PORT_ATTR_MAC = 4,
+	MVSW_MSG_PORT_ATTR_SPEED = 5,
+	MVSW_MSG_PORT_ATTR_ACCEPT_FRAME_TYPE = 6,
+	MVSW_MSG_PORT_ATTR_LEARNING = 7,
+	MVSW_MSG_PORT_ATTR_FLOOD = 8,
+	MVSW_MSG_PORT_ATTR_CAPABILITY = 9,
+	MVSW_MSG_PORT_ATTR_REMOTE_CAPABILITY = 10,
+	MVSW_MSG_PORT_ATTR_REMOTE_FC = 11,
+	MVSW_MSG_PORT_ATTR_LINK_MODE = 12,
+	MVSW_MSG_PORT_ATTR_TYPE = 13,
+	MVSW_MSG_PORT_ATTR_FEC = 14,
+	MVSW_MSG_PORT_ATTR_AUTONEG = 15,
+	MVSW_MSG_PORT_ATTR_DUPLEX = 16,
+	MVSW_MSG_PORT_ATTR_STATS = 17,
+	MVSW_MSG_PORT_ATTR_MDIX = 18,
+	MVSW_MSG_PORT_ATTR_AUTONEG_RESTART = 19,
+	MVSW_MSG_PORT_ATTR_MAX
+};
+
+enum mvsw_msg_switch_attr {
+	MVSW_MSG_SWITCH_ATTR_MAC = 1,
+	MVSW_MSG_SWITCH_ATTR_AGEING = 2,
+	MVSW_MSG_SWITCH_ATTR_TRAP_POLICER = 3,
+};
+
+enum {
+	MVSW_MSG_ACK_OK,
+	MVSW_MSG_ACK_FAILED,
+	MVSW_MSG_ACK_MAX
+};
+
+enum {
+	MVSW_PORT_TP_NA,
+	MVSW_PORT_TP_MDI,
+	MVSW_PORT_TP_MDIX,
+	MVSW_PORT_TP_AUTO
+};
+
+enum {
+	MVSW_PORT_GOOD_OCTETS_RCV_CNT,
+	MVSW_PORT_BAD_OCTETS_RCV_CNT,
+	MVSW_PORT_MAC_TRANSMIT_ERR_CNT,
+	MVSW_PORT_BRDC_PKTS_RCV_CNT,
+	MVSW_PORT_MC_PKTS_RCV_CNT,
+	MVSW_PORT_PKTS_64_OCTETS_CNT,
+	MVSW_PORT_PKTS_65TO127_OCTETS_CNT,
+	MVSW_PORT_PKTS_128TO255_OCTETS_CNT,
+	MVSW_PORT_PKTS_256TO511_OCTETS_CNT,
+	MVSW_PORT_PKTS_512TO1023_OCTETS_CNT,
+	MVSW_PORT_PKTS_1024TOMAX_OCTETS_CNT,
+	MVSW_PORT_EXCESSIVE_COLLISIONS_CNT,
+	MVSW_PORT_MC_PKTS_SENT_CNT,
+	MVSW_PORT_BRDC_PKTS_SENT_CNT,
+	MVSW_PORT_FC_SENT_CNT,
+	MVSW_PORT_GOOD_FC_RCV_CNT,
+	MVSW_PORT_DROP_EVENTS_CNT,
+	MVSW_PORT_UNDERSIZE_PKTS_CNT,
+	MVSW_PORT_FRAGMENTS_PKTS_CNT,
+	MVSW_PORT_OVERSIZE_PKTS_CNT,
+	MVSW_PORT_JABBER_PKTS_CNT,
+	MVSW_PORT_MAC_RCV_ERROR_CNT,
+	MVSW_PORT_BAD_CRC_CNT,
+	MVSW_PORT_COLLISIONS_CNT,
+	MVSW_PORT_LATE_COLLISIONS_CNT,
+	MVSW_PORT_GOOD_UC_PKTS_RCV_CNT,
+	MVSW_PORT_GOOD_UC_PKTS_SENT_CNT,
+	MVSW_PORT_MULTIPLE_PKTS_SENT_CNT,
+	MVSW_PORT_DEFERRED_PKTS_SENT_CNT,
+	MVSW_PORT_GOOD_OCTETS_SENT_CNT,
+	MVSW_PORT_CNT_MAX,
+};
+
+enum {
+	MVSW_FC_NONE,
+	MVSW_FC_SYMMETRIC,
+	MVSW_FC_ASYMMETRIC,
+	MVSW_FC_SYMM_ASYMM,
+};
+
+enum {
+	MVSW_HW_FDB_ENTRY_TYPE_REG_PORT = 0,
+	MVSW_HW_FDB_ENTRY_TYPE_LAG = 1,
+	MVSW_HW_FDB_ENTRY_TYPE_MAX = 2,
+};
+
+enum {
+	MVSW_PORT_FLOOD_TYPE_UC = 0,
+	MVSW_PORT_FLOOD_TYPE_MC = 1,
+	MVSW_PORT_FLOOD_TYPE_BC = 2,
+};
+
+struct mvsw_msg_buff {
+	u32 free;
+	u32 total;
+	u32 used;
+	void *data;
+};
+
+struct mvsw_msg_cmd {
+	u32 type;
+} __packed __aligned(4);
+
+struct mvsw_msg_ret {
+	struct mvsw_msg_cmd cmd;
+	u32 status;
+} __packed __aligned(4);
+
+struct mvsw_msg_common_request {
+	struct mvsw_msg_cmd cmd;
+} __packed __aligned(4);
+
+struct mvsw_msg_common_response {
+	struct mvsw_msg_ret ret;
+} __packed __aligned(4);
+
+union mvsw_msg_switch_param {
+	u32 ageing_timeout;
+	u8  mac[ETH_ALEN];
+	u32 trap_policer_profile;
+};
+
+struct mvsw_msg_switch_attr_cmd {
+	struct mvsw_msg_cmd cmd;
+	u32 attr;
+	union mvsw_msg_switch_param param;
+} __packed __aligned(4);
+
+struct mvsw_msg_switch_init_ret {
+	struct mvsw_msg_ret ret;
+	u32 port_count;
+	u32 mtu_max;
+	u8  switch_id;
+	u8  lag_max;
+	u8  lag_member_max;
+} __packed __aligned(4);
+
+struct mvsw_msg_port_autoneg_param {
+	u64 link_mode;
+	u8  enable;
+	u8  fec;
+};
+
+struct mvsw_msg_port_cap_param {
+	u64 link_mode;
+	u8  type;
+	u8  fec;
+	u8  transceiver;
+};
+
+struct mvsw_msg_port_mdix_param {
+	u8 status;
+	u8 admin_mode;
+};
+
+struct mvsw_msg_port_flood_param {
+	u8 type;
+	u8 enable;
+};
+
+union mvsw_msg_port_param {
+	u8  admin_state;
+	u8  oper_state;
+	u32 mtu;
+	u8  mac[ETH_ALEN];
+	u8  accept_frm_type;
+	u8  learning;
+	u32 speed;
+	u32 link_mode;
+	u8  type;
+	u8  duplex;
+	u8  fec;
+	u8  fc;
+	struct mvsw_msg_port_mdix_param mdix;
+	struct mvsw_msg_port_autoneg_param autoneg;
+	struct mvsw_msg_port_cap_param cap;
+	struct mvsw_msg_port_flood_param flood;
+};
+
+struct mvsw_msg_port_attr_cmd {
+	struct mvsw_msg_cmd cmd;
+	u32 attr;
+	u32 port;
+	u32 dev;
+	union mvsw_msg_port_param param;
+} __packed __aligned(4);
+
+struct mvsw_msg_port_attr_ret {
+	struct mvsw_msg_ret ret;
+	union mvsw_msg_port_param param;
+} __packed __aligned(4);
+
+struct mvsw_msg_port_stats_ret {
+	struct mvsw_msg_ret ret;
+	u64 stats[MVSW_PORT_CNT_MAX];
+} __packed __aligned(4);
+
+struct mvsw_msg_port_info_cmd {
+	struct mvsw_msg_cmd cmd;
+	u32 port;
+} __packed __aligned(4);
+
+struct mvsw_msg_port_info_ret {
+	struct mvsw_msg_ret ret;
+	u32 hw_id;
+	u32 dev_id;
+	u16 fp_id;
+} __packed __aligned(4);
+
+struct mvsw_msg_vlan_cmd {
+	struct mvsw_msg_cmd cmd;
+	u32 port;
+	u32 dev;
+	u16 vid;
+	u8  is_member;
+	u8  is_tagged;
+} __packed __aligned(4);
+
+struct mvsw_msg_fdb_cmd {
+	struct mvsw_msg_cmd cmd;
+	u8 dest_type;
+	union {
+		struct {
+			u32 port;
+			u32 dev;
+		};
+		u16 lag_id;
+	} dest;
+	u8  mac[ETH_ALEN];
+	u16 vid;
+	u8  dynamic;
+	u32 flush_mode;
+} __packed __aligned(4);
+
+struct mvsw_msg_log_lvl_set_cmd {
+	struct mvsw_msg_cmd cmd;
+	u32 lib;
+	u32 type;
+} __packed __aligned(4);
+
+struct mvsw_msg_acl_rule_cmd {
+	struct mvsw_msg_cmd cmd;
+	u32 id;
+	u16 ruleset_id;
+} __packed __aligned(4);
+
+struct mvsw_msg_acl_rule_ret {
+	struct mvsw_msg_ret ret;
+	u32 id;
+} __packed __aligned(4);
+
+struct mvsw_msg_acl_rule_stats_ret {
+	struct mvsw_msg_ret ret;
+	u64 packets;
+	u64 bytes;
+} __packed __aligned(4);
+
+struct mvsw_msg_acl_ruleset_bind_cmd {
+	struct mvsw_msg_cmd cmd;
+	u32 port;
+	u32 dev;
+	u16 ruleset_id;
+} __packed __aligned(4);
+
+struct mvsw_msg_acl_ruleset_cmd {
+	struct mvsw_msg_cmd cmd;
+	u16 id;
+} __packed __aligned(4);
+
+struct mvsw_msg_acl_ruleset_ret {
+	struct mvsw_msg_ret ret;
+	u16 id;
+} __packed __aligned(4);
+
+struct mvsw_msg_event {
+	u16 type;
+	u16 id;
+} __packed __aligned(4);
+
+struct mvsw_msg_event_log {
+	struct mvsw_msg_event id;
+	u32 log_string_size;
+	u8 log_string[0];
+} __packed __aligned(4);
+
+union mvsw_msg_event_fdb_param {
+	u8 mac[ETH_ALEN];
+};
+
+struct mvsw_msg_event_fdb {
+	struct mvsw_msg_event id;
+	u8 dest_type;
+	union {
+		u32 port_id;
+		u16 lag_id;
+	} dest;
+	u32 vid;
+	union mvsw_msg_event_fdb_param param;
+} __packed __aligned(4);
+
+union mvsw_msg_event_port_param {
+	u32 oper_state;
+};
+
+struct mvsw_msg_event_port {
+	struct mvsw_msg_event id;
+	u32 port_id;
+	union mvsw_msg_event_port_param param;
+} __packed __aligned(4);
+
+struct mvsw_msg_bridge_cmd {
+	struct mvsw_msg_cmd cmd;
+	u32 port;
+	u32 dev;
+	u16 bridge;
+} __packed __aligned(4);
+
+struct mvsw_msg_bridge_ret {
+	struct mvsw_msg_ret ret;
+	u16 bridge;
+} __packed __aligned(4);
+
+struct mvsw_msg_macvlan_cmd {
+	struct mvsw_msg_cmd cmd;
+	u16 vr_id;
+	u8 mac[ETH_ALEN];
+	u16 vid;
+} __packed __aligned(4);
+
+struct mvsw_msg_stp_cmd {
+	struct mvsw_msg_cmd cmd;
+	u32 port;
+	u32 dev;
+	u16 vid;
+	u8  state;
+} __packed __aligned(4);
+
+struct mvsw_msg_iface {
+	u8 type;
+	u16 vid;
+	u16 vr_id;
+	union {
+		struct {
+			u32 dev;
+			u32 port;
+		};
+		u16 lag_id;
+	};
+} __packed __aligned(4);
+
+struct mvsw_msg_rif_cmd {
+	struct mvsw_msg_cmd cmd;
+	struct mvsw_msg_iface iif;
+	u16 rif_id;
+	u8 mac[ETH_ALEN];
+	u32 mtu;
+} __packed __aligned(4);
+
+struct mvsw_msg_rif_ret {
+	struct mvsw_msg_ret ret;
+	u16 rif_id;
+} __packed __aligned(4);
+
+struct mvsw_msg_lpm_cmd {
+	struct mvsw_msg_cmd cmd;
+	u32 grp_id;
+	__be32 dst;
+	u32 dst_len;
+	u16 vr_id;
+} __packed __aligned(4);
+
+struct mvsw_msg_nh {
+	struct mvsw_msg_iface oif;
+	u8 is_active;
+	u32 hw_id;
+	u8 mac[ETH_ALEN];
+} __packed __aligned(4);
+
+struct mvsw_msg_nh_cmd {
+	struct mvsw_msg_cmd cmd;
+	u32 size;
+	u32 grp_id;
+	struct mvsw_msg_nh nh[MVSW_PR_NHGR_SIZE_MAX];
+} __packed __aligned(4);
+
+struct mvsw_msg_nh_ret {
+	struct mvsw_msg_ret ret;
+	struct mvsw_msg_nh nh[MVSW_PR_NHGR_SIZE_MAX];
+} __packed __aligned(4);
+
+struct mvsw_msg_nh_grp_cmd {
+	struct mvsw_msg_cmd cmd;
+	u32 grp_id;
+	u32 size;
+} __packed __aligned(4);
+
+struct mvsw_msg_nh_grp_ret {
+	struct mvsw_msg_ret ret;
+	u32 grp_id;
+} __packed __aligned(4);
+
+struct mvsw_msg_mp_cmd {
+	struct mvsw_msg_cmd cmd;
+	u8 hash_policy;
+} __packed __aligned(4);
+
+struct mvsw_msg_rxtx_cmd {
+	struct mvsw_msg_cmd cmd;
+	u8 use_sdma;
+} __packed __aligned(4);
+
+struct mvsw_msg_rxtx_ret {
+	struct mvsw_msg_ret ret;
+	u32 map_addr;
+} __packed __aligned(4);
+
+struct mvsw_msg_vr_cmd {
+	struct mvsw_msg_cmd cmd;
+	u16 vr_id;
+} __packed __aligned(4);
+
+struct mvsw_msg_vr_ret {
+	struct mvsw_msg_ret ret;
+	u16 vr_id;
+} __packed __aligned(4);
+
+struct mvsw_msg_lag_cmd {
+	struct mvsw_msg_cmd cmd;
+	u32 port;
+	u32 dev;
+	u16 lag_id;
+	u16 vr_id;
+} __packed __aligned(4);
+
+#define fw_check_resp(_response)	\
+({								\
+	int __er = 0;						\
+	typeof(_response) __r = (_response);			\
+	if (__r->ret.cmd.type != MVSW_MSG_TYPE_ACK)		\
+		__er = -EBADE;					\
+	else if (__r->ret.status != MVSW_MSG_ACK_OK)		\
+		__er = -EINVAL;					\
+	(__er);							\
+})
+
+#define __fw_send_req_resp(_switch, _type, _req, _req_size,	\
+_response, _wait)						\
+({								\
+	int __e;						\
+	typeof(_switch) __sw = (_switch);			\
+	typeof(_req) __req = (_req);				\
+	typeof(_response) __resp = (_response);			\
+	__req->cmd.type = (_type);				\
+	__e = __sw->dev->send_req(__sw->dev,			\
+		(u8 *)__req, _req_size,				\
+		(u8 *)__resp, sizeof(*__resp),			\
+		_wait);						\
+	if (!__e)						\
+		__e = fw_check_resp(__resp);			\
+	(__e);							\
+})
+
+#define fw_send_nreq_resp(_sw, _t, _req, _req_size, _resp)	\
+	__fw_send_req_resp(_sw, _t, _req, _req_size, _resp, 0)
+
+#define fw_send_req_resp(_sw, _t, _req, _resp)	\
+	__fw_send_req_resp(_sw, _t, _req, sizeof(*_req), _resp, 0)
+
+#define fw_send_req_resp_wait(_sw, _t, _req, _resp, _wait)	\
+	__fw_send_req_resp(_sw, _t, _req, sizeof(*_req), _resp, _wait)
+
+#define fw_send_req(_sw, _t, _req)	\
+({							\
+	struct mvsw_msg_common_response __re;		\
+	(fw_send_req_resp(_sw, _t, _req, &__re));	\
+})
+
+struct mvsw_fw_event_handler {
+	struct list_head list;
+	enum mvsw_pr_event_type type;
+	void (*func)(struct mvsw_pr_switch *sw,
+		     struct mvsw_pr_event *evt,
+		     void *arg);
+	void *arg;
+};
+
+static int fw_parse_port_evt(u8 *msg, struct mvsw_pr_event *evt)
+{
+	struct mvsw_msg_event_port *hw_evt = (struct mvsw_msg_event_port *)msg;
+
+	evt->port_evt.port_id = hw_evt->port_id;
+
+	if (evt->id == MVSW_PORT_EVENT_STATE_CHANGED)
+		evt->port_evt.data.oper_state = hw_evt->param.oper_state;
+	else
+		return -EINVAL;
+
+	return 0;
+}
+
+static int fw_parse_fdb_evt(u8 *msg, struct mvsw_pr_event *evt)
+{
+	struct mvsw_msg_event_fdb *hw_evt = (struct mvsw_msg_event_fdb *)msg;
+
+	switch (hw_evt->dest_type) {
+	case MVSW_HW_FDB_ENTRY_TYPE_REG_PORT:
+		evt->fdb_evt.type = MVSW_PR_FDB_ENTRY_TYPE_REG_PORT;
+		evt->fdb_evt.dest.port_id = hw_evt->dest.port_id;
+		break;
+	case MVSW_HW_FDB_ENTRY_TYPE_LAG:
+		evt->fdb_evt.type = MVSW_PR_FDB_ENTRY_TYPE_LAG;
+		evt->fdb_evt.dest.lag_id = hw_evt->dest.lag_id;
+		break;
+	default:
+		return -EINVAL;
+	}
+
+	evt->fdb_evt.vid = hw_evt->vid;
+
+	memcpy(&evt->fdb_evt.data, &hw_evt->param, sizeof(u8) * ETH_ALEN);
+
+	return 0;
+}
+
+static int fw_parse_log_evt(u8 *msg, struct mvsw_pr_event *evt)
+{
+	struct mvsw_msg_event_log *hw_evt = (struct mvsw_msg_event_log *)msg;
+
+	evt->fw_log_evt.log_len	= hw_evt->log_string_size;
+	evt->fw_log_evt.data	= hw_evt->log_string;
+
+	return 0;
+}
+
+struct mvsw_fw_evt_parser {
+	int (*func)(u8 *msg, struct mvsw_pr_event *evt);
+};
+
+static struct mvsw_fw_evt_parser fw_event_parsers[MVSW_EVENT_TYPE_MAX] = {
+	[MVSW_EVENT_TYPE_PORT] = {.func = fw_parse_port_evt},
+	[MVSW_EVENT_TYPE_FDB] = {.func = fw_parse_fdb_evt},
+	[MVSW_EVENT_TYPE_FW_LOG] = {.func = fw_parse_log_evt}
+};
+
+static struct mvsw_fw_event_handler *
+__find_event_handler(const struct mvsw_pr_switch *sw,
+		     enum mvsw_pr_event_type type)
+{
+	struct mvsw_fw_event_handler *eh;
+
+	list_for_each_entry_rcu(eh, &sw->event_handlers, list) {
+		if (eh->type == type)
+			return eh;
+	}
+
+	return NULL;
+}
+
+static int mvsw_find_event_handler(const struct mvsw_pr_switch *sw,
+				   enum mvsw_pr_event_type type,
+				   struct mvsw_fw_event_handler *eh)
+{
+	struct mvsw_fw_event_handler *tmp;
+	int err = 0;
+
+	rcu_read_lock();
+	tmp = __find_event_handler(sw, type);
+	if (tmp)
+		*eh = *tmp;
+	else
+		err = -EEXIST;
+	rcu_read_unlock();
+
+	return err;
+}
+
+static int fw_event_recv(struct prestera_device *dev, u8 *buf, size_t size)
+{
+	struct mvsw_msg_event *msg = (struct mvsw_msg_event *)buf;
+	struct mvsw_pr_switch *sw = dev->priv;
+	struct mvsw_fw_event_handler eh;
+	struct mvsw_pr_event evt;
+	int err;
+
+	if (msg->type >= MVSW_EVENT_TYPE_MAX)
+		return -EINVAL;
+
+	err = mvsw_find_event_handler(sw, msg->type, &eh);
+
+	if (err || !fw_event_parsers[msg->type].func)
+		return 0;
+
+	evt.id = msg->id;
+
+	err = fw_event_parsers[msg->type].func(buf, &evt);
+	if (!err)
+		eh.func(sw, &evt, eh.arg);
+
+	return err;
+}
+
+static void fw_pkt_recv(struct prestera_device *dev)
+{
+	struct mvsw_pr_switch *sw = dev->priv;
+	struct mvsw_fw_event_handler eh;
+	struct mvsw_pr_event ev;
+	int err;
+
+	ev.id = MVSW_RXTX_EVENT_RCV_PKT;
+
+	err = mvsw_find_event_handler(sw, MVSW_EVENT_TYPE_RXTX, &eh);
+	if (err)
+		return;
+
+	eh.func(sw, &ev, eh.arg);
+}
+
+static struct mvsw_msg_buff *mvsw_msg_buff_create(u16 head_size)
+{
+	struct mvsw_msg_buff *msg_buff;
+
+	msg_buff = kzalloc(sizeof(*msg_buff), GFP_KERNEL);
+	if (!msg_buff)
+		return NULL;
+
+	msg_buff->data = kzalloc(MVSW_PR_MSG_BUFF_CHUNK_SIZE + head_size,
+				 GFP_KERNEL);
+	if (!msg_buff->data) {
+		kfree(msg_buff);
+		return NULL;
+	}
+
+	msg_buff->total = MVSW_PR_MSG_BUFF_CHUNK_SIZE + head_size;
+	msg_buff->free = MVSW_PR_MSG_BUFF_CHUNK_SIZE;
+	msg_buff->used = head_size;
+
+	return msg_buff;
+}
+
+static void *mvsw_msg_buff_data(struct mvsw_msg_buff *msg_buff)
+{
+	return msg_buff->data;
+}
+
+static u32 mvsw_msg_buff_size(const struct mvsw_msg_buff *msg_buff)
+{
+	return msg_buff->used;
+}
+
+static int mvsw_msg_buff_resize(struct mvsw_msg_buff *msg_buff)
+{
+	void *data;
+
+	data = krealloc(msg_buff->data,
+			msg_buff->total + MVSW_PR_MSG_BUFF_CHUNK_SIZE,
+			GFP_KERNEL);
+	if (!data)
+		return -ENOMEM;
+
+	msg_buff->total += MVSW_PR_MSG_BUFF_CHUNK_SIZE;
+	msg_buff->free += MVSW_PR_MSG_BUFF_CHUNK_SIZE;
+	msg_buff->data = data;
+
+	return 0;
+}
+
+static int mvsw_msg_buff_put(struct mvsw_msg_buff *msg_buff,
+			     void *data, u16 size)
+{
+	void *data_ptr;
+	int err;
+
+	if (size > msg_buff->free) {
+		err = mvsw_msg_buff_resize(msg_buff);
+		if (err)
+			return err;
+	}
+	/* point to unused data */
+	data_ptr = msg_buff->data + msg_buff->used;
+
+	/* set the data */
+	memcpy(data_ptr, data, size);
+	msg_buff->used += size;
+	msg_buff->free -= size;
+
+	return 0;
+}
+
+static int mvsw_msg_buff_terminate(struct mvsw_msg_buff *msg_buff)
+{
+	u16 padding_size;
+	void *data_ptr;
+	int err;
+
+	/* the data should be aligned to 4 byte, so calculate
+	 * the padding leaving at least one byte for termination
+	 */
+	padding_size = ALIGN(msg_buff->used + sizeof(u8),
+			     sizeof(u32)) - msg_buff->used;
+	if (msg_buff->free < padding_size) {
+		err = mvsw_msg_buff_resize(msg_buff);
+		if (err)
+			return err;
+	}
+	/* point to unused data */
+	data_ptr = msg_buff->data + msg_buff->used;
+
+	/* terminate buffer by zero byte */
+	memset(data_ptr, 0, padding_size);
+	msg_buff->used += padding_size;
+	msg_buff->free -= padding_size;
+	data_ptr += padding_size;
+
+	return 0;
+}
+
+static void mvsw_msg_buff_destroy(struct mvsw_msg_buff *msg_buff)
+{
+	kfree(msg_buff->data);
+	kfree(msg_buff);
+}
+
+int mvsw_pr_hw_port_info_get(const struct mvsw_pr_port *port,
+			     u16 *fp_id, u32 *hw_id, u32 *dev_id)
+{
+	struct mvsw_msg_port_info_ret resp;
+	struct mvsw_msg_port_info_cmd req = {
+		.port = port->id
+	};
+	int err;
+
+	err = fw_send_req_resp(port->sw, MVSW_MSG_TYPE_PORT_INFO_GET,
+			       &req, &resp);
+	if (err)
+		return err;
+
+	*hw_id = resp.hw_id;
+	*dev_id = resp.dev_id;
+	*fp_id = resp.fp_id;
+
+	return 0;
+}
+
+int mvsw_pr_hw_switch_init(struct mvsw_pr_switch *sw)
+{
+	struct mvsw_msg_switch_init_ret resp;
+	struct mvsw_msg_common_request req;
+	int err = 0;
+
+	INIT_LIST_HEAD(&sw->event_handlers);
+
+	err = fw_send_req_resp_wait(sw, MVSW_MSG_TYPE_SWITCH_INIT, &req, &resp,
+				    MVSW_PR_INIT_TIMEOUT);
+	if (err)
+		return err;
+
+	sw->id = resp.switch_id;
+	sw->port_count = resp.port_count;
+	sw->mtu_min = MVSW_PR_MIN_MTU;
+	sw->mtu_max = resp.mtu_max;
+	sw->lag_max = resp.lag_max;
+	sw->lag_member_max = resp.lag_member_max;
+	sw->dev->recv_msg = fw_event_recv;
+	sw->dev->recv_pkt = fw_pkt_recv;
+
+	return err;
+}
+
+int mvsw_pr_hw_switch_ageing_set(const struct mvsw_pr_switch *sw,
+				 u32 ageing_time)
+{
+	struct mvsw_msg_switch_attr_cmd req = {
+		.param = {.ageing_timeout = ageing_time},
+		.attr = MVSW_MSG_SWITCH_ATTR_AGEING,
+	};
+
+	return fw_send_req(sw, MVSW_MSG_TYPE_SWITCH_ATTR_SET, &req);
+}
+
+int mvsw_pr_hw_switch_mac_set(const struct mvsw_pr_switch *sw, const u8 *mac)
+{
+	struct mvsw_msg_switch_attr_cmd req = {
+		.attr = MVSW_MSG_SWITCH_ATTR_MAC,
+	};
+
+	memcpy(req.param.mac, mac, sizeof(req.param.mac));
+
+	return fw_send_req(sw, MVSW_MSG_TYPE_SWITCH_ATTR_SET, &req);
+}
+
+int mvsw_pr_hw_switch_trap_policer_set(const struct mvsw_pr_switch *sw,
+				       u8 profile)
+{
+	struct mvsw_msg_switch_attr_cmd req = {
+		.param = {.trap_policer_profile = profile},
+		.attr = MVSW_MSG_SWITCH_ATTR_TRAP_POLICER,
+	};
+
+	return fw_send_req(sw, MVSW_MSG_TYPE_SWITCH_ATTR_SET, &req);
+}
+
+int mvsw_pr_hw_port_state_set(const struct mvsw_pr_port *port,
+			      bool admin_state)
+{
+	struct mvsw_msg_port_attr_cmd req = {
+		.attr = MVSW_MSG_PORT_ATTR_ADMIN_STATE,
+		.port = port->hw_id,
+		.dev = port->dev_id,
+		.param = {.admin_state = admin_state ? 1 : 0}
+	};
+
+	return fw_send_req(port->sw, MVSW_MSG_TYPE_PORT_ATTR_SET, &req);
+}
+
+int mvsw_pr_hw_port_state_get(const struct mvsw_pr_port *port,
+			      bool *admin_state, bool *oper_state)
+{
+	struct mvsw_msg_port_attr_ret resp;
+	struct mvsw_msg_port_attr_cmd req = {
+		.port = port->hw_id,
+		.dev = port->dev_id
+	};
+	int err;
+
+	if (admin_state) {
+		req.attr = MVSW_MSG_PORT_ATTR_ADMIN_STATE;
+		err = fw_send_req_resp(port->sw, MVSW_MSG_TYPE_PORT_ATTR_GET,
+				       &req, &resp);
+		if (err)
+			return err;
+		*admin_state = resp.param.admin_state != 0;
+	}
+
+	if (oper_state) {
+		req.attr = MVSW_MSG_PORT_ATTR_OPER_STATE;
+		err = fw_send_req_resp(port->sw, MVSW_MSG_TYPE_PORT_ATTR_GET,
+				       &req, &resp);
+		if (err)
+			return err;
+		*oper_state = resp.param.oper_state != 0;
+	}
+
+	return 0;
+}
+
+int mvsw_pr_hw_port_mtu_set(const struct mvsw_pr_port *port, u32 mtu)
+{
+	struct mvsw_msg_port_attr_cmd req = {
+		.attr = MVSW_MSG_PORT_ATTR_MTU,
+		.port = port->hw_id,
+		.dev = port->dev_id,
+		.param = {.mtu = mtu}
+	};
+
+	return fw_send_req(port->sw, MVSW_MSG_TYPE_PORT_ATTR_SET, &req);
+}
+
+int mvsw_pr_hw_port_mtu_get(const struct mvsw_pr_port *port, u32 *mtu)
+{
+	struct mvsw_msg_port_attr_ret resp;
+	struct mvsw_msg_port_attr_cmd req = {
+		.attr = MVSW_MSG_PORT_ATTR_MTU,
+		.port = port->hw_id,
+		.dev = port->dev_id
+	};
+	int err;
+
+	err = fw_send_req_resp(port->sw, MVSW_MSG_TYPE_PORT_ATTR_GET,
+			       &req, &resp);
+	if (err)
+		return err;
+
+	*mtu = resp.param.mtu;
+
+	return err;
+}
+
+int mvsw_pr_hw_port_mac_set(const struct mvsw_pr_port *port, char *mac)
+{
+	struct mvsw_msg_port_attr_cmd req = {
+		.attr = MVSW_MSG_PORT_ATTR_MAC,
+		.port = port->hw_id,
+		.dev = port->dev_id
+	};
+	memcpy(&req.param.mac, mac, sizeof(req.param.mac));
+
+	return fw_send_req(port->sw, MVSW_MSG_TYPE_PORT_ATTR_SET, &req);
+}
+
+int mvsw_pr_hw_port_mac_get(const struct mvsw_pr_port *port, char *mac)
+{
+	struct mvsw_msg_port_attr_ret resp;
+	struct mvsw_msg_port_attr_cmd req = {
+		.attr = MVSW_MSG_PORT_ATTR_MAC,
+		.port = port->hw_id,
+		.dev = port->dev_id
+	};
+	int err;
+
+	err = fw_send_req_resp(port->sw, MVSW_MSG_TYPE_PORT_ATTR_GET,
+			       &req, &resp);
+	if (err)
+		return err;
+
+	memcpy(mac, resp.param.mac, sizeof(resp.param.mac));
+
+	return err;
+}
+
+int mvsw_pr_hw_port_accept_frame_type_set(const struct mvsw_pr_port *port,
+					  enum mvsw_pr_accept_frame_type type)
+{
+	struct mvsw_msg_port_attr_cmd req = {
+		.attr = MVSW_MSG_PORT_ATTR_ACCEPT_FRAME_TYPE,
+		.port = port->hw_id,
+		.dev = port->dev_id,
+		.param = {.accept_frm_type = type}
+	};
+
+	return fw_send_req(port->sw, MVSW_MSG_TYPE_PORT_ATTR_SET, &req);
+}
+
+int mvsw_pr_hw_port_learning_set(const struct mvsw_pr_port *port, bool enable)
+{
+	struct mvsw_msg_port_attr_cmd req = {
+		.attr = MVSW_MSG_PORT_ATTR_LEARNING,
+		.port = port->hw_id,
+		.dev = port->dev_id,
+		.param = {.learning = enable ? 1 : 0}
+	};
+
+	return fw_send_req(port->sw, MVSW_MSG_TYPE_PORT_ATTR_SET, &req);
+}
+
+int mvsw_pr_hw_event_handler_register(struct mvsw_pr_switch *sw,
+				      enum mvsw_pr_event_type type,
+				      void (*cb)(struct mvsw_pr_switch *sw,
+						 struct mvsw_pr_event *evt,
+						 void *arg),
+				      void *arg)
+{
+	struct mvsw_fw_event_handler *eh;
+
+	eh = __find_event_handler(sw, type);
+	if (eh)
+		return -EEXIST;
+	eh = kmalloc(sizeof(*eh), GFP_KERNEL);
+	if (!eh)
+		return -ENOMEM;
+
+	eh->type = type;
+	eh->func = cb;
+	eh->arg = arg;
+
+	INIT_LIST_HEAD(&eh->list);
+
+	list_add_rcu(&eh->list, &sw->event_handlers);
+
+	return 0;
+}
+
+void mvsw_pr_hw_event_handler_unregister(struct mvsw_pr_switch *sw,
+					 enum mvsw_pr_event_type type)
+{
+	struct mvsw_fw_event_handler *eh;
+
+	eh = __find_event_handler(sw, type);
+	if (!eh)
+		return;
+
+	list_del_rcu(&eh->list);
+	synchronize_rcu();
+	kfree(eh);
+}
+
+int mvsw_pr_hw_vlan_create(const struct mvsw_pr_switch *sw, u16 vid)
+{
+	struct mvsw_msg_vlan_cmd req = {
+		.vid = vid,
+	};
+
+	return fw_send_req(sw, MVSW_MSG_TYPE_VLAN_CREATE, &req);
+}
+
+int mvsw_pr_hw_vlan_delete(const struct mvsw_pr_switch *sw, u16 vid)
+{
+	struct mvsw_msg_vlan_cmd req = {
+		.vid = vid,
+	};
+
+	return fw_send_req(sw, MVSW_MSG_TYPE_VLAN_DELETE, &req);
+}
+
+int mvsw_pr_hw_vlan_port_set(const struct mvsw_pr_port *port,
+			     u16 vid, bool is_member, bool untagged)
+{
+	struct mvsw_msg_vlan_cmd req = {
+		.port = port->hw_id,
+		.dev = port->dev_id,
+		.vid = vid,
+		.is_member = is_member ? 1 : 0,
+		.is_tagged = untagged ? 0 : 1
+	};
+
+	return fw_send_req(port->sw, MVSW_MSG_TYPE_VLAN_PORT_SET, &req);
+}
+
+int mvsw_pr_hw_vlan_port_vid_set(const struct mvsw_pr_port *port, u16 vid)
+{
+	struct mvsw_msg_vlan_cmd req = {
+		.port = port->hw_id,
+		.dev = port->dev_id,
+		.vid = vid
+	};
+
+	return fw_send_req(port->sw, MVSW_MSG_TYPE_VLAN_PVID_SET, &req);
+}
+
+int mvsw_pr_hw_port_vid_stp_set(struct mvsw_pr_port *port, u16 vid, u8 state)
+{
+	struct mvsw_msg_stp_cmd req = {
+		.port = port->hw_id,
+		.dev = port->dev_id,
+		.vid = vid,
+		.state = state
+	};
+
+	return fw_send_req(port->sw, MVSW_MSG_TYPE_STP_PORT_SET, &req);
+}
+
+int mvsw_pr_hw_port_speed_get(const struct mvsw_pr_port *port, u32 *speed)
+{
+	struct mvsw_msg_port_attr_ret resp;
+	struct mvsw_msg_port_attr_cmd req = {
+		.attr = MVSW_MSG_PORT_ATTR_SPEED,
+		.port = port->hw_id,
+		.dev = port->dev_id
+	};
+	int err;
+
+	err = fw_send_req_resp(port->sw, MVSW_MSG_TYPE_PORT_ATTR_GET,
+			       &req, &resp);
+	if (err)
+		return err;
+
+	*speed = resp.param.speed;
+
+	return err;
+}
+
+int mvsw_pr_hw_port_uc_flood_set(const struct mvsw_pr_port *port, bool flood)
+{
+	struct mvsw_msg_port_attr_cmd req = {
+		.attr = MVSW_MSG_PORT_ATTR_FLOOD,
+		.port = port->hw_id,
+		.dev = port->dev_id,
+		.param = {
+			.flood = {
+				.type = MVSW_PORT_FLOOD_TYPE_UC,
+				.enable = flood ? 1 : 0,
+			}
+		}
+	};
+
+	return fw_send_req(port->sw, MVSW_MSG_TYPE_PORT_ATTR_SET, &req);
+}
+
+int mvsw_pr_hw_port_mc_flood_set(const struct mvsw_pr_port *port, bool flood)
+{
+	struct mvsw_msg_port_attr_cmd req = {
+		.attr = MVSW_MSG_PORT_ATTR_FLOOD,
+		.port = port->hw_id,
+		.dev = port->dev_id,
+		.param = {
+			.flood = {
+				.type = MVSW_PORT_FLOOD_TYPE_MC,
+				.enable = flood ? 1 : 0,
+			}
+		}
+	};
+
+	return fw_send_req(port->sw, MVSW_MSG_TYPE_PORT_ATTR_SET, &req);
+}
+
+int mvsw_pr_hw_fdb_add(const struct mvsw_pr_port *port,
+		       const unsigned char *mac, u16 vid, bool dynamic)
+{
+	struct mvsw_msg_fdb_cmd req = {
+		.dest_type = MVSW_HW_FDB_ENTRY_TYPE_REG_PORT,
+		.dest = {
+			.dev = port->dev_id,
+			.port = port->hw_id,
+		},
+		.vid = vid,
+		.dynamic = dynamic ? 1 : 0
+	};
+
+	memcpy(req.mac, mac, sizeof(req.mac));
+
+	return fw_send_req(port->sw, MVSW_MSG_TYPE_FDB_ADD, &req);
+}
+
+int mvsw_pr_hw_lag_fdb_add(const struct mvsw_pr_switch *sw, u16 lag_id,
+			   const unsigned char *mac, u16 vid, bool dynamic)
+{
+	struct mvsw_msg_fdb_cmd req = {
+		.dest_type = MVSW_HW_FDB_ENTRY_TYPE_LAG,
+		.dest = { .lag_id = lag_id },
+		.vid = vid,
+		.dynamic = dynamic
+	};
+
+	memcpy(req.mac, mac, sizeof(req.mac));
+
+	return fw_send_req(sw, MVSW_MSG_TYPE_FDB_ADD, &req);
+}
+
+int mvsw_pr_hw_fdb_del(const struct mvsw_pr_port *port,
+		       const unsigned char *mac, u16 vid)
+{
+	struct mvsw_msg_fdb_cmd req = {
+		.dest_type = MVSW_HW_FDB_ENTRY_TYPE_REG_PORT,
+		.dest = {
+			.dev = port->dev_id,
+			.port = port->hw_id,
+		},
+		.vid = vid
+	};
+
+	memcpy(req.mac, mac, sizeof(req.mac));
+
+	return fw_send_req(port->sw, MVSW_MSG_TYPE_FDB_DELETE, &req);
+}
+
+int mvsw_pr_hw_lag_fdb_del(const struct mvsw_pr_switch *sw, u16 lag_id,
+			   const unsigned char *mac, u16 vid)
+{
+	struct mvsw_msg_fdb_cmd req = {
+		.dest_type = MVSW_HW_FDB_ENTRY_TYPE_LAG,
+		.dest = { .lag_id = lag_id },
+		.vid = vid
+	};
+
+	memcpy(req.mac, mac, sizeof(req.mac));
+
+	return fw_send_req(sw, MVSW_MSG_TYPE_FDB_DELETE, &req);
+}
+
+int mvsw_pr_hw_port_cap_get(const struct mvsw_pr_port *port,
+			    struct mvsw_pr_port_caps *caps)
+{
+	struct mvsw_msg_port_attr_ret resp;
+	struct mvsw_msg_port_attr_cmd req = {
+		.attr = MVSW_MSG_PORT_ATTR_CAPABILITY,
+		.port = port->hw_id,
+		.dev = port->dev_id
+	};
+	int err;
+
+	err = fw_send_req_resp(port->sw, MVSW_MSG_TYPE_PORT_ATTR_GET,
+			       &req, &resp);
+	if (err)
+		return err;
+
+	caps->supp_link_modes = resp.param.cap.link_mode;
+	caps->supp_fec = resp.param.cap.fec;
+	caps->type = resp.param.cap.type;
+	caps->transceiver = resp.param.cap.transceiver;
+
+	return err;
+}
+
+int mvsw_pr_hw_port_remote_cap_get(const struct mvsw_pr_port *port,
+				   u64 *link_mode_bitmap)
+{
+	struct mvsw_msg_port_attr_ret resp;
+	struct mvsw_msg_port_attr_cmd req = {
+		.attr = MVSW_MSG_PORT_ATTR_REMOTE_CAPABILITY,
+		.port = port->hw_id,
+		.dev = port->dev_id
+	};
+	int err;
+
+	err = fw_send_req_resp(port->sw, MVSW_MSG_TYPE_PORT_ATTR_GET,
+			       &req, &resp);
+	if (err)
+		return err;
+
+	*link_mode_bitmap = resp.param.cap.link_mode;
+
+	return err;
+}
+
+static u8 mvsw_mdix_to_eth(u8 mode)
+{
+	switch (mode) {
+	case MVSW_PORT_TP_MDI:
+		return ETH_TP_MDI;
+	case MVSW_PORT_TP_MDIX:
+		return ETH_TP_MDI_X;
+	case MVSW_PORT_TP_AUTO:
+		return ETH_TP_MDI_AUTO;
+	}
+
+	return ETH_TP_MDI_INVALID;
+}
+
+static u8 mvsw_mdix_from_eth(u8 mode)
+{
+	switch (mode) {
+	case ETH_TP_MDI:
+		return MVSW_PORT_TP_MDI;
+	case ETH_TP_MDI_X:
+		return MVSW_PORT_TP_MDIX;
+	case ETH_TP_MDI_AUTO:
+		return MVSW_PORT_TP_AUTO;
+	}
+
+	return MVSW_PORT_TP_NA;
+}
+
+int mvsw_pr_hw_port_mdix_get(const struct mvsw_pr_port *port, u8 *status,
+			     u8 *admin_mode)
+{
+	struct mvsw_msg_port_attr_ret resp;
+	struct mvsw_msg_port_attr_cmd req = {
+		.attr = MVSW_MSG_PORT_ATTR_MDIX,
+		.port = port->hw_id,
+		.dev = port->dev_id
+	};
+	int err;
+
+	err = fw_send_req_resp(port->sw, MVSW_MSG_TYPE_PORT_ATTR_GET,
+			       &req, &resp);
+	if (err)
+		return err;
+
+	*status = mvsw_mdix_to_eth(resp.param.mdix.status);
+	*admin_mode = mvsw_mdix_to_eth(resp.param.mdix.admin_mode);
+
+	return 0;
+}
+
+int mvsw_pr_hw_port_mdix_set(const struct mvsw_pr_port *port, u8 mode)
+{
+	struct mvsw_msg_port_attr_cmd req = {
+		.attr = MVSW_MSG_PORT_ATTR_MDIX,
+		.port = port->hw_id,
+		.dev = port->dev_id
+	};
+
+	req.param.mdix.admin_mode = mvsw_mdix_from_eth(mode);
+
+	return fw_send_req(port->sw, MVSW_MSG_TYPE_PORT_ATTR_SET, &req);
+}
+
+int mvsw_pr_hw_port_type_get(const struct mvsw_pr_port *port, u8 *type)
+{
+	struct mvsw_msg_port_attr_ret resp;
+	struct mvsw_msg_port_attr_cmd req = {
+		.attr = MVSW_MSG_PORT_ATTR_TYPE,
+		.port = port->hw_id,
+		.dev = port->dev_id
+	};
+	int err;
+
+	err = fw_send_req_resp(port->sw, MVSW_MSG_TYPE_PORT_ATTR_GET,
+			       &req, &resp);
+	if (err)
+		return err;
+
+	*type = resp.param.type;
+
+	return err;
+}
+
+int mvsw_pr_hw_port_fec_get(const struct mvsw_pr_port *port, u8 *fec)
+{
+	struct mvsw_msg_port_attr_ret resp;
+	struct mvsw_msg_port_attr_cmd req = {
+		.attr = MVSW_MSG_PORT_ATTR_FEC,
+		.port = port->hw_id,
+		.dev = port->dev_id
+	};
+	int err;
+
+	err = fw_send_req_resp(port->sw, MVSW_MSG_TYPE_PORT_ATTR_GET,
+			       &req, &resp);
+	if (err)
+		return err;
+
+	*fec = resp.param.fec;
+
+	return err;
+}
+
+int mvsw_pr_hw_port_fec_set(const struct mvsw_pr_port *port, u8 fec)
+{
+	struct mvsw_msg_port_attr_cmd req = {
+		.attr = MVSW_MSG_PORT_ATTR_FEC,
+		.port = port->hw_id,
+		.dev = port->dev_id,
+		.param = {.fec = fec}
+	};
+
+	return fw_send_req(port->sw, MVSW_MSG_TYPE_PORT_ATTR_SET, &req);
+}
+
+int mvsw_pr_hw_fw_log_level_set(const struct mvsw_pr_switch *sw,
+				u32 lib, u32 type)
+{
+	struct mvsw_msg_log_lvl_set_cmd req = {
+		.lib = lib,
+		.type = type
+	};
+	int err;
+
+	err = fw_send_req(sw, MVSW_MSG_TYPE_LOG_LEVEL_SET, &req);
+	if (err)
+		return err;
+
+	return 0;
+}
+
+int mvsw_pr_hw_port_autoneg_set(const struct mvsw_pr_port *port,
+				bool autoneg, u64 link_modes, u8 fec)
+{
+	struct mvsw_msg_port_attr_cmd req = {
+		.attr = MVSW_MSG_PORT_ATTR_AUTONEG,
+		.port = port->hw_id,
+		.dev = port->dev_id,
+		.param = {.autoneg = {.link_mode = link_modes,
+				      .enable = autoneg ? 1 : 0,
+				      .fec = fec}
+		}
+	};
+
+	return fw_send_req(port->sw, MVSW_MSG_TYPE_PORT_ATTR_SET, &req);
+}
+
+int mvsw_pr_hw_port_duplex_get(const struct mvsw_pr_port *port, u8 *duplex)
+{
+	struct mvsw_msg_port_attr_ret resp;
+	struct mvsw_msg_port_attr_cmd req = {
+		.attr = MVSW_MSG_PORT_ATTR_DUPLEX,
+		.port = port->hw_id,
+		.dev = port->dev_id
+	};
+	int err;
+
+	err = fw_send_req_resp(port->sw, MVSW_MSG_TYPE_PORT_ATTR_GET,
+			       &req, &resp);
+	if (err)
+		return err;
+
+	*duplex = resp.param.duplex;
+
+	return err;
+}
+
+int mvsw_pr_hw_port_stats_get(const struct mvsw_pr_port *port,
+			      struct mvsw_pr_port_stats *stats)
+{
+	struct mvsw_msg_port_stats_ret resp;
+	struct mvsw_msg_port_attr_cmd req = {
+		.attr = MVSW_MSG_PORT_ATTR_STATS,
+		.port = port->hw_id,
+		.dev = port->dev_id
+	};
+	u64 *hw_val = resp.stats;
+	int err;
+
+	err = fw_send_req_resp(port->sw, MVSW_MSG_TYPE_PORT_ATTR_GET,
+			       &req, &resp);
+	if (err)
+		return err;
+
+	stats->good_octets_received = hw_val[MVSW_PORT_GOOD_OCTETS_RCV_CNT];
+	stats->bad_octets_received = hw_val[MVSW_PORT_BAD_OCTETS_RCV_CNT];
+	stats->mac_trans_error = hw_val[MVSW_PORT_MAC_TRANSMIT_ERR_CNT];
+	stats->broadcast_frames_received = hw_val[MVSW_PORT_BRDC_PKTS_RCV_CNT];
+	stats->multicast_frames_received = hw_val[MVSW_PORT_MC_PKTS_RCV_CNT];
+	stats->frames_64_octets = hw_val[MVSW_PORT_PKTS_64_OCTETS_CNT];
+	stats->frames_65_to_127_octets =
+		hw_val[MVSW_PORT_PKTS_65TO127_OCTETS_CNT];
+	stats->frames_128_to_255_octets =
+		hw_val[MVSW_PORT_PKTS_128TO255_OCTETS_CNT];
+	stats->frames_256_to_511_octets =
+		hw_val[MVSW_PORT_PKTS_256TO511_OCTETS_CNT];
+	stats->frames_512_to_1023_octets =
+		hw_val[MVSW_PORT_PKTS_512TO1023_OCTETS_CNT];
+	stats->frames_1024_to_max_octets =
+		hw_val[MVSW_PORT_PKTS_1024TOMAX_OCTETS_CNT];
+	stats->excessive_collision = hw_val[MVSW_PORT_EXCESSIVE_COLLISIONS_CNT];
+	stats->multicast_frames_sent = hw_val[MVSW_PORT_MC_PKTS_SENT_CNT];
+	stats->broadcast_frames_sent = hw_val[MVSW_PORT_BRDC_PKTS_SENT_CNT];
+	stats->fc_sent = hw_val[MVSW_PORT_FC_SENT_CNT];
+	stats->fc_received = hw_val[MVSW_PORT_GOOD_FC_RCV_CNT];
+	stats->buffer_overrun = hw_val[MVSW_PORT_DROP_EVENTS_CNT];
+	stats->undersize = hw_val[MVSW_PORT_UNDERSIZE_PKTS_CNT];
+	stats->fragments = hw_val[MVSW_PORT_FRAGMENTS_PKTS_CNT];
+	stats->oversize = hw_val[MVSW_PORT_OVERSIZE_PKTS_CNT];
+	stats->jabber = hw_val[MVSW_PORT_JABBER_PKTS_CNT];
+	stats->rx_error_frame_received = hw_val[MVSW_PORT_MAC_RCV_ERROR_CNT];
+	stats->bad_crc = hw_val[MVSW_PORT_BAD_CRC_CNT];
+	stats->collisions = hw_val[MVSW_PORT_COLLISIONS_CNT];
+	stats->late_collision = hw_val[MVSW_PORT_LATE_COLLISIONS_CNT];
+	stats->unicast_frames_received = hw_val[MVSW_PORT_GOOD_UC_PKTS_RCV_CNT];
+	stats->unicast_frames_sent = hw_val[MVSW_PORT_GOOD_UC_PKTS_SENT_CNT];
+	stats->sent_multiple = hw_val[MVSW_PORT_MULTIPLE_PKTS_SENT_CNT];
+	stats->sent_deferred = hw_val[MVSW_PORT_DEFERRED_PKTS_SENT_CNT];
+	stats->good_octets_sent = hw_val[MVSW_PORT_GOOD_OCTETS_SENT_CNT];
+
+	return 0;
+}
+
+int mvsw_pr_hw_bridge_create(const struct mvsw_pr_switch *sw, u16 *bridge_id)
+{
+	struct mvsw_msg_bridge_cmd req;
+	struct mvsw_msg_bridge_ret resp;
+	int err;
+
+	err = fw_send_req_resp(sw, MVSW_MSG_TYPE_BRIDGE_CREATE, &req, &resp);
+	if (err)
+		return err;
+
+	*bridge_id = resp.bridge;
+	return err;
+}
+
+int mvsw_pr_hw_bridge_delete(const struct mvsw_pr_switch *sw, u16 bridge_id)
+{
+	struct mvsw_msg_bridge_cmd req = {
+		.bridge = bridge_id
+	};
+
+	return fw_send_req(sw, MVSW_MSG_TYPE_BRIDGE_DELETE, &req);
+}
+
+int mvsw_pr_hw_bridge_port_add(const struct mvsw_pr_port *port, u16 bridge_id)
+{
+	struct mvsw_msg_bridge_cmd req = {
+		.bridge = bridge_id,
+		.port = port->hw_id,
+		.dev = port->dev_id
+	};
+
+	return fw_send_req(port->sw, MVSW_MSG_TYPE_BRIDGE_PORT_ADD, &req);
+}
+
+int mvsw_pr_hw_bridge_port_delete(const struct mvsw_pr_port *port,
+				  u16 bridge_id)
+{
+	struct mvsw_msg_bridge_cmd req = {
+		.bridge = bridge_id,
+		.port = port->hw_id,
+		.dev = port->dev_id
+	};
+
+	return fw_send_req(port->sw, MVSW_MSG_TYPE_BRIDGE_PORT_DELETE, &req);
+}
+
+int mvsw_pr_hw_macvlan_add(const struct mvsw_pr_switch *sw, u16 vr_id,
+			   const u8 *mac, u16 vid)
+{
+	struct mvsw_msg_macvlan_cmd req = {
+		.vr_id = vr_id,
+		.vid = vid
+	};
+
+	memcpy(req.mac, mac, ETH_ALEN);
+
+	return fw_send_req(sw, MVSW_MSG_TYPE_FDB_MACVLAN_ADD, &req);
+}
+
+int mvsw_pr_hw_macvlan_del(const struct mvsw_pr_switch *sw, u16 vr_id,
+			   const u8 *mac, u16 vid)
+{
+	struct mvsw_msg_macvlan_cmd req = {
+		.vr_id = vr_id,
+		.vid = vid
+	};
+
+	memcpy(req.mac, mac, ETH_ALEN);
+
+	return fw_send_req(sw, MVSW_MSG_TYPE_FDB_MACVLAN_DEL, &req);
+}
+
+int mvsw_pr_hw_fdb_flush_port(const struct mvsw_pr_port *port, u32 mode)
+{
+	struct mvsw_msg_fdb_cmd req = {
+		.dest_type = MVSW_HW_FDB_ENTRY_TYPE_REG_PORT,
+		.dest = {
+			.dev = port->dev_id,
+			.port = port->hw_id,
+		},
+		.flush_mode = mode,
+	};
+
+	return fw_send_req(port->sw, MVSW_MSG_TYPE_FDB_FLUSH_PORT, &req);
+}
+
+int mvsw_pr_hw_fdb_flush_lag(const struct mvsw_pr_switch *sw, u16 lag_id,
+			     u32 mode)
+{
+	struct mvsw_msg_fdb_cmd req = {
+		.dest_type = MVSW_HW_FDB_ENTRY_TYPE_LAG,
+		.dest = { .lag_id = lag_id },
+		.flush_mode = mode,
+	};
+
+	return fw_send_req(sw, MVSW_MSG_TYPE_FDB_FLUSH_PORT, &req);
+}
+
+int mvsw_pr_hw_fdb_flush_vlan(const struct mvsw_pr_switch *sw, u16 vid,
+			      u32 mode)
+{
+	struct mvsw_msg_fdb_cmd req = {
+		.vid = vid,
+		.flush_mode = mode,
+	};
+
+	return fw_send_req(sw, MVSW_MSG_TYPE_FDB_FLUSH_VLAN, &req);
+}
+
+int mvsw_pr_hw_fdb_flush_port_vlan(const struct mvsw_pr_port *port, u16 vid,
+				   u32 mode)
+{
+	struct mvsw_msg_fdb_cmd req = {
+		.dest_type = MVSW_HW_FDB_ENTRY_TYPE_REG_PORT,
+		.dest = {
+			.dev = port->dev_id,
+			.port = port->hw_id,
+		},
+		.vid = vid,
+		.flush_mode = mode,
+	};
+
+	return fw_send_req(port->sw, MVSW_MSG_TYPE_FDB_FLUSH_PORT_VLAN, &req);
+}
+
+int mvsw_pr_hw_fdb_flush_lag_vlan(const struct mvsw_pr_switch *sw,
+				  u16 lag_id, u16 vid, u32 mode)
+{
+	struct mvsw_msg_fdb_cmd req = {
+		.dest_type = MVSW_HW_FDB_ENTRY_TYPE_LAG,
+		.dest = { .lag_id = lag_id },
+		.vid = vid,
+		.flush_mode = mode,
+	};
+
+	return fw_send_req(sw, MVSW_MSG_TYPE_FDB_FLUSH_PORT_VLAN, &req);
+}
+
+int mvsw_pr_hw_port_link_mode_get(const struct mvsw_pr_port *port,
+				  u32 *mode)
+{
+	struct mvsw_msg_port_attr_ret resp;
+	struct mvsw_msg_port_attr_cmd req = {
+		.attr = MVSW_MSG_PORT_ATTR_LINK_MODE,
+		.port = port->hw_id,
+		.dev = port->dev_id
+	};
+	int err;
+
+	err = fw_send_req_resp(port->sw, MVSW_MSG_TYPE_PORT_ATTR_GET,
+			       &req, &resp);
+	if (err)
+		return err;
+
+	*mode = resp.param.link_mode;
+
+	return err;
+}
+
+int mvsw_pr_hw_port_link_mode_set(const struct mvsw_pr_port *port,
+				  u32 mode)
+{
+	struct mvsw_msg_port_attr_cmd req = {
+		.attr = MVSW_MSG_PORT_ATTR_LINK_MODE,
+		.port = port->hw_id,
+		.dev = port->dev_id,
+		.param = {.link_mode = mode}
+	};
+
+	return fw_send_req(port->sw, MVSW_MSG_TYPE_PORT_ATTR_SET, &req);
+}
+
+static int mvsw_pr_iface_to_msg(struct mvsw_pr_iface *iface,
+				struct mvsw_msg_iface *msg_if)
+{
+	switch (iface->type) {
+	case MVSW_IF_PORT_E:
+	case MVSW_IF_VID_E:
+		msg_if->port = iface->dev_port.port_num;
+		msg_if->dev = iface->dev_port.hw_dev_num;
+		break;
+	case MVSW_IF_LAG_E:
+		msg_if->lag_id = iface->lag_id;
+		break;
+	default:
+		return -ENOTSUPP;
+	}
+
+	msg_if->vr_id = iface->vr_id;
+	msg_if->vid = iface->vlan_id;
+	msg_if->type = iface->type;
+	return 0;
+}
+
+int mvsw_pr_hw_rif_create(const struct mvsw_pr_switch *sw,
+			  struct mvsw_pr_iface *iif, u8 *mac, u16 *rif_id)
+{
+	struct mvsw_msg_rif_cmd req;
+	struct mvsw_msg_rif_ret resp;
+	int err;
+
+	memcpy(req.mac, mac, ETH_ALEN);
+
+	err = mvsw_pr_iface_to_msg(iif, &req.iif);
+	if (err)
+		return err;
+
+	err = fw_send_req_resp(sw, MVSW_MSG_TYPE_ROUTER_RIF_CREATE,
+			       &req, &resp);
+	if (err)
+		return err;
+
+	*rif_id = resp.rif_id;
+	return err;
+}
+
+int mvsw_pr_hw_rif_delete(const struct mvsw_pr_switch *sw, u16 rif_id,
+			  struct mvsw_pr_iface *iif)
+{
+	struct mvsw_msg_rif_cmd req = {
+		.rif_id = rif_id,
+	};
+	int err;
+
+	err = mvsw_pr_iface_to_msg(iif, &req.iif);
+	if (err)
+		return err;
+
+	return fw_send_req(sw, MVSW_MSG_TYPE_ROUTER_RIF_DELETE, &req);
+}
+
+int mvsw_pr_hw_rif_set(const struct mvsw_pr_switch *sw, u16 *rif_id,
+		       struct mvsw_pr_iface *iif, u8 *mac)
+{
+	struct mvsw_msg_rif_ret resp;
+	struct mvsw_msg_rif_cmd req = {
+		.rif_id = *rif_id,
+	};
+	int err;
+
+	memcpy(req.mac, mac, ETH_ALEN);
+
+	err = mvsw_pr_iface_to_msg(iif, &req.iif);
+	if (err)
+		return err;
+
+	err = fw_send_req_resp(sw, MVSW_MSG_TYPE_ROUTER_RIF_SET, &req, &resp);
+	if (err)
+		return err;
+
+	*rif_id = resp.rif_id;
+	return err;
+}
+
+int mvsw_pr_hw_vr_create(const struct mvsw_pr_switch *sw, u16 *vr_id)
+{
+	int err;
+	struct mvsw_msg_vr_ret resp;
+	struct mvsw_msg_vr_cmd req;
+
+	err = fw_send_req_resp(sw, MVSW_MSG_TYPE_ROUTER_VR_CREATE, &req, &resp);
+	if (err)
+		return err;
+
+	*vr_id = resp.vr_id;
+	return err;
+}
+
+int mvsw_pr_hw_vr_delete(const struct mvsw_pr_switch *sw, u16 vr_id)
+{
+	struct mvsw_msg_vr_cmd req = {
+		.vr_id = vr_id,
+	};
+
+	return fw_send_req(sw, MVSW_MSG_TYPE_ROUTER_VR_DELETE, &req);
+}
+
+int mvsw_pr_hw_vr_abort(const struct mvsw_pr_switch *sw, u16 vr_id)
+{
+	struct mvsw_msg_vr_cmd req = {
+		.vr_id = vr_id,
+	};
+
+	return fw_send_req(sw, MVSW_MSG_TYPE_ROUTER_VR_ABORT, &req);
+}
+
+int mvsw_pr_hw_lpm_add(const struct mvsw_pr_switch *sw, u16 vr_id,
+		       __be32 dst, u32 dst_len, u32 grp_id)
+{
+	struct mvsw_msg_lpm_cmd req = {
+		.dst = dst,
+		.dst_len = dst_len,
+		.vr_id = vr_id,
+		.grp_id = grp_id
+	};
+
+	return fw_send_req(sw, MVSW_MSG_TYPE_ROUTER_LPM_ADD, &req);
+}
+
+int mvsw_pr_hw_lpm_del(const struct mvsw_pr_switch *sw, u16 vr_id, __be32 dst,
+		       u32 dst_len)
+{
+	struct mvsw_msg_lpm_cmd req = {
+		.dst = dst,
+		.dst_len = dst_len,
+		.vr_id = vr_id,
+	};
+
+	return fw_send_req(sw, MVSW_MSG_TYPE_ROUTER_LPM_DELETE, &req);
+}
+
+int mvsw_pr_hw_nh_entries_set(const struct mvsw_pr_switch *sw, int count,
+			      struct mvsw_pr_neigh_info *nhs, u32 grp_id)
+{
+	struct mvsw_msg_nh_cmd req = { .size = count, .grp_id = grp_id };
+	int i, err;
+
+	for (i = 0; i < count; i++) {
+		req.nh[i].is_active = nhs[i].connected;
+		memcpy(&req.nh[i].mac, nhs[i].ha, ETH_ALEN);
+		err = mvsw_pr_iface_to_msg(&nhs[i].iface, &req.nh[i].oif);
+		if (err)
+			return err;
+	}
+
+	return fw_send_req(sw, MVSW_MSG_TYPE_ROUTER_NH_GRP_SET, &req);
+}
+
+/* TODO: more than one nh */
+/* For now "count = 1" supported only */
+int mvsw_pr_hw_nh_entries_get(const struct mvsw_pr_switch *sw, int count,
+			      struct mvsw_pr_neigh_info *nhs, u32 grp_id)
+{
+	struct mvsw_msg_nh_cmd req = { .size = count, .grp_id = grp_id };
+	struct mvsw_msg_nh_ret resp;
+	int err, i;
+
+	err = fw_send_req_resp(sw, MVSW_MSG_TYPE_ROUTER_NH_GRP_GET,
+			       &req, &resp);
+	if (err)
+		return err;
+
+	for (i = 0; i < count; i++)
+		nhs[i].connected = resp.nh[i].is_active;
+
+	return err;
+}
+
+int mvsw_pr_hw_nh_group_create(const struct mvsw_pr_switch *sw, u16 nh_count,
+			       u32 *grp_id)
+{
+	struct mvsw_msg_nh_grp_cmd req = { .size = nh_count };
+	struct mvsw_msg_nh_grp_ret resp;
+	int err;
+
+	err = fw_send_req_resp(sw, MVSW_MSG_TYPE_ROUTER_NH_GRP_ADD, &req,
+			       &resp);
+	if (err)
+		return err;
+
+	*grp_id = resp.grp_id;
+	return err;
+}
+
+int mvsw_pr_hw_nh_group_delete(const struct mvsw_pr_switch *sw, u16 nh_count,
+			       u32 grp_id)
+{
+	struct mvsw_msg_nh_grp_cmd req = {
+	    .grp_id = grp_id,
+	    .size = nh_count
+	};
+
+	return fw_send_req(sw, MVSW_MSG_TYPE_ROUTER_NH_GRP_DELETE, &req);
+}
+
+int mvsw_pr_hw_mp4_hash_set(const struct mvsw_pr_switch *sw, u8 hash_policy)
+{
+	struct mvsw_msg_mp_cmd req = { .hash_policy = hash_policy};
+
+	return fw_send_req(sw, MVSW_MSG_TYPE_ROUTER_MP_HASH_SET, &req);
+}
+
+int mvsw_pr_hw_rxtx_init(const struct mvsw_pr_switch *sw, bool use_sdma,
+			 u32 *map_addr)
+{
+	struct mvsw_msg_rxtx_ret resp;
+	struct mvsw_msg_rxtx_cmd req;
+	int err;
+
+	req.use_sdma = use_sdma;
+
+	err = fw_send_req_resp(sw, MVSW_MSG_TYPE_RXTX_INIT, &req, &resp);
+	if (err)
+		return err;
+
+	if (map_addr)
+		*map_addr = resp.map_addr;
+
+	return 0;
+}
+
+int mvsw_pr_hw_port_autoneg_restart(struct mvsw_pr_port *port)
+{
+	struct mvsw_msg_port_attr_cmd req = {
+		.attr = MVSW_MSG_PORT_ATTR_AUTONEG_RESTART,
+		.port = port->hw_id,
+		.dev = port->dev_id,
+	};
+
+	return fw_send_req(port->sw, MVSW_MSG_TYPE_PORT_ATTR_SET, &req);
+}
+
+int mvsw_pr_hw_port_remote_fc_get(const struct mvsw_pr_port *port,
+				  bool *pause, bool *asym_pause)
+{
+	struct mvsw_msg_port_attr_ret resp;
+	struct mvsw_msg_port_attr_cmd req = {
+		.attr = MVSW_MSG_PORT_ATTR_REMOTE_FC,
+		.port = port->hw_id,
+		.dev = port->dev_id
+	};
+	int err;
+
+	err = fw_send_req_resp(port->sw, MVSW_MSG_TYPE_PORT_ATTR_GET,
+			       &req, &resp);
+	if (err)
+		return err;
+
+	switch (resp.param.fc) {
+	case MVSW_FC_SYMMETRIC:
+		*pause = true;
+		*asym_pause = false;
+		break;
+	case MVSW_FC_ASYMMETRIC:
+		*pause = false;
+		*asym_pause = true;
+		break;
+	case MVSW_FC_SYMM_ASYMM:
+		*pause = true;
+		*asym_pause = true;
+		break;
+	default:
+		*pause = false;
+		*asym_pause = false;
+	};
+
+	return err;
+}
+
+/* ACL API */
+int mvsw_pr_hw_acl_ruleset_create(const struct mvsw_pr_switch *sw,
+				  u16 *ruleset_id)
+{
+	int err;
+	struct mvsw_msg_acl_ruleset_ret resp;
+	struct mvsw_msg_acl_ruleset_cmd req;
+
+	err = fw_send_req_resp(sw, MVSW_MSG_TYPE_ACL_RULESET_CREATE,
+			       &req, &resp);
+	if (err)
+		return err;
+
+	*ruleset_id = resp.id;
+	return 0;
+}
+
+int mvsw_pr_hw_acl_ruleset_del(const struct mvsw_pr_switch *sw,
+			       u16 ruleset_id)
+{
+	struct mvsw_msg_acl_ruleset_cmd req = {
+		.id = ruleset_id,
+	};
+
+	return fw_send_req(sw, MVSW_MSG_TYPE_ACL_RULESET_DELETE, &req);
+}
+
+static int acl_rule_add_put_actions(struct mvsw_msg_buff *msg,
+				    struct prestera_acl_rule *rule)
+{
+	struct list_head *a_list = prestera_acl_rule_action_list_get(rule);
+	u8 n_actions = prestera_acl_rule_action_len(rule);
+	struct prestera_acl_rule_action_entry *a_entry;
+	__be64 be64;
+	int err;
+
+	err = mvsw_msg_buff_put(msg, &n_actions, sizeof(n_actions));
+	if (err)
+		return err;
+
+	list_for_each_entry(a_entry, a_list, list) {
+		err = mvsw_msg_buff_put(msg, (u8 *)&a_entry->id, sizeof(u8));
+		if (err)
+			return err;
+
+		switch (a_entry->id) {
+		case MVSW_ACL_RULE_ACTION_ACCEPT:
+		case MVSW_ACL_RULE_ACTION_DROP:
+		case MVSW_ACL_RULE_ACTION_TRAP:
+			/* just rule action id, no specific data */
+			break;
+		case MVSW_ACL_RULE_ACTION_POLICE:
+			be64 = cpu_to_be64(a_entry->police.rate);
+			err = mvsw_msg_buff_put(msg, &be64, sizeof(be64));
+			be64 = cpu_to_be64(a_entry->police.burst);
+			err = mvsw_msg_buff_put(msg, &be64, sizeof(be64));
+			break;
+		default:
+			err = -EINVAL;
+		}
+		if (err)
+			return err;
+	}
+
+	return 0;
+}
+
+static int acl_rule_add_put_matches(struct mvsw_msg_buff *msg,
+				    struct prestera_acl_rule *rule)
+{
+	struct list_head *m_list = prestera_acl_rule_match_list_get(rule);
+	struct prestera_acl_rule_match_entry *m_entry;
+	int err;
+
+	list_for_each_entry(m_entry, m_list, list) {
+		err = mvsw_msg_buff_put(msg, (u8 *)&m_entry->type, sizeof(u8));
+		if (err)
+			return err;
+
+		switch (m_entry->type) {
+		case MVSW_ACL_RULE_MATCH_ENTRY_TYPE_ETH_TYPE:
+		case MVSW_ACL_RULE_MATCH_ENTRY_TYPE_L4_PORT_SRC:
+		case MVSW_ACL_RULE_MATCH_ENTRY_TYPE_L4_PORT_DST:
+		case MVSW_ACL_RULE_MATCH_ENTRY_TYPE_VLAN_ID:
+		case MVSW_ACL_RULE_MATCH_ENTRY_TYPE_VLAN_TPID:
+			err = mvsw_msg_buff_put
+				(msg, &m_entry->keymask.u16.key,
+				 sizeof(m_entry->keymask.u16.key));
+			err |= mvsw_msg_buff_put
+				(msg, &m_entry->keymask.u16.mask,
+				 sizeof(m_entry->keymask.u16.mask));
+			break;
+		case MVSW_ACL_RULE_MATCH_ENTRY_TYPE_ICMP_TYPE:
+		case MVSW_ACL_RULE_MATCH_ENTRY_TYPE_ICMP_CODE:
+		case MVSW_ACL_RULE_MATCH_ENTRY_TYPE_IP_PROTO:
+			err = mvsw_msg_buff_put
+				(msg, &m_entry->keymask.u8.key,
+				 sizeof(m_entry->keymask.u8.key));
+			err |= mvsw_msg_buff_put
+				(msg, &m_entry->keymask.u8.mask,
+				 sizeof(m_entry->keymask.u8.mask));
+			break;
+		case MVSW_ACL_RULE_MATCH_ENTRY_TYPE_ETH_SMAC:
+		case MVSW_ACL_RULE_MATCH_ENTRY_TYPE_ETH_DMAC:
+			err = mvsw_msg_buff_put
+				(msg, &m_entry->keymask.mac.key,
+				 sizeof(m_entry->keymask.mac.key));
+			err |= mvsw_msg_buff_put
+				(msg, &m_entry->keymask.mac.mask,
+				 sizeof(m_entry->keymask.mac.mask));
+			break;
+		case MVSW_ACL_RULE_MATCH_ENTRY_TYPE_IP_SRC:
+		case MVSW_ACL_RULE_MATCH_ENTRY_TYPE_IP_DST:
+		case MVSW_ACL_RULE_MATCH_ENTRY_TYPE_L4_PORT_RANGE_SRC:
+		case MVSW_ACL_RULE_MATCH_ENTRY_TYPE_L4_PORT_RANGE_DST:
+			err = mvsw_msg_buff_put
+				(msg, &m_entry->keymask.u32.key,
+				 sizeof(m_entry->keymask.u32.key));
+			err |= mvsw_msg_buff_put
+				(msg, &m_entry->keymask.u32.mask,
+				 sizeof(m_entry->keymask.u32.mask));
+			break;
+		case MVSW_ACL_RULE_MATCH_ENTRY_TYPE_PORT:
+			err = mvsw_msg_buff_put
+				(msg, &m_entry->keymask.u64.key,
+				 sizeof(m_entry->keymask.u64.key));
+			err |= mvsw_msg_buff_put
+				(msg, &m_entry->keymask.u64.mask,
+				 sizeof(m_entry->keymask.u64.mask));
+			break;
+		default:
+			err = -EINVAL;
+		}
+		if (err)
+			return err;
+	}
+
+	return 0;
+}
+
+int mvsw_pr_hw_acl_rule_add(const struct mvsw_pr_switch *sw,
+			    struct prestera_acl_rule *rule,
+			    u32 *rule_id)
+{
+	int err;
+	struct mvsw_msg_acl_rule_ret resp;
+	u8 hw_tc = prestera_acl_rule_hw_tc_get(rule);
+	u32 priority = prestera_acl_rule_priority_get(rule);
+	u16 ruleset_id = prestera_acl_rule_ruleset_id_get(rule);
+	struct mvsw_msg_acl_rule_cmd *req;
+	struct mvsw_msg_buff *msg;
+
+	msg = mvsw_msg_buff_create(sizeof(*req));
+	if (!msg)
+		return -ENOMEM;
+
+	/* put priority first */
+	err = mvsw_msg_buff_put(msg, &priority, sizeof(priority));
+	if (err)
+		goto free_msg;
+
+	/* put hw_tc into the message */
+	err = mvsw_msg_buff_put(msg, &hw_tc, sizeof(hw_tc));
+	if (err)
+		goto free_msg;
+
+	/* put acl actions into the message */
+	err = acl_rule_add_put_actions(msg, rule);
+	if (err)
+		goto free_msg;
+
+	/* put acl matches into the message */
+	err = acl_rule_add_put_matches(msg, rule);
+	if (err)
+		goto free_msg;
+
+	/* terminate message */
+	err = mvsw_msg_buff_terminate(msg);
+	if (err)
+		goto free_msg;
+
+	req = (struct mvsw_msg_acl_rule_cmd *)mvsw_msg_buff_data(msg);
+
+	req->ruleset_id = ruleset_id;
+
+	err = fw_send_nreq_resp(sw, MVSW_MSG_TYPE_ACL_RULE_ADD, req,
+				mvsw_msg_buff_size(msg), &resp);
+	if (err)
+		goto free_msg;
+
+	*rule_id = resp.id;
+free_msg:
+	mvsw_msg_buff_destroy(msg);
+	return err;
+}
+
+int mvsw_pr_hw_acl_rule_del(const struct mvsw_pr_switch *sw, u32 rule_id)
+{
+	struct mvsw_msg_acl_rule_cmd req = {
+		.id = rule_id
+	};
+
+	return fw_send_req(sw, MVSW_MSG_TYPE_ACL_RULE_DELETE, &req);
+}
+
+int mvsw_pr_hw_acl_rule_stats_get(const struct mvsw_pr_switch *sw, u32 rule_id,
+				  u64 *packets, u64 *bytes)
+{
+	int err;
+	struct mvsw_msg_acl_rule_stats_ret resp;
+	struct mvsw_msg_acl_rule_cmd req = {
+		.id = rule_id
+	};
+
+	err = fw_send_req_resp(sw, MVSW_MSG_TYPE_ACL_RULE_STATS_GET,
+			       &req, &resp);
+	if (err)
+		return err;
+
+	*packets = resp.packets;
+	*bytes = resp.bytes;
+	return 0;
+}
+
+int mvsw_pr_hw_acl_port_bind(const struct mvsw_pr_port *port, u16 ruleset_id)
+{
+	struct mvsw_msg_acl_ruleset_bind_cmd req = {
+		.port = port->hw_id,
+		.dev = port->dev_id,
+		.ruleset_id = ruleset_id,
+	};
+
+	return fw_send_req(port->sw, MVSW_MSG_TYPE_ACL_PORT_BIND, &req);
+}
+
+int mvsw_pr_hw_acl_port_unbind(const struct mvsw_pr_port *port, u16 ruleset_id)
+{
+	struct mvsw_msg_acl_ruleset_bind_cmd req = {
+		.port = port->hw_id,
+		.dev = port->dev_id,
+		.ruleset_id = ruleset_id,
+	};
+
+	return fw_send_req(port->sw, MVSW_MSG_TYPE_ACL_PORT_UNBIND, &req);
+}
+
+int mvsw_pr_hw_lag_member_add(struct mvsw_pr_port *port, u16 lag_id)
+{
+	struct mvsw_msg_lag_cmd req = {
+		.port = port->hw_id,
+		.dev = port->dev_id,
+		.lag_id = lag_id
+	};
+
+	return fw_send_req(port->sw, MVSW_MSG_TYPE_LAG_ADD, &req);
+}
+
+int mvsw_pr_hw_lag_member_del(struct mvsw_pr_port *port, u16 lag_id)
+{
+	struct mvsw_msg_lag_cmd req = {
+		.port = port->hw_id,
+		.dev = port->dev_id,
+		.lag_id = lag_id
+	};
+
+	return fw_send_req(port->sw, MVSW_MSG_TYPE_LAG_DELETE, &req);
+}
+
+int mvsw_pr_hw_lag_member_enable(struct mvsw_pr_port *port, u16 lag_id,
+				 bool enable)
+{
+	u32 cmd;
+	struct mvsw_msg_lag_cmd req = {
+		.port = port->hw_id,
+		.dev = port->dev_id,
+		.lag_id = lag_id
+	};
+
+	cmd = enable ? MVSW_MSG_TYPE_LAG_ENABLE : MVSW_MSG_TYPE_LAG_DISABLE;
+	return fw_send_req(port->sw, cmd, &req);
+}
+
+int mvsw_pr_hw_lag_member_rif_leave(const struct mvsw_pr_port *port,
+				    u16 lag_id, u16 vr_id)
+{
+	struct mvsw_msg_lag_cmd req = {
+		.port = port->hw_id,
+		.dev = port->dev_id,
+		.lag_id = lag_id,
+		.vr_id = vr_id
+	};
+
+	return fw_send_req(port->sw, MVSW_MSG_TYPE_LAG_ROUTER_LEAVE, &req);
+}
diff --git a/drivers/net/ethernet/marvell/prestera_sw/prestera_hw.h b/drivers/net/ethernet/marvell/prestera_sw/prestera_hw.h
new file mode 100644
index 000000000..ad6c4dadd
--- /dev/null
+++ b/drivers/net/ethernet/marvell/prestera_sw/prestera_hw.h
@@ -0,0 +1,324 @@
+/* SPDX-License-Identifier: BSD-3-Clause OR GPL-2.0
+ *
+ * Copyright (c) 2019-2020 Marvell International Ltd. All rights reserved.
+ *
+ */
+
+#ifndef _MVSW_PRESTERA_HW_H_
+#define _MVSW_PRESTERA_HW_H_
+
+#include <linux/types.h>
+
+enum mvsw_pr_accept_frame_type {
+	MVSW_ACCEPT_FRAME_TYPE_TAGGED,
+	MVSW_ACCEPT_FRAME_TYPE_UNTAGGED,
+	MVSW_ACCEPT_FRAME_TYPE_ALL
+};
+
+enum {
+	MVSW_LINK_MODE_10baseT_Half_BIT,
+	MVSW_LINK_MODE_10baseT_Full_BIT,
+	MVSW_LINK_MODE_100baseT_Half_BIT,
+	MVSW_LINK_MODE_100baseT_Full_BIT,
+	MVSW_LINK_MODE_1000baseT_Half_BIT,
+	MVSW_LINK_MODE_1000baseT_Full_BIT,
+	MVSW_LINK_MODE_1000baseX_Full_BIT,
+	MVSW_LINK_MODE_1000baseKX_Full_BIT,
+	MVSW_LINK_MODE_2500baseX_Full_BIT,
+	MVSW_LINK_MODE_10GbaseKR_Full_BIT,
+	MVSW_LINK_MODE_10GbaseSR_Full_BIT,
+	MVSW_LINK_MODE_10GbaseLR_Full_BIT,
+	MVSW_LINK_MODE_20GbaseKR2_Full_BIT,
+	MVSW_LINK_MODE_25GbaseCR_Full_BIT,
+	MVSW_LINK_MODE_25GbaseKR_Full_BIT,
+	MVSW_LINK_MODE_25GbaseSR_Full_BIT,
+	MVSW_LINK_MODE_40GbaseKR4_Full_BIT,
+	MVSW_LINK_MODE_40GbaseCR4_Full_BIT,
+	MVSW_LINK_MODE_40GbaseSR4_Full_BIT,
+	MVSW_LINK_MODE_50GbaseCR2_Full_BIT,
+	MVSW_LINK_MODE_50GbaseKR2_Full_BIT,
+	MVSW_LINK_MODE_50GbaseSR2_Full_BIT,
+	MVSW_LINK_MODE_100GbaseKR4_Full_BIT,
+	MVSW_LINK_MODE_100GbaseSR4_Full_BIT,
+	MVSW_LINK_MODE_100GbaseCR4_Full_BIT,
+	MVSW_LINK_MODE_MAX,
+};
+
+enum {
+	MVSW_PORT_TYPE_NONE,
+	MVSW_PORT_TYPE_TP,
+	MVSW_PORT_TYPE_AUI,
+	MVSW_PORT_TYPE_MII,
+	MVSW_PORT_TYPE_FIBRE,
+	MVSW_PORT_TYPE_BNC,
+	MVSW_PORT_TYPE_DA,
+	MVSW_PORT_TYPE_OTHER,
+	MVSW_PORT_TYPE_MAX,
+};
+
+enum {
+	MVSW_PORT_TRANSCEIVER_COPPER,
+	MVSW_PORT_TRANSCEIVER_SFP,
+	MVSW_PORT_TRANSCEIVER_MAX,
+};
+
+enum {
+	MVSW_PORT_FEC_OFF_BIT,
+	MVSW_PORT_FEC_BASER_BIT,
+	MVSW_PORT_FEC_RS_BIT,
+	MVSW_PORT_FEC_MAX,
+};
+
+enum {
+	MVSW_PORT_DUPLEX_HALF,
+	MVSW_PORT_DUPLEX_FULL
+};
+
+enum {
+	MVSW_STP_DISABLED,
+	MVSW_STP_BLOCK_LISTEN,
+	MVSW_STP_LEARN,
+	MVSW_STP_FORWARD
+};
+
+enum {
+	MVSW_FW_LOG_LIB_BRIDGE = 0,
+	MVSW_FW_LOG_LIB_CNC,
+	MVSW_FW_LOG_LIB_CONFIG,
+	MVSW_FW_LOG_LIB_COS,
+	MVSW_FW_LOG_LIB_HW_INIT,
+	MVSW_FW_LOG_LIB_CSCD,
+	MVSW_FW_LOG_LIB_CUT_THROUGH,
+	MVSW_FW_LOG_LIB_DIAG,
+	MVSW_FW_LOG_LIB_FABRIC,
+	MVSW_FW_LOG_LIB_IP,
+	MVSW_FW_LOG_LIB_IPFIX,
+	MVSW_FW_LOG_LIB_IP_LPM,
+	MVSW_FW_LOG_LIB_L2_MLL,
+	MVSW_FW_LOG_LIB_LOGICAL_TARGET,
+	MVSW_FW_LOG_LIB_LPM,
+	MVSW_FW_LOG_LIB_MIRROR,
+	MVSW_FW_LOG_LIB_MULTI_PORT_GROUP,
+	MVSW_FW_LOG_LIB_NETWORK_IF,
+	MVSW_FW_LOG_LIB_NST,
+	MVSW_FW_LOG_LIB_OAM,
+	MVSW_FW_LOG_LIB_PCL,
+	MVSW_FW_LOG_LIB_PHY,
+	MVSW_FW_LOG_LIB_POLICER,
+	MVSW_FW_LOG_LIB_PORT,
+	MVSW_FW_LOG_LIB_PROTECTION,
+	MVSW_FW_LOG_LIB_PTP,
+	MVSW_FW_LOG_LIB_SYSTEM_RECOVERY,
+	MVSW_FW_LOG_LIB_TCAM,
+	MVSW_FW_LOG_LIB_TM_GLUE,
+	MVSW_FW_LOG_LIB_TRUNK,
+	MVSW_FW_LOG_LIB_TTI,
+	MVSW_FW_LOG_LIB_TUNNEL,
+	MVSW_FW_LOG_LIB_VNT,
+	MVSW_FW_LOG_LIB_RESOURCE_MANAGER,
+	MVSW_FW_LOG_LIB_VERSION,
+	MVSW_FW_LOG_LIB_TM,
+	MVSW_FW_LOG_LIB_SMI,
+	MVSW_FW_LOG_LIB_INIT,
+	MVSW_FW_LOG_LIB_DRAGONITE,
+	MVSW_FW_LOG_LIB_VIRTUAL_TCAM,
+	MVSW_FW_LOG_LIB_INGRESS,
+	MVSW_FW_LOG_LIB_EGRESS,
+	MVSW_FW_LOG_LIB_LATENCY_MONITORING,
+	MVSW_FW_LOG_LIB_TAM,
+	MVSW_FW_LOG_LIB_EXACT_MATCH,
+	MVSW_FW_LOG_LIB_PHA,
+	MVSW_FW_LOG_LIB_PACKET_ANALYZER,
+	MVSW_FW_LOG_LIB_FLOW_MANAGER,
+	MVSW_FW_LOG_LIB_BRIDGE_FDB_MANAGER,
+	MVSW_FW_LOG_LIB_I2C,
+	MVSW_FW_LOG_LIB_PPU,
+	MVSW_FW_LOG_LIB_EXACT_MATCH_MANAGER,
+	MVSW_FW_LOG_LIB_MAC_SEC,
+	MVSW_FW_LOG_LIB_ALL,
+
+	MVSW_FW_LOG_LIB_MAX
+};
+
+enum {
+	MVSW_FW_LOG_TYPE_INFO = 0,
+	MVSW_FW_LOG_TYPE_ENTRY_LEVEL_FUNCTION,
+	MVSW_FW_LOG_TYPE_ERROR,
+	MVSW_FW_LOG_TYPE_ALL,
+	MVSW_FW_LOG_TYPE_NONE,
+
+	MVSW_FW_LOG_TYPE_MAX
+};
+
+struct mvsw_pr_switch;
+struct mvsw_pr_port;
+struct mvsw_pr_port_stats;
+struct mvsw_pr_port_caps;
+struct prestera_acl_rule;
+struct mvsw_pr_iface;
+struct mvsw_pr_neigh_info;
+
+enum mvsw_pr_event_type;
+struct mvsw_pr_event;
+
+/* Switch API */
+int mvsw_pr_hw_switch_init(struct mvsw_pr_switch *sw);
+int mvsw_pr_hw_switch_ageing_set(const struct mvsw_pr_switch *sw,
+				 u32 ageing_time);
+int mvsw_pr_hw_switch_mac_set(const struct mvsw_pr_switch *sw, const u8 *mac);
+int mvsw_pr_hw_switch_trap_policer_set(const struct mvsw_pr_switch *sw,
+				       u8 profile);
+
+/* Port API */
+int mvsw_pr_hw_port_info_get(const struct mvsw_pr_port *port,
+			     u16 *fp_id, u32 *hw_id, u32 *dev_id);
+int mvsw_pr_hw_port_state_set(const struct mvsw_pr_port *port,
+			      bool admin_state);
+int mvsw_pr_hw_port_state_get(const struct mvsw_pr_port *port,
+			      bool *admin_state, bool *oper_state);
+int mvsw_pr_hw_port_mtu_set(const struct mvsw_pr_port *port, u32 mtu);
+int mvsw_pr_hw_port_mtu_get(const struct mvsw_pr_port *port, u32 *mtu);
+int mvsw_pr_hw_port_mac_set(const struct mvsw_pr_port *port, char *mac);
+int mvsw_pr_hw_port_mac_get(const struct mvsw_pr_port *port, char *mac);
+int mvsw_pr_hw_port_accept_frame_type_set(const struct mvsw_pr_port *port,
+					  enum mvsw_pr_accept_frame_type type);
+int mvsw_pr_hw_port_learning_set(const struct mvsw_pr_port *port, bool enable);
+int mvsw_pr_hw_port_speed_get(const struct mvsw_pr_port *port, u32 *speed);
+int mvsw_pr_hw_port_uc_flood_set(const struct mvsw_pr_port *port, bool flood);
+int mvsw_pr_hw_port_mc_flood_set(const struct mvsw_pr_port *port, bool flood);
+int mvsw_pr_hw_port_cap_get(const struct mvsw_pr_port *port,
+			    struct mvsw_pr_port_caps *caps);
+int mvsw_pr_hw_port_remote_cap_get(const struct mvsw_pr_port *port,
+				   u64 *link_mode_bitmap);
+int mvsw_pr_hw_port_remote_fc_get(const struct mvsw_pr_port *port,
+				  bool *pause, bool *asym_pause);
+int mvsw_pr_hw_port_type_get(const struct mvsw_pr_port *port, u8 *type);
+int mvsw_pr_hw_port_fec_get(const struct mvsw_pr_port *port, u8 *fec);
+int mvsw_pr_hw_port_fec_set(const struct mvsw_pr_port *port, u8 fec);
+int mvsw_pr_hw_port_autoneg_set(const struct mvsw_pr_port *port,
+				bool autoneg, u64 link_modes, u8 fec);
+int mvsw_pr_hw_port_duplex_get(const struct mvsw_pr_port *port, u8 *duplex);
+int mvsw_pr_hw_port_stats_get(const struct mvsw_pr_port *port,
+			      struct mvsw_pr_port_stats *stats);
+int mvsw_pr_hw_port_link_mode_get(const struct mvsw_pr_port *port,
+				  u32 *mode);
+int mvsw_pr_hw_port_link_mode_set(const struct mvsw_pr_port *port,
+				  u32 mode);
+int mvsw_pr_hw_port_mdix_get(const struct mvsw_pr_port *port, u8 *status,
+			     u8 *admin_mode);
+int mvsw_pr_hw_port_mdix_set(const struct mvsw_pr_port *port, u8 mode);
+int mvsw_pr_hw_port_autoneg_restart(struct mvsw_pr_port *port);
+
+/* Vlan API */
+int mvsw_pr_hw_vlan_create(const struct mvsw_pr_switch *sw, u16 vid);
+int mvsw_pr_hw_vlan_delete(const struct mvsw_pr_switch *sw, u16 vid);
+int mvsw_pr_hw_vlan_port_set(const struct mvsw_pr_port *port,
+			     u16 vid, bool is_member, bool untagged);
+int mvsw_pr_hw_vlan_port_vid_set(const struct mvsw_pr_port *port, u16 vid);
+
+/* FDB API */
+int mvsw_pr_hw_fdb_add(const struct mvsw_pr_port *port,
+		       const unsigned char *mac, u16 vid, bool dynamic);
+int mvsw_pr_hw_fdb_del(const struct mvsw_pr_port *port,
+		       const unsigned char *mac, u16 vid);
+int mvsw_pr_hw_fdb_flush_port(const struct mvsw_pr_port *port, u32 mode);
+int mvsw_pr_hw_fdb_flush_vlan(const struct mvsw_pr_switch *sw, u16 vid,
+			      u32 mode);
+int mvsw_pr_hw_fdb_flush_port_vlan(const struct mvsw_pr_port *port, u16 vid,
+				   u32 mode);
+int mvsw_pr_hw_macvlan_add(const struct mvsw_pr_switch *sw, u16 vr_id,
+			   const u8 *mac, u16 vid);
+int mvsw_pr_hw_macvlan_del(const struct mvsw_pr_switch *sw, u16 vr_id,
+			   const u8 *mac, u16 vid);
+
+/* Bridge API */
+int mvsw_pr_hw_bridge_create(const struct mvsw_pr_switch *sw, u16 *bridge_id);
+int mvsw_pr_hw_bridge_delete(const struct mvsw_pr_switch *sw, u16 bridge_id);
+int mvsw_pr_hw_bridge_port_add(const struct mvsw_pr_port *port, u16 bridge_id);
+int mvsw_pr_hw_bridge_port_delete(const struct mvsw_pr_port *port,
+				  u16 bridge_id);
+
+/* STP API */
+int mvsw_pr_hw_port_vid_stp_set(struct mvsw_pr_port *port, u16 vid, u8 state);
+
+/* ACL API */
+int mvsw_pr_hw_acl_ruleset_create(const struct mvsw_pr_switch *sw,
+				  u16 *ruleset_id);
+int mvsw_pr_hw_acl_ruleset_del(const struct mvsw_pr_switch *sw,
+			       u16 ruleset_id);
+int mvsw_pr_hw_acl_rule_add(const struct mvsw_pr_switch *sw,
+			    struct prestera_acl_rule *rule,
+			    u32 *rule_id);
+int mvsw_pr_hw_acl_rule_del(const struct mvsw_pr_switch *sw, u32 rule_id);
+int mvsw_pr_hw_acl_rule_stats_get(const struct mvsw_pr_switch *sw, u32 rule_id,
+				  u64 *packets, u64 *bytes);
+int mvsw_pr_hw_acl_port_bind(const struct mvsw_pr_port *port, u16 ruleset_id);
+int mvsw_pr_hw_acl_port_unbind(const struct mvsw_pr_port *port, u16 ruleset_id);
+
+/* Router API */
+int mvsw_pr_hw_rif_create(const struct mvsw_pr_switch *sw,
+			  struct mvsw_pr_iface *iif, u8 *mac, u16 *rif_id);
+int mvsw_pr_hw_rif_delete(const struct mvsw_pr_switch *sw, u16 rif_id,
+			  struct mvsw_pr_iface *iif);
+int mvsw_pr_hw_rif_set(const struct mvsw_pr_switch *sw, u16 *rif_id,
+		       struct mvsw_pr_iface *iif, u8 *mac);
+
+/* Virtual Router API */
+int mvsw_pr_hw_vr_create(const struct mvsw_pr_switch *sw, u16 *vr_id);
+int mvsw_pr_hw_vr_delete(const struct mvsw_pr_switch *sw, u16 vr_id);
+int mvsw_pr_hw_vr_abort(const struct mvsw_pr_switch *sw, u16 vr_id);
+
+/* LPM API */
+int mvsw_pr_hw_lpm_add(const struct mvsw_pr_switch *sw, u16 vr_id,
+		       __be32 dst, u32 dst_len, u32 grp_id);
+int mvsw_pr_hw_lpm_del(const struct mvsw_pr_switch *sw, u16 vr_id, __be32 dst,
+		       u32 dst_len);
+
+/* NH API */
+int mvsw_pr_hw_nh_entries_set(const struct mvsw_pr_switch *sw, int count,
+			      struct mvsw_pr_neigh_info *nhs, u32 grp_id);
+int mvsw_pr_hw_nh_entries_get(const struct mvsw_pr_switch *sw, int count,
+			      struct mvsw_pr_neigh_info *nhs, u32 grp_id);
+int mvsw_pr_hw_nh_group_create(const struct mvsw_pr_switch *sw, u16 nh_count,
+			       u32 *grp_id);
+int mvsw_pr_hw_nh_group_delete(const struct mvsw_pr_switch *sw, u16 nh_count,
+			       u32 grp_id);
+
+/* MP API */
+int mvsw_pr_hw_mp4_hash_set(const struct mvsw_pr_switch *sw, u8 hash_policy);
+
+/* LAG API */
+int mvsw_pr_hw_lag_member_add(struct mvsw_pr_port *port, u16 lag_id);
+int mvsw_pr_hw_lag_member_del(struct mvsw_pr_port *port, u16 lag_id);
+int mvsw_pr_hw_lag_member_enable(struct mvsw_pr_port *port, u16 lag_id,
+				 bool enable);
+int mvsw_pr_hw_lag_fdb_add(const struct mvsw_pr_switch *sw, u16 lag_id,
+			   const unsigned char *mac, u16 vid, bool dynamic);
+int mvsw_pr_hw_lag_fdb_del(const struct mvsw_pr_switch *sw, u16 lag_id,
+			   const unsigned char *mac, u16 vid);
+int mvsw_pr_hw_fdb_flush_lag(const struct mvsw_pr_switch *sw, u16 lag_id,
+			     u32 mode);
+int mvsw_pr_hw_fdb_flush_lag_vlan(const struct mvsw_pr_switch *sw,
+				  u16 lag_id, u16 vid, u32 mode);
+int mvsw_pr_hw_lag_member_rif_leave(const struct mvsw_pr_port *port,
+				    u16 lag_id, u16 vr_id);
+
+/* Event handlers */
+int mvsw_pr_hw_event_handler_register(struct mvsw_pr_switch *sw,
+				      enum mvsw_pr_event_type type,
+				      void (*cb)(struct mvsw_pr_switch *sw,
+						 struct mvsw_pr_event *evt,
+						 void *arg),
+				      void *arg);
+
+void mvsw_pr_hw_event_handler_unregister(struct mvsw_pr_switch *sw,
+					 enum mvsw_pr_event_type type);
+
+/* FW Log API */
+int mvsw_pr_hw_fw_log_level_set(const struct mvsw_pr_switch *sw, u32 lib,
+				u32 type);
+
+int mvsw_pr_hw_rxtx_init(const struct mvsw_pr_switch *sw, bool use_sdma,
+			 u32 *map_addr);
+
+#endif /* _MVSW_PRESTERA_HW_H_ */
diff --git a/drivers/net/ethernet/marvell/prestera_sw/prestera_log.c b/drivers/net/ethernet/marvell/prestera_sw/prestera_log.c
new file mode 100644
index 000000000..bf2fb60a2
--- /dev/null
+++ b/drivers/net/ethernet/marvell/prestera_sw/prestera_log.c
@@ -0,0 +1,203 @@
+/* SPDX-License-Identifier: BSD-3-Clause OR GPL-2.0
+ *
+ * Copyright (c) 2019-2020 Marvell International Ltd. All rights reserved.
+ *
+ */
+#include "prestera_log.h"
+
+static const char unknown[] = "UNKNOWN";
+
+DEF_ENUM_MAP(netdev_cmd) = {
+	[NETDEV_UP] = "NETDEV_UP",
+	[NETDEV_DOWN] = "NETDEV_DOWN",
+	[NETDEV_REBOOT] = "NETDEV_REBOOT",
+	[NETDEV_CHANGE] = "NETDEV_CHANGE",
+	[NETDEV_REGISTER] = "NETDEV_REGISTER",
+	[NETDEV_UNREGISTER] = "NETDEV_UNREGISTER",
+	[NETDEV_CHANGEMTU] = "NETDEV_CHANGEMTU",
+	[NETDEV_CHANGEADDR] = "NETDEV_CHANGEADDR",
+	[NETDEV_PRE_CHANGEADDR] = "NETDEV_PRE_CHANGEADDR",
+	[NETDEV_GOING_DOWN] = "NETDEV_GOING_DOWN",
+	[NETDEV_CHANGENAME] = "NETDEV_CHANGENAME",
+	[NETDEV_FEAT_CHANGE] = "NETDEV_FEAT_CHANGE",
+	[NETDEV_BONDING_FAILOVER] = "NETDEV_BONDING_FAILOVER",
+	[NETDEV_PRE_UP] = "NETDEV_PRE_UP",
+	[NETDEV_PRE_TYPE_CHANGE] = "NETDEV_PRE_TYPE_CHANGE",
+	[NETDEV_POST_TYPE_CHANGE] = "NETDEV_POST_TYPE_CHANGE",
+	[NETDEV_POST_INIT] = "NETDEV_POST_INIT",
+	[NETDEV_RELEASE] = "NETDEV_RELEASE",
+	[NETDEV_NOTIFY_PEERS] = "NETDEV_NOTIFY_PEERS",
+	[NETDEV_JOIN] = "NETDEV_JOIN",
+	[NETDEV_CHANGEUPPER] = "NETDEV_CHANGEUPPER",
+	[NETDEV_RESEND_IGMP] = "NETDEV_RESEND_IGMP",
+	[NETDEV_PRECHANGEMTU] = "NETDEV_PRECHANGEMTU",
+	[NETDEV_CHANGEINFODATA] = "NETDEV_CHANGEINFODATA",
+	[NETDEV_BONDING_INFO] = "NETDEV_BONDING_INFO",
+	[NETDEV_PRECHANGEUPPER] = "NETDEV_PRECHANGEUPPER",
+	[NETDEV_CHANGELOWERSTATE] = "NETDEV_CHANGELOWERSTATE",
+	[NETDEV_UDP_TUNNEL_PUSH_INFO] = "NETDEV_UDP_TUNNEL_PUSH_INFO",
+	[NETDEV_UDP_TUNNEL_DROP_INFO] = "NETDEV_UDP_TUNNEL_DROP_INFO",
+	[NETDEV_CHANGE_TX_QUEUE_LEN] = "NETDEV_CHANGE_TX_QUEUE_LEN",
+	[NETDEV_CVLAN_FILTER_PUSH_INFO] = "NETDEV_CVLAN_FILTER_PUSH_INFO",
+	[NETDEV_CVLAN_FILTER_DROP_INFO] = "NETDEV_CVLAN_FILTER_DROP_INFO",
+	[NETDEV_SVLAN_FILTER_PUSH_INFO] = "NETDEV_SVLAN_FILTER_PUSH_INFO",
+	[NETDEV_SVLAN_FILTER_DROP_INFO] = "NETDEV_SVLAN_FILTER_DROP_INFO"
+};
+
+DEF_ENUM_MAP(switchdev_notifier_type) = {
+	[SWITCHDEV_FDB_ADD_TO_BRIDGE] = "SWITCHDEV_FDB_ADD_TO_BRIDGE",
+	[SWITCHDEV_FDB_DEL_TO_BRIDGE] = "SWITCHDEV_FDB_DEL_TO_BRIDGE",
+	[SWITCHDEV_FDB_ADD_TO_DEVICE] = "SWITCHDEV_FDB_ADD_TO_DEVICE",
+	[SWITCHDEV_FDB_DEL_TO_DEVICE] = "SWITCHDEV_FDB_DEL_TO_DEVICE",
+	[SWITCHDEV_FDB_OFFLOADED] = "SWITCHDEV_FDB_OFFLOADED",
+	[SWITCHDEV_PORT_OBJ_ADD] = "SWITCHDEV_PORT_OBJ_ADD",
+	[SWITCHDEV_PORT_OBJ_DEL] = "SWITCHDEV_PORT_OBJ_DEL",
+	[SWITCHDEV_PORT_ATTR_SET] = "SWITCHDEV_PORT_ATTR_SET",
+	[SWITCHDEV_VXLAN_FDB_ADD_TO_BRIDGE] =
+		"SWITCHDEV_VXLAN_FDB_ADD_TO_BRIDGE",
+	[SWITCHDEV_VXLAN_FDB_DEL_TO_BRIDGE] =
+		"SWITCHDEV_VXLAN_FDB_DEL_TO_BRIDGE",
+	[SWITCHDEV_VXLAN_FDB_ADD_TO_DEVICE] =
+		"SWITCHDEV_VXLAN_FDB_ADD_TO_DEVICE",
+	[SWITCHDEV_VXLAN_FDB_DEL_TO_DEVICE] =
+		"SWITCHDEV_VXLAN_FDB_DEL_TO_DEVICE",
+	[SWITCHDEV_VXLAN_FDB_OFFLOADED] = "SWITCHDEV_VXLAN_FDB_OFFLOADED"
+};
+
+DEF_ENUM_MAP(switchdev_attr_id) = {
+	[SWITCHDEV_ATTR_ID_UNDEFINED] =
+		"SWITCHDEV_ATTR_ID_UNDEFINED",
+	[SWITCHDEV_ATTR_ID_PORT_STP_STATE] =
+		"SWITCHDEV_ATTR_ID_PORT_STP_STATE",
+	[SWITCHDEV_ATTR_ID_PORT_BRIDGE_FLAGS] =
+		"SWITCHDEV_ATTR_ID_PORT_BRIDGE_FLAGS",
+	[SWITCHDEV_ATTR_ID_PORT_PRE_BRIDGE_FLAGS] =
+		"SWITCHDEV_ATTR_ID_PORT_PRE_BRIDGE_FLAGS",
+	[SWITCHDEV_ATTR_ID_PORT_MROUTER] =
+		"SWITCHDEV_ATTR_ID_PORT_MROUTER",
+	[SWITCHDEV_ATTR_ID_BRIDGE_AGEING_TIME] =
+		"SWITCHDEV_ATTR_ID_BRIDGE_AGEING_TIME",
+	[SWITCHDEV_ATTR_ID_BRIDGE_VLAN_FILTERING] =
+		"SWITCHDEV_ATTR_ID_BRIDGE_VLAN_FILTERING",
+	[SWITCHDEV_ATTR_ID_BRIDGE_MC_DISABLED] =
+		"SWITCHDEV_ATTR_ID_BRIDGE_MC_DISABLED",
+	[SWITCHDEV_ATTR_ID_BRIDGE_MROUTER] =
+		"SWITCHDEV_ATTR_ID_BRIDGE_MROUTER"
+};
+
+DEF_ENUM_MAP(switchdev_obj_id) = {
+	[SWITCHDEV_OBJ_ID_UNDEFINED] = "SWITCHDEV_OBJ_ID_UNDEFINED",
+	[SWITCHDEV_OBJ_ID_PORT_VLAN] = "SWITCHDEV_OBJ_ID_PORT_VLAN",
+	[SWITCHDEV_OBJ_ID_PORT_MDB] = "SWITCHDEV_OBJ_ID_PORT_MDB",
+	[SWITCHDEV_OBJ_ID_HOST_MDB] = "SWITCHDEV_OBJ_ID_HOST_MDB",
+};
+
+DEF_ENUM_MAP(fib_event_type) = {
+	[FIB_EVENT_ENTRY_REPLACE] = "FIB_EVENT_ENTRY_REPLACE",
+	[FIB_EVENT_ENTRY_APPEND] = "FIB_EVENT_ENTRY_APPEND",
+	[FIB_EVENT_ENTRY_ADD] = "FIB_EVENT_ENTRY_ADD",
+	[FIB_EVENT_ENTRY_DEL] = "FIB_EVENT_ENTRY_DEL",
+	[FIB_EVENT_RULE_ADD] = "FIB_EVENT_RULE_ADD",
+	[FIB_EVENT_RULE_DEL] = "FIB_EVENT_RULE_DEL",
+	[FIB_EVENT_NH_ADD] = "FIB_EVENT_NH_ADD",
+	[FIB_EVENT_NH_DEL] = "FIB_EVENT_NH_DEL",
+	[FIB_EVENT_VIF_ADD] = "FIB_EVENT_VIF_ADD",
+	[FIB_EVENT_VIF_DEL] = "FIB_EVENT_VIF_DEL",
+};
+
+DEF_ENUM_MAP(netevent_notif_type) = {
+	[NETEVENT_NEIGH_UPDATE] = "NETEVENT_NEIGH_UPDATE",
+	[NETEVENT_REDIRECT] = "NETEVENT_REDIRECT",
+	[NETEVENT_DELAY_PROBE_TIME_UPDATE] =
+		"NETEVENT_DELAY_PROBE_TIME_UPDATE",
+	[NETEVENT_IPV4_MPATH_HASH_UPDATE] =
+		"NETEVENT_IPV4_MPATH_HASH_UPDATE",
+	[NETEVENT_IPV6_MPATH_HASH_UPDATE] =
+		"NETEVENT_IPV6_MPATH_HASH_UPDATE",
+	[NETEVENT_IPV4_FWD_UPDATE_PRIORITY_UPDATE] =
+		"NETEVENT_IPV4_FWD_UPDATE_PRIORITY_UPDATE",
+};
+
+DEF_ENUM_MAP(tc_setup_type) = {
+	[TC_SETUP_QDISC_MQPRIO] = "TC_SETUP_QDISC_MQPRIO",
+	[TC_SETUP_CLSU32] = "TC_SETUP_CLSU32",
+	[TC_SETUP_CLSFLOWER] = "TC_SETUP_CLSFLOWER",
+	[TC_SETUP_CLSMATCHALL] = "TC_SETUP_CLSMATCHALL",
+	[TC_SETUP_CLSBPF] = "TC_SETUP_CLSBPF",
+	[TC_SETUP_BLOCK] = "TC_SETUP_BLOCK",
+	[TC_SETUP_QDISC_CBS] = "TC_SETUP_QDISC_CBS",
+	[TC_SETUP_QDISC_RED] = "TC_SETUP_QDISC_RED",
+	[TC_SETUP_QDISC_PRIO] = "TC_SETUP_QDISC_PRIO",
+	[TC_SETUP_QDISC_MQ] = "TC_SETUP_QDISC_MQ",
+	[TC_SETUP_QDISC_ETF] = "TC_SETUP_QDISC_ETF",
+	[TC_SETUP_ROOT_QDISC] = "TC_SETUP_ROOT_QDISC",
+	[TC_SETUP_QDISC_GRED] = "TC_SETUP_QDISC_GRED",
+};
+
+DEF_ENUM_MAP(flow_block_binder_type) = {
+	[FLOW_BLOCK_BINDER_TYPE_UNSPEC] =
+		"FLOW_BLOCK_BINDER_TYPE_UNSPEC",
+	[FLOW_BLOCK_BINDER_TYPE_CLSACT_INGRESS] =
+		"FLOW_BLOCK_BINDER_TYPE_CLSACT_INGRESS",
+	[FLOW_BLOCK_BINDER_TYPE_CLSACT_EGRESS] =
+		"FLOW_BLOCK_BINDER_TYPE_CLSACT_EGRESS",
+};
+
+DEF_ENUM_MAP(tc_matchall_command) = {
+	[TC_CLSMATCHALL_REPLACE] = "TC_CLSMATCHALL_REPLACE",
+	[TC_CLSMATCHALL_DESTROY] = "TC_CLSMATCHALL_DESTROY",
+	[TC_CLSMATCHALL_STATS] = "TC_CLSMATCHALL_STATS",
+};
+
+DEF_ENUM_MAP(flow_cls_command) = {
+	[FLOW_CLS_REPLACE] = "FLOW_CLS_REPLACE",
+	[FLOW_CLS_DESTROY] = "FLOW_CLS_DESTROY",
+	[FLOW_CLS_STATS] = "FLOW_CLS_STATS",
+	[FLOW_CLS_TMPLT_CREATE] = "FLOW_CLS_TMPLT_CREATE",
+	[FLOW_CLS_TMPLT_DESTROY] = "FLOW_CLS_TMPLT_DESTROY",
+};
+
+DEF_ENUM_MAP(flow_action_id) = {
+	[FLOW_ACTION_ACCEPT] = "FLOW_ACTION_ACCEPT",
+	[FLOW_ACTION_DROP] = "FLOW_ACTION_DROP",
+	[FLOW_ACTION_TRAP] = "FLOW_ACTION_TRAP",
+	[FLOW_ACTION_GOTO] = "FLOW_ACTION_GOTO",
+	[FLOW_ACTION_REDIRECT] = "FLOW_ACTION_REDIRECT",
+	[FLOW_ACTION_MIRRED] = "FLOW_ACTION_MIRRED",
+	[FLOW_ACTION_VLAN_PUSH] = "FLOW_ACTION_VLAN_PUSH",
+	[FLOW_ACTION_VLAN_POP] = "FLOW_ACTION_VLAN_POP",
+	[FLOW_ACTION_VLAN_MANGLE] = "FLOW_ACTION_VLAN_MANGLE",
+	[FLOW_ACTION_TUNNEL_ENCAP] = "FLOW_ACTION_TUNNEL_ENCAP",
+	[FLOW_ACTION_TUNNEL_DECAP] = "FLOW_ACTION_TUNNEL_DECAP",
+	[FLOW_ACTION_MANGLE] = "FLOW_ACTION_MANGLE",
+	[FLOW_ACTION_ADD] = "FLOW_ACTION_ADD",
+	[FLOW_ACTION_CSUM] = "FLOW_ACTION_CSUM",
+	[FLOW_ACTION_MARK] = "FLOW_ACTION_MARK",
+	[FLOW_ACTION_WAKE] = "FLOW_ACTION_WAKE",
+	[FLOW_ACTION_QUEUE] = "FLOW_ACTION_QUEUE",
+	[FLOW_ACTION_SAMPLE] = "FLOW_ACTION_SAMPLE",
+	[FLOW_ACTION_POLICE] = "FLOW_ACTION_POLICE",
+	[FLOW_ACTION_CT] = "FLOW_ACTION_CT",
+};
+
+DEF_ENUM_FUNC(netdev_cmd, NETDEV_UP, NETDEV_SVLAN_FILTER_DROP_INFO)
+
+DEF_ENUM_FUNC(switchdev_notifier_type, SWITCHDEV_FDB_ADD_TO_BRIDGE,
+	      SWITCHDEV_VXLAN_FDB_OFFLOADED)
+DEF_ENUM_FUNC(switchdev_attr_id, SWITCHDEV_ATTR_ID_UNDEFINED,
+	      SWITCHDEV_ATTR_ID_BRIDGE_MROUTER)
+DEF_ENUM_FUNC(switchdev_obj_id, SWITCHDEV_OBJ_ID_UNDEFINED,
+	      SWITCHDEV_OBJ_ID_HOST_MDB)
+
+DEF_ENUM_FUNC(fib_event_type, FIB_EVENT_ENTRY_REPLACE, FIB_EVENT_VIF_DEL)
+
+DEF_ENUM_FUNC(netevent_notif_type, NETEVENT_NEIGH_UPDATE,
+	      NETEVENT_IPV4_FWD_UPDATE_PRIORITY_UPDATE)
+
+/* TC traffic control */
+DEF_ENUM_FUNC(tc_setup_type, TC_SETUP_QDISC_MQPRIO, TC_SETUP_QDISC_GRED)
+DEF_ENUM_FUNC(flow_block_binder_type, FLOW_BLOCK_BINDER_TYPE_UNSPEC,
+	      FLOW_BLOCK_BINDER_TYPE_CLSACT_EGRESS)
+DEF_ENUM_FUNC(tc_matchall_command, TC_CLSMATCHALL_REPLACE, TC_CLSMATCHALL_STATS)
+DEF_ENUM_FUNC(flow_cls_command, FLOW_CLS_REPLACE, FLOW_CLS_TMPLT_DESTROY)
+DEF_ENUM_FUNC(flow_action_id, FLOW_ACTION_ACCEPT, FLOW_ACTION_CT)
diff --git a/drivers/net/ethernet/marvell/prestera_sw/prestera_log.h b/drivers/net/ethernet/marvell/prestera_sw/prestera_log.h
new file mode 100644
index 000000000..aacb296c7
--- /dev/null
+++ b/drivers/net/ethernet/marvell/prestera_sw/prestera_log.h
@@ -0,0 +1,58 @@
+/* SPDX-License-Identifier: BSD-3-Clause OR GPL-2.0
+ *
+ * Copyright (c) 2019-2020 Marvell International Ltd. All rights reserved.
+ *
+ */
+
+#ifndef _MVSW_PRESTERA_LOG_H_
+#define _MVSW_PRESTERA_LOG_H_
+
+#ifdef CONFIG_MRVL_PRESTERA_DEBUG
+
+#include <linux/netdevice.h>
+#include <linux/version.h>
+#include <net/switchdev.h>
+#include <net/fib_notifier.h>
+#include <net/netevent.h>
+#include <net/pkt_cls.h>
+
+#define DEF_ENUM_MAP(enum_name) \
+static const char *enum_name##_map[]
+
+#define DEF_ENUM_FUNC(enum_name, enum_min, enum_max) \
+const char *enum_name##_to_name(enum enum_name val) \
+{ \
+	if (val < enum_min || val > enum_max) \
+		return unknown; \
+	return enum_name##_map[val]; \
+}
+
+#define DEC_ENUM_FUNC(enum_name) \
+const char *enum_name##_to_name(enum enum_name)
+
+#define ENUM_TO_NAME(enum_name, val) enum_name##_to_name(val)
+
+#define MVSW_LOG_INFO(fmt, ...) \
+	pr_info("%s:%d: " fmt "\n", __func__, __LINE__, ##__VA_ARGS__)
+
+#define MVSW_LOG_ERROR(fmt, ...) \
+	pr_err("%s:%d: " fmt "\n", __func__, __LINE__, ##__VA_ARGS__)
+
+DEC_ENUM_FUNC(netdev_cmd);
+DEC_ENUM_FUNC(switchdev_notifier_type);
+DEC_ENUM_FUNC(switchdev_attr_id);
+DEC_ENUM_FUNC(switchdev_obj_id);
+DEC_ENUM_FUNC(fib_event_type);
+DEC_ENUM_FUNC(netevent_notif_type);
+DEC_ENUM_FUNC(tc_setup_type);
+DEC_ENUM_FUNC(flow_block_binder_type);
+DEC_ENUM_FUNC(tc_matchall_command);
+DEC_ENUM_FUNC(flow_cls_command);
+DEC_ENUM_FUNC(flow_action_id);
+
+#else /* CONFIG_MRVL_PRESTERA_DEBUG */
+#define MVSW_LOG_INFO(...)
+#define MVSW_LOG_ERROR(...)
+#endif /* CONFIG_MRVL_PRESTERA_DEBUG */
+
+#endif /* _MVSW_PRESTERA_LOG_H_ */
diff --git a/drivers/net/ethernet/marvell/prestera_sw/prestera_pci.c b/drivers/net/ethernet/marvell/prestera_sw/prestera_pci.c
new file mode 100644
index 000000000..a64a15663
--- /dev/null
+++ b/drivers/net/ethernet/marvell/prestera_sw/prestera_pci.c
@@ -0,0 +1,917 @@
+/* SPDX-License-Identifier: BSD-3-Clause OR GPL-2.0
+ *
+ * Copyright (c) 2019-2020 Marvell International Ltd. All rights reserved.
+ *
+ */
+
+#include <linux/module.h>
+#include <linux/kernel.h>
+#include <linux/device.h>
+#include <linux/pci.h>
+#include <linux/circ_buf.h>
+#include <linux/firmware.h>
+
+#include "prestera.h"
+
+#define MVSW_FW_FILENAME	"marvell/mvsw_prestera_fw.img"
+
+#define MVSW_SUPP_FW_MAJ_VER 2
+#define MVSW_SUPP_FW_MIN_VER 6
+#define MVSW_SUPP_FW_PATCH_VER 0
+
+#define mvsw_wait_timeout(cond, waitms) \
+({ \
+	unsigned long __wait_end = jiffies + msecs_to_jiffies(waitms); \
+	bool __wait_ret = false; \
+	do { \
+		if (cond) { \
+			__wait_ret = true; \
+			break; \
+		} \
+		cond_resched(); \
+	} while (time_before(jiffies, __wait_end)); \
+	__wait_ret; \
+})
+
+#define MVSW_FW_HDR_MAGIC 0x351D9D06
+#define MVSW_FW_DL_TIMEOUT 50000
+#define MVSW_FW_BLK_SZ 1024
+
+#define FW_VER_MAJ_MUL 1000000
+#define FW_VER_MIN_MUL 1000
+
+#define FW_VER_MAJ(v)	((v) / FW_VER_MAJ_MUL)
+
+#define FW_VER_MIN(v) \
+	(((v) - (FW_VER_MAJ(v) * FW_VER_MAJ_MUL)) / FW_VER_MIN_MUL)
+
+#define FW_VER_PATCH(v) \
+	(v - (FW_VER_MAJ(v) * FW_VER_MAJ_MUL) - (FW_VER_MIN(v) * FW_VER_MIN_MUL))
+
+struct mvsw_pr_fw_header {
+	__be32 magic_number;
+	__be32 version_value;
+	u8 reserved[8];
+} __packed;
+
+struct mvsw_pr_ldr_regs {
+	u32 ldr_ready;
+	u32 pad1;
+
+	u32 ldr_img_size;
+	u32 ldr_ctl_flags;
+
+	u32 ldr_buf_offs;
+	u32 ldr_buf_size;
+
+	u32 ldr_buf_rd;
+	u32 pad2;
+	u32 ldr_buf_wr;
+
+	u32 ldr_status;
+} __packed __aligned(4);
+
+#define MVSW_LDR_REG_OFFSET(f)	offsetof(struct mvsw_pr_ldr_regs, f)
+
+#define MVSW_LDR_READY_MAGIC	0xf00dfeed
+
+#define MVSW_LDR_STATUS_IMG_DL		BIT(0)
+#define MVSW_LDR_STATUS_START_FW	BIT(1)
+#define MVSW_LDR_STATUS_INVALID_IMG	BIT(2)
+#define MVSW_LDR_STATUS_NOMEM		BIT(3)
+
+#define mvsw_ldr_write(fw, reg, val) \
+	writel(val, (fw)->ldr_regs + (reg))
+#define mvsw_ldr_read(fw, reg)	\
+	readl((fw)->ldr_regs + (reg))
+
+/* fw loader registers */
+#define MVSW_LDR_READY_REG	MVSW_LDR_REG_OFFSET(ldr_ready)
+#define MVSW_LDR_IMG_SIZE_REG	MVSW_LDR_REG_OFFSET(ldr_img_size)
+#define MVSW_LDR_CTL_REG	MVSW_LDR_REG_OFFSET(ldr_ctl_flags)
+#define MVSW_LDR_BUF_SIZE_REG	MVSW_LDR_REG_OFFSET(ldr_buf_size)
+#define MVSW_LDR_BUF_OFFS_REG	MVSW_LDR_REG_OFFSET(ldr_buf_offs)
+#define MVSW_LDR_BUF_RD_REG	MVSW_LDR_REG_OFFSET(ldr_buf_rd)
+#define MVSW_LDR_BUF_WR_REG	MVSW_LDR_REG_OFFSET(ldr_buf_wr)
+#define MVSW_LDR_STATUS_REG	MVSW_LDR_REG_OFFSET(ldr_status)
+
+#define MVSW_LDR_CTL_DL_START	BIT(0)
+
+#define MVSW_LDR_WR_IDX_MOVE(fw, n) \
+do { \
+	typeof(fw) __fw = (fw); \
+	(__fw)->ldr_wr_idx = ((__fw)->ldr_wr_idx + (n)) & \
+				((__fw)->ldr_buf_len - 1); \
+} while (0)
+
+#define MVSW_LDR_WR_IDX_COMMIT(fw) \
+({ \
+	typeof(fw) __fw = (fw); \
+	mvsw_ldr_write((__fw), MVSW_LDR_BUF_WR_REG, \
+		       (__fw)->ldr_wr_idx); \
+})
+
+#define MVSW_LDR_WR_PTR(fw) \
+({ \
+	typeof(fw) __fw = (fw); \
+	((__fw)->ldr_ring_buf + (__fw)->ldr_wr_idx); \
+})
+
+#define MVSW_EVT_QNUM_MAX	4
+
+struct mvsw_pr_fw_evtq_regs {
+	u32 rd_idx;
+	u32 pad1;
+	u32 wr_idx;
+	u32 pad2;
+	u32 offs;
+	u32 len;
+};
+
+struct mvsw_pr_fw_regs {
+	u32 fw_ready;
+	u32 pad;
+	u32 cmd_offs;
+	u32 cmd_len;
+	u32 evt_offs;
+	u32 evt_qnum;
+
+	u32 cmd_req_ctl;
+	u32 cmd_req_len;
+	u32 cmd_rcv_ctl;
+	u32 cmd_rcv_len;
+
+	u32 fw_status;
+	u32 rx_status;
+
+	struct mvsw_pr_fw_evtq_regs evtq_list[MVSW_EVT_QNUM_MAX];
+};
+
+#define MVSW_FW_REG_OFFSET(f)	offsetof(struct mvsw_pr_fw_regs, f)
+
+#define MVSW_FW_READY_MAGIC	0xcafebabe
+
+/* fw registers */
+#define MVSW_FW_READY_REG		MVSW_FW_REG_OFFSET(fw_ready)
+
+#define MVSW_CMD_BUF_OFFS_REG		MVSW_FW_REG_OFFSET(cmd_offs)
+#define MVSW_CMD_BUF_LEN_REG		MVSW_FW_REG_OFFSET(cmd_len)
+#define MVSW_EVT_BUF_OFFS_REG		MVSW_FW_REG_OFFSET(evt_offs)
+#define MVSW_EVT_QNUM_REG		MVSW_FW_REG_OFFSET(evt_qnum)
+
+#define MVSW_CMD_REQ_CTL_REG		MVSW_FW_REG_OFFSET(cmd_req_ctl)
+#define MVSW_CMD_REQ_LEN_REG		MVSW_FW_REG_OFFSET(cmd_req_len)
+
+#define MVSW_CMD_RCV_CTL_REG		MVSW_FW_REG_OFFSET(cmd_rcv_ctl)
+#define MVSW_CMD_RCV_LEN_REG		MVSW_FW_REG_OFFSET(cmd_rcv_len)
+#define MVSW_FW_STATUS_REG		MVSW_FW_REG_OFFSET(fw_status)
+#define MVSW_RX_STATUS_REG		MVSW_FW_REG_OFFSET(rx_status)
+
+/* MVSW_CMD_REQ_CTL_REG flags */
+#define MVSW_CMD_F_REQ_SENT		BIT(0)
+#define MVSW_CMD_F_REPL_RCVD		BIT(1)
+
+/* MVSW_CMD_RCV_CTL_REG flags */
+#define MVSW_CMD_F_REPL_SENT		BIT(0)
+
+/* MVSW_FW_STATUS_REG flags */
+#define MVSW_STATUS_F_EVT_OFF		BIT(0)
+
+#define MVSW_EVTQ_REG_OFFSET(q, f)			\
+	(MVSW_FW_REG_OFFSET(evtq_list) +		\
+	 (q) * sizeof(struct mvsw_pr_fw_evtq_regs) +	\
+	 offsetof(struct mvsw_pr_fw_evtq_regs, f))
+
+#define MVSW_EVTQ_RD_IDX_REG(q)		MVSW_EVTQ_REG_OFFSET(q, rd_idx)
+#define MVSW_EVTQ_WR_IDX_REG(q)		MVSW_EVTQ_REG_OFFSET(q, wr_idx)
+#define MVSW_EVTQ_OFFS_REG(q)		MVSW_EVTQ_REG_OFFSET(q, offs)
+#define MVSW_EVTQ_LEN_REG(q)		MVSW_EVTQ_REG_OFFSET(q, len)
+
+#define mvsw_fw_write(fw, reg, val)	writel(val, (fw)->hw_regs + (reg))
+#define mvsw_fw_read(fw, reg)		readl((fw)->hw_regs + (reg))
+
+struct mvsw_pr_fw_evtq {
+	u8 __iomem *addr;
+	size_t len;
+};
+
+struct mvsw_pr_fw {
+	struct workqueue_struct *wq;
+	struct prestera_device dev;
+	struct pci_dev *pci_dev;
+	u8 __iomem *mem_addr;
+
+	u8 __iomem *ldr_regs;
+	u8 __iomem *hw_regs;
+
+	u8 __iomem *ldr_ring_buf;
+	u32 ldr_buf_len;
+	u32 ldr_wr_idx;
+	bool active;
+
+	/* serialize access to dev->send_req */
+	struct mutex cmd_mtx;
+	size_t cmd_mbox_len;
+	u8 __iomem *cmd_mbox;
+	struct mvsw_pr_fw_evtq evt_queue[MVSW_EVT_QNUM_MAX];
+	u8 evt_qnum;
+	struct work_struct evt_work;
+	u8 __iomem *evt_buf;
+	u8 *evt_msg;
+};
+
+#define mvsw_fw_dev(fw)	((fw)->dev.dev)
+
+#define PRESTERA_DEVICE(id) PCI_VDEVICE(MARVELL, (id))
+
+static struct mvsw_pr_pci_match {
+	struct pci_driver driver;
+	const struct pci_device_id id;
+	bool registered;
+} mvsw_pci_devices[] = {
+	{
+		.driver = { .name = "AC3x B2B 98DX3255", },
+		.id = { PRESTERA_DEVICE(0xC804), 0 },
+	},
+	{
+		.driver = { .name = "AC3x B2B 98DX3265", },
+		.id = { PRESTERA_DEVICE(0xC80C), 0 },
+	},
+	{
+		.driver = { .name = "Aldrin2", },
+		.id = { PRESTERA_DEVICE(0xCC1E), 0 },
+	},
+	{{ }, }
+};
+
+static int mvsw_pr_fw_load(struct mvsw_pr_fw *fw);
+
+static u32 mvsw_pr_fw_evtq_len(struct mvsw_pr_fw *fw, u8 qid)
+{
+	return fw->evt_queue[qid].len;
+}
+
+static u32 mvsw_pr_fw_evtq_avail(struct mvsw_pr_fw *fw, u8 qid)
+{
+	u32 wr_idx = mvsw_fw_read(fw, MVSW_EVTQ_WR_IDX_REG(qid));
+	u32 rd_idx = mvsw_fw_read(fw, MVSW_EVTQ_RD_IDX_REG(qid));
+
+	return CIRC_CNT(wr_idx, rd_idx, mvsw_pr_fw_evtq_len(fw, qid));
+}
+
+static void mvsw_pr_fw_evtq_rd_set(struct mvsw_pr_fw *fw,
+				   u8 qid, u32 idx)
+{
+	u32 rd_idx = idx & (mvsw_pr_fw_evtq_len(fw, qid) - 1);
+
+	mvsw_fw_write(fw, MVSW_EVTQ_RD_IDX_REG(qid), rd_idx);
+}
+
+static u8 __iomem *mvsw_pr_fw_evtq_buf(struct mvsw_pr_fw *fw,
+				       u8 qid)
+{
+	return fw->evt_queue[qid].addr;
+}
+
+static u32 mvsw_pr_fw_evtq_read32(struct mvsw_pr_fw *fw, u8 qid)
+{
+	u32 rd_idx = mvsw_fw_read(fw, MVSW_EVTQ_RD_IDX_REG(qid));
+	u32 val;
+
+	val = readl(mvsw_pr_fw_evtq_buf(fw, qid) + rd_idx);
+	mvsw_pr_fw_evtq_rd_set(fw, qid, rd_idx + 4);
+	return val;
+}
+
+static ssize_t mvsw_pr_fw_evtq_read_buf(struct mvsw_pr_fw *fw,
+					u8 qid, u8 *buf, size_t len)
+{
+	u32 idx = mvsw_fw_read(fw, MVSW_EVTQ_RD_IDX_REG(qid));
+	u8 __iomem *evtq_addr = mvsw_pr_fw_evtq_buf(fw, qid);
+	u32 *buf32 = (u32 *)buf;
+	int i;
+
+	for (i = 0; i < len / 4; buf32++, i++) {
+		*buf32 = readl_relaxed(evtq_addr + idx);
+		idx = (idx + 4) & (mvsw_pr_fw_evtq_len(fw, qid) - 1);
+	}
+
+	mvsw_pr_fw_evtq_rd_set(fw, qid, idx);
+
+	return i;
+}
+
+static u8 mvsw_pr_fw_evtq_pick(struct mvsw_pr_fw *fw)
+{
+	int qid;
+
+	for (qid = 0; qid < fw->evt_qnum; qid++) {
+		if (mvsw_pr_fw_evtq_avail(fw, qid) >= 4)
+			return qid;
+	}
+
+	return MVSW_EVT_QNUM_MAX;
+}
+
+static void mvsw_pr_fw_status_set(struct mvsw_pr_fw *fw, unsigned int val)
+{
+	u32 status = mvsw_fw_read(fw, MVSW_FW_STATUS_REG);
+
+	status |= val;
+
+	mvsw_fw_write(fw, MVSW_FW_STATUS_REG, status);
+}
+
+static void mvsw_pr_fw_status_clear(struct mvsw_pr_fw *fw, u32 val)
+{
+	u32 status = mvsw_fw_read(fw, MVSW_FW_STATUS_REG);
+
+	status &= ~val;
+
+	mvsw_fw_write(fw, MVSW_FW_STATUS_REG, status);
+}
+
+static void mvsw_pr_fw_evt_work_fn(struct work_struct *work)
+{
+	struct mvsw_pr_fw *fw;
+	u8 *msg;
+	u8 qid;
+
+	fw = container_of(work, struct mvsw_pr_fw, evt_work);
+	msg = fw->evt_msg;
+
+	mvsw_pr_fw_status_set(fw, MVSW_STATUS_F_EVT_OFF);
+
+	while ((qid = mvsw_pr_fw_evtq_pick(fw)) < MVSW_EVT_QNUM_MAX) {
+		u32 idx;
+		u32 len;
+
+		len = mvsw_pr_fw_evtq_read32(fw, qid);
+		idx = mvsw_fw_read(fw, MVSW_EVTQ_RD_IDX_REG(qid));
+
+		WARN_ON(mvsw_pr_fw_evtq_avail(fw, qid) < len);
+
+		if (WARN_ON(len > MVSW_MSG_MAX_SIZE)) {
+			mvsw_pr_fw_evtq_rd_set(fw, qid, idx + len);
+			continue;
+		}
+
+		mvsw_pr_fw_evtq_read_buf(fw, qid, msg, len);
+
+		if (fw->dev.recv_msg)
+			fw->dev.recv_msg(&fw->dev, msg, len);
+	}
+
+	mvsw_pr_fw_status_clear(fw, MVSW_STATUS_F_EVT_OFF);
+}
+
+static int mvsw_pr_fw_wait_reg32(struct mvsw_pr_fw *fw,
+				 u32 reg, u32 val, unsigned int wait)
+{
+	if (mvsw_wait_timeout(mvsw_fw_read(fw, reg) == val, wait))
+		return 0;
+
+	return -EBUSY;
+}
+
+static void mvsw_pci_copy_to(u8 __iomem *dst, u8 *src, size_t len)
+{
+	u32 __iomem *dst32 = (u32 __iomem *)dst;
+	u32 *src32 = (u32 *)src;
+	int i;
+
+	for (i = 0; i < (len / 4); dst32++, src32++, i++)
+		writel_relaxed(*src32, dst32);
+}
+
+static void mvsw_pci_copy_from(u8 *dst, u8 __iomem *src, size_t len)
+{
+	u32 *dst32 = (u32 *)dst;
+	u32 __iomem *src32 = (u32 __iomem *)src;
+	int i;
+
+	for (i = 0; i < (len / 4); dst32++, src32++, i++)
+		*dst32 = readl_relaxed(src32);
+}
+
+static int mvsw_pr_fw_cmd_send(struct mvsw_pr_fw *fw,
+			       u8 *in_msg, size_t in_size,
+			       u8 *out_msg, size_t out_size,
+			       unsigned int wait)
+{
+	u32 ret_size = 0;
+	int err = 0;
+
+	if (!wait)
+		wait = 30000;
+
+	if (ALIGN(in_size, 4) > fw->cmd_mbox_len)
+		return -EMSGSIZE;
+
+	/* wait for finish previous reply from FW */
+	err = mvsw_pr_fw_wait_reg32(fw, MVSW_CMD_RCV_CTL_REG, 0, 1000);
+	if (err) {
+		dev_err(mvsw_fw_dev(fw), "finish reply from FW is timed out\n");
+		return err;
+	}
+
+	mvsw_fw_write(fw, MVSW_CMD_REQ_LEN_REG, in_size);
+	mvsw_pci_copy_to(fw->cmd_mbox, in_msg, in_size);
+
+	mvsw_fw_write(fw, MVSW_CMD_REQ_CTL_REG, MVSW_CMD_F_REQ_SENT);
+
+	/* wait for reply from FW */
+	err = mvsw_pr_fw_wait_reg32(fw, MVSW_CMD_RCV_CTL_REG, MVSW_CMD_F_REPL_SENT,
+				    wait);
+	if (err) {
+		dev_err(mvsw_fw_dev(fw), "reply from FW is timed out\n");
+		fw->active = false;
+		goto cmd_exit;
+	}
+
+	ret_size = mvsw_fw_read(fw, MVSW_CMD_RCV_LEN_REG);
+	if (ret_size > out_size) {
+		dev_err(mvsw_fw_dev(fw), "ret_size (%u) > out_len(%zu)\n",
+			ret_size, out_size);
+		err = -EMSGSIZE;
+		goto cmd_exit;
+	}
+
+	mvsw_pci_copy_from(out_msg, fw->cmd_mbox + in_size, ret_size);
+
+cmd_exit:
+	mvsw_fw_write(fw, MVSW_CMD_REQ_CTL_REG, MVSW_CMD_F_REPL_RCVD);
+	return err;
+}
+
+static int mvsw_pr_fw_send_req(struct prestera_device *dev,
+			       u8 *in_msg, size_t in_size, u8 *out_msg,
+			       size_t out_size, unsigned int wait)
+{
+	struct mvsw_pr_fw *fw;
+	ssize_t ret;
+
+	fw = container_of(dev, struct mvsw_pr_fw, dev);
+
+	if (!fw->active)
+		return -1;
+
+	mutex_lock(&fw->cmd_mtx);
+	ret = mvsw_pr_fw_cmd_send(fw, in_msg, in_size, out_msg, out_size, wait);
+	mutex_unlock(&fw->cmd_mtx);
+
+	return ret;
+}
+
+static int mvsw_pr_fw_init(struct mvsw_pr_fw *fw)
+{
+	u8 __iomem *base;
+	int err;
+	u8 qid;
+
+	err = mvsw_pr_fw_load(fw);
+	if (err)
+		return err;
+
+	err = mvsw_pr_fw_wait_reg32(fw, MVSW_FW_READY_REG,
+				    MVSW_FW_READY_MAGIC, 20000);
+	if (err) {
+		dev_err(mvsw_fw_dev(fw), "FW is failed to start\n");
+		return err;
+	}
+
+	base = fw->mem_addr;
+
+	fw->cmd_mbox = base + mvsw_fw_read(fw, MVSW_CMD_BUF_OFFS_REG);
+	fw->cmd_mbox_len = mvsw_fw_read(fw, MVSW_CMD_BUF_LEN_REG);
+	mutex_init(&fw->cmd_mtx);
+
+	fw->evt_buf = base + mvsw_fw_read(fw, MVSW_EVT_BUF_OFFS_REG);
+	fw->evt_qnum = mvsw_fw_read(fw, MVSW_EVT_QNUM_REG);
+	fw->evt_msg = kmalloc(MVSW_MSG_MAX_SIZE, GFP_KERNEL);
+	if (!fw->evt_msg)
+		return -ENOMEM;
+
+	for (qid = 0; qid < fw->evt_qnum; qid++) {
+		u32 offs = mvsw_fw_read(fw, MVSW_EVTQ_OFFS_REG(qid));
+		struct mvsw_pr_fw_evtq *evtq = &fw->evt_queue[qid];
+
+		evtq->len = mvsw_fw_read(fw, MVSW_EVTQ_LEN_REG(qid));
+		evtq->addr = fw->evt_buf + offs;
+	}
+
+	return 0;
+}
+
+static void mvsw_pr_fw_uninit(struct mvsw_pr_fw *fw)
+{
+	kfree(fw->evt_msg);
+}
+
+static irqreturn_t mvsw_pci_irq_handler(int irq, void *dev_id)
+{
+	struct mvsw_pr_fw *fw = dev_id;
+
+	if (mvsw_fw_read(fw, MVSW_RX_STATUS_REG)) {
+		if (fw->dev.recv_pkt) {
+			mvsw_fw_write(fw, MVSW_RX_STATUS_REG, 0);
+			fw->dev.recv_pkt(&fw->dev);
+		}
+	}
+
+	queue_work(fw->wq, &fw->evt_work);
+
+	return IRQ_HANDLED;
+}
+
+static int mvsw_pr_ldr_wait_reg32(struct mvsw_pr_fw *fw,
+				  u32 reg, u32 val, unsigned int wait)
+{
+	if (mvsw_wait_timeout(mvsw_ldr_read(fw, reg) == val, wait))
+		return 0;
+
+	return -EBUSY;
+}
+
+static u32 mvsw_pr_ldr_buf_avail(struct mvsw_pr_fw *fw)
+{
+	u32 rd_idx = mvsw_ldr_read(fw, MVSW_LDR_BUF_RD_REG);
+
+	return CIRC_SPACE(fw->ldr_wr_idx, rd_idx, fw->ldr_buf_len);
+}
+
+static int mvsw_pr_ldr_send_buf(struct mvsw_pr_fw *fw, const u8 *buf,
+				size_t len)
+{
+	int i;
+
+	if (!mvsw_wait_timeout(mvsw_pr_ldr_buf_avail(fw) >= len, 100)) {
+		dev_err(mvsw_fw_dev(fw), "failed wait for sending firmware\n");
+		return -EBUSY;
+	}
+
+	for (i = 0; i < len; i += 4) {
+		writel_relaxed(*(u32 *)(buf + i), MVSW_LDR_WR_PTR(fw));
+		MVSW_LDR_WR_IDX_MOVE(fw, 4);
+	}
+
+	MVSW_LDR_WR_IDX_COMMIT(fw);
+	return 0;
+}
+
+static int mvsw_pr_ldr_send(struct mvsw_pr_fw *fw,
+			    const char *img, u32 fw_size)
+{
+	unsigned long mask;
+	u32 status;
+	u32 pos;
+	int err;
+
+	if (mvsw_pr_ldr_wait_reg32(fw, MVSW_LDR_STATUS_REG,
+				   MVSW_LDR_STATUS_IMG_DL, 1000)) {
+		dev_err(mvsw_fw_dev(fw), "Loader is not ready to load image\n");
+		return -EBUSY;
+	}
+
+	for (pos = 0; pos < fw_size; pos += MVSW_FW_BLK_SZ) {
+		if (pos + MVSW_FW_BLK_SZ > fw_size)
+			break;
+
+		err = mvsw_pr_ldr_send_buf(fw, img + pos, MVSW_FW_BLK_SZ);
+		if (err) {
+			if (mvsw_fw_read(fw, MVSW_LDR_STATUS_REG) ==
+					 MVSW_LDR_STATUS_NOMEM) {
+				dev_err(mvsw_fw_dev(fw),
+					"Fw image is too big or invalid\n");
+				return -EINVAL;
+			}
+			return err;
+		}
+	}
+
+	if (pos < fw_size) {
+		err = mvsw_pr_ldr_send_buf(fw, img + pos, fw_size - pos);
+		if (err)
+			return err;
+	}
+
+	/* Waiting for status IMG_DOWNLOADING to change to something else */
+	mask = ~(MVSW_LDR_STATUS_IMG_DL);
+
+	if (!mvsw_wait_timeout(mvsw_ldr_read(fw, MVSW_LDR_STATUS_REG) & mask,
+			       MVSW_FW_DL_TIMEOUT)) {
+		dev_err(mvsw_fw_dev(fw), "Timeout to load FW img [state=%d]",
+			mvsw_ldr_read(fw, MVSW_LDR_STATUS_REG));
+		return -ETIMEDOUT;
+	}
+
+	status = mvsw_ldr_read(fw, MVSW_LDR_STATUS_REG);
+	if (status != MVSW_LDR_STATUS_START_FW) {
+		switch (status) {
+		case MVSW_LDR_STATUS_INVALID_IMG:
+			dev_err(mvsw_fw_dev(fw), "FW img has bad crc\n");
+			return -EINVAL;
+		case MVSW_LDR_STATUS_NOMEM:
+			dev_err(mvsw_fw_dev(fw), "Loader has no enough mem\n");
+			return -ENOMEM;
+		default:
+			break;
+		}
+	}
+
+	return 0;
+}
+
+static bool mvsw_pr_ldr_is_ready(struct mvsw_pr_fw *fw)
+{
+	return mvsw_ldr_read(fw, MVSW_LDR_READY_REG) == MVSW_LDR_READY_MAGIC;
+}
+
+static void mvsw_pr_fw_rev_parse(const struct mvsw_pr_fw_header *hdr,
+				 struct prestera_fw_rev *rev)
+{
+	u32 version = be32_to_cpu(hdr->version_value);
+
+	rev->maj = FW_VER_MAJ(version);
+	rev->min = FW_VER_MIN(version);
+	rev->sub = FW_VER_PATCH(version);
+}
+
+static int mvsw_pr_fw_rev_check(struct mvsw_pr_fw *fw)
+{
+	struct prestera_fw_rev *rev = &fw->dev.fw_rev;
+
+	if (rev->maj == MVSW_SUPP_FW_MAJ_VER &&
+	    rev->min == MVSW_SUPP_FW_MIN_VER) {
+		return 0;
+	}
+
+	return -EINVAL;
+}
+
+static int mvsw_pr_fw_hdr_parse(struct mvsw_pr_fw *fw,
+				const struct firmware *img)
+{
+	struct mvsw_pr_fw_header *hdr = (struct mvsw_pr_fw_header *)img->data;
+	struct prestera_fw_rev *rev = &fw->dev.fw_rev;
+	u32 magic;
+
+	magic = be32_to_cpu(hdr->magic_number);
+	if (magic != MVSW_FW_HDR_MAGIC) {
+		dev_err(mvsw_fw_dev(fw), "FW img type is invalid");
+		return -EINVAL;
+	}
+
+	mvsw_pr_fw_rev_parse(hdr, rev);
+
+	dev_info(mvsw_fw_dev(fw), "FW version '%u.%u.%u'\n",
+		 rev->maj, rev->min, rev->sub);
+	dev_info(mvsw_fw_dev(fw), "Driver version '%u.%u.%u'\n",
+		 MVSW_SUPP_FW_MAJ_VER, MVSW_SUPP_FW_MIN_VER,
+		 MVSW_SUPP_FW_PATCH_VER);
+
+	if (mvsw_pr_fw_rev_check(fw)) {
+		dev_err(mvsw_fw_dev(fw),
+			"Driver is incomatible with FW: version mismatch");
+		return -EINVAL;
+	}
+
+	return 0;
+}
+
+static int mvsw_pr_fw_load(struct mvsw_pr_fw *fw)
+{
+	size_t hlen = sizeof(struct mvsw_pr_fw_header);
+	const struct firmware *f;
+	bool has_ldr;
+	int err;
+
+	has_ldr = mvsw_wait_timeout(mvsw_pr_ldr_is_ready(fw), 1000);
+	if (!has_ldr) {
+		dev_err(mvsw_fw_dev(fw), "waiting for FW loader is timed out");
+		return -ETIMEDOUT;
+	}
+
+	fw->ldr_ring_buf = fw->ldr_regs +
+		mvsw_ldr_read(fw, MVSW_LDR_BUF_OFFS_REG);
+
+	fw->ldr_buf_len =
+		mvsw_ldr_read(fw, MVSW_LDR_BUF_SIZE_REG);
+
+	fw->ldr_wr_idx = 0;
+
+	err = request_firmware_direct(&f, MVSW_FW_FILENAME, &fw->pci_dev->dev);
+	if (err) {
+		dev_err(mvsw_fw_dev(fw), "failed to request firmware file\n");
+		return err;
+	}
+
+	if (!IS_ALIGNED(f->size, 4)) {
+		dev_err(mvsw_fw_dev(fw), "FW image file is not aligned");
+		release_firmware(f);
+		return -EINVAL;
+	}
+
+	err = mvsw_pr_fw_hdr_parse(fw, f);
+	if (err) {
+		dev_err(mvsw_fw_dev(fw), "FW image is invalid\n");
+		release_firmware(f);
+		return err;
+	}
+
+	mvsw_ldr_write(fw, MVSW_LDR_IMG_SIZE_REG, f->size - hlen);
+	mvsw_ldr_write(fw, MVSW_LDR_CTL_REG, MVSW_LDR_CTL_DL_START);
+
+	dev_info(mvsw_fw_dev(fw), "Loading prestera FW image ...");
+
+	err = mvsw_pr_ldr_send(fw, f->data + hlen, f->size - hlen);
+
+	release_firmware(f);
+	return err;
+}
+
+static int mvsw_pr_pci_probe(struct pci_dev *pdev,
+			     const struct pci_device_id *id)
+{
+	const char *driver_name = pdev->driver->name;
+	u8 __iomem *mem_addr, *pp_addr;
+	struct mvsw_pr_fw *fw;
+	int err;
+
+	err = pci_enable_device(pdev);
+	if (err) {
+		dev_err(&pdev->dev, "pci_enable_device failed\n");
+		goto err_pci_enable_device;
+	}
+
+	err = pci_request_regions(pdev, driver_name);
+	if (err) {
+		dev_err(&pdev->dev, "pci_request_regions failed\n");
+		goto err_pci_request_regions;
+	}
+
+	if (dma_set_mask_and_coherent(&pdev->dev, DMA_BIT_MASK(30))) {
+		dev_err(&pdev->dev, "fail to set DMA mask\n");
+		goto err_dma_mask;
+	}
+
+	mem_addr = pci_ioremap_bar(pdev, 2);
+	if (!mem_addr) {
+		dev_err(&pdev->dev, "pci mem ioremap failed\n");
+		err = -EIO;
+		goto err_mem_ioremap;
+	}
+
+	pp_addr = ioremap(pci_resource_start(pdev, 4),
+			  pci_resource_len(pdev, 4));
+	if (!pp_addr) {
+		dev_err(&pdev->dev, "pp regs ioremap failed\n");
+		err = -EIO;
+		goto err_pp_ioremap;
+	}
+
+	pci_set_master(pdev);
+
+	fw = kzalloc(sizeof(*fw), GFP_KERNEL);
+	if (!fw) {
+		err = -ENOMEM;
+		goto err_pci_dev_alloc;
+	}
+
+	fw->pci_dev = pdev;
+	fw->dev.dev = &pdev->dev;
+	fw->dev.send_req = mvsw_pr_fw_send_req;
+	fw->dev.pp_regs = pp_addr;
+	fw->mem_addr = mem_addr;
+	fw->ldr_regs = mem_addr;
+	fw->hw_regs = mem_addr;
+	fw->active = true;
+
+	fw->wq = alloc_workqueue("mvsw_fw_wq", WQ_HIGHPRI, 1);
+	if (!fw->wq)
+		goto err_wq_alloc;
+
+	INIT_WORK(&fw->evt_work, mvsw_pr_fw_evt_work_fn);
+
+	err = pci_alloc_irq_vectors(pdev, 1, 1, PCI_IRQ_MSI);
+	if (err < 0) {
+		dev_err(&pdev->dev, "MSI IRQ init failed\n");
+		goto err_irq_alloc;
+	}
+
+	err = request_irq(pci_irq_vector(pdev, 0), mvsw_pci_irq_handler,
+			  0, driver_name, fw);
+	if (err) {
+		dev_err(&pdev->dev, "fail to request IRQ\n");
+		goto err_request_irq;
+	}
+
+	pci_set_drvdata(pdev, fw);
+
+	err = mvsw_pr_fw_init(fw);
+	if (err)
+		goto err_mvsw_fw_init;
+
+	dev_info(mvsw_fw_dev(fw), "Prestera Switch FW is ready\n");
+
+	err = prestera_device_register(&fw->dev);
+	if (err)
+		goto err_mvsw_dev_register;
+
+	return 0;
+
+err_mvsw_dev_register:
+	mvsw_pr_fw_uninit(fw);
+err_mvsw_fw_init:
+	free_irq(pci_irq_vector(pdev, 0), fw);
+err_request_irq:
+	pci_free_irq_vectors(pdev);
+err_irq_alloc:
+	destroy_workqueue(fw->wq);
+err_wq_alloc:
+	kfree(fw);
+err_pci_dev_alloc:
+	iounmap(pp_addr);
+err_pp_ioremap:
+	iounmap(mem_addr);
+err_mem_ioremap:
+err_dma_mask:
+	pci_release_regions(pdev);
+err_pci_request_regions:
+	pci_disable_device(pdev);
+err_pci_enable_device:
+	return err;
+}
+
+static void mvsw_pr_pci_remove(struct pci_dev *pdev)
+{
+	struct mvsw_pr_fw *fw = pci_get_drvdata(pdev);
+
+	free_irq(pci_irq_vector(pdev, 0), fw);
+	pci_free_irq_vectors(pdev);
+	prestera_device_unregister(&fw->dev);
+	flush_workqueue(fw->wq);
+	destroy_workqueue(fw->wq);
+	mvsw_pr_fw_uninit(fw);
+	iounmap(fw->dev.pp_regs);
+	iounmap(fw->mem_addr);
+	pci_release_regions(pdev);
+	pci_disable_device(pdev);
+	kfree(fw);
+}
+
+static int __init mvsw_pr_pci_init(void)
+{
+	struct mvsw_pr_pci_match *match;
+	int err = 0;
+
+	for (match = mvsw_pci_devices; match->driver.name; match++) {
+		match->driver.probe = mvsw_pr_pci_probe;
+		match->driver.remove = mvsw_pr_pci_remove;
+		match->driver.id_table = &match->id;
+
+		err = pci_register_driver(&match->driver);
+		if (err) {
+			pr_err("prestera_pci: failed to register %s\n",
+			       match->driver.name);
+			break;
+		}
+
+		match->registered = true;
+	}
+
+	if (err) {
+		for (match = mvsw_pci_devices; match->driver.name; match++) {
+			if (!match->registered)
+				break;
+
+			pci_unregister_driver(&match->driver);
+		}
+
+		return err;
+	}
+
+	pr_info("prestera_pci: Registered Marvell Prestera PCI driver\n");
+	return 0;
+}
+
+static void __exit mvsw_pr_pci_exit(void)
+{
+	struct mvsw_pr_pci_match *match;
+
+	for (match = mvsw_pci_devices; match->driver.name; match++) {
+		if (!match->registered)
+			break;
+
+		pci_unregister_driver(&match->driver);
+	}
+
+	pr_info("prestera_pci: Unregistered Marvell Prestera PCI driver\n");
+}
+
+module_init(mvsw_pr_pci_init);
+module_exit(mvsw_pr_pci_exit);
+
+MODULE_AUTHOR("Marvell Semi.");
+MODULE_LICENSE("GPL");
+MODULE_DESCRIPTION("Marvell Prestera switch PCI interface");
diff --git a/drivers/net/ethernet/marvell/prestera_sw/prestera_router.c b/drivers/net/ethernet/marvell/prestera_sw/prestera_router.c
new file mode 100644
index 000000000..5891d03cc
--- /dev/null
+++ b/drivers/net/ethernet/marvell/prestera_sw/prestera_router.c
@@ -0,0 +1,3060 @@
+// SPDX-License-Identifier: BSD-3-Clause OR GPL-2.0
+/*
+ * Copyright (c) 2019-2020 Marvell International Ltd. All rights reserved.
+ *
+ */
+
+#include <linux/kernel.h>
+#include <linux/types.h>
+#include <linux/notifier.h>
+#include <linux/sort.h>
+#include <linux/inetdevice.h>
+#include <linux/netdevice.h>
+#include <linux/if_bridge.h>
+#include <net/netevent.h>
+#include <net/neighbour.h>
+#include <net/addrconf.h>
+#include <net/fib_notifier.h>
+#include <net/switchdev.h>
+#include <net/arp.h>
+#include <net/nexthop.h>
+
+#include "prestera.h"
+#include "prestera_hw.h"
+#include "prestera_log.h"
+
+#define MVSW_PR_IMPLICITY_RESOLVE_DEAD_NEIGH
+#define MVSW_PR_NH_PROBE_INTERVAL 5000 /* ms */
+#define MVSW_PR_NH_ACTIVE_JIFFER_FILTER 3000 /* ms */
+#define MVSW_PR_NHGR_UNUSED (0)
+#define MVSW_PR_NHGR_DROP (0xFFFFFFFF)
+
+static const char mvsw_driver_name[] = "mrvl_switchdev";
+
+struct mvsw_pr_rif {
+	struct mvsw_pr_iface iface;
+	struct net_device *dev;
+	struct list_head router_node;
+	unsigned char addr[ETH_ALEN];
+	unsigned int mtu;
+	bool is_active;
+	u16 rif_id;
+	struct mvsw_pr_vr *vr;
+	struct mvsw_pr_switch *sw;
+	unsigned int ref_cnt;
+};
+
+struct mvsw_pr_rif_params {
+	struct net_device *dev;
+	u16 vid;
+};
+
+struct mvsw_pr_fib_node {
+	struct rhash_head ht_node; /* node of mvsw_pr_vr */
+	struct mvsw_pr_fib_key key;
+	struct mvsw_pr_fib_info info; /* action related info */
+};
+
+struct mvsw_pr_vr {
+	u16 hw_vr_id;			/* virtual router ID */
+	u32 tb_id;			/* key (kernel fib table id) */
+	struct list_head router_node;
+	unsigned int ref_cnt;
+};
+
+struct mvsw_pr_nexthop_group_key {
+	struct mvsw_pr_nh_neigh_key neigh[MVSW_PR_NHGR_SIZE_MAX];
+};
+
+struct mvsw_pr_nexthop_group {
+	struct mvsw_pr_nexthop_group_key key;
+	/* Store intermediate object here.
+	 * This prevent overhead kzalloc call.
+	 */
+	/* nh_neigh is used only to notify nexthop_group */
+	struct mvsw_pr_nh_neigh_head {
+		struct mvsw_pr_nexthop_group *this;
+		struct list_head head;
+		/* ptr to neigh is not necessary.
+		 * It used to prevent lookup of nh_neigh by key (n) on destroy
+		 */
+		struct mvsw_pr_nh_neigh *neigh;
+	} nh_neigh_head[MVSW_PR_NHGR_SIZE_MAX];
+	u32 grp_id; /* hw */
+	unsigned long hw_last_connected; /* jiffies */
+	struct rhash_head ht_node; /* node of mvsw_pr_vr */
+	unsigned int ref_cnt;
+};
+
+enum mvsw_pr_mp_hash_policy {
+	MVSW_MP_L3_HASH_POLICY,
+	MVSW_MP_L4_HASH_POLICY,
+	MVSW_MP_HASH_POLICY_MAX,
+};
+
+struct mvsw_pr_kern_neigh_cache {
+	struct mvsw_pr_nh_neigh_key key;
+	bool offloaded;
+	struct rhash_head ht_node;
+	struct list_head kern_fib_cache_list;
+	bool lpm_added; /* Indicate if neigh is reachable by connected route */
+	bool in_kernel; /* Valid in kernel */
+};
+
+/* Used to track offloaded fib entries */
+struct mvsw_pr_kern_fib_cache {
+	struct mvsw_pr_fib_key key;
+	struct fib_info *fi;
+	struct rhash_head ht_node; /* node of mvsw_pr_router */
+	struct mvsw_pr_kern_neigh_cache_head {
+		struct mvsw_pr_kern_fib_cache *this;
+		struct list_head head;
+		struct mvsw_pr_kern_neigh_cache *n_cache;
+	} kern_neigh_cache_head[MVSW_PR_NHGR_SIZE_MAX];
+};
+
+static const struct rhashtable_params __mvsw_pr_kern_neigh_cache_ht_params = {
+	.key_offset  = offsetof(struct mvsw_pr_kern_neigh_cache, key),
+	.head_offset = offsetof(struct mvsw_pr_kern_neigh_cache, ht_node),
+	.key_len     = sizeof(struct mvsw_pr_nh_neigh_key),
+	.automatic_shrinking = true,
+};
+
+static const struct rhashtable_params __mvsw_pr_kern_fib_cache_ht_params = {
+	.key_offset  = offsetof(struct mvsw_pr_kern_fib_cache, key),
+	.head_offset = offsetof(struct mvsw_pr_kern_fib_cache, ht_node),
+	.key_len     = sizeof(struct mvsw_pr_fib_key),
+	.automatic_shrinking = true,
+};
+
+static const struct rhashtable_params __mvsw_pr_fib_ht_params = {
+	.key_offset  = offsetof(struct mvsw_pr_fib_node, key),
+	.head_offset = offsetof(struct mvsw_pr_fib_node, ht_node),
+	.key_len     = sizeof(struct mvsw_pr_fib_key),
+	.automatic_shrinking = true,
+};
+
+static const struct rhashtable_params __mvsw_pr_nh_neigh_ht_params = {
+	.key_offset  = offsetof(struct mvsw_pr_nh_neigh, key),
+	.key_len     = sizeof(struct mvsw_pr_nh_neigh_key),
+	.head_offset = offsetof(struct mvsw_pr_nh_neigh, ht_node),
+};
+
+static const struct rhashtable_params __mvsw_pr_nexthop_group_ht_params = {
+	.key_offset  = offsetof(struct mvsw_pr_nexthop_group, key),
+	.key_len     = sizeof(struct mvsw_pr_nexthop_group_key),
+	.head_offset = offsetof(struct mvsw_pr_nexthop_group, ht_node),
+};
+
+static struct workqueue_struct *mvsw_r_wq;
+static struct workqueue_struct *mvsw_r_owq;
+
+static DEFINE_MUTEX(mvsw_owq_mutex_wip); /* owq function is in progress */
+static bool mvsw_owq_flushing;
+static void mvsw_owq_lock(void)
+{
+	while (true) {
+		if (!mutex_trylock(&mvsw_owq_mutex_wip))
+			goto wip_again;
+
+		if (!rtnl_trylock() && !READ_ONCE(mvsw_owq_flushing))
+			goto rtnl_again;
+
+		break;
+rtnl_again:
+		mutex_unlock(&mvsw_owq_mutex_wip);
+wip_again:
+		schedule();
+	}
+}
+
+static void mvsw_owq_unlock(void)
+{
+	if (!READ_ONCE(mvsw_owq_flushing))
+		rtnl_unlock();
+
+	mutex_unlock(&mvsw_owq_mutex_wip);
+}
+
+/* Must be called under rtnl_lock */
+static void mvsw_owq_flush(void)
+{
+	/* Sanity check */
+	if (rtnl_trylock())
+		panic("%s: called without rtnl_lock !", __func__);
+
+	mutex_lock(&mvsw_owq_mutex_wip);
+	WRITE_ONCE(mvsw_owq_flushing, true);
+	mutex_unlock(&mvsw_owq_mutex_wip);
+
+	flush_workqueue(mvsw_r_owq);
+
+	mutex_lock(&mvsw_owq_mutex_wip);
+	WRITE_ONCE(mvsw_owq_flushing, false);
+	mutex_unlock(&mvsw_owq_mutex_wip);
+}
+
+static const unsigned char mvsw_pr_mac_mask[ETH_ALEN] = {
+	0xff, 0xff, 0xff, 0xff, 0xfc, 0x00
+};
+
+static struct mvsw_pr_vr *mvsw_pr_vr_get(struct mvsw_pr_switch *sw, u32 tb_id,
+					 struct netlink_ext_ack *extack);
+static u32 mvsw_pr_fix_tb_id(u32 tb_id);
+static void mvsw_pr_vr_put(struct mvsw_pr_switch *sw, struct mvsw_pr_vr *vr);
+static void mvsw_pr_vr_util_hw_abort(struct mvsw_pr_switch *sw);
+static struct mvsw_pr_rif *mvsw_pr_rif_create(struct mvsw_pr_switch *sw,
+					      const struct mvsw_pr_rif_params
+					      *params,
+					      struct netlink_ext_ack *extack);
+static int mvsw_pr_rif_vr_update(struct mvsw_pr_switch *sw,
+				 struct mvsw_pr_rif *rif,
+				 struct netlink_ext_ack *extack);
+static void mvsw_pr_rif_destroy(struct mvsw_pr_rif *rif);
+static void mvsw_pr_rif_put(struct mvsw_pr_rif *rif);
+static int mvsw_pr_rif_update(struct mvsw_pr_rif *rif, char *mac);
+static struct mvsw_pr_rif *mvsw_pr_rif_find(const struct mvsw_pr_switch *sw,
+					    const struct net_device *dev);
+static u16 mvsw_pr_rif_vr_id(struct mvsw_pr_rif *rif);
+static bool
+mvsw_pr_nh_neigh_util_hw_state(struct mvsw_pr_switch *sw,
+			       struct mvsw_pr_nh_neigh *nh_neigh);
+static bool
+mvsw_pr_nexthop_group_util_hw_state(struct mvsw_pr_switch *sw,
+				    struct mvsw_pr_nexthop_group *nh_grp);
+static struct mvsw_pr_nh_neigh *
+mvsw_pr_nh_neigh_find(struct mvsw_pr_switch *sw,
+		      struct mvsw_pr_nh_neigh_key *key);
+static int mvsw_pr_nh_neigh_set(struct mvsw_pr_switch *sw,
+				struct mvsw_pr_nh_neigh *neigh);
+static int mvsw_pr_nexthop_group_set(struct mvsw_pr_switch *sw,
+				     struct mvsw_pr_nexthop_group *nh_grp);
+static struct mvsw_pr_fib_node *
+mvsw_pr_fib_node_find(struct mvsw_pr_switch *sw, struct mvsw_pr_fib_key *key);
+static void mvsw_pr_fib_node_destroy(struct mvsw_pr_switch *sw,
+				     struct mvsw_pr_fib_node *fib_node);
+static struct mvsw_pr_fib_node *
+mvsw_pr_fib_node_uc_nh_create(struct mvsw_pr_switch *sw,
+			      struct mvsw_pr_fib_key *key,
+			      struct mvsw_pr_nexthop_group_key *nh_grp_key);
+static bool mvsw_pr_fi_is_direct(struct fib_info *fi);
+static bool mvsw_pr_fi_is_nh(struct fib_info *fi);
+static bool
+mvsw_pr_fib_node_util_is_neighbour(struct mvsw_pr_fib_node *fib_node);
+static void
+mvsw_pr_kern_fib_cache_offload_set(struct mvsw_pr_switch *sw,
+				   struct mvsw_pr_kern_fib_cache *fib_cache);
+
+static u16 mvsw_pr_nh_dev_to_vid(struct mvsw_pr_switch *sw,
+				 struct net_device *dev)
+{
+	struct macvlan_dev *vlan;
+	u16 vid = 0;
+
+	if (is_vlan_dev(dev) &&
+	    netif_is_bridge_master(vlan_dev_real_dev(dev))) {
+		vid = vlan_dev_vlan_id(dev);
+	} else if (netif_is_bridge_master(dev) && br_vlan_enabled(dev)) {
+		br_vlan_get_pvid(dev, &vid);
+	} else if (netif_is_bridge_master(dev)) {
+		vid = mvsw_pr_vlan_dev_vlan_id(sw->bridge, dev);
+	} else if (netif_is_macvlan(dev)) {
+		vlan = netdev_priv(dev);
+		return mvsw_pr_nh_dev_to_vid(sw, vlan->lowerdev);
+	}
+
+	return vid;
+}
+
+static struct net_device*
+mvsw_pr_nh_dev_egress(struct mvsw_pr_switch *sw, struct net_device *dev,
+		      u8 *ha)
+{
+	struct net_device *bridge_dev, *egress_dev = dev;
+	u16 vid = mvsw_pr_nh_dev_to_vid(sw, dev);
+	struct macvlan_dev *vlan;
+
+	if (is_vlan_dev(dev) &&
+	    netif_is_bridge_master(vlan_dev_real_dev(dev))) {
+		bridge_dev = vlan_dev_priv(dev)->real_dev;
+		egress_dev = br_fdb_find_port(bridge_dev, ha,
+					      vid);
+	} else if (netif_is_bridge_master(dev) && br_vlan_enabled(dev)) {
+		egress_dev = br_fdb_find_port(dev, ha, vid);
+	} else if (netif_is_bridge_master(dev)) {
+		/* vid in .1d bridge is 0 */
+		egress_dev = br_fdb_find_port(dev, ha, 0);
+	} else if (netif_is_macvlan(dev)) {
+		vlan = netdev_priv(dev);
+		return mvsw_pr_nh_dev_egress(sw, vlan->lowerdev, ha);
+	}
+
+	return egress_dev;
+}
+
+static u16 mvsw_pr_rif_vr_id(struct mvsw_pr_rif *rif)
+{
+	return rif->vr->hw_vr_id;
+}
+
+static int
+mvsw_pr_rif_iface_init(struct mvsw_pr_rif *rif)
+{
+	struct net_device *dev = rif->dev;
+	struct mvsw_pr_switch *sw = rif->sw;
+	struct mvsw_pr_port *port;
+	int if_type = mvsw_pr_dev_if_type(dev);
+
+	switch (if_type) {
+	case MVSW_IF_PORT_E:
+		port = netdev_priv(dev);
+		rif->iface.dev_port.hw_dev_num = port->dev_id;
+		rif->iface.dev_port.port_num = port->hw_id;
+		break;
+	case MVSW_IF_LAG_E:
+		prestera_lag_id_find(sw, dev, &rif->iface.lag_id);
+		break;
+	case MVSW_IF_VID_E:
+		break;
+	default:
+		pr_err("Unsupported rif type");
+		return -EINVAL;
+	}
+
+	rif->iface.type = if_type;
+	rif->iface.vlan_id = mvsw_pr_nh_dev_to_vid(sw, dev);
+	rif->iface.vr_id = rif->vr->hw_vr_id;
+
+	return 0;
+}
+
+static int
+__mvsw_pr_neigh_iface_init(struct mvsw_pr_switch *sw,
+			   struct mvsw_pr_iface *iface,
+			   struct neighbour *n,
+			   struct net_device *dev)
+{
+	bool is_nud_perm = n->nud_state & NUD_PERMANENT;
+	struct net_device *egress_dev;
+	struct mvsw_pr_port *port;
+
+	iface->type = mvsw_pr_dev_if_type(dev);
+
+	switch (iface->type) {
+	case MVSW_IF_PORT_E:
+	case MVSW_IF_VID_E:
+		egress_dev = mvsw_pr_nh_dev_egress(sw, dev, n->ha);
+		if (!egress_dev && is_nud_perm) {
+		/* Permanent neighbours on a bridge are not bounded to any
+		 * of the ports which is needed by the hardware, therefore
+		 * use any valid lower
+		 */
+			port = mvsw_pr_port_dev_lower_find(dev);
+			egress_dev = port->net_dev;
+		}
+		if (!egress_dev)
+			return -ENOENT;
+
+		if (!mvsw_pr_netdev_check(egress_dev))
+			return __mvsw_pr_neigh_iface_init(sw, iface, n,
+							  egress_dev);
+
+		port = netdev_priv(egress_dev);
+		iface->dev_port.hw_dev_num = port->dev_id;
+		iface->dev_port.port_num = port->hw_id;
+		break;
+	case MVSW_IF_LAG_E:
+		prestera_lag_id_find(sw, dev, &iface->lag_id);
+		break;
+	default:
+		MVSW_LOG_ERROR("Unsupported nexthop device");
+		return -EINVAL;
+	}
+
+	return 0;
+}
+
+static int
+mvsw_pr_neigh_iface_init(struct mvsw_pr_switch *sw,
+			struct mvsw_pr_iface *iface,
+			struct neighbour *n)
+{
+	/* TODO vr_id is obsolete in iface ? */
+	iface->vlan_id = mvsw_pr_nh_dev_to_vid(sw, n->dev);
+	return __mvsw_pr_neigh_iface_init(sw, iface, n, n->dev);
+}
+
+static void mvsw_pr_util_kern_set_neigh_offload(struct neighbour *n,
+						bool offloaded)
+{
+	if (offloaded)
+		n->flags |= NTF_OFFLOADED;
+	else
+		n->flags &= ~NTF_OFFLOADED;
+}
+
+static void
+__mvsw_pr_util_kern_unset_allneigh_offload_cb(struct neighbour *n,
+					      void *cookie)
+{
+	mvsw_pr_util_kern_set_neigh_offload(n, false);
+}
+
+static void mvsw_pr_util_kern_unset_allneigh_offload(void)
+{
+	/* Walk through every neighbour in kernel */
+	neigh_for_each(&arp_tbl,
+		       __mvsw_pr_util_kern_unset_allneigh_offload_cb,
+		       NULL);
+}
+
+static void
+mvsw_pr_util_kern_set_nh_offload(struct fib_nh *fib_nh, bool offloaded)
+{
+		if (offloaded)
+			fib_nh->fib_nh_flags |= RTNH_F_OFFLOAD;
+		else
+			fib_nh->fib_nh_flags &= ~RTNH_F_OFFLOAD;
+}
+
+/* must be called with rcu_read_lock() */
+static int mvsw_pr_util_kern_get_route(struct fib_result *res,
+				       u32 tb_id,
+				       struct mvsw_pr_ip_addr *addr)
+{
+	struct fib_table *tb;
+	struct flowi4 fl4;
+	int ret;
+
+	/* TODO: walkthrough appropriate tables in kernel
+	 * to know if the same prefix exists in several tables
+	 */
+	tb = fib_new_table(&init_net, tb_id);
+	if (!tb)
+		return -ENOENT;
+
+	memset(&fl4, 0, sizeof(fl4));
+	fl4.daddr = addr->u.ipv4;
+	ret = fib_table_lookup(tb, &fl4, res, FIB_LOOKUP_NOREF);
+	if (ret)
+		return ret;
+
+	return 0;
+}
+
+static int
+mvsw_pr_util_fib_nh2nh_neigh_key(struct mvsw_pr_switch *sw,
+				 struct fib_nh *fib_nh,
+				 struct mvsw_pr_nh_neigh_key *nh_key)
+{
+	memset(nh_key, 0, sizeof(*nh_key));
+	nh_key->addr.u.ipv4 = fib_nh->fib_nh_gw4;
+	nh_key->rif = mvsw_pr_rif_find(sw, fib_nh->fib_nh_dev);
+	if (!nh_key->rif)
+		return -ENOENT;
+
+	return 0;
+}
+
+static struct mvsw_pr_kern_neigh_cache *
+mvsw_pr_kern_neigh_cache_find(struct mvsw_pr_switch *sw,
+			      struct mvsw_pr_nh_neigh_key *key)
+{
+	struct mvsw_pr_kern_neigh_cache *n_cache;
+
+	n_cache =
+	 rhashtable_lookup_fast(&sw->router->kern_neigh_cache_ht, key,
+				__mvsw_pr_kern_neigh_cache_ht_params);
+	return IS_ERR(n_cache) ? NULL : n_cache;
+}
+
+static void
+__mvsw_pr_kern_neigh_cache_destroy(struct mvsw_pr_switch *sw,
+				   struct mvsw_pr_kern_neigh_cache *n_cache)
+{
+	n_cache->key.rif->ref_cnt--;
+	mvsw_pr_rif_put(n_cache->key.rif);
+	rhashtable_remove_fast(&sw->router->kern_neigh_cache_ht,
+			       &n_cache->ht_node,
+			       __mvsw_pr_kern_neigh_cache_ht_params);
+	kfree(n_cache);
+}
+
+static struct mvsw_pr_kern_neigh_cache *
+__mvsw_pr_kern_neigh_cache_create(struct mvsw_pr_switch *sw,
+				  struct mvsw_pr_nh_neigh_key *key)
+{
+	struct mvsw_pr_kern_neigh_cache *n_cache;
+	int err;
+
+	n_cache = kzalloc(sizeof(*n_cache), GFP_KERNEL);
+	if (!n_cache)
+		goto err_kzalloc;
+
+	memcpy(&n_cache->key, key, sizeof(*key));
+	n_cache->key.rif->ref_cnt++;
+
+	INIT_LIST_HEAD(&n_cache->kern_fib_cache_list);
+	err = rhashtable_insert_fast(&sw->router->kern_neigh_cache_ht,
+				     &n_cache->ht_node,
+				     __mvsw_pr_kern_neigh_cache_ht_params);
+	if (err)
+		goto err_ht_insert;
+
+	return n_cache;
+
+err_ht_insert:
+	n_cache->key.rif->ref_cnt--;
+	mvsw_pr_rif_put(n_cache->key.rif);
+	kfree(n_cache);
+err_kzalloc:
+	return NULL;
+}
+
+static struct mvsw_pr_kern_neigh_cache *
+mvsw_pr_kern_neigh_cache_get(struct mvsw_pr_switch *sw,
+			     struct mvsw_pr_nh_neigh_key *key)
+{
+	struct mvsw_pr_kern_neigh_cache *n_cache;
+
+	n_cache = mvsw_pr_kern_neigh_cache_find(sw, key);
+	if (!n_cache)
+		n_cache = __mvsw_pr_kern_neigh_cache_create(sw, key);
+
+	return n_cache;
+}
+
+static void
+mvsw_pr_kern_neigh_cache_put(struct mvsw_pr_switch *sw,
+			     struct mvsw_pr_kern_neigh_cache *n_cache)
+{
+	if (!n_cache->in_kernel && !n_cache->lpm_added &&
+	    list_empty(&n_cache->kern_fib_cache_list))
+		__mvsw_pr_kern_neigh_cache_destroy(sw, n_cache);
+}
+
+static void
+mvsw_pr_kern_neigh_cache_offload_set(struct mvsw_pr_switch *sw,
+				     struct mvsw_pr_kern_neigh_cache *n_cache)
+{
+	struct mvsw_pr_kern_neigh_cache_head *n_head;
+
+	list_for_each_entry(n_head, &n_cache->kern_fib_cache_list, head) {
+		mvsw_pr_kern_fib_cache_offload_set(sw, n_head->this);
+	}
+}
+
+static void
+mvsw_pr_kern_neigh_cache_lpm_set(struct mvsw_pr_switch *sw,
+				 struct mvsw_pr_kern_neigh_cache *n_cache)
+{
+	struct mvsw_pr_fib_key fib_key;
+	struct mvsw_pr_fib_node *fib_node;
+	struct mvsw_pr_nexthop_group_key nh_grp_key;
+
+	memset(&fib_key, 0, sizeof(fib_key));
+	fib_key.addr = n_cache->key.addr;
+	fib_key.prefix_len = 32;
+	fib_key.tb_id = n_cache->key.rif->vr->tb_id;
+	fib_node = mvsw_pr_fib_node_find(sw, &fib_key);
+	if (!n_cache->lpm_added && fib_node) {
+		if (mvsw_pr_fib_node_util_is_neighbour(fib_node))
+			mvsw_pr_fib_node_destroy(sw, fib_node);
+		return;
+	}
+
+	if (n_cache->lpm_added && !fib_node) {
+		memset(&nh_grp_key, 0, sizeof(nh_grp_key));
+		nh_grp_key.neigh[0] = n_cache->key;
+		fib_node = mvsw_pr_fib_node_uc_nh_create(sw, &fib_key,
+							 &nh_grp_key);
+		if (!fib_node)
+			MVSW_LOG_ERROR("%s failed ip=%pI4n",
+				       "mvsw_pr_fib_node_uc_nh_create",
+				       &fib_key.addr.u.ipv4);
+		return;
+	}
+}
+
+static struct mvsw_pr_kern_fib_cache *
+mvsw_pr_kern_fib_cache_find(struct mvsw_pr_switch *sw,
+			    struct mvsw_pr_fib_key *key)
+{
+	struct mvsw_pr_kern_fib_cache *fib_cache;
+
+	fib_cache =
+	 rhashtable_lookup_fast(&sw->router->kern_fib_cache_ht, key,
+				__mvsw_pr_kern_fib_cache_ht_params);
+	return IS_ERR(fib_cache) ? NULL : fib_cache;
+}
+
+static void
+__mvsw_pr_kern_fib_cache_destruct(struct mvsw_pr_switch *sw,
+				  struct mvsw_pr_kern_fib_cache *fib_cache)
+{
+	int i;
+	struct mvsw_pr_kern_neigh_cache *n_cache;
+	struct fib_nh *fib_nh;
+
+	if (mvsw_pr_fi_is_direct(fib_cache->fi)) {
+		fib_nh = fib_info_nh(fib_cache->fi, 0);
+		mvsw_pr_util_kern_set_nh_offload(fib_nh, true);
+		goto out;
+	}
+
+	for (i = 0; i < MVSW_PR_NHGR_SIZE_MAX; i++) {
+		n_cache = fib_cache->kern_neigh_cache_head[i].n_cache;
+		if (n_cache) {
+			list_del(&fib_cache->kern_neigh_cache_head[i].head);
+			mvsw_pr_kern_neigh_cache_put(sw, n_cache);
+			fib_nh = fib_info_nh(fib_cache->fi, i);
+			mvsw_pr_util_kern_set_nh_offload(fib_nh, false);
+		}
+	}
+
+out:
+	fib_info_put(fib_cache->fi);
+}
+
+static void
+mvsw_pr_kern_fib_cache_offload_set(struct mvsw_pr_switch *sw,
+				   struct mvsw_pr_kern_fib_cache *fib_cache)
+{
+	int i;
+	struct mvsw_pr_kern_neigh_cache *n_cache;
+	struct fib_nh *fib_nh;
+	bool offloaded;
+
+	if (mvsw_pr_fi_is_direct(fib_cache->fi)) {
+		fib_nh = fib_info_nh(fib_cache->fi, 0);
+		if (mvsw_pr_rif_find(sw, fib_nh->fib_nh_dev))
+			mvsw_pr_util_kern_set_nh_offload(fib_nh, true);
+		goto out;
+	}
+
+	for (i = 0; i < MVSW_PR_NHGR_SIZE_MAX; i++) {
+		n_cache = fib_cache->kern_neigh_cache_head[i].n_cache;
+		if (!n_cache)
+			continue;
+
+		offloaded = n_cache->offloaded;
+		fib_nh = fib_info_nh(fib_cache->fi, i);
+		mvsw_pr_util_kern_set_nh_offload(fib_nh, offloaded);
+	}
+
+out:
+	return;
+}
+
+static void
+mvsw_pr_kern_fib_cache_destroy(struct mvsw_pr_switch *sw,
+			       struct mvsw_pr_kern_fib_cache *fib_cache)
+{
+	__mvsw_pr_kern_fib_cache_destruct(sw, fib_cache);
+	rhashtable_remove_fast(&sw->router->kern_fib_cache_ht,
+			       &fib_cache->ht_node,
+			       __mvsw_pr_kern_fib_cache_ht_params);
+	kfree(fib_cache);
+}
+
+static void
+mvsw_pr_kern_fib_cache_destroy_ht(struct mvsw_pr_switch *sw)
+{
+	struct mvsw_pr_kern_fib_cache *fib_cache, *tfib_cache;
+	struct rhashtable_iter iter;
+
+	tfib_cache = NULL;
+	rhashtable_walk_enter(&sw->router->kern_fib_cache_ht, &iter);
+	rhashtable_walk_start(&iter);
+	while (1) {
+		fib_cache = rhashtable_walk_next(&iter);
+		if (tfib_cache) {
+			rhashtable_remove_fast(&sw->router->kern_fib_cache_ht,
+					       &tfib_cache->ht_node,
+					    __mvsw_pr_kern_fib_cache_ht_params);
+			kfree(tfib_cache);
+			tfib_cache = NULL;
+		}
+
+		if (!fib_cache)
+			break;
+
+		if (IS_ERR(fib_cache))
+			continue;
+
+		__mvsw_pr_kern_fib_cache_destruct(sw, fib_cache);
+		tfib_cache = fib_cache;
+	}
+	rhashtable_walk_stop(&iter);
+	rhashtable_walk_exit(&iter);
+}
+
+static struct mvsw_pr_kern_fib_cache *
+mvsw_pr_kern_fib_cache_create(struct mvsw_pr_switch *sw,
+			      struct mvsw_pr_fib_key *key,
+			      struct fib_info *fi)
+{
+	struct mvsw_pr_kern_fib_cache *fib_cache;
+	struct mvsw_pr_kern_neigh_cache *n_cache;
+	struct mvsw_pr_nh_neigh_key nh_key;
+	struct fib_nh *fib_nh;
+	int err, i, nhs;
+
+	fib_cache = kzalloc(sizeof(*fib_cache), GFP_KERNEL);
+	if (!fib_cache)
+		goto err_kzalloc;
+
+	memcpy(&fib_cache->key, key, sizeof(*key));
+	fib_info_hold(fi);
+	fib_cache->fi = fi;
+
+	err = rhashtable_insert_fast(&sw->router->kern_fib_cache_ht,
+				     &fib_cache->ht_node,
+				     __mvsw_pr_kern_fib_cache_ht_params);
+	if (err)
+		goto err_ht_insert;
+
+	if (!mvsw_pr_fi_is_nh(fi))
+		goto out;
+
+	nhs = fib_info_num_path(fi);
+	for (i = 0; i < nhs; i++) {
+		fib_nh = fib_info_nh(fi, i);
+		err = mvsw_pr_util_fib_nh2nh_neigh_key(sw, fib_nh, &nh_key);
+		if (err)
+			continue;
+
+		n_cache = mvsw_pr_kern_neigh_cache_get(sw, &nh_key);
+		if (!n_cache)
+			continue;
+
+		fib_cache->kern_neigh_cache_head[i].this = fib_cache;
+		fib_cache->kern_neigh_cache_head[i].n_cache = n_cache;
+		list_add(&fib_cache->kern_neigh_cache_head[i].head,
+			 &n_cache->kern_fib_cache_list);
+	}
+
+out:
+	mvsw_pr_kern_fib_cache_offload_set(sw, fib_cache);
+
+	return fib_cache;
+
+err_ht_insert:
+	fib_info_put(fi);
+	kfree(fib_cache);
+err_kzalloc:
+	return NULL;
+}
+
+static int mvsw_pr_util_neigh2nh_neigh_key(struct mvsw_pr_switch *sw,
+					   struct neighbour *n,
+					   struct mvsw_pr_nh_neigh_key *key)
+{
+	memset(key, 0, sizeof(*key));
+	key->addr.u.ipv4 = *(__be32 *)n->primary_key;
+	key->rif = mvsw_pr_rif_find(sw, n->dev);
+	if (!key->rif)
+		return -ENOENT;
+
+	return 0;
+}
+
+static void __mvsw_pr_neigh2nh_neigh_update(struct mvsw_pr_switch *sw,
+					    struct neighbour *n)
+{
+	struct mvsw_pr_nh_neigh_key nh_neigh_key;
+	struct mvsw_pr_nh_neigh *nh_neigh;
+	struct mvsw_pr_neigh_info new_info;
+	struct mvsw_pr_kern_neigh_cache *n_cache;
+	bool offloaded;
+	int err;
+
+	err = mvsw_pr_util_neigh2nh_neigh_key(sw, n, &nh_neigh_key);
+	if (err)
+		return;
+
+	nh_neigh = mvsw_pr_nh_neigh_find(sw, &nh_neigh_key);
+	if (!nh_neigh)
+		return;
+
+	memset(&new_info, 0, sizeof(new_info));
+	read_lock_bh(&n->lock);
+	if (n->nud_state & NUD_VALID && !n->dead) {
+		memcpy(&new_info.ha[0], &n->ha[0], ETH_ALEN);
+		err = mvsw_pr_neigh_iface_init(sw, &new_info.iface, n);
+		if (err) {
+			MVSW_LOG_ERROR("Cannot initialize iface for %pI4n %pM",
+				       n->primary_key, &n->ha[0]);
+			new_info.connected = false;
+		} else {
+			new_info.connected = true;
+		}
+	} else {
+		new_info.connected = false;
+	}
+	read_unlock_bh(&n->lock);
+
+	offloaded = new_info.connected;
+	/* Do hw update only if something changed to prevent nh flap */
+	if (memcmp(&new_info, &nh_neigh->info, sizeof(new_info))) {
+		memcpy(&nh_neigh->info, &new_info, sizeof(new_info));
+		err = mvsw_pr_nh_neigh_set(sw, nh_neigh);
+		if (err) {
+			offloaded = false;
+			MVSW_LOG_ERROR("%s failed with err=%d ip=%pI4n mac=%pM",
+				       "mvsw_pr_nh_neigh_set", err,
+				       &nh_neigh->key.addr.u.ipv4,
+				       &nh_neigh->info.ha[0]);
+		}
+	}
+
+	mvsw_pr_util_kern_set_neigh_offload(n, offloaded);
+	n_cache = mvsw_pr_kern_neigh_cache_find(sw, &nh_neigh_key);
+	if (!n_cache) {
+		MVSW_LOG_ERROR("Cannot get neigh cache for %pI4n %pM",
+			       n->primary_key, &n->ha[0]);
+	} else {
+		n_cache->offloaded = offloaded;
+		mvsw_pr_kern_neigh_cache_offload_set(sw, n_cache);
+	}
+}
+
+static void __mvsw_pr_neigh2nh_neigh_update_all(struct mvsw_pr_switch *sw)
+{
+	struct mvsw_pr_nh_neigh *nh_neigh;
+	struct rhashtable_iter iter;
+	struct neighbour *n;
+	struct mvsw_pr_nh_neigh_key *nkey;
+
+	rhashtable_walk_enter(&sw->router->nh_neigh_ht, &iter);
+	rhashtable_walk_start(&iter);
+	while ((nh_neigh = rhashtable_walk_next(&iter))) {
+		if (IS_ERR(nh_neigh))
+			continue;
+
+		nkey = &nh_neigh->key;
+		n = neigh_lookup(&arp_tbl, &nkey->addr.u.ipv4, nkey->rif->dev);
+		if (n) {
+			__mvsw_pr_neigh2nh_neigh_update(sw, n);
+			neigh_release(n);
+		}
+	}
+	rhashtable_walk_stop(&iter);
+	rhashtable_walk_exit(&iter);
+}
+
+/* I dont think that optimiztaion of this
+ * function withone neighbour will make sense...
+ * Just select direction nh_neigh -> kernel or vice versa
+ */
+static void
+__mvsw_pr_neigh_hwstate_update_all(struct mvsw_pr_switch *sw)
+{
+	struct mvsw_pr_nh_neigh *nh_neigh;
+	struct rhashtable_iter iter;
+	struct neighbour *n;
+	struct mvsw_pr_nh_neigh_key *nkey;
+	bool n_resolved, hw_active;
+
+	rhashtable_walk_enter(&sw->router->nh_neigh_ht, &iter);
+	rhashtable_walk_start(&iter);
+	while ((nh_neigh = rhashtable_walk_next(&iter))) {
+		if (IS_ERR(nh_neigh))
+			continue;
+
+		hw_active = mvsw_pr_nh_neigh_util_hw_state(sw, nh_neigh);
+		nkey = &nh_neigh->key;
+		n = neigh_lookup(&arp_tbl, &nkey->addr.u.ipv4, nkey->rif->dev);
+		if (n) {
+			read_lock_bh(&n->lock);
+			if (n->dead || !(n->nud_state & NUD_VALID))
+				n_resolved = false;
+			else
+				n_resolved = true;
+			read_unlock_bh(&n->lock);
+		} else {
+			n_resolved = false;
+		}
+
+#ifdef MVSW_PR_IMPLICITY_RESOLVE_DEAD_NEIGH
+		if (!hw_active && n_resolved)
+			goto next_nh_neigh;
+#else /* MVSW_PR_IMPLICITY_RESOLVE_DEAD_NEIGH */
+		if (!hw_active)
+			goto next_nh_neigh;
+#endif /* MVSW_PR_IMPLICITY_RESOLVE_DEAD_NEIGH */
+
+		if (!n) {
+			MVSW_LOG_INFO("Push active neighbour %pI4n to kernel",
+				      &nkey->addr.u.ipv4);
+			n = neigh_create(&arp_tbl, &nkey->addr.u.ipv4,
+					 nkey->rif->dev);
+			if (IS_ERR(n)) {
+				n = NULL;
+				MVSW_LOG_ERROR("Cannot create neighbour %pI4n",
+					       &nkey->addr.u.ipv4);
+
+				goto next_nh_neigh;
+			}
+		}
+
+		neigh_event_send(n, NULL);
+
+next_nh_neigh:
+		if (n)
+			neigh_release(n);
+	}
+	rhashtable_walk_stop(&iter);
+	rhashtable_walk_exit(&iter);
+}
+
+static void
+__mvsw_pr_sync_neigh_cache_kernel(struct mvsw_pr_switch *sw,
+				  struct mvsw_pr_kern_neigh_cache *n_cache)
+{
+	struct neighbour *n;
+	struct fib_result res;
+	struct fib_nh *fib_nh;
+	struct mvsw_pr_nh_neigh_key *key;
+
+	key = &n_cache->key;
+
+	n_cache->in_kernel = false;
+	n_cache->lpm_added = false;
+	n = neigh_lookup(&arp_tbl, &key->addr.u.ipv4, key->rif->dev);
+	if (n) {
+		read_lock_bh(&n->lock);
+		if (!n->dead && (n->nud_state & NUD_VALID))
+			n_cache->in_kernel = true;
+		read_unlock_bh(&n->lock);
+	}
+
+	if (n_cache->in_kernel) {
+		if (!mvsw_pr_util_kern_get_route(&res, key->rif->vr->tb_id,
+						 &key->addr))
+			if (res.type == RTN_UNICAST &&
+			    mvsw_pr_fi_is_direct(res.fi)) {
+				fib_nh = fib_info_nh(res.fi, 0);
+				if (n->dev == fib_nh->fib_nh_dev)
+					n_cache->lpm_added = true;
+			}
+	}
+
+	if (n)
+		neigh_release(n);
+
+	mvsw_pr_kern_neigh_cache_lpm_set(sw, n_cache);
+}
+
+static void __mvsw_pr_sync_neigh_cache_kernel_all(struct mvsw_pr_switch *sw)
+{
+	struct mvsw_pr_kern_neigh_cache *n_cache, *tn_cache;
+	struct rhashtable_iter iter;
+
+	tn_cache = NULL;
+	rhashtable_walk_enter(&sw->router->kern_neigh_cache_ht, &iter);
+	rhashtable_walk_start(&iter);
+	while (1) {
+		n_cache = rhashtable_walk_next(&iter);
+		if (tn_cache) {
+			mvsw_pr_kern_neigh_cache_put(sw, tn_cache);
+			tn_cache = NULL;
+		}
+
+		if (!n_cache)
+			break;
+
+		if (IS_ERR(n_cache))
+			continue;
+
+		__mvsw_pr_sync_neigh_cache_kernel(sw, n_cache);
+		tn_cache = n_cache;
+	}
+	rhashtable_walk_stop(&iter);
+	rhashtable_walk_exit(&iter);
+}
+
+/* Propagate kernel event to hw */
+static void mvsw_pr_neigh_arbiter_n_evt(struct mvsw_pr_switch *sw,
+					struct neighbour *n)
+{
+	struct mvsw_pr_kern_neigh_cache *n_cache;
+	struct mvsw_pr_nh_neigh_key n_key;
+	int err;
+
+	err = mvsw_pr_util_neigh2nh_neigh_key(sw, n, &n_key);
+	if (err)
+		return;
+
+	n_cache = mvsw_pr_kern_neigh_cache_get(sw, &n_key);
+	if (!n_cache)
+		return;
+
+	__mvsw_pr_sync_neigh_cache_kernel(sw, n_cache);
+	mvsw_pr_kern_neigh_cache_put(sw, n_cache);
+
+	__mvsw_pr_neigh2nh_neigh_update(sw, n);
+}
+
+/* Propagate hw state to kernel */
+static void mvsw_pr_neigh_arbiter_hw_evt(struct mvsw_pr_switch *sw)
+{
+	__mvsw_pr_neigh_hwstate_update_all(sw);
+}
+
+/* Propagate fib changes to hw neighs */
+static void
+mvsw_pr_neigh_arbiter_fib_evt(struct mvsw_pr_switch *sw,
+			      bool replace, /* replace or del */
+			      struct mvsw_pr_fib_key *fib_key,
+			      struct fib_info *fi)
+{
+	struct mvsw_pr_kern_fib_cache *fib_cache;
+
+	fib_cache = mvsw_pr_kern_fib_cache_find(sw, fib_key);
+		if (fib_cache)
+			mvsw_pr_kern_fib_cache_destroy(sw, fib_cache);
+
+	/* TODO: add util function IS_NH / IS_DIR */
+	if (replace) {
+		fib_cache = mvsw_pr_kern_fib_cache_create(sw, fib_key, fi);
+		if (!fib_cache)
+			MVSW_LOG_ERROR("%s failed for %pI4n",
+				       "mvsw_pr_kern_fib_cache_create",
+				       &fib_key->addr.u.ipv4);
+	}
+
+	__mvsw_pr_sync_neigh_cache_kernel_all(sw);
+	__mvsw_pr_neigh2nh_neigh_update_all(sw);
+}
+
+struct mvsw_pr_netevent_work {
+	struct work_struct work;
+	struct mvsw_pr_switch *sw;
+	struct neighbour *n;
+};
+
+static void mvsw_pr_router_neigh_event_work(struct work_struct *work)
+{
+	struct mvsw_pr_netevent_work *net_work =
+		container_of(work, struct mvsw_pr_netevent_work, work);
+	struct mvsw_pr_switch *sw = net_work->sw;
+	struct neighbour *n = net_work->n;
+
+	/* neigh - its not hw related object. It stored only in kernel. So... */
+	mvsw_owq_lock();
+
+	if (sw->router->aborted)
+		goto out;
+
+	mvsw_pr_neigh_arbiter_n_evt(sw, n);
+
+out:
+	neigh_release(n);
+	mvsw_owq_unlock();
+	kfree(net_work);
+}
+
+static int mvsw_pr_router_netevent_event(struct notifier_block *nb,
+					 unsigned long event, void *ptr)
+{
+	struct mvsw_pr_netevent_work *net_work;
+	struct mvsw_pr_router *router;
+	struct mvsw_pr_rif *rif;
+	struct neighbour *n = ptr;
+
+	router = container_of(nb, struct mvsw_pr_router, netevent_nb);
+
+	switch (event) {
+	case NETEVENT_NEIGH_UPDATE:
+		if (n->tbl != &arp_tbl)
+			return NOTIFY_DONE;
+
+		rif = mvsw_pr_rif_find(router->sw, n->dev);
+		if (!rif)
+			return NOTIFY_DONE;
+
+		net_work = kzalloc(sizeof(*net_work), GFP_ATOMIC);
+		if (WARN_ON(!net_work))
+			return NOTIFY_BAD;
+
+		neigh_clone(n);
+		net_work->n = n;
+		net_work->sw = router->sw;
+		INIT_WORK(&net_work->work, mvsw_pr_router_neigh_event_work);
+		queue_work(mvsw_r_owq, &net_work->work);
+	}
+
+	return NOTIFY_DONE;
+}
+
+static void
+mvsw_pr_router_neighs_update_interval_init(struct mvsw_pr_router *router)
+{
+	router->neighs_update.interval = MVSW_PR_NH_PROBE_INTERVAL;
+}
+
+static void mvsw_pr_router_update_neighs_work(struct work_struct *work)
+{
+	struct mvsw_pr_router *router;
+
+	router = container_of(work, struct mvsw_pr_router,
+			      neighs_update.dw.work);
+	rtnl_lock();
+
+	if (router->aborted)
+		goto out;
+
+	mvsw_pr_neigh_arbiter_hw_evt(router->sw);
+
+out:
+	rtnl_unlock();
+	mvsw_pr_router_neighs_update_interval_init(router);
+	queue_delayed_work(mvsw_r_wq, &router->neighs_update.dw,
+			   msecs_to_jiffies(router->neighs_update.interval));
+}
+
+static int mvsw_pr_neigh_init(struct mvsw_pr_switch *sw)
+{
+	int err;
+
+	err = rhashtable_init(&sw->router->nh_neigh_ht,
+			      &__mvsw_pr_nh_neigh_ht_params);
+	if (err)
+		return err;
+
+	mvsw_pr_router_neighs_update_interval_init(sw->router);
+
+	INIT_DELAYED_WORK(&sw->router->neighs_update.dw,
+			  mvsw_pr_router_update_neighs_work);
+	queue_delayed_work(mvsw_r_wq, &sw->router->neighs_update.dw, 0);
+	return 0;
+}
+
+static void mvsw_pr_neigh_fini(struct mvsw_pr_switch *sw)
+{
+	cancel_delayed_work_sync(&sw->router->neighs_update.dw);
+	rhashtable_destroy(&sw->router->nh_neigh_ht);
+}
+
+static struct mvsw_pr_rif*
+mvsw_pr_rif_find(const struct mvsw_pr_switch *sw,
+		 const struct net_device *dev)
+{
+	struct mvsw_pr_rif *rif;
+
+	list_for_each_entry(rif, &sw->router->rif_list, router_node) {
+		if (rif->dev == dev)
+			return rif;
+	}
+
+	return NULL;
+}
+
+bool mvsw_pr_rif_exists(const struct mvsw_pr_switch *sw,
+			const struct net_device *dev)
+{
+	return !!mvsw_pr_rif_find(sw, dev);
+}
+
+static int
+mvsw_pr_port_vlan_router_join(struct mvsw_pr_port_vlan *mvsw_pr_port_vlan,
+			      struct net_device *dev,
+			      struct netlink_ext_ack *extack)
+{
+	struct mvsw_pr_port *mvsw_pr_port = mvsw_pr_port_vlan->mvsw_pr_port;
+	struct mvsw_pr_switch *sw = mvsw_pr_port->sw;
+
+	struct mvsw_pr_rif_params params = {
+		.dev = dev,
+	};
+	struct mvsw_pr_rif *rif;
+
+	MVSW_LOG_ERROR("NOT IMPLEMENTED!!!");
+
+	rif = mvsw_pr_rif_find(sw, dev);
+	if (!rif)
+		rif = mvsw_pr_rif_create(sw, &params, extack);
+
+	if (IS_ERR(rif))
+		return PTR_ERR(rif);
+
+	rif->is_active = true;
+
+	/* TODO:
+	 * - vid learning set (false)
+	 * - stp state set (FORWARDING)
+	 */
+
+	return 0;
+}
+
+static void
+mvsw_pr_port_vlan_router_leave(struct mvsw_pr_port_vlan *mvsw_pr_port_vlan,
+			       struct net_device *dev)
+
+{
+	struct mvsw_pr_port *mvsw_pr_port = mvsw_pr_port_vlan->mvsw_pr_port;
+	struct mvsw_pr_switch *sw = mvsw_pr_port->sw;
+	struct mvsw_pr_rif *rif;
+
+	rif = mvsw_pr_rif_find(sw, dev);
+
+	if (rif)
+		rif->is_active = false;
+	/* TODO:
+	 * - stp state set (BLOCKING)
+	 * - vid learning set (true)
+	 */
+}
+
+static int
+mvsw_pr_port_router_join(struct mvsw_pr_port *mvsw_pr_port,
+			 struct net_device *dev,
+			 struct netlink_ext_ack *extack)
+{
+	struct mvsw_pr_switch *sw = mvsw_pr_port->sw;
+
+	struct mvsw_pr_rif_params params = {
+		.dev = dev,
+	};
+	struct mvsw_pr_rif *rif;
+
+	rif = mvsw_pr_rif_find(sw, dev);
+	if (!rif)
+		rif = mvsw_pr_rif_create(sw, &params, extack);
+
+	if (IS_ERR(rif))
+		return PTR_ERR(rif);
+
+	rif->is_active = true;
+
+	/* TODO:
+	 * - vid learning set (false)
+	 * - stp state set (FORWARDING)
+	 */
+
+	return 0;
+}
+
+void mvsw_pr_port_router_leave(struct mvsw_pr_port *mvsw_pr_port)
+{
+	struct mvsw_pr_rif *rif;
+
+	/* TODO:
+	 * - stp state set (BLOCKING)
+	 * - vid learning set (true)
+	 */
+
+	rif = mvsw_pr_rif_find(mvsw_pr_port->sw, mvsw_pr_port->net_dev);
+	if (rif) {
+		rif->is_active = false;
+		mvsw_pr_rif_put(rif);
+	}
+}
+
+static int mvsw_pr_rif_fdb_op(struct mvsw_pr_rif *rif, const char *mac,
+			      bool adding)
+{
+	if (adding)
+		mvsw_pr_macvlan_add(rif->sw, mvsw_pr_rif_vr_id(rif), mac,
+				    rif->iface.vlan_id);
+	else
+		mvsw_pr_macvlan_del(rif->sw, mvsw_pr_rif_vr_id(rif), mac,
+				    rif->iface.vlan_id);
+
+	return 0;
+}
+
+static int mvsw_pr_rif_macvlan_add(struct mvsw_pr_switch *sw,
+				   const struct net_device *macvlan_dev,
+				   struct netlink_ext_ack *extack)
+{
+	struct macvlan_dev *vlan = netdev_priv(macvlan_dev);
+	struct mvsw_pr_rif *rif;
+	int err;
+
+	rif = mvsw_pr_rif_find(sw, vlan->lowerdev);
+	if (!rif) {
+		NL_SET_ERR_MSG_MOD(extack,
+				   "macvlan is only supported on top of RIF");
+		return -EOPNOTSUPP;
+	}
+
+	err = mvsw_pr_rif_fdb_op(rif, macvlan_dev->dev_addr, true);
+	if (err)
+		return err;
+
+	return err;
+}
+
+static void __mvsw_pr_rif_macvlan_del(struct mvsw_pr_switch *sw,
+				      const struct net_device *macvlan_dev)
+{
+	struct macvlan_dev *vlan = netdev_priv(macvlan_dev);
+	struct mvsw_pr_rif *rif;
+
+	rif = mvsw_pr_rif_find(sw, vlan->lowerdev);
+	if (!rif)
+		return;
+
+	mvsw_pr_rif_fdb_op(rif, macvlan_dev->dev_addr,  false);
+}
+
+static void mvsw_pr_rif_macvlan_del(struct mvsw_pr_switch *sw,
+				    const struct net_device *macvlan_dev)
+{
+	__mvsw_pr_rif_macvlan_del(sw, macvlan_dev);
+}
+
+static int mvsw_pr_inetaddr_macvlan_event(struct mvsw_pr_switch *sw,
+					  struct net_device *macvlan_dev,
+					  unsigned long event,
+					  struct netlink_ext_ack *extack)
+{
+	switch (event) {
+	case NETDEV_UP:
+		return mvsw_pr_rif_macvlan_add(sw, macvlan_dev, extack);
+	case NETDEV_DOWN:
+		mvsw_pr_rif_macvlan_del(sw, macvlan_dev);
+		break;
+	}
+
+	return 0;
+}
+
+static int mvsw_pr_router_port_check_rif_addr(struct mvsw_pr_switch *sw,
+					      struct net_device *dev,
+					      const unsigned char *dev_addr,
+					      struct netlink_ext_ack *extack)
+{
+	if (netif_is_macvlan(dev) || netif_is_l3_master(dev))
+		return 0;
+
+	if (!ether_addr_equal_masked(sw->base_mac, dev_addr,
+				     mvsw_pr_mac_mask)) {
+		NL_SET_ERR_MSG_MOD(extack,
+				   "RIF MAC must have the same prefix");
+		return -EINVAL;
+	}
+
+	return 0;
+}
+
+static int mvsw_pr_inetaddr_port_event(struct net_device *port_dev,
+				       unsigned long event,
+				       struct netlink_ext_ack *extack)
+{
+	struct mvsw_pr_port *mvsw_pr_port = netdev_priv(port_dev);
+
+	MVSW_LOG_ERROR("dev=%s", port_dev->name);
+
+	switch (event) {
+	case NETDEV_UP:
+		if (netif_is_bridge_port(port_dev) ||
+		    netif_is_lag_port(port_dev) || netif_is_ovs_port(port_dev))
+			return 0;
+		return mvsw_pr_port_router_join(mvsw_pr_port, port_dev, extack);
+	case NETDEV_DOWN:
+		mvsw_pr_port_router_leave(mvsw_pr_port);
+		break;
+	}
+
+	return 0;
+}
+
+static int mvsw_pr_inetaddr_bridge_event(struct mvsw_pr_switch *sw,
+					 struct net_device *dev,
+					 unsigned long event,
+					 struct netlink_ext_ack *extack)
+{
+	struct mvsw_pr_rif_params params = {
+		.dev = dev,
+	};
+	struct mvsw_pr_rif *rif;
+
+	switch (event) {
+	case NETDEV_UP:
+		rif = mvsw_pr_rif_find(sw, dev);
+		if (!rif)
+			rif = mvsw_pr_rif_create(sw, &params, extack);
+
+		if (IS_ERR(rif))
+			return PTR_ERR(rif);
+		rif->is_active = true;
+		break;
+	case NETDEV_DOWN:
+		rif = mvsw_pr_rif_find(sw, dev);
+		rif->is_active = false;
+		mvsw_pr_rif_put(rif);
+		break;
+	}
+
+	return 0;
+}
+
+static int mvsw_pr_inetaddr_port_vlan_event(struct net_device *l3_dev,
+					    struct net_device *port_dev,
+					    unsigned long event, u16 vid,
+					    struct netlink_ext_ack *extack)
+{
+	struct mvsw_pr_port *mvsw_pr_port = netdev_priv(port_dev);
+	struct mvsw_pr_port_vlan *mvsw_pr_port_vlan;
+
+	mvsw_pr_port_vlan = mvsw_pr_port_vlan_find_by_vid(mvsw_pr_port, vid);
+	if (WARN_ON(!mvsw_pr_port_vlan))
+		return -EINVAL;
+
+	switch (event) {
+	case NETDEV_UP:
+		return mvsw_pr_port_vlan_router_join(mvsw_pr_port_vlan,
+						     l3_dev, extack);
+	case NETDEV_DOWN:
+		mvsw_pr_port_vlan_router_leave(mvsw_pr_port_vlan, l3_dev);
+		break;
+	}
+
+	return 0;
+}
+
+static int mvsw_pr_inetaddr_vlan_event(struct mvsw_pr_switch *sw,
+				       struct net_device *vlan_dev,
+				       unsigned long event,
+				       struct netlink_ext_ack *extack)
+{
+	struct net_device *real_dev = vlan_dev_real_dev(vlan_dev);
+	u16 vid = vlan_dev_vlan_id(vlan_dev);
+
+	MVSW_LOG_ERROR("vlan_dev=%s, real_dev=%s", vlan_dev->name,
+		       real_dev->name);
+	if (netif_is_bridge_port(vlan_dev))
+		return 0;
+
+	if (mvsw_pr_netdev_check(real_dev))
+		return mvsw_pr_inetaddr_port_vlan_event(vlan_dev, real_dev,
+							event, vid, extack);
+	else if (netif_is_bridge_master(real_dev) && br_vlan_enabled(real_dev))
+		return mvsw_pr_inetaddr_bridge_event(sw, vlan_dev, event,
+						     extack);
+
+	return 0;
+}
+
+static int mvsw_pr_inetaddr_lag_event(struct mvsw_pr_switch *sw,
+				      struct net_device *lag_dev,
+				      unsigned long event,
+				      struct netlink_ext_ack *extack)
+{
+	struct mvsw_pr_rif_params params = {
+		.dev = lag_dev,
+	};
+	struct mvsw_pr_rif *rif;
+
+	MVSW_LOG_ERROR("lag_dev=%s", lag_dev->name);
+
+	switch (event) {
+	case NETDEV_UP:
+		rif = mvsw_pr_rif_find(sw, lag_dev);
+		if (!rif)
+			rif = mvsw_pr_rif_create(sw, &params, extack);
+
+		if (IS_ERR(rif))
+			return PTR_ERR(rif);
+		rif->is_active = true;
+		break;
+	case NETDEV_DOWN:
+		rif = mvsw_pr_rif_find(sw, lag_dev);
+		rif->is_active = false;
+		mvsw_pr_rif_put(rif);
+		break;
+	}
+
+	return 0;
+}
+
+static int __mvsw_pr_inetaddr_event(struct mvsw_pr_switch *sw,
+				    struct net_device *dev,
+				    unsigned long event,
+				    struct netlink_ext_ack *extack)
+{
+	if (mvsw_pr_netdev_check(dev))
+		return mvsw_pr_inetaddr_port_event(dev, event, extack);
+	else if (is_vlan_dev(dev))
+		return mvsw_pr_inetaddr_vlan_event(sw, dev, event, extack);
+	else if (netif_is_bridge_master(dev))
+		return mvsw_pr_inetaddr_bridge_event(sw, dev, event, extack);
+	else if (netif_is_lag_master(dev))
+		return mvsw_pr_inetaddr_lag_event(sw, dev, event, extack);
+	else if (netif_is_macvlan(dev))
+		return mvsw_pr_inetaddr_macvlan_event(sw, dev, event, extack);
+	else
+		return 0;
+}
+
+static bool
+mvsw_pr_rif_should_config(struct mvsw_pr_rif *rif, struct net_device *dev,
+			  unsigned long event)
+{
+	bool addr_list_empty = true;
+	struct in_device *idev;
+
+	switch (event) {
+	case NETDEV_UP:
+		return !rif;
+	case NETDEV_DOWN:
+		idev = __in_dev_get_rtnl(dev);
+		if (idev && idev->ifa_list)
+			addr_list_empty = false;
+
+		if (netif_is_macvlan(dev) && addr_list_empty)
+			return true;
+
+		if (rif && addr_list_empty)
+			return true;
+
+		return false;
+	}
+
+	return false;
+}
+
+static int mvsw_pr_inetaddr_event(struct notifier_block *nb,
+				  unsigned long event, void *ptr)
+{
+	struct in_ifaddr *ifa = (struct in_ifaddr *)ptr;
+	struct net_device *dev = ifa->ifa_dev->dev;
+	struct mvsw_pr_router *router;
+	struct mvsw_pr_switch *sw;
+	struct mvsw_pr_rif *rif;
+	int err = 0;
+
+	/* Wait until previously created works finished (e.g. neigh events) */
+	mvsw_owq_flush();
+	/* NETDEV_UP event is handled by mvsw_pr_inetaddr_valid_event */
+	if (event == NETDEV_UP)
+		goto out;
+
+	MVSW_LOG_ERROR("dev=%s", dev->name);
+	router = container_of(nb, struct mvsw_pr_router, inetaddr_nb);
+	sw = router->sw;
+
+	if (netif_is_macvlan(dev))
+		goto mac_vlan;
+
+	rif = mvsw_pr_rif_find(sw, dev);
+	if (!rif)
+		goto out;
+
+	if (!mvsw_pr_rif_should_config(rif, dev, event))
+		goto out;
+mac_vlan:
+	err = __mvsw_pr_inetaddr_event(sw, dev, event, NULL);
+out:
+	return notifier_from_errno(err);
+}
+
+int mvsw_pr_inetaddr_valid_event(struct notifier_block *unused,
+				 unsigned long event, void *ptr)
+{
+	struct in_validator_info *ivi = (struct in_validator_info *)ptr;
+	struct net_device *dev = ivi->ivi_dev->dev;
+	struct mvsw_pr_switch *sw;
+	struct mvsw_pr_rif *rif;
+	int err = 0;
+
+	sw = mvsw_pr_switch_get(dev);
+	if (!sw)
+		goto out;
+
+	/* Wait until previously created works finished (e.g. neigh events) */
+	mvsw_owq_flush();
+
+	if (ipv4_is_multicast(ivi->ivi_addr))
+		return notifier_from_errno(-EINVAL);
+
+	rif = mvsw_pr_rif_find(sw, dev);
+	if (!mvsw_pr_rif_should_config(rif, dev, event))
+		goto out;
+
+	err = mvsw_pr_router_port_check_rif_addr(sw, dev, dev->dev_addr,
+						 ivi->extack);
+	if (err)
+		goto out;
+
+	err = __mvsw_pr_inetaddr_event(sw, dev, event, ivi->extack);
+out:
+	return notifier_from_errno(err);
+}
+
+static bool __mvsw_pr_fi_is_direct(struct fib_info *fi)
+{
+	struct fib_nh *fib_nh;
+
+	if (fib_info_num_path(fi) == 1) {
+		fib_nh = fib_info_nh(fi, 0);
+		if (fib_nh->fib_nh_scope == RT_SCOPE_HOST)
+			return true;
+	}
+
+	return false;
+}
+
+static bool mvsw_pr_fi_is_direct(struct fib_info *fi)
+{
+	if (fi->fib_type != RTN_UNICAST)
+		return false;
+
+	return __mvsw_pr_fi_is_direct(fi);
+}
+
+static bool mvsw_pr_fi_is_nh(struct fib_info *fi)
+{
+	if (fi->fib_type != RTN_UNICAST)
+		return false;
+
+	return !__mvsw_pr_fi_is_direct(fi);
+}
+
+static void __mvsw_pr_nh_neigh_destroy(struct mvsw_pr_switch *sw,
+				       struct mvsw_pr_nh_neigh *neigh)
+{
+	neigh->key.rif->ref_cnt--;
+	mvsw_pr_rif_put(neigh->key.rif);
+	rhashtable_remove_fast(&sw->router->nh_neigh_ht,
+			       &neigh->ht_node,
+			       __mvsw_pr_nh_neigh_ht_params);
+	kfree(neigh);
+}
+
+static struct mvsw_pr_nh_neigh *
+__mvsw_pr_nh_neigh_create(struct mvsw_pr_switch *sw,
+			  struct mvsw_pr_nh_neigh_key *key)
+{
+	struct mvsw_pr_nh_neigh *neigh;
+	int err;
+
+	neigh = kzalloc(sizeof(*neigh), GFP_KERNEL);
+	if (!neigh)
+		goto err_kzalloc;
+
+	memcpy(&neigh->key, key, sizeof(*key));
+	neigh->key.rif->ref_cnt++;
+	neigh->info.connected = false;
+	INIT_LIST_HEAD(&neigh->nexthop_group_list);
+	err = rhashtable_insert_fast(&sw->router->nh_neigh_ht,
+				     &neigh->ht_node,
+				     __mvsw_pr_nh_neigh_ht_params);
+	if (err)
+		goto err_rhashtable_insert;
+
+	return neigh;
+
+err_rhashtable_insert:
+	kfree(neigh);
+err_kzalloc:
+	return NULL;
+}
+
+static struct mvsw_pr_nh_neigh *
+mvsw_pr_nh_neigh_find(struct mvsw_pr_switch *sw,
+		      struct mvsw_pr_nh_neigh_key *key)
+{
+	struct mvsw_pr_nh_neigh *nh_neigh;
+
+	nh_neigh = rhashtable_lookup_fast(&sw->router->nh_neigh_ht,
+					  key, __mvsw_pr_nh_neigh_ht_params);
+	return IS_ERR(nh_neigh) ? NULL : nh_neigh;
+}
+
+static struct mvsw_pr_nh_neigh *
+mvsw_pr_nh_neigh_get(struct mvsw_pr_switch *sw,
+		     struct mvsw_pr_nh_neigh_key *key)
+{
+	struct mvsw_pr_nh_neigh *neigh;
+
+	neigh = mvsw_pr_nh_neigh_find(sw, key);
+	if (!neigh)
+		return __mvsw_pr_nh_neigh_create(sw, key);
+
+	return neigh;
+}
+
+static void mvsw_pr_nh_neigh_put(struct mvsw_pr_switch *sw,
+				 struct mvsw_pr_nh_neigh *neigh)
+{
+	if (list_empty(&neigh->nexthop_group_list))
+		__mvsw_pr_nh_neigh_destroy(sw, neigh);
+}
+
+/* Updates new mvsw_pr_neigh_info */
+static int mvsw_pr_nh_neigh_set(struct mvsw_pr_switch *sw,
+				struct mvsw_pr_nh_neigh *neigh)
+{
+	struct mvsw_pr_nh_neigh_head *nh_head;
+	struct mvsw_pr_nexthop_group *nh_grp;
+	int err;
+
+	list_for_each_entry(nh_head, &neigh->nexthop_group_list, head) {
+		nh_grp = nh_head->this;
+		err = mvsw_pr_nexthop_group_set(sw, nh_grp);
+		if (err)
+			return err;
+	}
+
+	return 0;
+}
+
+static bool __mvsw_pr_nh_neigh_key_is_valid(struct mvsw_pr_nh_neigh_key *key)
+{
+	return memchr_inv(key, 0, sizeof(*key)) ? true : false;
+}
+
+static bool
+mvsw_pr_nh_neigh_util_hw_state(struct mvsw_pr_switch *sw,
+			       struct mvsw_pr_nh_neigh *nh_neigh)
+{
+	bool state;
+	struct mvsw_pr_nh_neigh_head *nh_head, *tmp;
+
+	state = false;
+	list_for_each_entry_safe(nh_head, tmp,
+				 &nh_neigh->nexthop_group_list, head) {
+		state = mvsw_pr_nexthop_group_util_hw_state(sw, nh_head->this);
+		if (state)
+			break;
+	}
+
+	return state;
+}
+
+static size_t
+__mvsw_pr_nexthop_group_key_size(struct mvsw_pr_nexthop_group_key *key)
+{
+	size_t nh_cnt;
+
+	for (nh_cnt = 0; nh_cnt < MVSW_PR_NHGR_SIZE_MAX; nh_cnt++) {
+		if (!__mvsw_pr_nh_neigh_key_is_valid(&key->neigh[nh_cnt]))
+			break;
+	}
+
+	return nh_cnt;
+}
+
+static struct mvsw_pr_nexthop_group *
+__mvsw_pr_nexthop_group_create(struct mvsw_pr_switch *sw,
+			       struct mvsw_pr_nexthop_group_key *key)
+{
+	struct mvsw_pr_nexthop_group *nh_grp;
+	struct mvsw_pr_nh_neigh *nh_neigh;
+	int nh_cnt, err;
+
+	nh_grp = kzalloc(sizeof(*nh_grp), GFP_KERNEL);
+	if (!nh_grp)
+		goto err_kzalloc;
+
+	memcpy(&nh_grp->key, key, sizeof(*key));
+	for (nh_cnt = 0; nh_cnt < MVSW_PR_NHGR_SIZE_MAX; nh_cnt++) {
+		if (!__mvsw_pr_nh_neigh_key_is_valid(&nh_grp->key.neigh[nh_cnt])
+		   )
+			break;
+
+		nh_neigh = mvsw_pr_nh_neigh_get(sw, &nh_grp->key.neigh[nh_cnt]);
+		if (!nh_neigh)
+			goto err_nh_neigh_get;
+
+		nh_grp->nh_neigh_head[nh_cnt].neigh = nh_neigh;
+		nh_grp->nh_neigh_head[nh_cnt].this = nh_grp;
+		list_add(&nh_grp->nh_neigh_head[nh_cnt].head,
+			 &nh_neigh->nexthop_group_list);
+	}
+
+	err = mvsw_pr_nh_group_create(sw, nh_cnt, &nh_grp->grp_id);
+	if (err)
+		goto err_nh_group_create;
+
+	err = mvsw_pr_nexthop_group_set(sw, nh_grp);
+	if (err)
+		goto err_nexthop_group_set;
+
+	err = rhashtable_insert_fast(&sw->router->nexthop_group_ht,
+				     &nh_grp->ht_node,
+				     __mvsw_pr_nexthop_group_ht_params);
+	if (err)
+		goto err_ht_insert;
+
+	return nh_grp;
+
+err_ht_insert:
+err_nexthop_group_set:
+	mvsw_pr_nh_group_delete(sw, nh_cnt, nh_grp->grp_id);
+err_nh_group_create:
+err_nh_neigh_get:
+	for (nh_cnt--; nh_cnt >= 0; nh_cnt--) {
+		list_del(&nh_grp->nh_neigh_head[nh_cnt].head);
+		mvsw_pr_nh_neigh_put(sw, nh_grp->nh_neigh_head[nh_cnt].neigh);
+	}
+
+	kfree(nh_grp);
+err_kzalloc:
+	return NULL;
+}
+
+static void
+__mvsw_pr_nexthop_group_destroy(struct mvsw_pr_switch *sw,
+				struct mvsw_pr_nexthop_group *nh_grp)
+{
+	struct mvsw_pr_nh_neigh *nh_neigh;
+	int nh_cnt;
+
+	rhashtable_remove_fast(&sw->router->nexthop_group_ht,
+			       &nh_grp->ht_node,
+			       __mvsw_pr_nexthop_group_ht_params);
+
+	for (nh_cnt = 0; nh_cnt < MVSW_PR_NHGR_SIZE_MAX; nh_cnt++) {
+		nh_neigh = nh_grp->nh_neigh_head[nh_cnt].neigh;
+		if (!nh_neigh)
+			break;
+
+		list_del(&nh_grp->nh_neigh_head[nh_cnt].head);
+		mvsw_pr_nh_neigh_put(sw, nh_neigh);
+	}
+
+	mvsw_pr_nh_group_delete(sw, nh_cnt, nh_grp->grp_id);
+	kfree(nh_grp);
+}
+
+static struct mvsw_pr_nexthop_group *
+mvsw_pr_nexthop_group_find(struct mvsw_pr_switch *sw,
+			   struct mvsw_pr_nexthop_group_key *key)
+{
+	struct mvsw_pr_nexthop_group *nh_grp;
+
+	nh_grp = rhashtable_lookup_fast(&sw->router->nexthop_group_ht,
+					key, __mvsw_pr_nexthop_group_ht_params);
+	return IS_ERR(nh_grp) ? NULL : nh_grp;
+}
+
+static struct mvsw_pr_nexthop_group *
+mvsw_pr_nexthop_group_get(struct mvsw_pr_switch *sw,
+			  struct mvsw_pr_nexthop_group_key *key)
+{
+	struct mvsw_pr_nexthop_group *nh_grp;
+
+	nh_grp = mvsw_pr_nexthop_group_find(sw, key);
+	if (!nh_grp)
+		return __mvsw_pr_nexthop_group_create(sw, key);
+
+	return nh_grp;
+}
+
+static void mvsw_pr_nexthop_group_put(struct mvsw_pr_switch *sw,
+				      struct mvsw_pr_nexthop_group *nh_grp)
+{
+	if (!nh_grp->ref_cnt)
+		__mvsw_pr_nexthop_group_destroy(sw, nh_grp);
+}
+
+/* Updates with new nh_neigh's info */
+static int mvsw_pr_nexthop_group_set(struct mvsw_pr_switch *sw,
+				     struct mvsw_pr_nexthop_group *nh_grp)
+{
+	struct mvsw_pr_neigh_info info[MVSW_PR_NHGR_SIZE_MAX];
+	struct mvsw_pr_nh_neigh *neigh;
+	int nh_cnt;
+
+	memset(&info[0], 0, sizeof(info));
+	for (nh_cnt = 0; nh_cnt < MVSW_PR_NHGR_SIZE_MAX; nh_cnt++) {
+		neigh = nh_grp->nh_neigh_head[nh_cnt].neigh;
+		if (!neigh)
+			break;
+
+		memcpy(&info[nh_cnt], &neigh->info, sizeof(neigh->info));
+	}
+
+	return mvsw_pr_nh_entries_set(sw, nh_cnt, &info[0], nh_grp->grp_id);
+}
+
+static bool
+mvsw_pr_nexthop_group_util_hw_state(struct mvsw_pr_switch *sw,
+				    struct mvsw_pr_nexthop_group *nh_grp)
+{
+	int err, nh_cnt;
+	struct mvsw_pr_neigh_info info[MVSW_PR_NHGR_SIZE_MAX];
+	size_t grp_size;
+
+	/* Antijitter
+	 * Prevent situation, when we read state of nh_grp twice in short time,
+	 * and state bit is still cleared on second call. So just stuck active
+	 * state for MVSW_PR_NH_ACTIVE_JIFFER_FILTER, after last occurred.
+	 */
+	if (time_before(jiffies, nh_grp->hw_last_connected +
+			msecs_to_jiffies(MVSW_PR_NH_ACTIVE_JIFFER_FILTER)))
+		return true;
+
+	grp_size =  __mvsw_pr_nexthop_group_key_size(&nh_grp->key);
+	err = mvsw_pr_nh_entries_get(sw, grp_size, &info[0], nh_grp->grp_id);
+	if (err) {
+		MVSW_LOG_ERROR("Failed to get hw state of nh_grp %d",
+			       nh_grp->grp_id);
+		return false;
+	}
+
+	for (nh_cnt = 0; nh_cnt < grp_size; nh_cnt++)
+		if (info[nh_cnt].connected) {
+			nh_grp->hw_last_connected = jiffies;
+			return true;
+		}
+
+	return false;
+}
+
+static struct mvsw_pr_fib_node *
+mvsw_pr_fib_node_find(struct mvsw_pr_switch *sw, struct mvsw_pr_fib_key *key)
+{
+	struct mvsw_pr_fib_node *fib_node;
+
+	fib_node = rhashtable_lookup_fast(&sw->router->fib_ht, key,
+					  __mvsw_pr_fib_ht_params);
+	return IS_ERR(fib_node) ? NULL : fib_node;
+}
+
+static void __mvsw_pr_fib_node_destruct(struct mvsw_pr_switch *sw,
+					struct mvsw_pr_fib_node *fib_node)
+{
+	struct mvsw_pr_vr *vr;
+
+	vr = fib_node->info.vr;
+	mvsw_pr_lpm_del(sw, vr->hw_vr_id, &fib_node->key.addr,
+			fib_node->key.prefix_len);
+	switch (fib_node->info.type) {
+	case MVSW_PR_FIB_TYPE_UC_NH:
+		fib_node->info.nh_grp->ref_cnt--;
+		mvsw_pr_nexthop_group_put(sw, fib_node->info.nh_grp);
+		break;
+	case MVSW_PR_FIB_TYPE_TRAP:
+		break;
+	case MVSW_PR_FIB_TYPE_DROP:
+		break;
+	default:
+	      MVSW_LOG_ERROR("Unknown fib_node->info.type = %d",
+			     fib_node->info.type);
+	}
+
+	vr->ref_cnt--;
+	mvsw_pr_vr_put(sw, vr);
+}
+
+static void mvsw_pr_fib_node_destroy(struct mvsw_pr_switch *sw,
+				     struct mvsw_pr_fib_node *fib_node)
+{
+	__mvsw_pr_fib_node_destruct(sw, fib_node);
+	rhashtable_remove_fast(&sw->router->fib_ht, &fib_node->ht_node,
+			       __mvsw_pr_fib_ht_params);
+	kfree(fib_node);
+}
+
+static void mvsw_pr_fib_node_destroy_ht(struct mvsw_pr_switch *sw)
+{
+	struct mvsw_pr_fib_node *node, *tnode;
+	struct rhashtable_iter iter;
+
+	tnode = NULL;
+	rhashtable_walk_enter(&sw->router->fib_ht, &iter);
+	rhashtable_walk_start(&iter);
+	while (1) {
+		node = rhashtable_walk_next(&iter);
+		if (tnode) {
+			rhashtable_remove_fast(&sw->router->fib_ht,
+					       &tnode->ht_node,
+					       __mvsw_pr_fib_ht_params);
+			kfree(tnode);
+			tnode = NULL;
+		}
+
+		if (!node)
+			break;
+
+		if (IS_ERR(node))
+			continue;
+
+		__mvsw_pr_fib_node_destruct(sw, node);
+		tnode = node;
+	}
+	rhashtable_walk_stop(&iter);
+	rhashtable_walk_exit(&iter);
+}
+
+static struct mvsw_pr_fib_node *
+mvsw_pr_fib_node_uc_nh_create(struct mvsw_pr_switch *sw,
+			      struct mvsw_pr_fib_key *key,
+			      struct mvsw_pr_nexthop_group_key *nh_grp_key)
+{
+	struct mvsw_pr_fib_node *fib_node;
+	struct mvsw_pr_nexthop_group *nh_grp;
+	struct mvsw_pr_vr *vr;
+	int err;
+
+	fib_node = kzalloc(sizeof(*fib_node), GFP_KERNEL);
+	if (!fib_node)
+		goto err_kzalloc;
+
+	memcpy(&fib_node->key, key, sizeof(*key));
+	fib_node->info.type = MVSW_PR_FIB_TYPE_UC_NH;
+
+	vr = mvsw_pr_vr_get(sw, key->tb_id, NULL);
+	if (!vr)
+		goto err_vr_get;
+
+	fib_node->info.vr = vr;
+	vr->ref_cnt++;
+
+	nh_grp = mvsw_pr_nexthop_group_get(sw, nh_grp_key);
+	if (!nh_grp)
+		goto err_nh_grp_get;
+
+	fib_node->info.nh_grp = nh_grp;
+	nh_grp->ref_cnt++;
+
+	err = mvsw_pr_lpm_add(sw, vr->hw_vr_id, &key->addr,
+			      key->prefix_len, nh_grp->grp_id);
+	if (err)
+		goto err_lpm_add;
+
+	err = rhashtable_insert_fast(&sw->router->fib_ht, &fib_node->ht_node,
+				     __mvsw_pr_fib_ht_params);
+	if (err)
+		goto err_ht_insert;
+
+	return fib_node;
+
+err_ht_insert:
+	mvsw_pr_lpm_del(sw, vr->hw_vr_id, &key->addr, key->prefix_len);
+err_lpm_add:
+	nh_grp->ref_cnt--;
+	mvsw_pr_nexthop_group_put(sw, nh_grp);
+err_nh_grp_get:
+	vr->ref_cnt--;
+	mvsw_pr_vr_put(sw, vr);
+err_vr_get:
+	kfree(fib_node);
+err_kzalloc:
+	return NULL;
+}
+
+/* Decided, that uc_nh route with key==nh is obviously neighbour route */
+static bool
+mvsw_pr_fib_node_util_is_neighbour(struct mvsw_pr_fib_node *fib_node)
+{
+	if (fib_node->info.type != MVSW_PR_FIB_TYPE_UC_NH)
+		return false;
+
+	if (fib_node->info.nh_grp->nh_neigh_head[1].neigh)
+		return false;
+
+	if (!fib_node->info.nh_grp->nh_neigh_head[0].neigh)
+		return false;
+
+	if (memcmp(&fib_node->info.nh_grp->nh_neigh_head[0].neigh->key.addr,
+		   &fib_node->key.addr, sizeof(struct mvsw_pr_ip_addr)))
+		return false;
+
+	return true;
+}
+
+static struct mvsw_pr_fib_node *
+mvsw_pr_fib_node_trap_create(struct mvsw_pr_switch *sw,
+			     struct mvsw_pr_fib_key *key)
+{
+	struct mvsw_pr_fib_node *fib_node;
+	struct mvsw_pr_vr *vr;
+	int err;
+
+	fib_node = kzalloc(sizeof(*fib_node), GFP_KERNEL);
+	if (!fib_node)
+		goto err_kzalloc;
+
+	vr = mvsw_pr_vr_get(sw, key->tb_id, NULL);
+	if (!vr)
+		goto err_vr_get;
+
+	fib_node->info.vr = vr;
+	vr->ref_cnt++;
+
+	memcpy(&fib_node->key, key, sizeof(*key));
+	fib_node->info.type = MVSW_PR_FIB_TYPE_TRAP;
+	err = mvsw_pr_lpm_add(sw, vr->hw_vr_id, &key->addr,
+			      key->prefix_len, MVSW_PR_NHGR_UNUSED);
+	if (err)
+		goto err_lpm_add;
+
+	err = rhashtable_insert_fast(&sw->router->fib_ht, &fib_node->ht_node,
+				     __mvsw_pr_fib_ht_params);
+	if (err)
+		goto err_ht_insert;
+
+	return fib_node;
+
+err_ht_insert:
+	mvsw_pr_lpm_del(sw, vr->hw_vr_id, &key->addr, key->prefix_len);
+err_lpm_add:
+	vr->ref_cnt--;
+	mvsw_pr_vr_put(sw, vr);
+err_vr_get:
+	kfree(fib_node);
+err_kzalloc:
+	return NULL;
+}
+
+static struct mvsw_pr_fib_node *
+mvsw_pr_fib_node_drop_create(struct mvsw_pr_switch *sw,
+			     struct mvsw_pr_fib_key *key)
+{
+	struct mvsw_pr_fib_node *fib_node;
+	struct mvsw_pr_vr *vr;
+	int err;
+
+	fib_node = kzalloc(sizeof(*fib_node), GFP_KERNEL);
+	if (!fib_node)
+		goto err_kzalloc;
+
+	vr = mvsw_pr_vr_get(sw, key->tb_id, NULL);
+	if (!vr)
+		goto err_vr_get;
+
+	fib_node->info.vr = vr;
+	vr->ref_cnt++;
+
+	memcpy(&fib_node->key, key, sizeof(*key));
+	fib_node->info.type = MVSW_PR_FIB_TYPE_DROP;
+	err = mvsw_pr_lpm_add(sw, vr->hw_vr_id,
+			      &key->addr, key->prefix_len, MVSW_PR_NHGR_DROP);
+	if (err)
+		goto err_lpm_add;
+
+	err = rhashtable_insert_fast(&sw->router->fib_ht, &fib_node->ht_node,
+				     __mvsw_pr_fib_ht_params);
+	if (err)
+		goto err_ht_insert;
+
+	return fib_node;
+
+err_ht_insert:
+	mvsw_pr_lpm_del(sw, vr->hw_vr_id,
+			&key->addr, key->prefix_len);
+err_lpm_add:
+	vr->ref_cnt--;
+	mvsw_pr_vr_put(sw, vr);
+err_vr_get:
+	kfree(fib_node);
+err_kzalloc:
+	return NULL;
+}
+
+static void mvsw_pr_fib_nh_del2nh_neigh_set(struct mvsw_pr_switch *sw,
+					    struct fib_nh *fib_nh)
+{
+	struct mvsw_pr_nh_neigh_key nh_neigh_key;
+	struct mvsw_pr_nh_neigh *nh_neigh;
+
+	memset(&nh_neigh_key, 0, sizeof(nh_neigh_key));
+	nh_neigh_key.addr.u.ipv4 = fib_nh->fib_nh_gw4;
+	nh_neigh_key.rif = mvsw_pr_rif_find(sw, fib_nh->fib_nh_dev);
+	if (!nh_neigh_key.rif)
+		return;
+
+	nh_neigh = mvsw_pr_nh_neigh_find(sw, &nh_neigh_key);
+	if (!nh_neigh)
+		return;
+
+	nh_neigh->info.connected = false;
+	mvsw_pr_nh_neigh_set(sw, nh_neigh);
+}
+
+static void __mvsw_pr_fen_info2fib_key(struct fib_entry_notifier_info *fen_info,
+				       struct mvsw_pr_fib_key *key)
+{
+	memset(key, 0, sizeof(*key));
+	key->addr.u.ipv4 = cpu_to_be32(fen_info->dst);
+	key->prefix_len = fen_info->dst_len;
+	key->tb_id = mvsw_pr_fix_tb_id(fen_info->tb_id);
+}
+
+static int
+mvsw_pr_fi2nh_gr_key(struct mvsw_pr_switch *sw, struct fib_info *fi,
+		     size_t limit, struct mvsw_pr_nexthop_group_key *grp_key)
+{
+	int i, nhs, err;
+	struct fib_nh *fib_nh;
+
+	nhs = fib_info_num_path(fi);
+	if (nhs > limit)
+		return 0;
+
+	memset(grp_key, 0, sizeof(*grp_key));
+	for (i = 0; i < nhs; i++) {
+		fib_nh = fib_info_nh(fi, i);
+		err = mvsw_pr_util_fib_nh2nh_neigh_key(sw,
+						       fib_nh,
+						       &grp_key->neigh[i]);
+		if (err)
+			return 0;
+	}
+
+	return nhs;
+}
+
+static int
+mvsw_pr_router_fib_replace(struct mvsw_pr_switch *sw,
+			   struct fib_entry_notifier_info *fen_info)
+{
+	struct mvsw_pr_fib_node *fib_node;
+	struct mvsw_pr_fib_key fib_key;
+	struct mvsw_pr_nexthop_group_key nh_grp_key;
+	int nh_cnt;
+
+	__mvsw_pr_fen_info2fib_key(fen_info, &fib_key);
+
+	fib_node = mvsw_pr_fib_node_find(sw, &fib_key);
+	if (fib_node) {
+		MVSW_LOG_INFO("fib_node found. destroy.");
+		mvsw_pr_fib_node_destroy(sw, fib_node);
+	}
+
+	/* TODO: fib lookup here to check if another route with the same key
+	 * is occurred in another kernel's table
+	 */
+	switch (fen_info->fi->fib_type) {
+	case RTN_UNICAST:
+		if (mvsw_pr_fi_is_nh(fen_info->fi))
+			nh_cnt = mvsw_pr_fi2nh_gr_key(sw, fen_info->fi,
+						      MVSW_PR_NHGR_SIZE_MAX,
+						      &nh_grp_key);
+		else
+			nh_cnt = 0;
+
+		if (nh_cnt) {
+			fib_node = mvsw_pr_fib_node_uc_nh_create(sw, &fib_key,
+								 &nh_grp_key);
+		} else {
+			fib_node = mvsw_pr_fib_node_trap_create(sw, &fib_key);
+		}
+
+		if (!fib_node)
+			goto err_fib_create;
+
+		break;
+	/* Unsupported. Leave it for kernel: */
+	case RTN_BROADCAST:
+	case RTN_MULTICAST:
+	/* Routes we must trap by design: */
+	case RTN_LOCAL:
+	case RTN_UNREACHABLE:
+	case RTN_PROHIBIT:
+		fib_node = mvsw_pr_fib_node_trap_create(sw, &fib_key);
+		if (!fib_node)
+			goto err_fib_create;
+		break;
+	case RTN_BLACKHOLE:
+		fib_node = mvsw_pr_fib_node_drop_create(sw, &fib_key);
+		if (!fib_node)
+			goto err_fib_create;
+
+		break;
+	default:
+		goto err_type;
+	}
+
+	mvsw_pr_neigh_arbiter_fib_evt(sw, true, &fib_key, fen_info->fi);
+
+	return 0;
+
+err_fib_create:
+	MVSW_LOG_ERROR("fib_create failed");
+	goto err_out;
+err_type:
+	MVSW_LOG_ERROR("Invalid fen_info->fi->fib_type %d",
+		       fen_info->fi->fib_type);
+	goto err_out;
+err_out:
+	MVSW_LOG_ERROR("Error when processing %pI4h/%d", &fen_info->dst,
+		       fen_info->dst_len);
+	return -EINVAL;
+}
+
+static void mvsw_pr_router_fib_del(struct mvsw_pr_switch *sw,
+				   struct fib_entry_notifier_info *fen_info)
+{
+	struct mvsw_pr_fib_node *fib_node;
+	struct mvsw_pr_fib_key fib_key;
+
+	/* TODO: fib lookup here to check if another route with the same key
+	 * is occurred in another kernel's table
+	 */
+	__mvsw_pr_fen_info2fib_key(fen_info, &fib_key);
+
+	fib_node = mvsw_pr_fib_node_find(sw, &fib_key);
+	if (fib_node) {
+		mvsw_pr_fib_node_destroy(sw, fib_node);
+	} else {
+		MVSW_LOG_ERROR("Cant find fib_node");
+		goto err_out;
+	}
+
+	mvsw_pr_neigh_arbiter_fib_evt(sw, false, &fib_key, fen_info->fi);
+
+	return;
+
+err_out:
+	MVSW_LOG_ERROR("Error when processing %pI4h/%d", &fen_info->dst,
+		       fen_info->dst_len);
+}
+
+static void mvsw_pr_router_fib_abort(struct mvsw_pr_switch *sw)
+{
+	mvsw_pr_vr_util_hw_abort(sw);
+	mvsw_pr_fib_node_destroy_ht(sw);
+	mvsw_pr_kern_fib_cache_destroy_ht(sw);
+	mvsw_pr_util_kern_unset_allneigh_offload();
+}
+
+struct mvsw_pr_fib_event_work {
+	struct work_struct work;
+	struct mvsw_pr_switch *sw;
+	union {
+		struct fib_entry_notifier_info fen_info;
+		struct fib_nh_notifier_info fnh_info;
+	};
+	unsigned long event;
+};
+
+static void mvsw_pr_router_fib4_event_work(struct work_struct *work)
+{
+	struct mvsw_pr_fib_event_work *fib_work =
+			container_of(work, struct mvsw_pr_fib_event_work, work);
+	struct mvsw_pr_switch *sw = fib_work->sw;
+	int err;
+
+	mvsw_owq_lock();
+
+	if (sw->router->aborted)
+		goto out;
+
+	switch (fib_work->event) {
+	case FIB_EVENT_ENTRY_REPLACE:
+		err = mvsw_pr_router_fib_replace(sw,
+						 &fib_work->fen_info);
+		if (err)
+			goto abort_out;
+		break;
+	case FIB_EVENT_ENTRY_DEL:
+		mvsw_pr_router_fib_del(sw, &fib_work->fen_info);
+		break;
+	}
+
+	goto out;
+
+abort_out:
+	sw->router->aborted = true;
+	mvsw_pr_router_fib_abort(sw);
+	dev_err(sw->dev->dev, "Abort. HW routing offloading disabled");
+out:
+	fib_info_put(fib_work->fen_info.fi);
+	mvsw_owq_unlock();
+	kfree(fib_work);
+}
+
+static void mvsw_pr_router_nh_update_event_work(struct work_struct *work)
+{
+	struct mvsw_pr_fib_event_work *fib_work =
+			container_of(work, struct mvsw_pr_fib_event_work, work);
+	struct mvsw_pr_switch *sw = fib_work->sw;
+	struct fib_nh *fib_nh = fib_work->fnh_info.fib_nh;
+
+	mvsw_owq_lock();
+
+	if (sw->router->aborted)
+		goto out;
+
+	/* For now provided only deletion */
+	if (fib_work->event == FIB_EVENT_NH_DEL)
+		mvsw_pr_fib_nh_del2nh_neigh_set(sw, fib_nh);
+
+out:
+	fib_info_put(fib_nh->nh_parent);
+	mvsw_owq_unlock();
+	kfree(fib_work);
+	return;
+}
+
+/* Called with rcu_read_lock() */
+static int mvsw_pr_router_fib_event(struct notifier_block *nb,
+				    unsigned long event, void *ptr)
+{
+	struct fib_entry_notifier_info *fen_info;
+	struct fib_nh_notifier_info *fnh_info;
+	struct fib_notifier_info *info = ptr;
+	struct mvsw_pr_fib_event_work *fib_work;
+	struct mvsw_pr_router *router;
+	struct fib_info *fi;
+
+	if (info->family != AF_INET)
+		return NOTIFY_DONE;
+
+	router = container_of(nb, struct mvsw_pr_router, fib_nb);
+
+	switch (event) {
+	case FIB_EVENT_ENTRY_REPLACE:
+	case FIB_EVENT_ENTRY_DEL:
+		fen_info = container_of(info, struct fib_entry_notifier_info,
+					info);
+		if (!fen_info->fi)
+			return NOTIFY_DONE;
+		else
+			fi = fen_info->fi;
+
+		/* Sanity */
+		if (event == FIB_EVENT_ENTRY_REPLACE) {
+			if (fi->nh)
+				return notifier_from_errno(-EINVAL);
+
+			if (fi->fib_nh_is_v6)
+				return notifier_from_errno(-EINVAL);
+
+			if (fib_info_num_path(fi) > MVSW_PR_NHGR_SIZE_MAX) {
+				NL_SET_ERR_MSG_MOD(info->extack,
+						   "Exceeded number of nexthops per route"
+						   );
+				return notifier_from_errno(-EINVAL);
+			}
+		}
+
+		fib_work = kzalloc(sizeof(*fib_work), GFP_ATOMIC);
+		if (WARN_ON(!fib_work))
+			return NOTIFY_BAD;
+
+		fib_info_hold(fi);
+		fib_work->fen_info = *fen_info;
+		fib_work->event = event;
+		fib_work->sw = router->sw;
+		INIT_WORK(&fib_work->work, mvsw_pr_router_fib4_event_work);
+		queue_work(mvsw_r_owq, &fib_work->work);
+		break;
+	case FIB_EVENT_NH_DEL:
+		/* Set down nh as fast as possible */
+		fnh_info = container_of(info, struct fib_nh_notifier_info,
+					info);
+		if (!fnh_info->fib_nh->nh_parent)
+			return NOTIFY_DONE;
+
+		fi = fnh_info->fib_nh->nh_parent;
+
+		fib_work = kzalloc(sizeof(*fib_work), GFP_ATOMIC);
+		if (WARN_ON(!fib_work))
+			return NOTIFY_BAD;
+
+		fib_info_hold(fi);
+		fib_work->fnh_info = *fnh_info;
+		fib_work->event = event;
+		fib_work->sw = router->sw;
+		INIT_WORK(&fib_work->work, mvsw_pr_router_nh_update_event_work);
+		queue_work(mvsw_r_owq, &fib_work->work);
+		break;
+	default:
+		return NOTIFY_DONE;
+	}
+
+	return NOTIFY_DONE;
+}
+
+static void mvsw_pr_router_fib_dump_flush(struct notifier_block *nb)
+{
+	struct mvsw_pr_router *router;
+
+	/* Flush pending FIB notifications and then flush the device's
+	 * table before requesting another dump. The FIB notification
+	 * block is unregistered, so no need to take RTNL.
+	 * No neighbours are expected to be present since FIBs  are not
+	 * registered yet
+	 */
+	router = container_of(nb, struct mvsw_pr_router, fib_nb);
+	flush_workqueue(mvsw_r_owq);
+	flush_workqueue(mvsw_r_wq);
+	mvsw_pr_fib_node_destroy_ht(router->sw);
+}
+
+static int
+mvsw_pr_router_port_change(struct mvsw_pr_rif *rif)
+{
+	struct net_device *dev = rif->dev;
+	int err;
+
+	err = mvsw_pr_rif_update(rif, dev->dev_addr);
+	if (err)
+		return err;
+
+	ether_addr_copy(rif->addr, dev->dev_addr);
+	rif->mtu = dev->mtu;
+
+	netdev_dbg(dev, "Updated RIF=%d\n", rif->rif_id);
+
+	return 0;
+}
+
+static int
+mvsw_pr_router_port_pre_change(struct mvsw_pr_rif *rif,
+			       struct netdev_notifier_pre_changeaddr_info *info)
+{
+	struct netlink_ext_ack *extack;
+
+	extack = netdev_notifier_info_to_extack(&info->info);
+	return mvsw_pr_router_port_check_rif_addr(rif->sw, rif->dev,
+						  info->dev_addr, extack);
+}
+
+int mvsw_pr_netdevice_router_port_event(struct net_device *dev,
+					unsigned long event, void *ptr)
+{
+	struct mvsw_pr_switch *sw;
+	struct mvsw_pr_rif *rif;
+
+	sw = mvsw_pr_switch_get(dev);
+	if (!sw)
+		return 0;
+
+	rif = mvsw_pr_rif_find(sw, dev);
+	if (!rif)
+		return 0;
+
+	switch (event) {
+	case NETDEV_CHANGEADDR:
+		return mvsw_pr_router_port_change(rif);
+	case NETDEV_PRE_CHANGEADDR:
+		return mvsw_pr_router_port_pre_change(rif, ptr);
+	}
+
+	return 0;
+}
+
+static int mvsw_pr_port_vrf_join(struct mvsw_pr_switch *sw,
+				 struct net_device *dev,
+				 struct netlink_ext_ack *extack)
+{
+	struct mvsw_pr_rif *rif;
+
+	/* If netdev is already associated with a RIF, then we need to
+	 * destroy it and create a new one with the new virtual router ID.
+	 */
+	rif = mvsw_pr_rif_find(sw, dev);
+	if (rif)
+		__mvsw_pr_inetaddr_event(sw, dev, NETDEV_DOWN, extack);
+
+	__mvsw_pr_inetaddr_event(sw, dev, NETDEV_UP, extack);
+	rif = mvsw_pr_rif_find(sw, dev);
+	return mvsw_pr_rif_vr_update(sw, rif, extack);
+}
+
+static void mvsw_pr_port_vrf_leave(struct mvsw_pr_switch *sw,
+				   struct net_device *dev,
+				   struct netlink_ext_ack *extack)
+{
+	struct mvsw_pr_rif *rif;
+	struct in_device *idev;
+
+	rif = mvsw_pr_rif_find(sw, dev);
+	if (!rif)
+		return;
+
+	__mvsw_pr_inetaddr_event(sw, dev, NETDEV_DOWN, NULL);
+
+	rif = mvsw_pr_rif_find(sw, dev);
+	if (rif)
+		mvsw_pr_rif_vr_update(sw, rif, extack);
+
+	idev = __in_dev_get_rtnl(dev);
+	/* Restore rif in the default vrf: do so only if IF address's present*/
+	if (idev && idev->ifa_list)
+		__mvsw_pr_inetaddr_event(sw, dev, NETDEV_UP, NULL);
+}
+
+int mvsw_pr_netdevice_vrf_event(struct net_device *dev, unsigned long event,
+				struct netdev_notifier_changeupper_info *info)
+{
+	struct mvsw_pr_switch *sw = mvsw_pr_switch_get(dev);
+	struct netlink_ext_ack *extack = NULL;
+	int err = 0;
+
+	if (!sw || netif_is_macvlan(dev))
+		return 0;
+
+	switch (event) {
+	case NETDEV_PRECHANGEUPPER:
+		return 0;
+	case NETDEV_CHANGEUPPER:
+		extack = netdev_notifier_info_to_extack(&info->info);
+		if (info->linking)
+			err = mvsw_pr_port_vrf_join(sw, dev, extack);
+		else
+			mvsw_pr_port_vrf_leave(sw, dev, extack);
+		break;
+	}
+
+	return err;
+}
+
+static int __mvsw_pr_rif_macvlan_flush(struct net_device *dev, void *data)
+{
+	struct mvsw_pr_rif *rif = data;
+
+	if (!netif_is_macvlan(dev))
+		return 0;
+
+	return mvsw_pr_rif_fdb_op(rif, dev->dev_addr, false);
+}
+
+static int mvsw_pr_rif_macvlan_flush(struct mvsw_pr_rif *rif)
+{
+	if (!netif_is_macvlan_port(rif->dev))
+		return 0;
+
+	netdev_warn(rif->dev,
+		    "Router interface is deleted. Upper macvlans will not work\n");
+	return netdev_walk_all_upper_dev_rcu(rif->dev,
+					     __mvsw_pr_rif_macvlan_flush, rif);
+}
+
+#ifdef CONFIG_IP_ROUTE_MULTIPATH
+static int mvsw_pr_mp_hash_init(struct mvsw_pr_switch *sw)
+{
+	u8 hash_policy;
+
+	hash_policy = init_net.ipv4.sysctl_fib_multipath_hash_policy;
+	return  mvsw_pr_mp4_hash_set(sw, hash_policy);
+}
+#else
+static int mvsw_pr_mp_hash_init(struct mvsw_pr_switch *sw)
+{
+	return 0;
+}
+#endif
+
+static struct notifier_block mvsw_pr_inetaddr_valid_nb __read_mostly = {
+	.notifier_call = mvsw_pr_inetaddr_valid_event,
+};
+
+int mvsw_pr_router_init(struct mvsw_pr_switch *sw)
+{
+	struct mvsw_pr_router *router;
+	int err;
+
+	router = kzalloc(sizeof(*sw->router), GFP_KERNEL);
+	if (!router)
+		return -ENOMEM;
+	sw->router = router;
+	router->sw = sw;
+
+	err = mvsw_pr_mp_hash_init(sw);
+	if (err)
+		goto err_mp_hash_init;
+
+	err = rhashtable_init(&router->nexthop_group_ht,
+			      &__mvsw_pr_nexthop_group_ht_params);
+	if (err)
+		goto err_nexthop_grp_ht_init;
+
+	err = rhashtable_init(&router->fib_ht,
+			      &__mvsw_pr_fib_ht_params);
+	if (err)
+		goto err_fib_ht_init;
+
+	err = rhashtable_init(&router->kern_fib_cache_ht,
+			      &__mvsw_pr_kern_fib_cache_ht_params);
+	if (err)
+		goto err_kern_fib_cache_ht_init;
+
+	err = rhashtable_init(&router->kern_neigh_cache_ht,
+			      &__mvsw_pr_kern_neigh_cache_ht_params);
+	if (err)
+		goto err_kern_neigh_cache_ht_init;
+
+	INIT_LIST_HEAD(&sw->router->rif_list);
+	INIT_LIST_HEAD(&sw->router->vr_list);
+
+	mvsw_r_wq = alloc_workqueue(mvsw_driver_name, 0, 0);
+	if (!mvsw_r_wq) {
+		err = -ENOMEM;
+		goto err_alloc_workqueue;
+	}
+
+	mvsw_r_owq = alloc_ordered_workqueue("%s_ordered", 0, "mvsw_prestera");
+	if (!mvsw_r_owq) {
+		err = -ENOMEM;
+		goto err_alloc_oworkqueue;
+	}
+
+	err = register_inetaddr_validator_notifier(&mvsw_pr_inetaddr_valid_nb);
+	if (err)
+		goto err_register_inetaddr_validator_notifier;
+
+	router->inetaddr_nb.notifier_call = mvsw_pr_inetaddr_event;
+	err = register_inetaddr_notifier(&router->inetaddr_nb);
+	if (err)
+		goto err_register_inetaddr_notifier;
+
+	err = mvsw_pr_neigh_init(sw);
+	if (err)
+		goto err_neigh_init;
+
+	sw->router->netevent_nb.notifier_call = mvsw_pr_router_netevent_event;
+	err = register_netevent_notifier(&sw->router->netevent_nb);
+	if (err)
+		goto err_register_netevent_notifier;
+
+	sw->router->fib_nb.notifier_call = mvsw_pr_router_fib_event;
+	err = register_fib_notifier(&init_net, &sw->router->fib_nb,
+				    mvsw_pr_router_fib_dump_flush, NULL);
+	if (err)
+		goto err_register_fib_notifier;
+
+	return 0;
+
+err_register_fib_notifier:
+	unregister_netevent_notifier(&sw->router->netevent_nb);
+err_register_netevent_notifier:
+	mvsw_pr_neigh_fini(sw);
+err_neigh_init:
+	unregister_inetaddr_notifier(&router->inetaddr_nb);
+err_register_inetaddr_notifier:
+	unregister_inetaddr_validator_notifier(&mvsw_pr_inetaddr_valid_nb);
+err_register_inetaddr_validator_notifier:
+	destroy_workqueue(mvsw_r_owq);
+err_alloc_oworkqueue:
+	destroy_workqueue(mvsw_r_wq);
+err_alloc_workqueue:
+	rhashtable_destroy(&router->kern_neigh_cache_ht);
+err_kern_neigh_cache_ht_init:
+	rhashtable_destroy(&router->kern_fib_cache_ht);
+err_kern_fib_cache_ht_init:
+	rhashtable_destroy(&router->fib_ht);
+err_fib_ht_init:
+	rhashtable_destroy(&router->nexthop_group_ht);
+err_nexthop_grp_ht_init:
+err_mp_hash_init:
+	kfree(sw->router);
+	return err;
+}
+
+static void mvsw_pr_rifs_fini(struct mvsw_pr_switch *sw)
+{
+	struct mvsw_pr_rif *rif, *tmp;
+
+	list_for_each_entry_safe(rif, tmp, &sw->router->rif_list, router_node) {
+		rif->is_active = false;
+		mvsw_pr_rif_destroy(rif);
+	}
+}
+
+void mvsw_pr_router_fini(struct mvsw_pr_switch *sw)
+{
+	unregister_fib_notifier(&init_net, &sw->router->fib_nb);
+	unregister_netevent_notifier(&sw->router->netevent_nb);
+	mvsw_pr_neigh_fini(sw);
+	/* TODO: check if vrs necessary ? */
+	mvsw_pr_rifs_fini(sw);
+	unregister_inetaddr_notifier(&sw->router->inetaddr_nb);
+	unregister_inetaddr_validator_notifier(&mvsw_pr_inetaddr_valid_nb);
+
+	rhashtable_destroy(&sw->router->fib_ht);
+	rhashtable_destroy(&sw->router->nexthop_group_ht);
+
+	flush_workqueue(mvsw_r_wq);
+	flush_workqueue(mvsw_r_owq);
+	destroy_workqueue(mvsw_r_wq);
+	destroy_workqueue(mvsw_r_owq);
+
+	WARN_ON(!list_empty(&sw->router->rif_list));
+
+	kfree(sw->router);
+	sw->router = NULL;
+}
+
+static u32 mvsw_pr_fix_tb_id(u32 tb_id)
+{
+	if (tb_id == RT_TABLE_UNSPEC ||
+	    tb_id == RT_TABLE_LOCAL ||
+	    tb_id == RT_TABLE_DEFAULT)
+		return tb_id = RT_TABLE_MAIN;
+
+	return tb_id;
+}
+
+static struct mvsw_pr_vr *__mvsw_pr_vr_find(struct mvsw_pr_switch *sw,
+					    u32 tb_id)
+{
+	struct mvsw_pr_vr *vr;
+
+	list_for_each_entry(vr, &sw->router->vr_list, router_node) {
+		if (vr->tb_id == tb_id)
+			return vr;
+	}
+
+	return NULL;
+}
+
+static struct mvsw_pr_vr *__mvsw_pr_vr_create(struct mvsw_pr_switch *sw,
+					      u32 tb_id,
+					      struct netlink_ext_ack *extack)
+{
+	struct mvsw_pr_vr *vr;
+	u16 hw_vr_id;
+	int err;
+
+	err = mvsw_pr_hw_vr_create(sw, &hw_vr_id);
+	if (err)
+		return ERR_PTR(-ENOMEM);
+
+	vr = kzalloc(sizeof(*vr), GFP_KERNEL);
+	if (!vr) {
+		err = -ENOMEM;
+		goto err_alloc_vr;
+	}
+
+	vr->tb_id = tb_id;
+	vr->hw_vr_id = hw_vr_id;
+
+	list_add(&vr->router_node, &sw->router->vr_list);
+
+	return vr;
+
+err_alloc_vr:
+	mvsw_pr_hw_vr_delete(sw, hw_vr_id);
+	kfree(vr);
+	return ERR_PTR(err);
+}
+
+static void __mvsw_pr_vr_destroy(struct mvsw_pr_switch *sw,
+				 struct mvsw_pr_vr *vr)
+{
+	mvsw_pr_hw_vr_delete(sw, vr->hw_vr_id);
+	list_del(&vr->router_node);
+	kfree(vr);
+}
+
+static struct mvsw_pr_vr *mvsw_pr_vr_get(struct mvsw_pr_switch *sw, u32 tb_id,
+					 struct netlink_ext_ack *extack)
+{
+	struct mvsw_pr_vr *vr;
+
+	vr = __mvsw_pr_vr_find(sw, tb_id);
+	if (!vr)
+		vr = __mvsw_pr_vr_create(sw, tb_id, extack);
+	if (IS_ERR(vr))
+		return ERR_CAST(vr);
+
+	return vr;
+}
+
+static void mvsw_pr_vr_put(struct mvsw_pr_switch *sw, struct mvsw_pr_vr *vr)
+{
+	if (!vr->ref_cnt)
+		__mvsw_pr_vr_destroy(sw, vr);
+}
+
+static void mvsw_pr_vr_util_hw_abort(struct mvsw_pr_switch *sw)
+{
+	struct mvsw_pr_vr *vr, *vr_tmp;
+
+	list_for_each_entry_safe(vr, vr_tmp,
+				 &sw->router->vr_list, router_node)
+		mvsw_pr_hw_vr_abort(sw, vr->hw_vr_id);
+}
+
+static struct mvsw_pr_rif*
+mvsw_pr_rif_alloc(struct mvsw_pr_switch *sw,
+		  struct mvsw_pr_vr *vr,
+		  const struct mvsw_pr_rif_params *params)
+{
+	struct mvsw_pr_rif *rif;
+	int err;
+
+	rif = kzalloc(sizeof(*rif), GFP_KERNEL);
+	if (!rif) {
+		err = -ENOMEM;
+		goto err_rif_alloc;
+	}
+
+	rif->sw = sw;
+	rif->vr = vr;
+	rif->dev = params->dev;
+	err = mvsw_pr_rif_iface_init(rif);
+	if (err)
+		goto err_rif_iface_init;
+
+	ether_addr_copy(rif->addr, params->dev->dev_addr);
+	rif->mtu = params->dev->mtu;
+
+	return rif;
+
+err_rif_iface_init:
+	kfree(rif);
+err_rif_alloc:
+	return ERR_PTR(err);
+}
+
+static int mvsw_pr_rif_offload(struct mvsw_pr_rif *rif)
+{
+	return mvsw_pr_hw_rif_create(rif->sw, &rif->iface, rif->addr,
+				     &rif->rif_id);
+}
+
+static struct mvsw_pr_rif *mvsw_pr_rif_create(struct mvsw_pr_switch *sw,
+					      const struct mvsw_pr_rif_params
+					      *params,
+					      struct netlink_ext_ack *extack)
+{
+	u32 tb_id = mvsw_pr_fix_tb_id(l3mdev_fib_table(params->dev));
+	struct mvsw_pr_rif *rif;
+	struct mvsw_pr_vr *vr;
+	int err;
+
+	vr = mvsw_pr_vr_get(sw, tb_id, extack);
+	if (IS_ERR(vr))
+		return ERR_CAST(vr);
+
+	rif = mvsw_pr_rif_alloc(sw, vr, params);
+	if (IS_ERR(rif)) {
+		mvsw_pr_vr_put(sw, vr);
+		return rif;
+	}
+
+	err = mvsw_pr_rif_offload(rif);
+	if (err)  {
+		NL_SET_ERR_MSG_MOD(extack,
+				   "Exceeded number of supported rifs");
+		goto err_rif_offload;
+	}
+
+	vr->ref_cnt++;
+	dev_hold(rif->dev);
+	list_add(&rif->router_node, &sw->router->rif_list);
+
+	return rif;
+
+err_rif_offload:
+	kfree(rif);
+	return ERR_PTR(err);
+}
+
+static int mvsw_pr_rif_delete(struct mvsw_pr_rif *rif)
+{
+	return mvsw_pr_hw_rif_delete(rif->sw, rif->rif_id, &rif->iface);
+}
+
+static void mvsw_pr_rif_destroy(struct mvsw_pr_rif *rif)
+{
+	mvsw_pr_rif_macvlan_flush(rif);
+	if (!rif->is_active) {
+		mvsw_pr_rif_delete(rif);
+		list_del(&rif->router_node);
+		dev_put(rif->dev);
+		rif->vr->ref_cnt--;
+		mvsw_pr_vr_put(rif->sw, rif->vr);
+		kfree(rif);
+	}
+}
+
+static void mvsw_pr_rif_put(struct mvsw_pr_rif *rif)
+{
+	if (!rif->ref_cnt)
+		mvsw_pr_rif_destroy(rif);
+}
+
+void mvsw_pr_rif_enable(struct mvsw_pr_switch *sw,
+			struct net_device *dev, bool enable)
+{
+	struct mvsw_pr_rif *rif;
+
+	rif = mvsw_pr_rif_find(sw, dev);
+	if (!rif)
+		return;
+
+	if (enable)
+		mvsw_pr_rif_offload(rif);
+	else
+		mvsw_pr_rif_delete(rif);
+}
+
+static int mvsw_pr_rif_update(struct mvsw_pr_rif *rif, char *mac)
+{
+	return mvsw_pr_hw_rif_set(rif->sw, &rif->rif_id, &rif->iface, mac);
+}
+
+static int mvsw_pr_rif_vr_update(struct mvsw_pr_switch *sw,
+				 struct mvsw_pr_rif *rif,
+				 struct netlink_ext_ack *extack)
+{
+	u32 tb_id = mvsw_pr_fix_tb_id(l3mdev_fib_table(rif->dev));
+	struct mvsw_pr_vr *vr;
+
+	rif->vr->ref_cnt--;
+	mvsw_pr_rif_delete(rif);
+	mvsw_pr_vr_put(sw, rif->vr);
+	vr = mvsw_pr_vr_get(sw, tb_id, extack);
+	if (IS_ERR(vr))
+		return PTR_ERR(vr);
+	rif->vr = vr;
+	mvsw_pr_rif_iface_init(rif);
+	mvsw_pr_rif_offload(rif);
+	rif->vr->ref_cnt++;
+
+	return 0;
+}
+
+void mvsw_pr_router_lag_member_leave(const struct mvsw_pr_port *port,
+				     const struct net_device *dev)
+{
+	struct mvsw_pr_rif *rif;
+	u16 vr_id;
+
+	rif = mvsw_pr_rif_find(port->sw, dev);
+	if (!rif)
+		return;
+
+	vr_id = mvsw_pr_rif_vr_id(rif);
+	prestera_lag_member_rif_leave(port, port->lag_id, vr_id);
+}
+
+void prestera_lag_router_leave(struct mvsw_pr_switch *sw,
+			       struct net_device *lag_dev)
+{
+	struct mvsw_pr_rif *rif;
+
+	rif = mvsw_pr_rif_find(sw, lag_dev);
+	if (rif) {
+		rif->is_active = false;
+		mvsw_pr_rif_put(rif);
+	}
+}
+
+static int mvsw_pr_bridge_device_rif_put(struct net_device *bridge_dev,
+					 void *data)
+{
+	struct mvsw_pr_rif *rif;
+	struct mvsw_pr_switch *sw = data;
+
+	rif = mvsw_pr_rif_find(sw, bridge_dev);
+	if (rif) {
+		rif->is_active = false;
+		mvsw_pr_rif_put(rif);
+	}
+
+	return 0;
+}
+
+void mvsw_pr_bridge_device_rifs_destroy(struct mvsw_pr_switch *sw,
+					struct net_device *bridge_dev)
+{
+	mvsw_pr_bridge_device_rif_put(bridge_dev, sw);
+	netdev_walk_all_upper_dev_rcu(bridge_dev,
+				      mvsw_pr_bridge_device_rif_put,
+				      sw);
+}
diff --git a/drivers/net/ethernet/marvell/prestera_sw/prestera_rxtx.c b/drivers/net/ethernet/marvell/prestera_sw/prestera_rxtx.c
new file mode 100644
index 000000000..50b811ebf
--- /dev/null
+++ b/drivers/net/ethernet/marvell/prestera_sw/prestera_rxtx.c
@@ -0,0 +1,222 @@
+// SPDX-License-Identifier: BSD-3-Clause OR GPL-2.0
+/*
+ * Copyright (c) 2019-2020 Marvell International Ltd. All rights reserved.
+ *
+ */
+#include "prestera.h"
+#include "prestera_rxtx_priv.h"
+#include "prestera_dsa.h"
+
+#include <linux/if_vlan.h>
+#include <net/ip.h>
+
+#define MVSW_DSA_TAG_ARP_BROADCAST 5
+#define MVSW_DSA_TAG_IPV4_BROADCAST 19
+#define MVSW_DSA_TAG_IPV4_IPV6_LINK_LOCAL_MC_1 29
+#define MVSW_DSA_TAG_IPV4_IPV6_LINK_LOCAL_MC_2 30
+#define MVSW_DSA_TAG_UDP_BROADCAST 33
+#define MVSW_DSA_TAG_ARP_BROADCAST_TO_ME 179
+
+struct mvsw_pr_rxtx;
+
+enum mvsw_pr_rxtx_type {
+	MVSW_PR_RXTX_MVPP,
+	MVSW_PR_RXTX_ETH,
+	MVSW_PR_RXTX_SDMA,
+};
+
+static struct mvsw_pr_rxtx *rxtx_registered;
+
+static u64 *cpu_code_stats;
+
+netdev_tx_t mvsw_pr_rxtx_xmit(struct sk_buff *skb,
+			      struct mvsw_pr_rxtx_info *info)
+{
+	struct mvsw_pr_dsa dsa;
+	struct mvsw_pr_dsa_from_cpu *from_cpu;
+	struct net_device *dev = skb->dev;
+	struct mvsw_pr_port *port = netdev_priv(dev);
+	size_t dsa_resize_len = MVSW_PR_DSA_HLEN;
+
+	if (!rxtx_registered)
+		return NET_XMIT_DROP;
+
+	/* common DSA tag fill-up */
+	memset(&dsa, 0, sizeof(dsa));
+	dsa.dsa_cmd = MVSW_NET_DSA_CMD_FROM_CPU_E;
+
+	from_cpu = &dsa.dsa_info.from_cpu;
+	from_cpu->egr_filter_en = false;
+	from_cpu->egr_filter_registered = false;
+	from_cpu->dst_eport = port->hw_id;
+
+	from_cpu->dst_iface.dev_port.port_num = port->hw_id;
+	from_cpu->dst_iface.dev_port.hw_dev_num = port->dev_id;
+	from_cpu->dst_iface.type = MVSW_IF_PORT_E;
+
+	/* epmorary removing due to issue with vlan sub interface
+	 * on 1.Q bridge
+	 */
+	/* If (skb->protocol == htons(ETH_P_8021Q)) { */
+		/* 802.1q packet tag size is 4 bytes, so DSA len would
+		 * need only allocation of MVSW_PR_DSA_HLEN - size of
+		 * 802.1q tag
+		 */
+		/*dsa.common_params.vpt = skb_vlan_tag_get_prio(skb);
+		 * dsa.common_params.cfi_bit = skb_vlan_tag_get_cfi(skb);
+		 * dsa.common_params.vid = skb_vlan_tag_get_id(skb);
+		 * dsa_resize_len -= VLAN_HLEN;
+		 */
+	/* } */
+
+
+	if (skb_cow_head(skb, dsa_resize_len) < 0)
+		return NET_XMIT_DROP;
+
+	/* expects skb->data at mac header */
+	skb_push(skb, dsa_resize_len);
+	memmove(skb->data, skb->data + dsa_resize_len, 2 * ETH_ALEN);
+
+	if (mvsw_pr_dsa_build(&dsa, skb->data + 2 * ETH_ALEN) != 0)
+		return NET_XMIT_DROP;
+
+	return rxtx_registered->ops->rxtx_xmit(rxtx_registered, skb);
+}
+
+int mvsw_pr_rxtx_recv_skb(struct mvsw_pr_rxtx *rxtx, struct sk_buff *skb)
+{
+	const struct mvsw_pr_port *port;
+	struct mvsw_pr_dsa dsa;
+	u32 hw_port, hw_id;
+	int err;
+
+	skb_pull(skb, ETH_HLEN);
+
+	/* parse/process DSA tag
+	 * ethertype field is part of the dsa header
+	 */
+	err = mvsw_pr_dsa_parse(skb->data - ETH_TLEN, &dsa);
+	if (err)
+		return err;
+
+	/* get switch port */
+	hw_port = dsa.dsa_info.to_cpu.iface.port_num;
+	hw_id = dsa.dsa_info.to_cpu.hw_dev_num;
+	port = mvsw_pr_port_find(hw_id, hw_port);
+	if (unlikely(!port)) {
+		pr_warn_ratelimited("prestera: received pkt for non-existent port(%u, %u)\n",
+				    hw_id, hw_port);
+		return -EEXIST;
+	}
+
+	if (unlikely(!pskb_may_pull(skb, MVSW_PR_DSA_HLEN)))
+		return -EINVAL;
+
+	/* remove DSA tag and update checksum */
+	skb_pull_rcsum(skb, MVSW_PR_DSA_HLEN);
+
+	memmove(skb->data - ETH_HLEN, skb->data - ETH_HLEN - MVSW_PR_DSA_HLEN,
+		ETH_ALEN * 2);
+
+	skb_push(skb, ETH_HLEN);
+
+	skb->protocol = eth_type_trans(skb, port->net_dev);
+
+	if (dsa.dsa_info.to_cpu.is_tagged) {
+		u16 tci = dsa.common_params.vid & VLAN_VID_MASK;
+
+		tci |= dsa.common_params.vpt << VLAN_PRIO_SHIFT;
+		if (dsa.common_params.cfi_bit)
+			tci |= VLAN_CFI_MASK;
+
+		__vlan_hwaccel_put_tag(skb, htons(ETH_P_8021Q), tci);
+	}
+
+	switch (dsa.dsa_info.to_cpu.cpu_code) {
+	case MVSW_DSA_TAG_ARP_BROADCAST:
+	case MVSW_DSA_TAG_IPV4_BROADCAST:
+	case MVSW_DSA_TAG_IPV4_IPV6_LINK_LOCAL_MC_1:
+	case MVSW_DSA_TAG_IPV4_IPV6_LINK_LOCAL_MC_2:
+	case MVSW_DSA_TAG_UDP_BROADCAST:
+	case MVSW_DSA_TAG_ARP_BROADCAST_TO_ME:
+		skb->offload_fwd_mark = 1;
+	}
+	++cpu_code_stats[dsa.dsa_info.to_cpu.cpu_code];
+
+	return 0;
+}
+
+static struct mvsw_pr_rxtx_ops rxtx_driver_ops[] = {
+	[MVSW_PR_RXTX_SDMA] = {
+		.rxtx_init = mvsw_pr_rxtx_sdma_init,
+		.rxtx_fini = mvsw_pr_rxtx_sdma_fini,
+		.rxtx_switch_init = mvsw_pr_rxtx_sdma_switch_init,
+		.rxtx_switch_fini = mvsw_pr_rxtx_sdma_switch_fini,
+		.rxtx_xmit = mvsw_pr_rxtx_sdma_xmit,
+	},
+};
+
+int mvsw_pr_rxtx_init(void)
+{
+	cpu_code_stats = kzalloc(sizeof(u64) * MVSW_PR_RXTX_CPU_CODE_MAX_NUM,
+				 GFP_KERNEL);
+	if (!cpu_code_stats)
+		return -ENOMEM;
+
+	rxtx_registered = kzalloc(sizeof(*rxtx_registered), GFP_KERNEL);
+	if (!rxtx_registered) {
+		kfree(cpu_code_stats);
+		return -ENOMEM;
+	}
+
+	rxtx_registered->ops = &rxtx_driver_ops[MVSW_PR_RXTX_SDMA];
+
+	if (rxtx_registered->ops->rxtx_init)
+		return rxtx_registered->ops->rxtx_init(rxtx_registered);
+
+	return 0;
+}
+
+void mvsw_pr_rxtx_fini(void)
+{
+	struct mvsw_pr_rxtx *rxtx = rxtx_registered;
+
+	if (rxtx->ops->rxtx_fini)
+		rxtx->ops->rxtx_fini(rxtx);
+
+	kfree(rxtx_registered);
+	rxtx_registered = NULL;
+	kfree(cpu_code_stats);
+}
+
+int mvsw_pr_rxtx_switch_init(struct mvsw_pr_switch *sw)
+{
+	int err;
+
+	if (!rxtx_registered) {
+		pr_info("No RxTx driver registered");
+		return 0;
+	}
+
+	if (!rxtx_registered->ops->rxtx_switch_init)
+		return 0;
+
+	err = rxtx_registered->ops->rxtx_switch_init(rxtx_registered, sw);
+	if (err)
+		return err;
+
+	return 0;
+}
+
+void mvsw_pr_rxtx_switch_fini(struct mvsw_pr_switch *sw)
+{
+	if (!rxtx_registered || !rxtx_registered->ops->rxtx_switch_init)
+		return;
+
+	return rxtx_registered->ops->rxtx_switch_fini(rxtx_registered, sw);
+}
+
+u64 mvsw_pr_rxtx_get_cpu_code_stats(u8 cpu_code)
+{
+	return cpu_code_stats[cpu_code];
+}
diff --git a/drivers/net/ethernet/marvell/prestera_sw/prestera_rxtx.h b/drivers/net/ethernet/marvell/prestera_sw/prestera_rxtx.h
new file mode 100644
index 000000000..a10522538
--- /dev/null
+++ b/drivers/net/ethernet/marvell/prestera_sw/prestera_rxtx.h
@@ -0,0 +1,32 @@
+/* SPDX-License-Identifier: BSD-3-Clause OR GPL-2.0
+ *
+ * Copyright (c) 2019-2020 Marvell International Ltd. All rights reserved.
+ *
+ */
+
+#ifndef _MVSW_PRESTERA_RXTX_H_
+#define _MVSW_PRESTERA_RXTX_H_
+
+#include <linux/netdevice.h>
+
+#define MVSW_PR_RXTX_CPU_CODE_MAX_NUM	256
+
+struct mvsw_pr_switch;
+
+struct mvsw_pr_rxtx_info {
+	u32 port_id;
+	u32 dev_id;
+};
+
+int mvsw_pr_rxtx_init(void);
+void mvsw_pr_rxtx_fini(void);
+
+int mvsw_pr_rxtx_switch_init(struct mvsw_pr_switch *sw);
+void mvsw_pr_rxtx_switch_fini(struct mvsw_pr_switch *sw);
+
+netdev_tx_t mvsw_pr_rxtx_xmit(struct sk_buff *skb,
+			      struct mvsw_pr_rxtx_info *info);
+
+u64 mvsw_pr_rxtx_get_cpu_code_stats(u8 cpu_code);
+
+#endif /* _MVSW_PRESTERA_RXTX_H_ */
diff --git a/drivers/net/ethernet/marvell/prestera_sw/prestera_rxtx_priv.h b/drivers/net/ethernet/marvell/prestera_sw/prestera_rxtx_priv.h
new file mode 100644
index 000000000..13527b9d3
--- /dev/null
+++ b/drivers/net/ethernet/marvell/prestera_sw/prestera_rxtx_priv.h
@@ -0,0 +1,61 @@
+/* SPDX-License-Identifier: BSD-3-Clause OR GPL-2.0
+ *
+ * Copyright (c) 2019-2020 Marvell International Ltd. All rights reserved.
+ *
+ */
+#include <linux/kernel.h>
+#include <linux/netdevice.h>
+#include <linux/rtnetlink.h>
+#include <linux/platform_device.h>
+#include <linux/of.h>
+#include <linux/of_device.h>
+
+#include "prestera_rxtx.h"
+
+struct mvsw_pr_rxtx;
+
+struct mvsw_pr_rxtx_ops {
+	int (*rxtx_init)(struct mvsw_pr_rxtx *rxtx);
+	int (*rxtx_fini)(struct mvsw_pr_rxtx *rxtx);
+
+	int (*rxtx_switch_init)(struct mvsw_pr_rxtx *rxtx,
+				struct mvsw_pr_switch *sw);
+	void (*rxtx_switch_fini)(struct mvsw_pr_rxtx *rxtx,
+				 struct mvsw_pr_switch *sw);
+
+	netdev_tx_t (*rxtx_xmit)(struct mvsw_pr_rxtx *rxtx,
+				 struct sk_buff *skb);
+};
+
+struct mvsw_pr_rxtx {
+	struct platform_device *pdev;
+	struct device *dev;
+
+	const struct mvsw_pr_rxtx_ops *ops;
+	void *priv;
+};
+
+int mvsw_pr_rxtx_recv_skb(struct mvsw_pr_rxtx *rxtx, struct sk_buff *skb);
+
+int mvsw_pr_rxtx_eth_init(struct mvsw_pr_rxtx *rxtx);
+int mvsw_pr_rxtx_eth_fini(struct mvsw_pr_rxtx *rxtx);
+netdev_tx_t mvsw_pr_rxtx_eth_xmit(struct mvsw_pr_rxtx *rxtx,
+				  struct sk_buff *skb);
+
+int mvsw_pr_rxtx_mvpp_init(struct mvsw_pr_rxtx *rxtx);
+int mvsw_pr_rxtx_mvpp_fini(struct mvsw_pr_rxtx *rxtx);
+int mvsw_pr_rxtx_eth_switch_init(struct mvsw_pr_rxtx *rxtx,
+				 struct mvsw_pr_switch *sw);
+void mvsw_pr_rxtx_eth_switch_fini(struct mvsw_pr_rxtx *rxtx,
+				  struct mvsw_pr_switch *sw);
+netdev_tx_t mvsw_pr_rxtx_mvpp_xmit(struct mvsw_pr_rxtx *rxtx,
+				   struct sk_buff *skb);
+
+int mvsw_pr_rxtx_sdma_init(struct mvsw_pr_rxtx *rxtx);
+int mvsw_pr_rxtx_sdma_fini(struct mvsw_pr_rxtx *rxtx);
+int mvsw_pr_rxtx_sdma_switch_init(struct mvsw_pr_rxtx *rxtx,
+				  struct mvsw_pr_switch *sw);
+void mvsw_pr_rxtx_sdma_switch_fini(struct mvsw_pr_rxtx *rxtx,
+				   struct mvsw_pr_switch *sw);
+netdev_tx_t mvsw_pr_rxtx_sdma_xmit(struct mvsw_pr_rxtx *rxtx,
+				   struct sk_buff *skb);
diff --git a/drivers/net/ethernet/marvell/prestera_sw/prestera_rxtx_sdma.c b/drivers/net/ethernet/marvell/prestera_sw/prestera_rxtx_sdma.c
new file mode 100644
index 000000000..9a65f3cd3
--- /dev/null
+++ b/drivers/net/ethernet/marvell/prestera_sw/prestera_rxtx_sdma.c
@@ -0,0 +1,769 @@
+// SPDX-License-Identifier: BSD-3-Clause OR GPL-2.0
+/*
+ * Copyright (c) 2019-2020 Marvell International Ltd. All rights reserved.
+ *
+ */
+
+#include <linux/platform_device.h>
+#include <linux/of.h>
+#include <linux/of_address.h>
+#include <linux/of_device.h>
+#include <linux/dmapool.h>
+
+#include "prestera.h"
+#include "prestera_hw.h"
+#include "prestera_rxtx_priv.h"
+
+struct mvsw_sdma_desc {
+	__le32 word1;
+	__le32 word2;
+	__le32 buff;
+	__le32 next;
+} __packed __aligned(16);
+
+#define SDMA_BUFF_SIZE_MAX	1544
+
+#define SDMA_RX_DESC_PKT_LEN(desc) \
+	((le32_to_cpu((desc)->word2) >> 16) & 0x3FFF)
+
+#define SDMA_RX_DESC_OWNER(desc) \
+	((le32_to_cpu((desc)->word1) & BIT(31)) >> 31)
+
+#define SDMA_RX_DESC_CPU_OWN	0
+#define SDMA_RX_DESC_DMA_OWN	1
+
+#define SDMA_RX_QUEUE_NUM	8
+
+#define SDMA_RX_DESC_PER_Q	1000
+
+#define SDMA_TX_DESC_PER_Q	1000
+#define SDMA_TX_MAX_BURST	32
+
+#define SDMA_TX_DESC_OWNER(desc) \
+	((le32_to_cpu((desc)->word1) & BIT(31)) >> 31)
+
+#define SDMA_TX_DESC_CPU_OWN	0
+#define SDMA_TX_DESC_DMA_OWN	1
+
+#define SDMA_TX_DESC_IS_SENT(desc) \
+	(SDMA_TX_DESC_OWNER(desc) == SDMA_TX_DESC_CPU_OWN)
+
+#define SDMA_TX_DESC_LAST	BIT(20)
+#define SDMA_TX_DESC_FIRST	BIT(21)
+#define SDMA_TX_DESC_SINGLE	(SDMA_TX_DESC_FIRST | SDMA_TX_DESC_LAST)
+#define SDMA_TX_DESC_CALC_CRC	BIT(12)
+
+#define mvsw_reg_write(sw, reg, val) \
+	writel(val, (sw)->dev->pp_regs + (reg))
+#define mvsw_reg_read(sw, reg) \
+	readl((sw)->dev->pp_regs + (reg))
+
+#define SDMA_RX_INTR_MASK_REG		0x2814
+#define SDMA_RX_QUEUE_STATUS_REG	0x2680
+#define SDMA_RX_QUEUE_DESC_REG(n)	(0x260C + (n) * 16)
+
+#define SDMA_TX_QUEUE_DESC_REG		0x26C0
+#define SDMA_TX_QUEUE_START_REG		0x2868
+
+struct mvsw_sdma_buf {
+	struct mvsw_sdma_desc *desc;
+	dma_addr_t desc_dma;
+	struct sk_buff *skb;
+	dma_addr_t buf_dma;
+	bool is_used;
+};
+
+struct mvsw_sdma_rx_ring {
+	struct mvsw_sdma_buf *bufs;
+	int next_rx;
+	int weight;
+	int recvd;
+};
+
+struct mvsw_sdma_tx_ring {
+	struct mvsw_sdma_buf *bufs;
+	int next_tx;
+	int max_burst;
+	int burst;
+};
+
+struct mvsw_pr_rxtx_sdma {
+	struct mvsw_sdma_rx_ring rx_ring[SDMA_RX_QUEUE_NUM];
+	struct mvsw_sdma_tx_ring tx_ring;
+	const struct mvsw_pr_switch *sw;
+	struct dma_pool *desc_pool;
+	struct mvsw_pr_rxtx *rxtx;
+	struct work_struct tx_work;
+	struct napi_struct rx_napi;
+	int next_rxq;
+	struct net_device napi_dev;
+	/* protect SDMA with concurrrent access from multiple CPUs */
+	spinlock_t tx_lock;
+	u32 map_addr;
+	u64 dma_mask;
+};
+
+static int prestera_rx_weight_map[SDMA_RX_QUEUE_NUM] = {
+	1, 2, 2, 2, 2, 4, 4, 8
+};
+
+static int mvsw_sdma_buf_desc_alloc(struct mvsw_pr_rxtx_sdma *sdma,
+				    struct mvsw_sdma_buf *buf)
+{
+	struct device *dma_dev = sdma->sw->dev->dev;
+	struct mvsw_sdma_desc *desc;
+	dma_addr_t dma;
+
+	desc = dma_pool_alloc(sdma->desc_pool, GFP_DMA | GFP_KERNEL, &dma);
+	if (!desc)
+		return -ENOMEM;
+
+	if (dma + sizeof(struct mvsw_sdma_desc) > sdma->dma_mask) {
+		dev_err(dma_dev, "failed to alloc desc\n");
+		dma_pool_free(sdma->desc_pool, desc, dma);
+		return -ENOMEM;
+	}
+
+	buf->desc_dma = dma;
+	buf->desc = desc;
+
+	return 0;
+}
+
+static u32 mvsw_sdma_addr_phy(struct mvsw_pr_rxtx_sdma *sdma, dma_addr_t pa)
+{
+	return sdma->map_addr + pa;
+}
+
+static void mvsw_sdma_rx_desc_set_len(struct mvsw_sdma_desc *desc, size_t val)
+{
+	u32 word = le32_to_cpu(desc->word2);
+
+	word = (word & ~GENMASK(15, 0)) | val;
+	desc->word2 = cpu_to_le32(word);
+}
+
+static void mvsw_sdma_rx_desc_init(struct mvsw_pr_rxtx_sdma *sdma,
+				   struct mvsw_sdma_desc *desc,
+				   dma_addr_t buf)
+{
+	mvsw_sdma_rx_desc_set_len(desc, SDMA_BUFF_SIZE_MAX);
+	desc->buff = cpu_to_le32(mvsw_sdma_addr_phy(sdma, buf));
+	/* make sure buffer is set before reset the descriptor */
+	wmb();
+	desc->word1 = cpu_to_le32(0xA0000000);
+}
+
+static void mvsw_sdma_rx_desc_set_next(struct mvsw_pr_rxtx_sdma *sdma,
+				       struct mvsw_sdma_desc *desc,
+				       dma_addr_t next)
+{
+	desc->next = cpu_to_le32(mvsw_sdma_addr_phy(sdma, next));
+}
+
+static int mvsw_sdma_rx_dma_alloc(struct mvsw_pr_rxtx_sdma *sdma,
+				  struct mvsw_sdma_buf *buf)
+{
+	struct device *dev = sdma->sw->dev->dev;
+
+	buf->skb = alloc_skb(SDMA_BUFF_SIZE_MAX, GFP_DMA | GFP_ATOMIC);
+	if (!buf->skb)
+		return -ENOMEM;
+
+	buf->buf_dma = dma_map_single(dev, buf->skb->data, buf->skb->len,
+				      DMA_FROM_DEVICE);
+
+	if (dma_mapping_error(dev, buf->buf_dma))
+		goto err_dma_map;
+	if (buf->buf_dma + buf->skb->len > sdma->dma_mask)
+		goto err_dma_range;
+
+	return 0;
+
+err_dma_range:
+	dma_unmap_single(dev, buf->buf_dma, buf->skb->len, DMA_FROM_DEVICE);
+	buf->buf_dma = DMA_MAPPING_ERROR;
+err_dma_map:
+	kfree_skb(buf->skb);
+	buf->skb = NULL;
+
+	return -ENOMEM;
+}
+
+static struct sk_buff *mvsw_sdma_rx_buf_get(struct mvsw_pr_rxtx_sdma *sdma,
+					    struct mvsw_sdma_buf *buf)
+{
+	struct sk_buff *skb_orig = buf->skb;
+	dma_addr_t buf_dma = buf->buf_dma;
+	u32 len = skb_orig->len;
+	int err;
+
+	err = mvsw_sdma_rx_dma_alloc(sdma, buf);
+	if (err) {
+		struct sk_buff *skb;
+
+		buf->buf_dma = buf_dma;
+		buf->skb = skb_orig;
+
+		skb = alloc_skb(SDMA_BUFF_SIZE_MAX, GFP_ATOMIC);
+		if (!skb)
+			return NULL;
+
+		skb_copy_from_linear_data(buf->skb, skb_put(skb, len), len);
+		return skb;
+	}
+
+	return skb_orig;
+}
+
+static void mvsw_sdma_rx_set_next_queue(struct mvsw_pr_rxtx_sdma *sdma, int rxq)
+{
+	sdma->next_rxq = rxq % SDMA_RX_QUEUE_NUM;
+}
+
+static int mvsw_sdma_rx_pick_next_queue(struct mvsw_pr_rxtx_sdma *sdma)
+{
+	struct mvsw_sdma_rx_ring *ring = &sdma->rx_ring[sdma->next_rxq];
+
+	if (ring->recvd >= ring->weight) {
+		mvsw_sdma_rx_set_next_queue(sdma, sdma->next_rxq + 1);
+		ring->recvd = 0;
+	}
+
+	return sdma->next_rxq;
+}
+
+static int mvsw_sdma_rx_poll(struct napi_struct *napi, int budget)
+{
+	unsigned int qmask = GENMASK(SDMA_RX_QUEUE_NUM - 1, 0);
+	struct mvsw_pr_rxtx_sdma *sdma;
+	unsigned int rxq_done_map = 0;
+	struct list_head rx_list;
+	int pkts_done = 0;
+
+	INIT_LIST_HEAD(&rx_list);
+
+	sdma = container_of(napi, struct mvsw_pr_rxtx_sdma, rx_napi);
+
+	while (pkts_done < budget && rxq_done_map != qmask) {
+		struct mvsw_sdma_rx_ring *ring;
+		struct mvsw_sdma_desc *desc;
+		struct mvsw_sdma_buf *buf;
+		struct sk_buff *skb;
+		int buf_idx;
+		int rxq;
+
+		rxq = mvsw_sdma_rx_pick_next_queue(sdma);
+		ring = &sdma->rx_ring[rxq];
+
+		buf_idx = ring->next_rx;
+		buf = &ring->bufs[buf_idx];
+		desc = buf->desc;
+
+		if (SDMA_RX_DESC_OWNER(desc) != SDMA_RX_DESC_CPU_OWN) {
+			mvsw_sdma_rx_set_next_queue(sdma, rxq + 1);
+			rxq_done_map |= BIT(rxq);
+			continue;
+		} else {
+			rxq_done_map &= ~BIT(rxq);
+		}
+
+		ring->recvd++;
+		pkts_done++;
+
+		__skb_trim(buf->skb, SDMA_RX_DESC_PKT_LEN(desc));
+
+		skb = mvsw_sdma_rx_buf_get(sdma, buf);
+		if (!skb)
+			goto rx_reset_buf;
+
+		if (unlikely(mvsw_pr_rxtx_recv_skb(sdma->rxtx, skb)))
+			goto rx_reset_buf;
+
+		list_add_tail(&skb->list, &rx_list);
+rx_reset_buf:
+		mvsw_sdma_rx_desc_init(sdma, buf->desc, buf->buf_dma);
+		ring->next_rx = (buf_idx + 1) % SDMA_RX_DESC_PER_Q;
+	}
+
+	if (pkts_done < budget && napi_complete_done(napi, pkts_done))
+		mvsw_reg_write(sdma->sw, SDMA_RX_INTR_MASK_REG, 0xff << 2);
+
+	netif_receive_skb_list(&rx_list);
+
+	return pkts_done;
+}
+
+static void mvsw_sdma_rx_fini(struct mvsw_pr_rxtx_sdma *sdma)
+{
+	int q, b;
+
+	/* disable all rx queues */
+	mvsw_reg_write(sdma->sw, SDMA_RX_QUEUE_STATUS_REG, 0xff00);
+
+	for (q = 0; q < SDMA_RX_QUEUE_NUM; q++) {
+		struct mvsw_sdma_rx_ring *ring = &sdma->rx_ring[q];
+
+		if (!ring->bufs)
+			break;
+
+		for (b = 0; b < SDMA_RX_DESC_PER_Q; b++) {
+			struct mvsw_sdma_buf *buf = &ring->bufs[b];
+
+			if (buf->desc_dma)
+				dma_pool_free(sdma->desc_pool, buf->desc,
+					      buf->desc_dma);
+
+			if (!buf->skb)
+				continue;
+
+			if (buf->buf_dma != DMA_MAPPING_ERROR)
+				dma_unmap_single(sdma->sw->dev->dev,
+						 buf->buf_dma, buf->skb->len,
+						 DMA_FROM_DEVICE);
+			kfree_skb(buf->skb);
+		}
+	}
+}
+
+static int mvsw_sdma_rx_init(struct mvsw_pr_rxtx_sdma *sdma)
+{
+	int q, b;
+	int err;
+
+	/* disable all rx queues */
+	mvsw_reg_write(sdma->sw, SDMA_RX_QUEUE_STATUS_REG, 0xff00);
+
+	for (q = 0; q < SDMA_RX_QUEUE_NUM; q++) {
+		struct mvsw_sdma_rx_ring *ring = &sdma->rx_ring[q];
+		struct mvsw_sdma_buf *head;
+
+		ring->bufs = kmalloc_array(SDMA_RX_DESC_PER_Q, sizeof(*head),
+					   GFP_KERNEL);
+		if (!ring->bufs)
+			return -ENOMEM;
+
+		ring->weight = prestera_rx_weight_map[q];
+		ring->recvd = 0;
+		ring->next_rx = 0;
+
+		head = &ring->bufs[0];
+
+		for (b = 0; b < SDMA_RX_DESC_PER_Q; b++) {
+			struct mvsw_sdma_buf *buf = &ring->bufs[b];
+
+			err = mvsw_sdma_buf_desc_alloc(sdma, buf);
+			if (err)
+				return err;
+
+			err = mvsw_sdma_rx_dma_alloc(sdma, buf);
+			if (err)
+				return err;
+
+			mvsw_sdma_rx_desc_init(sdma, buf->desc, buf->buf_dma);
+
+			if (b == 0)
+				continue;
+
+			mvsw_sdma_rx_desc_set_next(sdma, ring->bufs[b - 1].desc,
+						   buf->desc_dma);
+
+			if (b == SDMA_RX_DESC_PER_Q - 1)
+				mvsw_sdma_rx_desc_set_next(sdma, buf->desc,
+							   head->desc_dma);
+		}
+
+		mvsw_reg_write(sdma->sw, SDMA_RX_QUEUE_DESC_REG(q),
+			       mvsw_sdma_addr_phy(sdma, head->desc_dma));
+	}
+
+	/* make sure all rx descs are filled before enabling all rx queues */
+	wmb();
+	mvsw_reg_write(sdma->sw, SDMA_RX_QUEUE_STATUS_REG, 0xff);
+
+	return 0;
+}
+
+static void mvsw_sdma_tx_desc_init(struct mvsw_pr_rxtx_sdma *sdma,
+				   struct mvsw_sdma_desc *desc)
+{
+	desc->word1 = cpu_to_le32(SDMA_TX_DESC_SINGLE | SDMA_TX_DESC_CALC_CRC);
+	desc->word2 = 0;
+}
+
+static void mvsw_sdma_tx_desc_set_next(struct mvsw_pr_rxtx_sdma *sdma,
+				       struct mvsw_sdma_desc *desc,
+				       dma_addr_t next)
+{
+	desc->next = cpu_to_le32(mvsw_sdma_addr_phy(sdma, next));
+}
+
+static void mvsw_sdma_tx_desc_set_buf(struct mvsw_pr_rxtx_sdma *sdma,
+				      struct mvsw_sdma_desc *desc,
+				      dma_addr_t buf, size_t len)
+{
+	u32 word = le32_to_cpu(desc->word2);
+
+	word = (word & ~GENMASK(30, 16)) | ((len + 4) << 16);
+
+	desc->buff = cpu_to_le32(mvsw_sdma_addr_phy(sdma, buf));
+	desc->word2 = cpu_to_le32(word);
+}
+
+static void mvsw_sdma_tx_desc_xmit(struct mvsw_sdma_desc *desc)
+{
+	u32 word = le32_to_cpu(desc->word1);
+
+	word |= (SDMA_TX_DESC_DMA_OWN << 31);
+
+	/* make sure everything is written before enable xmit */
+	wmb();
+	desc->word1 = cpu_to_le32(word);
+}
+
+static int mvsw_sdma_tx_buf_map(struct mvsw_pr_rxtx_sdma *sdma,
+				struct mvsw_sdma_buf *buf,
+				struct sk_buff *skb)
+{
+	struct device *dma_dev = sdma->sw->dev->dev;
+	struct sk_buff *new_skb;
+	size_t len = skb->len;
+	dma_addr_t dma;
+
+	dma = dma_map_single(dma_dev, skb->data, len, DMA_TO_DEVICE);
+	if (!dma_mapping_error(dma_dev, dma) && dma + len <= sdma->dma_mask) {
+		buf->buf_dma = dma;
+		buf->skb = skb;
+		return 0;
+	}
+
+	if (!dma_mapping_error(dma_dev, dma))
+		dma_unmap_single(dma_dev, dma, len, DMA_TO_DEVICE);
+
+	new_skb = alloc_skb(len, GFP_ATOMIC | GFP_DMA);
+	if (!new_skb)
+		goto err_alloc_skb;
+
+	dma = dma_map_single(dma_dev, new_skb->data, len, DMA_TO_DEVICE);
+	if (dma_mapping_error(dma_dev, dma))
+		goto err_dma_map;
+	if (dma + len > sdma->dma_mask)
+		goto err_dma_range;
+
+	skb_copy_from_linear_data(skb, skb_put(new_skb, len), len);
+
+	dev_consume_skb_any(skb);
+
+	buf->skb = new_skb;
+	buf->buf_dma = dma;
+
+	return 0;
+
+err_dma_range:
+	dma_unmap_single(dma_dev, dma, len, DMA_TO_DEVICE);
+err_dma_map:
+	dev_kfree_skb(new_skb);
+err_alloc_skb:
+	dev_kfree_skb(skb);
+
+	return -ENOMEM;
+}
+
+static void mvsw_sdma_tx_buf_unmap(struct mvsw_pr_rxtx_sdma *sdma,
+				   struct mvsw_sdma_buf *buf)
+{
+	struct device *dma_dev = sdma->sw->dev->dev;
+
+	dma_unmap_single(dma_dev, buf->buf_dma, buf->skb->len, DMA_TO_DEVICE);
+}
+
+static void mvsw_sdma_tx_recycle_work_fn(struct work_struct *work)
+{
+	struct mvsw_sdma_tx_ring *tx_ring;
+	struct mvsw_pr_rxtx_sdma *sdma;
+	struct device *dma_dev;
+	int b;
+
+	sdma = container_of(work, struct mvsw_pr_rxtx_sdma, tx_work);
+
+	dma_dev = sdma->sw->dev->dev;
+	tx_ring = &sdma->tx_ring;
+
+	for (b = 0; b < SDMA_TX_DESC_PER_Q; b++) {
+		struct mvsw_sdma_buf *buf = &tx_ring->bufs[b];
+
+		if (!buf->is_used)
+			continue;
+
+		if (!SDMA_TX_DESC_IS_SENT(buf->desc))
+			continue;
+
+		mvsw_sdma_tx_buf_unmap(sdma, buf);
+		dev_consume_skb_any(buf->skb);
+		buf->skb = NULL;
+
+		/* make sure everything is cleaned up */
+		wmb();
+
+		buf->is_used = false;
+	}
+}
+
+static int mvsw_sdma_tx_init(struct mvsw_pr_rxtx_sdma *sdma)
+{
+	struct mvsw_sdma_tx_ring *tx_ring = &sdma->tx_ring;
+	struct mvsw_sdma_buf *head;
+	int err;
+	int b;
+
+	spin_lock_init(&sdma->tx_lock);
+
+	INIT_WORK(&sdma->tx_work, mvsw_sdma_tx_recycle_work_fn);
+
+	tx_ring->bufs = kmalloc_array(SDMA_TX_DESC_PER_Q, sizeof(*head),
+				      GFP_KERNEL);
+	if (!tx_ring->bufs)
+		return -ENOMEM;
+
+	head = &tx_ring->bufs[0];
+
+	tx_ring->max_burst = SDMA_TX_MAX_BURST;
+	tx_ring->burst = tx_ring->max_burst;
+	tx_ring->next_tx = 0;
+
+	for (b = 0; b < SDMA_TX_DESC_PER_Q; b++) {
+		struct mvsw_sdma_buf *buf = &tx_ring->bufs[b];
+
+		err = mvsw_sdma_buf_desc_alloc(sdma, buf);
+		if (err)
+			return err;
+
+		mvsw_sdma_tx_desc_init(sdma, buf->desc);
+
+		buf->is_used = false;
+		buf->skb = NULL;
+
+		if (b == 0)
+			continue;
+
+		mvsw_sdma_tx_desc_set_next(sdma, tx_ring->bufs[b - 1].desc,
+					   buf->desc_dma);
+
+		if (b == SDMA_TX_DESC_PER_Q - 1)
+			mvsw_sdma_tx_desc_set_next(sdma, buf->desc,
+						   head->desc_dma);
+	}
+
+	/* make sure descriptors are written */
+	wmb();
+	mvsw_reg_write(sdma->sw, SDMA_TX_QUEUE_DESC_REG,
+		       mvsw_sdma_addr_phy(sdma, head->desc_dma));
+
+	return 0;
+}
+
+static void mvsw_sdma_tx_fini(struct mvsw_pr_rxtx_sdma *sdma)
+{
+	struct mvsw_sdma_tx_ring *ring = &sdma->tx_ring;
+	int b;
+
+	cancel_work_sync(&sdma->tx_work);
+
+	if (!ring->bufs)
+		return;
+
+	for (b = 0; b < SDMA_TX_DESC_PER_Q; b++) {
+		struct mvsw_sdma_buf *buf = &ring->bufs[b];
+
+		if (buf->desc)
+			dma_pool_free(sdma->desc_pool, buf->desc,
+				      buf->desc_dma);
+
+		if (!buf->skb)
+			continue;
+
+		dma_unmap_single(sdma->sw->dev->dev, buf->buf_dma,
+				 buf->skb->len, DMA_TO_DEVICE);
+
+		dev_consume_skb_any(buf->skb);
+	}
+}
+
+int mvsw_pr_rxtx_sdma_init(struct mvsw_pr_rxtx *rxtx)
+{
+	struct mvsw_pr_rxtx_sdma *sdma;
+
+	sdma = kzalloc(sizeof(*sdma), GFP_KERNEL);
+	if (!sdma)
+		return -ENOMEM;
+
+	rxtx->priv = sdma;
+	sdma->rxtx = rxtx;
+
+	return 0;
+}
+
+int mvsw_pr_rxtx_sdma_fini(struct mvsw_pr_rxtx *rxtx)
+{
+	kfree(rxtx->priv);
+	return 0;
+}
+
+static void mvsw_rxtx_handle_event(struct mvsw_pr_switch *sw,
+				   struct mvsw_pr_event *evt, void *arg)
+{
+	struct mvsw_pr_rxtx_sdma *sdma = arg;
+
+	if (evt->id != MVSW_RXTX_EVENT_RCV_PKT)
+		return;
+
+	mvsw_reg_write(sdma->sw, SDMA_RX_INTR_MASK_REG, 0);
+	napi_schedule(&sdma->rx_napi);
+}
+
+int mvsw_pr_rxtx_sdma_switch_init(struct mvsw_pr_rxtx *rxtx,
+				  struct mvsw_pr_switch *sw)
+{
+	struct mvsw_pr_rxtx_sdma *sdma = rxtx->priv;
+	int err;
+
+	err = mvsw_pr_hw_rxtx_init(sw, true, &sdma->map_addr);
+	if (err) {
+		dev_err(sw->dev->dev, "failed to init rxtx by hw\n");
+		return err;
+	}
+
+	sdma->dma_mask = dma_get_mask(sw->dev->dev);
+	sdma->sw = sw;
+
+	sdma->desc_pool = dma_pool_create("desc_pool", sdma->sw->dev->dev,
+					  sizeof(struct mvsw_sdma_desc), 16, 0);
+	if (!sdma->desc_pool)
+		return -ENOMEM;
+
+	err = mvsw_sdma_rx_init(sdma);
+	if (err) {
+		dev_err(sw->dev->dev, "failed to init rx ring\n");
+		goto err_rx_init;
+	}
+
+	err = mvsw_sdma_tx_init(sdma);
+	if (err) {
+		dev_err(sw->dev->dev, "failed to init tx ring\n");
+		goto err_tx_init;
+	}
+
+	err = mvsw_pr_hw_event_handler_register(sw, MVSW_EVENT_TYPE_RXTX,
+						mvsw_rxtx_handle_event, sdma);
+	if (err)
+		goto err_evt_register;
+
+	init_dummy_netdev(&sdma->napi_dev);
+
+	netif_napi_add(&sdma->napi_dev, &sdma->rx_napi, mvsw_sdma_rx_poll, 64);
+	napi_enable(&sdma->rx_napi);
+
+	return 0;
+
+err_evt_register:
+err_tx_init:
+	mvsw_sdma_tx_fini(sdma);
+err_rx_init:
+	mvsw_sdma_rx_fini(sdma);
+
+	dma_pool_destroy(sdma->desc_pool);
+	return err;
+}
+
+void mvsw_pr_rxtx_sdma_switch_fini(struct mvsw_pr_rxtx *rxtx,
+				   struct mvsw_pr_switch *sw)
+{
+	struct mvsw_pr_rxtx_sdma *sdma = rxtx->priv;
+
+	mvsw_pr_hw_event_handler_unregister(sw, MVSW_EVENT_TYPE_RXTX);
+	napi_disable(&sdma->rx_napi);
+	netif_napi_del(&sdma->rx_napi);
+	mvsw_sdma_rx_fini(sdma);
+	mvsw_sdma_tx_fini(sdma);
+	dma_pool_destroy(sdma->desc_pool);
+}
+
+static int mvsw_sdma_wait_tx(struct mvsw_pr_rxtx_sdma *sdma,
+			     struct mvsw_sdma_tx_ring *tx_ring)
+{
+	int tx_retry_num = 10 * tx_ring->max_burst;
+
+	while (--tx_retry_num) {
+		if (!(mvsw_reg_read(sdma->sw, SDMA_TX_QUEUE_START_REG) & 1))
+			return 0;
+
+		udelay(5);
+	}
+
+	return -EBUSY;
+}
+
+static void mvsw_sdma_start_tx(struct mvsw_pr_rxtx_sdma *sdma)
+{
+	mvsw_reg_write(sdma->sw, SDMA_TX_QUEUE_START_REG, 1);
+	schedule_work(&sdma->tx_work);
+}
+
+netdev_tx_t mvsw_pr_rxtx_sdma_xmit(struct mvsw_pr_rxtx *rxtx,
+				   struct sk_buff *skb)
+{
+	struct mvsw_pr_rxtx_sdma *sdma = rxtx->priv;
+	struct device *dma_dev = sdma->sw->dev->dev;
+	struct mvsw_sdma_tx_ring *tx_ring;
+	struct net_device *dev = skb->dev;
+	struct mvsw_sdma_buf *buf;
+	int err;
+
+	spin_lock(&sdma->tx_lock);
+
+	tx_ring = &sdma->tx_ring;
+
+	buf = &tx_ring->bufs[tx_ring->next_tx];
+	if (buf->is_used) {
+		schedule_work(&sdma->tx_work);
+		goto drop_skb;
+	}
+
+	if (unlikely(skb_put_padto(skb, ETH_ZLEN)))
+		goto drop_skb_nofree;
+
+	err = mvsw_sdma_tx_buf_map(sdma, buf, skb);
+	if (err)
+		goto drop_skb;
+
+	mvsw_sdma_tx_desc_set_buf(sdma, buf->desc, buf->buf_dma, skb->len);
+
+	dma_sync_single_for_device(dma_dev, buf->buf_dma, skb->len,
+				   DMA_TO_DEVICE);
+
+	if (!tx_ring->burst--) {
+		tx_ring->burst = tx_ring->max_burst;
+
+		err = mvsw_sdma_wait_tx(sdma, tx_ring);
+		if (err)
+			goto drop_skb_unmap;
+	}
+
+	tx_ring->next_tx = (tx_ring->next_tx + 1) % SDMA_TX_DESC_PER_Q;
+	mvsw_sdma_tx_desc_xmit(buf->desc);
+	buf->is_used = true;
+
+	mvsw_sdma_start_tx(sdma);
+
+	goto tx_done;
+
+drop_skb_unmap:
+	mvsw_sdma_tx_buf_unmap(sdma, buf);
+drop_skb:
+	dev_consume_skb_any(skb);
+drop_skb_nofree:
+	dev->stats.tx_dropped++;
+tx_done:
+	spin_unlock(&sdma->tx_lock);
+	return NETDEV_TX_OK;
+}
diff --git a/drivers/net/ethernet/marvell/prestera_sw/prestera_switchdev.c b/drivers/net/ethernet/marvell/prestera_sw/prestera_switchdev.c
new file mode 100644
index 000000000..969daebe7
--- /dev/null
+++ b/drivers/net/ethernet/marvell/prestera_sw/prestera_switchdev.c
@@ -0,0 +1,1647 @@
+/* SPDX-License-Identifier: BSD-3-Clause OR GPL-2.0
+ *
+ * Copyright (c) 2019-2020 Marvell International Ltd. All rights reserved.
+ *
+ */
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/if_vlan.h>
+#include <linux/if_bridge.h>
+#include <linux/notifier.h>
+#include <net/switchdev.h>
+#include <net/netevent.h>
+#include <net/vxlan.h>
+
+#include "prestera.h"
+
+#define MVSW_PR_VID_ALL (0xffff)
+
+struct mvsw_pr_bridge {
+	struct mvsw_pr_switch *sw;
+	u32 ageing_time;
+	struct list_head bridge_list;
+	bool bridge_8021q_exists;
+};
+
+struct mvsw_pr_bridge_device {
+	struct net_device *dev;
+	struct list_head bridge_node;
+	struct list_head port_list;
+	u16 bridge_id;
+	u8 vlan_enabled:1, multicast_enabled:1, mrouter:1;
+};
+
+struct mvsw_pr_bridge_port {
+	struct net_device *dev;
+	struct mvsw_pr_bridge_device *bridge_device;
+	struct list_head bridge_device_node;
+	struct list_head vlan_list;
+	unsigned int ref_count;
+	u8 stp_state;
+	unsigned long flags;
+};
+
+struct mvsw_pr_bridge_vlan {
+	struct list_head bridge_port_node;
+	struct list_head port_vlan_list;
+	u16 vid;
+};
+
+struct mvsw_pr_event_work {
+	struct work_struct work;
+	struct switchdev_notifier_fdb_info fdb_info;
+	struct net_device *dev;
+	unsigned long event;
+};
+
+static struct workqueue_struct *mvsw_owq;
+
+static struct mvsw_pr_bridge_port *
+mvsw_pr_bridge_port_get(struct mvsw_pr_bridge *bridge,
+			struct net_device *brport_dev);
+
+static void mvsw_pr_bridge_port_put(struct mvsw_pr_bridge *bridge,
+				    struct mvsw_pr_bridge_port *br_port);
+
+struct mvsw_pr_bridge_device *
+mvsw_pr_bridge_device_find(const struct mvsw_pr_bridge *bridge,
+			   const struct net_device *br_dev)
+{
+	struct mvsw_pr_bridge_device *bridge_device;
+
+	list_for_each_entry(bridge_device, &bridge->bridge_list,
+			    bridge_node)
+		if (bridge_device->dev == br_dev)
+			return bridge_device;
+
+	return NULL;
+}
+
+static bool
+mvsw_pr_bridge_device_is_offloaded(const struct mvsw_pr_switch *sw,
+				   const struct net_device *br_dev)
+{
+	return !!mvsw_pr_bridge_device_find(sw->bridge, br_dev);
+}
+
+static struct mvsw_pr_bridge_port *
+__mvsw_pr_bridge_port_find(const struct mvsw_pr_bridge_device *bridge_device,
+			   const struct net_device *brport_dev)
+{
+	struct mvsw_pr_bridge_port *br_port;
+
+	list_for_each_entry(br_port, &bridge_device->port_list,
+			    bridge_device_node) {
+		if (br_port->dev == brport_dev)
+			return br_port;
+	}
+
+	return NULL;
+}
+
+static struct mvsw_pr_bridge_port *
+mvsw_pr_bridge_port_find(struct mvsw_pr_bridge *bridge,
+			 struct net_device *brport_dev)
+{
+	struct net_device *br_dev = netdev_master_upper_dev_get(brport_dev);
+	struct mvsw_pr_bridge_device *bridge_device;
+
+	if (!br_dev)
+		return NULL;
+
+	bridge_device = mvsw_pr_bridge_device_find(bridge, br_dev);
+	if (!bridge_device)
+		return NULL;
+
+	return __mvsw_pr_bridge_port_find(bridge_device, brport_dev);
+}
+
+static struct mvsw_pr_bridge_vlan *
+mvsw_pr_bridge_vlan_find(const struct mvsw_pr_bridge_port *br_port, u16 vid)
+{
+	struct mvsw_pr_bridge_vlan *br_vlan;
+
+	list_for_each_entry(br_vlan, &br_port->vlan_list, bridge_port_node) {
+		if (br_vlan->vid == vid)
+			return br_vlan;
+	}
+
+	return NULL;
+}
+
+u16 mvsw_pr_vlan_dev_vlan_id(struct mvsw_pr_bridge *bridge,
+			     struct net_device *dev)
+{
+	struct mvsw_pr_bridge_device *bridge_dev;
+
+	bridge_dev = mvsw_pr_bridge_device_find(bridge, dev);
+
+	return bridge_dev ? bridge_dev->bridge_id : 0;
+}
+
+static struct mvsw_pr_bridge_vlan *
+mvsw_pr_bridge_vlan_create(struct mvsw_pr_bridge_port *br_port, u16 vid)
+{
+	struct mvsw_pr_bridge_vlan *br_vlan;
+
+	br_vlan = kzalloc(sizeof(*br_vlan), GFP_KERNEL);
+	if (!br_vlan)
+		return NULL;
+
+	INIT_LIST_HEAD(&br_vlan->port_vlan_list);
+	br_vlan->vid = vid;
+	list_add(&br_vlan->bridge_port_node, &br_port->vlan_list);
+
+	return br_vlan;
+}
+
+static void
+mvsw_pr_bridge_vlan_destroy(struct mvsw_pr_bridge_vlan *br_vlan)
+{
+	list_del(&br_vlan->bridge_port_node);
+	WARN_ON(!list_empty(&br_vlan->port_vlan_list));
+	kfree(br_vlan);
+}
+
+static struct mvsw_pr_bridge_vlan *
+mvsw_pr_bridge_vlan_get(struct mvsw_pr_bridge_port *br_port, u16 vid)
+{
+	struct mvsw_pr_bridge_vlan *br_vlan;
+
+	br_vlan = mvsw_pr_bridge_vlan_find(br_port, vid);
+	if (br_vlan)
+		return br_vlan;
+
+	return mvsw_pr_bridge_vlan_create(br_port, vid);
+}
+
+static void mvsw_pr_bridge_vlan_put(struct mvsw_pr_bridge_vlan *br_vlan)
+{
+	if (list_empty(&br_vlan->port_vlan_list))
+		mvsw_pr_bridge_vlan_destroy(br_vlan);
+}
+
+static int
+mvsw_pr_port_vlan_bridge_join(struct mvsw_pr_port_vlan *port_vlan,
+			      struct mvsw_pr_bridge_port *br_port,
+			      struct netlink_ext_ack *extack)
+{
+	struct mvsw_pr_port *port = port_vlan->mvsw_pr_port;
+	struct mvsw_pr_bridge_vlan *br_vlan;
+	u16 vid = port_vlan->vid;
+	int err;
+
+	if (port_vlan->bridge_port)
+		return 0;
+
+	err = mvsw_pr_port_uc_flood_set(port, br_port->flags & BR_FLOOD);
+	if (err)
+		return err;
+
+	err = mvsw_pr_port_mc_flood_set(port, br_port->flags & BR_MCAST_FLOOD);
+	if (err)
+		return err;
+
+	err = mvsw_pr_port_learning_set(port, br_port->flags & BR_LEARNING);
+	if (err)
+		goto err_port_learning_set;
+
+	err = mvsw_pr_port_vid_stp_set(port, vid, br_port->stp_state);
+	if (err)
+		goto err_port_vid_stp_set;
+
+	br_vlan = mvsw_pr_bridge_vlan_get(br_port, vid);
+	if (!br_vlan) {
+		err = -ENOMEM;
+		goto err_bridge_vlan_get;
+	}
+
+	list_add(&port_vlan->bridge_vlan_node, &br_vlan->port_vlan_list);
+
+	mvsw_pr_bridge_port_get(port->sw->bridge, br_port->dev);
+	port_vlan->bridge_port = br_port;
+
+	return 0;
+
+err_bridge_vlan_get:
+	mvsw_pr_port_vid_stp_set(port, vid, BR_STATE_FORWARDING);
+err_port_vid_stp_set:
+	mvsw_pr_port_learning_set(port, false);
+err_port_learning_set:
+	return err;
+}
+
+static int
+mvsw_pr_bridge_vlan_port_count_get(struct mvsw_pr_bridge_device *bridge_device,
+				   u16 vid)
+{
+	int count = 0;
+	struct mvsw_pr_bridge_port *br_port;
+	struct mvsw_pr_bridge_vlan *br_vlan;
+
+	list_for_each_entry(br_port, &bridge_device->port_list,
+			    bridge_device_node) {
+		list_for_each_entry(br_vlan, &br_port->vlan_list,
+				    bridge_port_node) {
+			if (br_vlan->vid == vid) {
+				count += 1;
+				break;
+			}
+		}
+	}
+
+	return count;
+}
+
+void
+mvsw_pr_port_vlan_bridge_leave(struct mvsw_pr_port_vlan *port_vlan)
+{
+	struct mvsw_pr_port *port = port_vlan->mvsw_pr_port;
+	u32 mode = MVSW_PR_FDB_FLUSH_MODE_DYNAMIC;
+	struct mvsw_pr_bridge_vlan *br_vlan;
+	struct mvsw_pr_bridge_port *br_port;
+	u16 vid = port_vlan->vid;
+	bool last_port, last_vlan;
+	int port_count;
+
+	br_port = port_vlan->bridge_port;
+	last_vlan = list_is_singular(&br_port->vlan_list);
+	port_count =
+	    mvsw_pr_bridge_vlan_port_count_get(br_port->bridge_device, vid);
+	br_vlan = mvsw_pr_bridge_vlan_find(br_port, vid);
+	last_port = port_count == 1;
+	if (last_vlan)
+		mvsw_pr_fdb_flush_port(port, mode);
+	else if (last_port)
+		mvsw_pr_fdb_flush_vlan(port->sw, vid, mode);
+	else
+		mvsw_pr_fdb_flush_port_vlan(port, vid, mode);
+
+	list_del(&port_vlan->bridge_vlan_node);
+	mvsw_pr_bridge_vlan_put(br_vlan);
+	mvsw_pr_port_vid_stp_set(port, vid, BR_STATE_FORWARDING);
+	mvsw_pr_bridge_port_put(port->sw->bridge, br_port);
+	port_vlan->bridge_port = NULL;
+}
+
+static int
+mvsw_pr_bridge_port_vlan_add(struct mvsw_pr_port *port,
+			     struct mvsw_pr_bridge_port *br_port,
+			     u16 vid, bool is_untagged, bool is_pvid,
+			     struct netlink_ext_ack *extack)
+{
+	u16 pvid;
+	struct mvsw_pr_port_vlan *port_vlan;
+	u16 old_pvid = port->pvid;
+	int err;
+
+	if (is_pvid)
+		pvid = vid;
+	else
+		pvid = port->pvid == vid ? 0 : port->pvid;
+
+	port_vlan = mvsw_pr_port_vlan_find_by_vid(port, vid);
+	if (port_vlan && port_vlan->bridge_port != br_port)
+		return -EEXIST;
+
+	if (!port_vlan) {
+		port_vlan = mvsw_pr_port_vlan_create(port, vid, is_untagged);
+		if (IS_ERR(port_vlan))
+			return PTR_ERR(port_vlan);
+	} else {
+		err = mvsw_pr_port_vlan_set(port, vid, true, is_untagged);
+		if (err)
+			goto err_port_vlan_set;
+	}
+
+	err = mvsw_pr_port_pvid_set(port, pvid);
+	if (err)
+		goto err_port_pvid_set;
+
+	err = mvsw_pr_port_vlan_bridge_join(port_vlan, br_port, extack);
+	if (err)
+		goto err_port_vlan_bridge_join;
+
+	return 0;
+
+err_port_vlan_bridge_join:
+	mvsw_pr_port_pvid_set(port, old_pvid);
+err_port_pvid_set:
+	mvsw_pr_port_vlan_set(port, vid, false, false);
+err_port_vlan_set:
+	mvsw_pr_port_vlan_destroy(port_vlan);
+
+	return err;
+}
+
+static int mvsw_pr_port_vlans_add(struct mvsw_pr_port *port,
+				  const struct switchdev_obj_port_vlan *vlan,
+				  struct switchdev_trans *trans,
+				  struct netlink_ext_ack *extack)
+{
+	bool flag_untagged = vlan->flags & BRIDGE_VLAN_INFO_UNTAGGED;
+	bool flag_pvid = vlan->flags & BRIDGE_VLAN_INFO_PVID;
+	struct net_device *orig_dev = vlan->obj.orig_dev;
+	struct mvsw_pr_bridge_port *br_port;
+	struct mvsw_pr_bridge_device *bridge_device;
+	struct mvsw_pr_switch *sw = port->sw;
+	u16 vid;
+
+	if (netif_is_bridge_master(orig_dev))
+		return 0;
+
+	if (switchdev_trans_ph_commit(trans))
+		return 0;
+
+	br_port = mvsw_pr_bridge_port_find(sw->bridge, orig_dev);
+	if (WARN_ON(!br_port))
+		return -EINVAL;
+
+	bridge_device = br_port->bridge_device;
+	if (!bridge_device->vlan_enabled)
+		return 0;
+
+	for (vid = vlan->vid_begin; vid <= vlan->vid_end; vid++) {
+		int err;
+
+		err = mvsw_pr_bridge_port_vlan_add(port, br_port,
+						   vid, flag_untagged,
+						   flag_pvid, extack);
+		if (err)
+			return err;
+	}
+
+	if (list_is_singular(&bridge_device->port_list))
+		mvsw_pr_rif_enable(port->sw, bridge_device->dev, true);
+
+	return 0;
+}
+
+static int mvsw_pr_port_obj_add(struct net_device *dev,
+				const struct switchdev_obj *obj,
+				struct switchdev_trans *trans,
+				struct netlink_ext_ack *extack)
+{
+	int err = 0;
+	struct mvsw_pr_port *port = netdev_priv(dev);
+	const struct switchdev_obj_port_vlan *vlan;
+
+	switch (obj->id) {
+	case SWITCHDEV_OBJ_ID_PORT_VLAN:
+		vlan = SWITCHDEV_OBJ_PORT_VLAN(obj);
+		err = mvsw_pr_port_vlans_add(port, vlan, trans, extack);
+		break;
+	default:
+		err = -EOPNOTSUPP;
+	}
+
+	return err;
+}
+
+static void
+mvsw_pr_bridge_port_vlan_del(struct mvsw_pr_port *port,
+			     struct mvsw_pr_bridge_port *br_port, u16 vid)
+{
+	u16 pvid = port->pvid == vid ? 0 : port->pvid;
+	struct mvsw_pr_port_vlan *port_vlan;
+
+	port_vlan = mvsw_pr_port_vlan_find_by_vid(port, vid);
+	if (WARN_ON(!port_vlan))
+		return;
+
+	mvsw_pr_port_vlan_bridge_leave(port_vlan);
+	mvsw_pr_port_pvid_set(port, pvid);
+	mvsw_pr_port_vlan_destroy(port_vlan);
+}
+
+static int mvsw_pr_port_vlans_del(struct mvsw_pr_port *port,
+				  const struct switchdev_obj_port_vlan *vlan)
+{
+	struct mvsw_pr_switch *sw = port->sw;
+	struct net_device *orig_dev = vlan->obj.orig_dev;
+	struct mvsw_pr_bridge_port *br_port;
+	u16 vid;
+
+	if (netif_is_bridge_master(orig_dev))
+		return -EOPNOTSUPP;
+
+	br_port = mvsw_pr_bridge_port_find(sw->bridge, orig_dev);
+	if (WARN_ON(!br_port))
+		return -EINVAL;
+
+	if (!br_port->bridge_device->vlan_enabled)
+		return 0;
+
+	for (vid = vlan->vid_begin; vid <= vlan->vid_end; vid++)
+		mvsw_pr_bridge_port_vlan_del(port, br_port, vid);
+
+	return 0;
+}
+
+static int mvsw_pr_port_obj_del(struct net_device *dev,
+				const struct switchdev_obj *obj)
+{
+	int err = 0;
+	struct mvsw_pr_port *port = netdev_priv(dev);
+
+	switch (obj->id) {
+	case SWITCHDEV_OBJ_ID_PORT_VLAN:
+		err = mvsw_pr_port_vlans_del(port,
+					     SWITCHDEV_OBJ_PORT_VLAN(obj));
+		break;
+	default:
+		err = -EOPNOTSUPP;
+		break;
+	}
+
+	return err;
+}
+
+static int mvsw_pr_port_attr_br_vlan_set(struct mvsw_pr_port *port,
+					 struct switchdev_trans *trans,
+					 struct net_device *orig_dev,
+					 bool vlan_enabled)
+{
+	struct mvsw_pr_switch *sw = port->sw;
+	struct mvsw_pr_bridge_device *bridge_device;
+
+	if (!switchdev_trans_ph_prepare(trans))
+		return 0;
+
+	bridge_device = mvsw_pr_bridge_device_find(sw->bridge, orig_dev);
+	if (WARN_ON(!bridge_device))
+		return -EINVAL;
+
+	if (bridge_device->vlan_enabled == vlan_enabled)
+		return 0;
+
+	netdev_err(bridge_device->dev,
+		   "VLAN filtering can't be changed for existing bridge\n");
+	return -EINVAL;
+}
+
+static int mvsw_pr_port_attr_br_flags_set(struct mvsw_pr_port *port,
+					  struct switchdev_trans *trans,
+					  struct net_device *orig_dev,
+					  unsigned long flags)
+{
+	struct mvsw_pr_bridge_port *br_port;
+	int err;
+
+	if (switchdev_trans_ph_prepare(trans))
+		return 0;
+
+	br_port = mvsw_pr_bridge_port_find(port->sw->bridge, orig_dev);
+	if (!br_port)
+		return 0;
+
+	err = mvsw_pr_port_uc_flood_set(port, flags & BR_FLOOD);
+	if (err)
+		return err;
+
+	err = mvsw_pr_port_mc_flood_set(port, flags & BR_MCAST_FLOOD);
+	if (err)
+		return err;
+
+	err = mvsw_pr_port_learning_set(port, flags & BR_LEARNING);
+	if (err)
+		return err;
+
+	memcpy(&br_port->flags, &flags, sizeof(flags));
+	return 0;
+}
+
+static int mvsw_pr_port_attr_br_ageing_set(struct mvsw_pr_port *port,
+					   struct switchdev_trans *trans,
+					   unsigned long ageing_clock_t)
+{
+	int err;
+	struct mvsw_pr_switch *sw = port->sw;
+	unsigned long ageing_jiffies = clock_t_to_jiffies(ageing_clock_t);
+	u32 ageing_time = jiffies_to_msecs(ageing_jiffies);
+
+	if (switchdev_trans_ph_prepare(trans)) {
+		if (ageing_time < MVSW_PR_MIN_AGEING_TIME ||
+		    ageing_time > MVSW_PR_MAX_AGEING_TIME)
+			return -ERANGE;
+		else
+			return 0;
+	}
+
+	err = mvsw_pr_switch_ageing_set(sw, ageing_time);
+	if (!err)
+		sw->bridge->ageing_time = ageing_time;
+
+	return err;
+}
+
+static int
+mvsw_pr_port_bridge_vlan_stp_set(struct mvsw_pr_port *port,
+				 struct mvsw_pr_bridge_vlan *br_vlan,
+				 u8 state)
+{
+	struct mvsw_pr_port_vlan *port_vlan;
+
+	list_for_each_entry(port_vlan, &br_vlan->port_vlan_list,
+			    bridge_vlan_node) {
+		if (port_vlan->mvsw_pr_port != port)
+			continue;
+		return mvsw_pr_port_vid_stp_set(port, br_vlan->vid, state);
+	}
+
+	return 0;
+}
+
+static int mvsw_pr_port_attr_stp_state_set(struct mvsw_pr_port *port,
+					   struct switchdev_trans *trans,
+					   struct net_device *orig_dev,
+					   u8 state)
+{
+	struct mvsw_pr_bridge_port *br_port;
+	struct mvsw_pr_bridge_vlan *br_vlan;
+	int err;
+	u16 vid;
+
+	if (switchdev_trans_ph_prepare(trans))
+		return 0;
+
+	br_port = mvsw_pr_bridge_port_find(port->sw->bridge, orig_dev);
+	if (!br_port)
+		return 0;
+
+	if (!br_port->bridge_device->vlan_enabled) {
+		vid = br_port->bridge_device->bridge_id;
+		err = mvsw_pr_port_vid_stp_set(port, vid, state);
+		if (err)
+			goto err_port_bridge_stp_set;
+	} else {
+		list_for_each_entry(br_vlan, &br_port->vlan_list,
+				    bridge_port_node) {
+			err = mvsw_pr_port_bridge_vlan_stp_set(port, br_vlan,
+							       state);
+			if (err)
+				goto err_port_bridge_vlan_stp_set;
+		}
+	}
+
+	br_port->stp_state = state;
+
+	return 0;
+
+err_port_bridge_vlan_stp_set:
+	list_for_each_entry_continue_reverse(br_vlan, &br_port->vlan_list,
+					     bridge_port_node)
+		mvsw_pr_port_bridge_vlan_stp_set(port, br_vlan,
+						 br_port->stp_state);
+	return err;
+
+err_port_bridge_stp_set:
+	mvsw_pr_port_vid_stp_set(port, vid, br_port->stp_state);
+
+	return err;
+}
+
+static int mvsw_pr_port_attr_br_mc_disabled_set(struct mvsw_pr_port *port,
+						struct switchdev_trans *trans,
+						struct net_device *orig_dev,
+						bool mc_disabled)
+{
+	struct mvsw_pr_switch *sw = port->sw;
+	struct mvsw_pr_bridge_device *br_dev;
+	struct mvsw_pr_bridge_port *br_port;
+	bool enabled = !mc_disabled;
+	int err;
+
+	if (!switchdev_trans_ph_prepare(trans))
+		return 0;
+
+	br_dev = mvsw_pr_bridge_device_find(sw->bridge, orig_dev);
+	if (!br_dev)
+		return 0;
+
+	if (br_dev->multicast_enabled == enabled)
+		return 0;
+
+	list_for_each_entry(br_port, &br_dev->port_list, bridge_device_node) {
+		err = mvsw_pr_port_mc_flood_set(netdev_priv(br_port->dev),
+						enabled);
+		if (err)
+			return err;
+	}
+
+	br_dev->multicast_enabled = enabled;
+
+	return 0;
+}
+
+static int mvsw_pr_port_obj_attr_set(struct net_device *dev,
+				     const struct switchdev_attr *attr,
+				     struct switchdev_trans *trans)
+{
+	int err = 0;
+	struct mvsw_pr_port *port = netdev_priv(dev);
+
+	switch (attr->id) {
+	case SWITCHDEV_ATTR_ID_PORT_STP_STATE:
+		err = mvsw_pr_port_attr_stp_state_set(port, trans,
+						      attr->orig_dev,
+						      attr->u.stp_state);
+		break;
+	case SWITCHDEV_ATTR_ID_PORT_PRE_BRIDGE_FLAGS:
+		if (attr->u.brport_flags &
+		    ~(BR_LEARNING | BR_FLOOD | BR_MCAST_FLOOD))
+			err = -EINVAL;
+		break;
+	case SWITCHDEV_ATTR_ID_PORT_BRIDGE_FLAGS:
+		err = mvsw_pr_port_attr_br_flags_set(port, trans,
+						     attr->orig_dev,
+						     attr->u.brport_flags);
+		break;
+	case SWITCHDEV_ATTR_ID_BRIDGE_AGEING_TIME:
+		err = mvsw_pr_port_attr_br_ageing_set(port, trans,
+						      attr->u.ageing_time);
+		break;
+	case SWITCHDEV_ATTR_ID_BRIDGE_VLAN_FILTERING:
+		err = mvsw_pr_port_attr_br_vlan_set(port, trans,
+						    attr->orig_dev,
+						    attr->u.vlan_filtering);
+		break;
+	case SWITCHDEV_ATTR_ID_BRIDGE_MC_DISABLED:
+		err = mvsw_pr_port_attr_br_mc_disabled_set(port, trans,
+							   attr->orig_dev,
+							   attr->u.mc_disabled);
+		break;
+	default:
+		err = -EOPNOTSUPP;
+	}
+
+	return err;
+}
+
+static void mvsw_fdb_offload_notify(struct mvsw_pr_port *port,
+				    struct switchdev_notifier_fdb_info *info)
+{
+	struct switchdev_notifier_fdb_info send_info;
+
+	send_info.addr = info->addr;
+	send_info.vid = info->vid;
+	send_info.offloaded = true;
+	call_switchdev_notifiers(SWITCHDEV_FDB_OFFLOADED,
+				 port->net_dev, &send_info.info, NULL);
+}
+
+static int
+mvsw_pr_port_fdb_set(struct mvsw_pr_port *port,
+		     struct switchdev_notifier_fdb_info *fdb_info, bool adding)
+{
+	struct mvsw_pr_switch *sw = port->sw;
+	struct mvsw_pr_bridge_port *br_port;
+	struct mvsw_pr_bridge_device *bridge_device;
+	struct net_device *orig_dev = fdb_info->info.dev;
+	int err;
+	u16 vid;
+
+	br_port = mvsw_pr_bridge_port_find(sw->bridge, orig_dev);
+	if (!br_port)
+		return -EINVAL;
+
+	bridge_device = br_port->bridge_device;
+
+	if (bridge_device->vlan_enabled)
+		vid = fdb_info->vid;
+	else
+		vid = bridge_device->bridge_id;
+
+	if (adding)
+		err = mvsw_pr_fdb_add(port, fdb_info->addr, vid, false);
+	else
+		err = mvsw_pr_fdb_del(port, fdb_info->addr, vid);
+
+	return err;
+}
+
+static void mvsw_pr_bridge_fdb_event_work(struct work_struct *work)
+{
+	int err = 0;
+	struct mvsw_pr_event_work *switchdev_work =
+	    container_of(work, struct mvsw_pr_event_work, work);
+	struct net_device *dev = switchdev_work->dev;
+	struct switchdev_notifier_fdb_info *fdb_info;
+	struct mvsw_pr_port *port;
+
+	rtnl_lock();
+	if (netif_is_vxlan(dev))
+		goto out;
+
+	port = mvsw_pr_port_dev_lower_find(dev);
+	if (!port)
+		goto out;
+
+	switch (switchdev_work->event) {
+	case SWITCHDEV_FDB_ADD_TO_DEVICE:
+		fdb_info = &switchdev_work->fdb_info;
+		if (!fdb_info->added_by_user)
+			break;
+		err = mvsw_pr_port_fdb_set(port, fdb_info, true);
+		if (err)
+			break;
+		mvsw_fdb_offload_notify(port, fdb_info);
+		break;
+	case SWITCHDEV_FDB_DEL_TO_DEVICE:
+		fdb_info = &switchdev_work->fdb_info;
+		mvsw_pr_port_fdb_set(port, fdb_info, false);
+		break;
+	case SWITCHDEV_FDB_ADD_TO_BRIDGE:
+	case SWITCHDEV_FDB_DEL_TO_BRIDGE:
+		break;
+	}
+
+out:
+	rtnl_unlock();
+	kfree(switchdev_work->fdb_info.addr);
+	kfree(switchdev_work);
+	dev_put(dev);
+}
+
+static int prestera_switchdev_event(struct notifier_block *unused,
+				    unsigned long event, void *ptr)
+{
+	int err = 0;
+	struct net_device *net_dev = switchdev_notifier_info_to_dev(ptr);
+	struct mvsw_pr_event_work *switchdev_work;
+	struct switchdev_notifier_fdb_info *fdb_info;
+	struct switchdev_notifier_info *info = ptr;
+	struct net_device *upper_br;
+
+	if (event == SWITCHDEV_PORT_ATTR_SET) {
+		err = switchdev_handle_port_attr_set(net_dev, ptr,
+						     mvsw_pr_netdev_check,
+						     mvsw_pr_port_obj_attr_set);
+		return notifier_from_errno(err);
+	}
+
+	upper_br = netdev_master_upper_dev_get_rcu(net_dev);
+	if (!upper_br)
+		return NOTIFY_DONE;
+
+	if (!netif_is_bridge_master(upper_br))
+		return NOTIFY_DONE;
+
+	switchdev_work = kzalloc(sizeof(*switchdev_work), GFP_ATOMIC);
+	if (!switchdev_work)
+		return NOTIFY_BAD;
+
+	switchdev_work->dev = net_dev;
+	switchdev_work->event = event;
+
+	switch (event) {
+	case SWITCHDEV_FDB_ADD_TO_DEVICE:
+	case SWITCHDEV_FDB_DEL_TO_DEVICE:
+		fdb_info = container_of(info,
+					struct switchdev_notifier_fdb_info,
+					info);
+
+		INIT_WORK(&switchdev_work->work, mvsw_pr_bridge_fdb_event_work);
+		memcpy(&switchdev_work->fdb_info, ptr,
+		       sizeof(switchdev_work->fdb_info));
+		switchdev_work->fdb_info.addr = kzalloc(ETH_ALEN, GFP_ATOMIC);
+		if (!switchdev_work->fdb_info.addr)
+			goto out;
+		ether_addr_copy((u8 *)switchdev_work->fdb_info.addr,
+				fdb_info->addr);
+		dev_hold(net_dev);
+
+		break;
+	case SWITCHDEV_FDB_ADD_TO_BRIDGE:
+	case SWITCHDEV_FDB_DEL_TO_BRIDGE:
+	case SWITCHDEV_VXLAN_FDB_ADD_TO_DEVICE:
+	case SWITCHDEV_VXLAN_FDB_DEL_TO_DEVICE:
+	default:
+		kfree(switchdev_work);
+		return NOTIFY_DONE;
+	}
+
+	queue_work(mvsw_owq, &switchdev_work->work);
+	return NOTIFY_DONE;
+out:
+	kfree(switchdev_work);
+	return NOTIFY_BAD;
+}
+
+static int prestera_switchdev_blocking_event(struct notifier_block *unused,
+					     unsigned long event, void *ptr)
+{
+	int err = 0;
+	struct net_device *net_dev = switchdev_notifier_info_to_dev(ptr);
+
+	switch (event) {
+	case SWITCHDEV_PORT_OBJ_ADD:
+		if (netif_is_vxlan(net_dev)) {
+			err = -EOPNOTSUPP;
+		} else {
+			err = switchdev_handle_port_obj_add
+			    (net_dev, ptr, mvsw_pr_netdev_check,
+			     mvsw_pr_port_obj_add);
+		}
+		break;
+	case SWITCHDEV_PORT_OBJ_DEL:
+		if (netif_is_vxlan(net_dev)) {
+			err = -EOPNOTSUPP;
+		} else {
+			err = switchdev_handle_port_obj_del
+			    (net_dev, ptr, mvsw_pr_netdev_check,
+			     mvsw_pr_port_obj_del);
+		}
+		break;
+	case SWITCHDEV_PORT_ATTR_SET:
+		err = switchdev_handle_port_attr_set
+		    (net_dev, ptr, mvsw_pr_netdev_check,
+		    mvsw_pr_port_obj_attr_set);
+		break;
+	default:
+		err = -EOPNOTSUPP;
+	}
+
+	return notifier_from_errno(err);
+}
+
+static struct mvsw_pr_bridge_device *
+mvsw_pr_bridge_device_create(struct mvsw_pr_bridge *bridge,
+			     struct net_device *br_dev)
+{
+	struct mvsw_pr_bridge_device *bridge_device;
+	bool vlan_enabled = br_vlan_enabled(br_dev);
+	u16 bridge_id;
+	int err;
+
+	if (vlan_enabled && bridge->bridge_8021q_exists) {
+		netdev_err(br_dev, "Only one VLAN-aware bridge is supported\n");
+		return ERR_PTR(-EINVAL);
+	}
+
+	bridge_device = kzalloc(sizeof(*bridge_device), GFP_KERNEL);
+	if (!bridge_device)
+		return ERR_PTR(-ENOMEM);
+
+	if (vlan_enabled) {
+		bridge->bridge_8021q_exists = true;
+	} else {
+		err = mvsw_pr_8021d_bridge_create(bridge->sw, &bridge_id);
+		if (err) {
+			kfree(bridge_device);
+			return ERR_PTR(err);
+		}
+
+		bridge_device->bridge_id = bridge_id;
+	}
+
+	bridge_device->dev = br_dev;
+	bridge_device->vlan_enabled = vlan_enabled;
+	bridge_device->multicast_enabled = br_multicast_enabled(br_dev);
+	bridge_device->mrouter = br_multicast_router(br_dev);
+	INIT_LIST_HEAD(&bridge_device->port_list);
+
+	list_add(&bridge_device->bridge_node, &bridge->bridge_list);
+
+	return bridge_device;
+}
+
+static void
+mvsw_pr_bridge_device_destroy(struct mvsw_pr_bridge *bridge,
+			      struct mvsw_pr_bridge_device *bridge_device)
+{
+	list_del(&bridge_device->bridge_node);
+	if (bridge_device->vlan_enabled)
+		bridge->bridge_8021q_exists = false;
+	else
+		mvsw_pr_8021d_bridge_delete(bridge->sw,
+					    bridge_device->bridge_id);
+
+	WARN_ON(!list_empty(&bridge_device->port_list));
+	kfree(bridge_device);
+}
+
+static struct mvsw_pr_bridge_device *
+mvsw_pr_bridge_device_get(struct mvsw_pr_bridge *bridge,
+			  struct net_device *br_dev)
+{
+	struct mvsw_pr_bridge_device *bridge_device;
+
+	bridge_device = mvsw_pr_bridge_device_find(bridge, br_dev);
+	if (bridge_device)
+		return bridge_device;
+
+	return mvsw_pr_bridge_device_create(bridge, br_dev);
+}
+
+static void
+mvsw_pr_bridge_device_put(struct mvsw_pr_bridge *bridge,
+			  struct mvsw_pr_bridge_device *bridge_device)
+{
+	if (list_empty(&bridge_device->port_list))
+		mvsw_pr_bridge_device_destroy(bridge, bridge_device);
+}
+
+static struct mvsw_pr_bridge_port *
+mvsw_pr_bridge_port_create(struct mvsw_pr_bridge_device *bridge_device,
+			   struct net_device *brport_dev)
+{
+	struct mvsw_pr_bridge_port *br_port;
+	struct mvsw_pr_port *port;
+
+	br_port = kzalloc(sizeof(*br_port), GFP_KERNEL);
+	if (!br_port)
+		return NULL;
+
+	port = mvsw_pr_port_dev_lower_find(brport_dev);
+
+	br_port->dev = brport_dev;
+	br_port->bridge_device = bridge_device;
+	br_port->stp_state = BR_STATE_DISABLED;
+	br_port->flags = BR_LEARNING | BR_FLOOD | BR_LEARNING_SYNC |
+				BR_MCAST_FLOOD;
+	INIT_LIST_HEAD(&br_port->vlan_list);
+	list_add(&br_port->bridge_device_node, &bridge_device->port_list);
+	br_port->ref_count = 1;
+
+	return br_port;
+}
+
+static void
+mvsw_pr_bridge_port_destroy(struct mvsw_pr_bridge_port *br_port)
+{
+	list_del(&br_port->bridge_device_node);
+	WARN_ON(!list_empty(&br_port->vlan_list));
+	kfree(br_port);
+}
+
+static struct mvsw_pr_bridge_port *
+mvsw_pr_bridge_port_get(struct mvsw_pr_bridge *bridge,
+			struct net_device *brport_dev)
+{
+	struct net_device *br_dev = netdev_master_upper_dev_get(brport_dev);
+	struct mvsw_pr_bridge_device *bridge_device;
+	struct mvsw_pr_bridge_port *br_port;
+	int err;
+
+	br_port = mvsw_pr_bridge_port_find(bridge, brport_dev);
+	if (br_port) {
+		br_port->ref_count++;
+		return br_port;
+	}
+
+	bridge_device = mvsw_pr_bridge_device_get(bridge, br_dev);
+	if (IS_ERR(bridge_device))
+		return ERR_CAST(bridge_device);
+
+	br_port = mvsw_pr_bridge_port_create(bridge_device, brport_dev);
+	if (!br_port) {
+		err = -ENOMEM;
+		goto err_brport_create;
+	}
+
+	return br_port;
+
+err_brport_create:
+	mvsw_pr_bridge_device_put(bridge, bridge_device);
+	return ERR_PTR(err);
+}
+
+static void mvsw_pr_bridge_port_put(struct mvsw_pr_bridge *bridge,
+				    struct mvsw_pr_bridge_port *br_port)
+{
+	struct mvsw_pr_bridge_device *bridge_device;
+
+	if (--br_port->ref_count != 0)
+		return;
+	bridge_device = br_port->bridge_device;
+	mvsw_pr_bridge_port_destroy(br_port);
+	if (list_empty(&bridge_device->port_list)) {
+		mvsw_pr_rif_enable(bridge->sw, bridge_device->dev, false);
+		mvsw_pr_bridge_device_rifs_destroy(bridge->sw,
+						   bridge_device->dev);
+	}
+	mvsw_pr_bridge_device_put(bridge, bridge_device);
+}
+
+static int
+mvsw_pr_bridge_8021q_port_join(struct mvsw_pr_bridge_device *bridge_device,
+			       struct mvsw_pr_bridge_port *br_port,
+			       struct mvsw_pr_port *port,
+			       struct netlink_ext_ack *extack)
+{
+	if (is_vlan_dev(br_port->dev)) {
+		NL_SET_ERR_MSG_MOD(extack,
+				   "Can not enslave a VLAN device to a VLAN-aware bridge");
+		return -EINVAL;
+	}
+
+	return 0;
+}
+
+static int
+mvsw_pr_bridge_8021d_port_join(struct mvsw_pr_bridge_device *bridge_device,
+			       struct mvsw_pr_bridge_port *br_port,
+			       struct mvsw_pr_port *port,
+			       struct netlink_ext_ack *extack)
+{
+	int err;
+
+	if (is_vlan_dev(br_port->dev)) {
+		NL_SET_ERR_MSG_MOD(extack,
+				   "Enslaving of a VLAN device is not supported");
+		return -ENOTSUPP;
+	}
+	err = mvsw_pr_8021d_bridge_port_add(port, bridge_device->bridge_id);
+	if (err)
+		return err;
+
+	err = mvsw_pr_port_uc_flood_set(port, br_port->flags & BR_FLOOD);
+	if (err)
+		goto err_port_uc_flood_set;
+
+	err = mvsw_pr_port_mc_flood_set(port, br_port->flags & BR_MCAST_FLOOD);
+	if (err)
+		goto err_port_mc_flood_set;
+
+	err = mvsw_pr_port_learning_set(port, br_port->flags & BR_LEARNING);
+	if (err)
+		goto err_port_learning_set;
+
+	if (list_is_singular(&bridge_device->port_list))
+		mvsw_pr_rif_enable(port->sw, bridge_device->dev, true);
+
+	return err;
+
+err_port_learning_set:
+	mvsw_pr_port_mc_flood_set(port, false);
+err_port_mc_flood_set:
+	mvsw_pr_port_uc_flood_set(port, false);
+err_port_uc_flood_set:
+	mvsw_pr_8021d_bridge_port_delete(port, bridge_device->bridge_id);
+	return err;
+}
+
+static int mvsw_pr_port_bridge_join(struct mvsw_pr_port *port,
+				    struct net_device *brport_dev,
+				    struct net_device *br_dev,
+				    struct netlink_ext_ack *extack)
+{
+	struct mvsw_pr_bridge_device *bridge_device;
+	struct mvsw_pr_switch *sw = port->sw;
+	struct mvsw_pr_bridge_port *br_port;
+	int err;
+
+	br_port = mvsw_pr_bridge_port_get(sw->bridge, brport_dev);
+	if (IS_ERR(br_port))
+		return PTR_ERR(br_port);
+
+	bridge_device = br_port->bridge_device;
+
+	/* Enslaved port is not usable as a router interface */
+	if (mvsw_pr_rif_exists(sw, port->net_dev))
+		mvsw_pr_rif_enable(sw, port->net_dev, false);
+
+	if (bridge_device->vlan_enabled) {
+		err = mvsw_pr_bridge_8021q_port_join(bridge_device, br_port,
+						     port, extack);
+	} else {
+		err = mvsw_pr_bridge_8021d_port_join(bridge_device, br_port,
+						     port, extack);
+	}
+
+	if (err)
+		goto err_port_join;
+
+	return 0;
+
+err_port_join:
+	mvsw_pr_bridge_port_put(sw->bridge, br_port);
+	return err;
+}
+
+static void
+mvsw_pr_bridge_8021d_port_leave(struct mvsw_pr_bridge_device *bridge_device,
+				struct mvsw_pr_bridge_port *br_port,
+				struct mvsw_pr_port *port)
+{
+	mvsw_pr_fdb_flush_port(port, MVSW_PR_FDB_FLUSH_MODE_ALL);
+	mvsw_pr_8021d_bridge_port_delete(port, bridge_device->bridge_id);
+}
+
+static void
+mvsw_pr_bridge_8021q_port_leave(struct mvsw_pr_bridge_device *bridge_device,
+				struct mvsw_pr_bridge_port *br_port,
+				struct mvsw_pr_port *port)
+{
+	mvsw_pr_fdb_flush_port(port, MVSW_PR_FDB_FLUSH_MODE_ALL);
+	mvsw_pr_port_pvid_set(port, MVSW_PR_DEFAULT_VID);
+}
+
+static void mvsw_pr_port_bridge_leave(struct mvsw_pr_port *port,
+				      struct net_device *brport_dev,
+				      struct net_device *br_dev)
+{
+	struct mvsw_pr_switch *sw = port->sw;
+	struct mvsw_pr_bridge_device *bridge_device;
+	struct mvsw_pr_bridge_port *br_port;
+
+	bridge_device = mvsw_pr_bridge_device_find(sw->bridge, br_dev);
+	if (!bridge_device)
+		return;
+	br_port = __mvsw_pr_bridge_port_find(bridge_device, brport_dev);
+	if (!br_port)
+		return;
+
+	if (bridge_device->vlan_enabled)
+		mvsw_pr_bridge_8021q_port_leave(bridge_device, br_port, port);
+	else
+		mvsw_pr_bridge_8021d_port_leave(bridge_device, br_port, port);
+
+	mvsw_pr_port_learning_set(port, false);
+	mvsw_pr_port_uc_flood_set(port, false);
+	mvsw_pr_port_mc_flood_set(port, false);
+	mvsw_pr_port_vid_stp_set(port, MVSW_PR_VID_ALL, BR_STATE_FORWARDING);
+	mvsw_pr_bridge_port_put(sw->bridge, br_port);
+
+	/* Offload rif that was previosly disabled */
+	if (mvsw_pr_rif_exists(sw, port->net_dev))
+		mvsw_pr_rif_enable(sw, port->net_dev, true);
+
+}
+
+static bool
+prestera_lag_master_check(struct mvsw_pr_switch *sw, struct net_device *lag_dev,
+			  struct netdev_lag_upper_info *upper_info,
+			  struct netlink_ext_ack *ext_ack)
+{
+	u16 lag_id;
+
+	if (prestera_lag_id_find(sw, lag_dev, &lag_id)) {
+		NL_SET_ERR_MSG_MOD(ext_ack,
+				   "Exceeded max supported LAG devices");
+		return false;
+	}
+	if (upper_info->tx_type != NETDEV_LAG_TX_TYPE_HASH) {
+		NL_SET_ERR_MSG_MOD(ext_ack, "Unsupported LAG Tx type");
+		return false;
+	}
+	return true;
+}
+
+static void mvsw_pr_port_lag_clean(struct mvsw_pr_port *port,
+				   struct net_device *lag_dev)
+{
+	struct net_device *br_dev = netdev_master_upper_dev_get(lag_dev);
+	struct mvsw_pr_port_vlan *port_vlan, *tmp;
+	struct net_device *upper_dev;
+	struct list_head *iter;
+
+	list_for_each_entry_safe(port_vlan, tmp, &port->vlans_list, list) {
+		mvsw_pr_port_vlan_bridge_leave(port_vlan);
+		mvsw_pr_port_vlan_destroy(port_vlan);
+	}
+
+	if (netif_is_bridge_port(lag_dev))
+		mvsw_pr_port_bridge_leave(port, lag_dev, br_dev);
+
+	netdev_for_each_upper_dev_rcu(lag_dev, upper_dev, iter) {
+		if (!netif_is_bridge_port(upper_dev))
+			continue;
+		br_dev = netdev_master_upper_dev_get(upper_dev);
+		mvsw_pr_port_bridge_leave(port, upper_dev, br_dev);
+	}
+
+	mvsw_pr_port_pvid_set(port, MVSW_PR_DEFAULT_VID);
+}
+
+static int mvsw_pr_port_lag_join(struct mvsw_pr_port *port,
+				 struct net_device *lag_dev)
+{
+	u16 lag_id;
+	int err;
+
+	err = prestera_lag_id_find(port->sw, lag_dev, &lag_id);
+	if (err)
+		return err;
+
+	err = prestera_lag_member_add(port, lag_dev, lag_id);
+		return err;
+
+	/* TODO: Port should no be longer usable as a router interface */
+
+	return 0;
+}
+
+static void mvsw_pr_port_lag_leave(struct mvsw_pr_port *port,
+				   struct net_device *lag_dev)
+{
+	mvsw_pr_router_lag_member_leave(port, lag_dev);
+
+	if (prestera_lag_member_del(port))
+		return;
+
+	mvsw_pr_port_lag_clean(port, lag_dev);
+}
+
+static int mvsw_pr_netdevice_port_upper_event(struct net_device *lower_dev,
+					      struct net_device *dev,
+					      unsigned long event, void *ptr)
+{
+	struct netdev_notifier_changeupper_info *info;
+	struct mvsw_pr_port *port;
+	struct netlink_ext_ack *extack;
+	struct net_device *upper_dev;
+	struct mvsw_pr_switch *sw;
+	int err = 0;
+
+	port = netdev_priv(dev);
+	sw = port->sw;
+	info = ptr;
+	extack = netdev_notifier_info_to_extack(&info->info);
+
+	switch (event) {
+	case NETDEV_PRECHANGEUPPER:
+		upper_dev = info->upper_dev;
+		if (!netif_is_bridge_master(upper_dev) &&
+		    !netif_is_lag_master(upper_dev) &&
+		    !netif_is_macvlan(upper_dev)) {
+			NL_SET_ERR_MSG_MOD(extack, "Unknown upper device type");
+			return -EINVAL;
+		}
+		if (!info->linking)
+			break;
+		if (netdev_has_any_upper_dev(upper_dev) &&
+		    (!netif_is_bridge_master(upper_dev) ||
+		     !mvsw_pr_bridge_device_is_offloaded(sw, upper_dev))) {
+			NL_SET_ERR_MSG_MOD(extack,
+					   "Enslaving a port to a device that already has an upper device is not supported");
+			return -EINVAL;
+		}
+		if (netif_is_lag_master(upper_dev) &&
+		    !prestera_lag_master_check(sw, upper_dev,
+					      info->upper_info, extack))
+			return -EINVAL;
+		if (netif_is_lag_master(upper_dev) && vlan_uses_dev(dev)) {
+			NL_SET_ERR_MSG_MOD(extack,
+					   "Master device is a LAG master and port has a VLAN");
+			return -EINVAL;
+		}
+		if (netif_is_lag_port(dev) && is_vlan_dev(upper_dev) &&
+		    !netif_is_lag_master(vlan_dev_real_dev(upper_dev))) {
+			NL_SET_ERR_MSG_MOD(extack,
+					   "Can not put a VLAN on a LAG port");
+			return -EINVAL;
+		}
+		if (netif_is_macvlan(upper_dev) &&
+		    !mvsw_pr_rif_exists(sw, lower_dev)) {
+			NL_SET_ERR_MSG_MOD(extack,
+					   "macvlan is only supported on top of router interfaces");
+			return -EOPNOTSUPP;
+		}
+		break;
+	case NETDEV_CHANGEUPPER:
+		upper_dev = info->upper_dev;
+		if (netif_is_bridge_master(upper_dev)) {
+			if (info->linking)
+				err = mvsw_pr_port_bridge_join(port,
+							       lower_dev,
+							       upper_dev,
+							       extack);
+			else
+				mvsw_pr_port_bridge_leave(port,
+							  lower_dev,
+							  upper_dev);
+		} else if (netif_is_lag_master(upper_dev)) {
+			if (info->linking)
+				err = mvsw_pr_port_lag_join(port,
+							    upper_dev);
+			else
+				mvsw_pr_port_lag_leave(port,
+						       upper_dev);
+		}
+		break;
+	}
+
+	return err;
+}
+
+static int mvsw_pr_netdevice_port_lower_event(struct net_device *dev,
+					      unsigned long event, void *ptr)
+{
+	struct netdev_notifier_changelowerstate_info *info = ptr;
+	struct netdev_lag_lower_state_info *lower_state_info;
+	struct mvsw_pr_port *port = netdev_priv(dev);
+	bool enabled;
+
+	if (event != NETDEV_CHANGELOWERSTATE)
+		return 0;
+	if (!netif_is_lag_port(dev))
+		return 0;
+	if (!mvsw_pr_port_is_lag_member(port))
+		return 0;
+
+	lower_state_info = info->lower_state_info;
+	enabled = lower_state_info->tx_enabled;
+	return prestera_lag_member_enable(port, enabled);
+}
+
+static int mvsw_pr_netdevice_port_event(struct net_device *lower_dev,
+					struct net_device *port_dev,
+					unsigned long event, void *ptr)
+{
+	switch (event) {
+	case NETDEV_PRECHANGEUPPER:
+	case NETDEV_CHANGEUPPER:
+		return mvsw_pr_netdevice_port_upper_event(lower_dev, port_dev,
+							  event, ptr);
+	case NETDEV_CHANGELOWERSTATE:
+		return mvsw_pr_netdevice_port_lower_event(port_dev,
+							  event, ptr);
+	}
+
+	return 0;
+}
+
+static int mvsw_pr_netdevice_bridge_event(struct net_device *br_dev,
+					  unsigned long event, void *ptr)
+{
+	struct mvsw_pr_switch *sw = mvsw_pr_switch_get(br_dev);
+	struct netdev_notifier_changeupper_info *info = ptr;
+	struct netlink_ext_ack *extack;
+	struct net_device *upper_dev;
+
+	if (!sw)
+		return 0;
+
+	extack = netdev_notifier_info_to_extack(&info->info);
+
+	switch (event) {
+	case NETDEV_PRECHANGEUPPER:
+		upper_dev = info->upper_dev;
+		if (!is_vlan_dev(upper_dev) && !netif_is_macvlan(upper_dev)) {
+			NL_SET_ERR_MSG_MOD(extack, "Unknown upper device type");
+			return -EOPNOTSUPP;
+		}
+		if (!info->linking)
+			break;
+		if (netif_is_macvlan(upper_dev) &&
+		    !mvsw_pr_rif_exists(sw, br_dev)) {
+			NL_SET_ERR_MSG_MOD(extack,
+					   "macvlan is only supported on top of router interfaces");
+			return -EOPNOTSUPP;
+		}
+		break;
+	case NETDEV_CHANGEUPPER:
+		/* TODO:  */
+		break;
+	}
+
+	return 0;
+}
+
+static int mvsw_pr_netdevice_macvlan_event(struct net_device *macvlan_dev,
+					   unsigned long event, void *ptr)
+{
+	struct mvsw_pr_switch *sw = mvsw_pr_switch_get(macvlan_dev);
+	struct netdev_notifier_changeupper_info *info = ptr;
+	struct netlink_ext_ack *extack;
+
+	if (!sw || event != NETDEV_PRECHANGEUPPER)
+		return 0;
+
+	extack = netdev_notifier_info_to_extack(&info->info);
+
+	NL_SET_ERR_MSG_MOD(extack, "Unknown upper device type");
+
+	return -EOPNOTSUPP;
+}
+
+static bool mvsw_pr_is_vrf_event(unsigned long event, void *ptr)
+{
+	struct netdev_notifier_changeupper_info *info = ptr;
+
+	if (event != NETDEV_PRECHANGEUPPER && event != NETDEV_CHANGEUPPER)
+		return false;
+
+	return netif_is_l3_master(info->upper_dev);
+}
+
+static int mvsw_pr_netdevice_lag_event(struct net_device *lag_dev,
+				       unsigned long event, void *ptr)
+{
+	struct net_device *dev;
+	struct list_head *iter;
+	int err;
+
+	netdev_for_each_lower_dev(lag_dev, dev, iter) {
+		if (mvsw_pr_netdev_check(dev)) {
+			err = mvsw_pr_netdevice_port_event(lag_dev, dev, event,
+							   ptr);
+			if (err)
+				return err;
+		}
+	}
+
+	return 0;
+}
+
+static int mvsw_pr_netdevice_vlan_event(struct net_device *vlan_dev,
+					unsigned long event, void *ptr)
+{
+	struct net_device *real_dev = vlan_dev_real_dev(vlan_dev);
+	struct mvsw_pr_switch *sw = mvsw_pr_switch_get(real_dev);
+	struct netdev_notifier_changeupper_info *info = ptr;
+	struct netlink_ext_ack *extack;
+	struct net_device *upper_dev;
+
+	if (!sw)
+		return 0;
+
+	extack = netdev_notifier_info_to_extack(&info->info);
+
+	switch (event) {
+	case NETDEV_PRECHANGEUPPER:
+		upper_dev = info->upper_dev;
+
+		if (!info->linking)
+			break;
+
+		if (mvsw_pr_bridge_device_is_offloaded(sw, real_dev) &&
+		    netif_is_bridge_master(upper_dev)) {
+			NL_SET_ERR_MSG_MOD(extack,
+					   "Enslaving offloaded bridge to a bridge is not supported");
+			return -EOPNOTSUPP;
+		}
+		break;
+	case NETDEV_CHANGEUPPER:
+		/* empty */
+		break;
+	}
+
+	return 0;
+}
+
+static int mvsw_pr_netdevice_event(struct notifier_block *nb,
+				   unsigned long event, void *ptr)
+{
+	struct net_device *dev = netdev_notifier_info_to_dev(ptr);
+	struct mvsw_pr_switch *sw;
+	int err = 0;
+
+	sw = container_of(nb, struct mvsw_pr_switch, netdevice_nb);
+
+	if (event == NETDEV_PRE_CHANGEADDR ||
+	    event == NETDEV_CHANGEADDR)
+		err = mvsw_pr_netdevice_router_port_event(dev, event, ptr);
+	else if (mvsw_pr_is_vrf_event(event, ptr))
+		err = mvsw_pr_netdevice_vrf_event(dev, event, ptr);
+	else if (mvsw_pr_netdev_check(dev))
+		err = mvsw_pr_netdevice_port_event(dev, dev, event, ptr);
+	else if (netif_is_bridge_master(dev))
+		err = mvsw_pr_netdevice_bridge_event(dev, event, ptr);
+	else if (netif_is_lag_master(dev))
+		err = mvsw_pr_netdevice_lag_event(dev, event, ptr);
+	else if (is_vlan_dev(dev))
+		err = mvsw_pr_netdevice_vlan_event(dev, event, ptr);
+	else if (netif_is_macvlan(dev))
+		err = mvsw_pr_netdevice_macvlan_event(dev, event, ptr);
+
+	return notifier_from_errno(err);
+}
+
+static int mvsw_pr_fdb_init(struct mvsw_pr_switch *sw)
+{
+	int err;
+
+	err = mvsw_pr_switch_ageing_set(sw, MVSW_PR_DEFAULT_AGEING_TIME);
+	if (err)
+		return err;
+
+	return 0;
+}
+
+static int prestera_switchdev_init(struct mvsw_pr_switch *sw)
+{
+	int err = 0;
+	struct prestera_switchdev *swdev;
+	struct mvsw_pr_bridge *bridge;
+
+	if (sw->switchdev)
+		return -EPERM;
+
+	bridge = kzalloc(sizeof(*sw->bridge), GFP_KERNEL);
+	if (!bridge)
+		return -ENOMEM;
+
+	swdev = kzalloc(sizeof(*sw->switchdev), GFP_KERNEL);
+	if (!swdev) {
+		kfree(bridge);
+		return -ENOMEM;
+	}
+
+	sw->bridge = bridge;
+	bridge->sw = sw;
+	sw->switchdev = swdev;
+	swdev->sw = sw;
+
+	INIT_LIST_HEAD(&sw->bridge->bridge_list);
+
+	mvsw_owq = alloc_ordered_workqueue("%s_ordered", 0, "prestera_sw");
+	if (!mvsw_owq) {
+		err = -ENOMEM;
+		goto err_alloc_workqueue;
+	}
+
+	swdev->swdev_n.notifier_call = prestera_switchdev_event;
+	err = register_switchdev_notifier(&swdev->swdev_n);
+	if (err)
+		goto err_register_switchdev_notifier;
+
+	swdev->swdev_blocking_n.notifier_call =
+			prestera_switchdev_blocking_event;
+	err = register_switchdev_blocking_notifier(&swdev->swdev_blocking_n);
+	if (err)
+		goto err_register_block_switchdev_notifier;
+
+	mvsw_pr_fdb_init(sw);
+
+	return 0;
+
+err_register_block_switchdev_notifier:
+	unregister_switchdev_notifier(&swdev->swdev_n);
+err_register_switchdev_notifier:
+	destroy_workqueue(mvsw_owq);
+err_alloc_workqueue:
+	kfree(swdev);
+	kfree(bridge);
+	return err;
+}
+
+static void prestera_switchdev_fini(struct mvsw_pr_switch *sw)
+{
+	if (!sw->switchdev)
+		return;
+
+	unregister_switchdev_notifier(&sw->switchdev->swdev_n);
+	unregister_switchdev_blocking_notifier
+	    (&sw->switchdev->swdev_blocking_n);
+	flush_workqueue(mvsw_owq);
+	destroy_workqueue(mvsw_owq);
+	kfree(sw->switchdev);
+	sw->switchdev = NULL;
+	kfree(sw->bridge);
+}
+
+static int mvsw_pr_netdev_init(struct mvsw_pr_switch *sw)
+{
+	int err = 0;
+
+	if (sw->netdevice_nb.notifier_call)
+		return -EPERM;
+
+	sw->netdevice_nb.notifier_call = mvsw_pr_netdevice_event;
+	err = register_netdevice_notifier(&sw->netdevice_nb);
+	return err;
+}
+
+static void mvsw_pr_netdev_fini(struct mvsw_pr_switch *sw)
+{
+	if (sw->netdevice_nb.notifier_call)
+		unregister_netdevice_notifier(&sw->netdevice_nb);
+}
+
+int prestera_switchdev_register(struct mvsw_pr_switch *sw)
+{
+	int err;
+
+	err = prestera_switchdev_init(sw);
+	if (err)
+		return err;
+
+	err = mvsw_pr_router_init(sw);
+
+	if (err) {
+		pr_err("Failed to initialize fib notifier\n");
+		goto err_fib_notifier;
+	}
+
+	err = mvsw_pr_netdev_init(sw);
+	if (err)
+		goto err_netdevice_notifier;
+
+	return 0;
+
+err_netdevice_notifier:
+	mvsw_pr_router_fini(sw);
+err_fib_notifier:
+	prestera_switchdev_fini(sw);
+	return err;
+}
+
+void prestera_switchdev_unregister(struct mvsw_pr_switch *sw)
+{
+	mvsw_pr_netdev_fini(sw);
+	mvsw_pr_router_fini(sw);
+	prestera_switchdev_fini(sw);
+}
-- 
2.25.1

