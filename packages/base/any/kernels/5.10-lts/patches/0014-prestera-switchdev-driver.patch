From 4e977aa42340d42c3b0868495d9c99cbb8ff938e Mon Sep 17 00:00:00 2001
From: Taras Chornyi <taras.chornyi@plvision.eu>
Date: Wed, 10 Feb 2021 10:10:22 +0200
Subject: [PATCH] net: prestera: Update prestera driver

New features:
* Ipv4 offload
* TC offload
* phylink support
* ECMP offload

Co-developed-by: Andrii Savka <andrii.savka@plvision.eu>
Signed-off-by: Andrii Savka <andrii.savka@plvision.eu>
Co-developed-by: Oleksandr Mazur <oleksandr.mazur@plvision.eu>
Signed-off-by: Oleksandr Mazur <oleksandr.mazur@plvision.eu>
Co-developed-by: Serhiy Boiko <serhiy.boiko@plvision.eu>
Signed-off-by: Serhiy Boiko <serhiy.boiko@plvision.eu>
Co-developed-by: Serhiy Pshyk <serhiy.pshyk@plvision.eu>
Signed-off-by: Serhiy Pshyk <serhiy.pshyk@plvision.eu>
Co-developed-by: Volodymyr Mytnyk <volodymyr.mytnyk@plvision.eu>
Signed-off-by: Volodymyr Mytnyk <volodymyr.mytnyk@plvision.eu>
Co-developed-by: Vadym Kochan <vadym.kochan@plvision.eu>
Signed-off-by: Vadym Kochan <vadym.kochan@plvision.eu>
Signed-off-by: Taras Chornyi <taras.chornyi@plvision.eu>
---
 .../net/ethernet/marvell/prestera/Makefile    |   10 +-
 .../net/ethernet/marvell/prestera/prestera.h  |  544 ++-
 .../ethernet/marvell/prestera/prestera_acl.c  |  402 +++
 .../marvell/prestera/prestera_debugfs.c       |  160 +
 .../marvell/prestera/prestera_debugfs.h       |   14 +
 .../marvell/prestera/prestera_devlink.c       |   32 +-
 .../marvell/prestera/prestera_devlink.h       |   18 +-
 .../marvell/prestera/prestera_drv_ver.h       |   23 +
 .../ethernet/marvell/prestera/prestera_dsa.c  |  340 +-
 .../ethernet/marvell/prestera/prestera_dsa.h  |   70 +-
 .../marvell/prestera/prestera_ethtool.c       |  780 -----
 .../marvell/prestera/prestera_ethtool.h       |   11 -
 .../marvell/prestera/prestera_flower.c        |  430 +++
 .../marvell/prestera/prestera_fw_log.c        |  422 +++
 .../marvell/prestera/prestera_fw_log.h        |   15 +
 .../ethernet/marvell/prestera/prestera_hw.c   | 2483 +++++++++----
 .../ethernet/marvell/prestera/prestera_hw.h   |  428 ++-
 .../ethernet/marvell/prestera/prestera_log.c  |  203 ++
 .../ethernet/marvell/prestera/prestera_log.h  |   58 +
 .../ethernet/marvell/prestera/prestera_main.c | 2599 ++++++++++++--
 .../ethernet/marvell/prestera/prestera_pci.c  |  901 +++--
 .../marvell/prestera/prestera_router.c        | 3069 +++++++++++++++++
 .../ethernet/marvell/prestera/prestera_rxtx.c |  878 +----
 .../ethernet/marvell/prestera/prestera_rxtx.h |   35 +-
 .../marvell/prestera/prestera_rxtx_priv.h     |   61 +
 .../marvell/prestera/prestera_rxtx_sdma.c     |  769 +++++
 .../marvell/prestera/prestera_switchdev.c     | 2058 ++++++-----
 .../marvell/prestera/prestera_switchdev.h     |   13 -
 28 files changed, 12625 insertions(+), 4201 deletions(-)
 create mode 100644 drivers/net/ethernet/marvell/prestera/prestera_acl.c
 create mode 100644 drivers/net/ethernet/marvell/prestera/prestera_debugfs.c
 create mode 100644 drivers/net/ethernet/marvell/prestera/prestera_debugfs.h
 create mode 100644 drivers/net/ethernet/marvell/prestera/prestera_drv_ver.h
 delete mode 100644 drivers/net/ethernet/marvell/prestera/prestera_ethtool.c
 delete mode 100644 drivers/net/ethernet/marvell/prestera/prestera_ethtool.h
 create mode 100644 drivers/net/ethernet/marvell/prestera/prestera_flower.c
 create mode 100644 drivers/net/ethernet/marvell/prestera/prestera_fw_log.c
 create mode 100644 drivers/net/ethernet/marvell/prestera/prestera_fw_log.h
 create mode 100644 drivers/net/ethernet/marvell/prestera/prestera_log.c
 create mode 100644 drivers/net/ethernet/marvell/prestera/prestera_log.h
 create mode 100644 drivers/net/ethernet/marvell/prestera/prestera_router.c
 create mode 100644 drivers/net/ethernet/marvell/prestera/prestera_rxtx_priv.h
 create mode 100644 drivers/net/ethernet/marvell/prestera/prestera_rxtx_sdma.c
 delete mode 100644 drivers/net/ethernet/marvell/prestera/prestera_switchdev.h

diff --git a/drivers/net/ethernet/marvell/prestera/Makefile b/drivers/net/ethernet/marvell/prestera/Makefile
index 93129e32e..29093e4a5 100644
--- a/drivers/net/ethernet/marvell/prestera/Makefile
+++ b/drivers/net/ethernet/marvell/prestera/Makefile
@@ -1,7 +1,11 @@
 # SPDX-License-Identifier: GPL-2.0
 obj-$(CONFIG_PRESTERA)	+= prestera.o
-prestera-objs		:= prestera_main.o prestera_hw.o prestera_dsa.o \
-			   prestera_rxtx.o prestera_devlink.o prestera_ethtool.o \
-			   prestera_switchdev.o
+prestera-objs		:= prestera_main.o \
+			   prestera_hw.o prestera_switchdev.o prestera_devlink.o prestera_fw_log.o \
+			   prestera_rxtx.o prestera_rxtx_sdma.o prestera_dsa.o prestera_router.o \
+			   prestera_acl.o prestera_flower.o prestera_debugfs.o
 
 obj-$(CONFIG_PRESTERA_PCI)	+= prestera_pci.o
+
+prestera-$(CONFIG_MRVL_PRESTERA_DEBUG) += prestera_log.o
+ccflags-$(CONFIG_MRVL_PRESTERA_DEBUG) += -DCONFIG_MRVL_PRESTERA_DEBUG
diff --git a/drivers/net/ethernet/marvell/prestera/prestera.h b/drivers/net/ethernet/marvell/prestera/prestera.h
index 55aa4bf8a..87acd308e 100644
--- a/drivers/net/ethernet/marvell/prestera/prestera.h
+++ b/drivers/net/ethernet/marvell/prestera/prestera.h
@@ -1,18 +1,32 @@
-/* SPDX-License-Identifier: BSD-3-Clause OR GPL-2.0 */
-/* Copyright (c) 2019-2020 Marvell International Ltd. All rights reserved. */
+/* SPDX-License-Identifier: BSD-3-Clause OR GPL-2.0
+ *
+ * Copyright (c) 2019-2020 Marvell International Ltd. All rights reserved.
+ *
+ */
 
-#ifndef _PRESTERA_H_
-#define _PRESTERA_H_
+#ifndef _MVSW_PRESTERA_H_
+#define _MVSW_PRESTERA_H_
 
-#include <linux/notifier.h>
 #include <linux/skbuff.h>
+#include <linux/notifier.h>
+#include <uapi/linux/if_ether.h>
+#include <linux/if_macvlan.h>
 #include <linux/workqueue.h>
+#include <linux/phylink.h>
+#include <net/pkt_cls.h>
 #include <net/devlink.h>
-#include <uapi/linux/if_ether.h>
 
-#define PRESTERA_DRV_NAME	"prestera"
+#define PRESTERA_DRV_NAME       "prestera"
+
+#define MVSW_MSG_MAX_SIZE 1500
 
-#define PRESTERA_DEFAULT_VID    1
+#define MVSW_PR_DEFAULT_VID 1
+
+#define MVSW_PR_MIN_AGEING_TIME 32000
+#define MVSW_PR_MAX_AGEING_TIME 1000000000
+#define MVSW_PR_DEFAULT_AGEING_TIME 300000
+
+#define MVSW_PR_NHGR_SIZE_MAX 4
 
 struct prestera_fw_rev {
 	u16 maj;
@@ -20,7 +34,21 @@ struct prestera_fw_rev {
 	u16 sub;
 };
 
-struct prestera_port_stats {
+struct mvsw_pr_bridge_port;
+struct prestera_acl;
+struct prestera_acl_block;
+struct prestera_acl_rule;
+struct prestera_acl_ruleset;
+
+struct mvsw_pr_port_vlan {
+	struct list_head list;
+	struct mvsw_pr_port *mvsw_pr_port;
+	u16 vid;
+	struct mvsw_pr_bridge_port *bridge_port;
+	struct list_head bridge_vlan_node;
+};
+
+struct mvsw_pr_port_stats {
 	u64 good_octets_received;
 	u64 bad_octets_received;
 	u64 mac_trans_error;
@@ -53,154 +81,512 @@ struct prestera_port_stats {
 	u64 good_octets_sent;
 };
 
-struct prestera_port_caps {
+struct mvsw_pr_port_caps {
 	u64 supp_link_modes;
 	u8 supp_fec;
 	u8 type;
 	u8 transceiver;
 };
 
-struct prestera_port {
-	struct net_device *dev;
-	struct prestera_switch *sw;
+struct mvsw_pr_port {
 	struct devlink_port dl_port;
+	struct net_device *net_dev;
+	struct mvsw_pr_switch *sw;
 	u32 id;
 	u32 hw_id;
 	u32 dev_id;
 	u16 fp_id;
 	u16 pvid;
 	bool autoneg;
+	bool hw_oper_state; /* RS (PCS) */
+	u32 hw_speed;
+	u8 hw_duplex;
 	u64 adver_link_modes;
 	u8 adver_fec;
-	struct prestera_port_caps caps;
+	u16 lag_id;
+	struct mvsw_pr_port_caps caps;
 	struct list_head list;
 	struct list_head vlans_list;
 	struct {
-		struct prestera_port_stats stats;
+		struct mvsw_pr_port_stats stats;
 		struct delayed_work caching_dw;
 	} cached_hw_stats;
+	struct prestera_acl_block *acl_block;
+
+	struct phylink_config phy_config;
+	struct phylink *phy_link;
+};
+
+struct prestera_switchdev {
+	struct mvsw_pr_switch *sw;
+	struct notifier_block swdev_n;
+	struct notifier_block swdev_blocking_n;
+};
+
+struct mvsw_pr_fib {
+	struct mvsw_pr_switch *sw;
+	struct notifier_block fib_nb;
+	struct notifier_block netevent_nb;
 };
 
 struct prestera_device {
 	struct device *dev;
-	u8 __iomem *ctl_regs;
-	u8 __iomem *pp_regs;
 	struct prestera_fw_rev fw_rev;
+	struct workqueue_struct *dev_wq;
+	u8 __iomem *pp_regs;
 	void *priv;
 
 	/* called by device driver to handle received packets */
 	void (*recv_pkt)(struct prestera_device *dev);
 
 	/* called by device driver to pass event up to the higher layer */
-	int (*recv_msg)(struct prestera_device *dev, void *msg, size_t size);
+	int (*recv_msg)(struct prestera_device *dev, u8 *msg, size_t size);
 
 	/* called by higher layer to send request to the firmware */
-	int (*send_req)(struct prestera_device *dev, void *in_msg,
-			size_t in_size, void *out_msg, size_t out_size,
+	int (*send_req)(struct prestera_device *dev, u8 *in_msg,
+			size_t in_size, u8 *out_msg, size_t out_size,
 			unsigned int wait);
 };
 
-enum prestera_event_type {
-	PRESTERA_EVENT_TYPE_UNSPEC,
+enum mvsw_pr_event_type {
+	MVSW_EVENT_TYPE_UNSPEC,
+	MVSW_EVENT_TYPE_PORT,
+	MVSW_EVENT_TYPE_FDB,
+	MVSW_EVENT_TYPE_RXTX,
+	MVSW_EVENT_TYPE_FW_LOG,
 
-	PRESTERA_EVENT_TYPE_PORT,
-	PRESTERA_EVENT_TYPE_FDB,
-	PRESTERA_EVENT_TYPE_RXTX,
-
-	PRESTERA_EVENT_TYPE_MAX
+	MVSW_EVENT_TYPE_MAX,
 };
 
-enum prestera_rxtx_event_id {
-	PRESTERA_RXTX_EVENT_UNSPEC,
-	PRESTERA_RXTX_EVENT_RCV_PKT,
+enum mvsw_pr_rxtx_event_id {
+	MVSW_RXTX_EVENT_UNSPEC,
+
+	MVSW_RXTX_EVENT_RCV_PKT,
+
+	MVSW_RXTX_EVENT_MAX,
 };
 
-enum prestera_port_event_id {
-	PRESTERA_PORT_EVENT_UNSPEC,
-	PRESTERA_PORT_EVENT_STATE_CHANGED,
+enum mvsw_pr_port_event_id {
+	MVSW_PORT_EVENT_UNSPEC,
+	MVSW_PORT_EVENT_STATE_CHANGED,
+
+	MVSW_PORT_EVENT_MAX,
 };
 
-struct prestera_port_event {
-	u32 port_id;
-	union {
-		u32 oper_state;
-	} data;
+enum mvsw_pr_fdb_event_id {
+	MVSW_FDB_EVENT_UNSPEC,
+	MVSW_FDB_EVENT_LEARNED,
+	MVSW_FDB_EVENT_AGED,
+
+	MVSW_FDB_EVENT_MAX,
 };
 
-enum prestera_fdb_event_id {
-	PRESTERA_FDB_EVENT_UNSPEC,
-	PRESTERA_FDB_EVENT_LEARNED,
-	PRESTERA_FDB_EVENT_AGED,
+enum mvsw_pr_fdb_entry_type {
+	MVSW_PR_FDB_ENTRY_TYPE_REG_PORT,
+	MVSW_PR_FDB_ENTRY_TYPE_LAG,
+	MVSW_PR_FDB_ENTRY_TYPE_MAX
 };
 
-struct prestera_fdb_event {
-	u32 port_id;
+struct mvsw_pr_fdb_event {
+	enum mvsw_pr_fdb_entry_type type;
+	union {
+		u32 port_id;
+		u16 lag_id;
+	} dest;
 	u32 vid;
 	union {
 		u8 mac[ETH_ALEN];
 	} data;
 };
 
-struct prestera_event {
+struct mvsw_pr_port_event {
+	u32 port_id;
+	struct {
+		u8 oper_state;
+		u8 duplex;
+		u32 speed;
+	} data;
+};
+
+struct mvsw_pr_fw_log_event {
+	u32 log_len;
+	u8 *data;
+};
+
+struct mvsw_pr_event {
 	u16 id;
 	union {
-		struct prestera_port_event port_evt;
-		struct prestera_fdb_event fdb_evt;
+		struct mvsw_pr_port_event port_evt;
+		struct mvsw_pr_fdb_event fdb_evt;
+		struct mvsw_pr_fw_log_event fw_log_evt;
 	};
 };
 
-struct prestera_switchdev;
-struct prestera_rxtx;
+struct prestera_lag_member {
+	struct list_head list;
+	struct mvsw_pr_port *port;
+};
 
-struct prestera_switch {
+struct prestera_lag {
+	struct net_device *dev;
+	u16 member_count;
+	struct list_head members;
+};
+
+enum mvsw_pr_if_type {
+	/* the interface is of port type (dev,port) */
+	MVSW_IF_PORT_E = 0,
+
+	/* the interface is of lag type (lag-id) */
+	MVSW_IF_LAG_E = 1,
+
+	/* the interface is of Vid type (vlan-id) */
+	MVSW_IF_VID_E = 3,
+};
+
+struct mvsw_pr_iface {
+	enum mvsw_pr_if_type type;
+	struct {
+		u32 hw_dev_num;
+		u32 port_num;
+	} dev_port;
+	u16 vr_id;
+	u16 lag_id;
+	u16 vlan_id;
+	u32 hw_dev_num;
+};
+
+struct mvsw_pr_bridge;
+struct mvsw_pr_router;
+struct mvsw_pr_rif;
+
+struct mvsw_pr_switch {
+	struct list_head list;
 	struct prestera_device *dev;
-	struct prestera_switchdev *swdev;
-	struct prestera_rxtx *rxtx;
 	struct list_head event_handlers;
-	struct notifier_block netdev_nb;
 	char base_mac[ETH_ALEN];
 	struct list_head port_list;
-	rwlock_t port_list_lock;
 	u32 port_count;
 	u32 mtu_min;
 	u32 mtu_max;
 	u8 id;
+	u8 lag_max;
+	u8 lag_member_max;
+	struct prestera_acl *acl;
+	struct mvsw_pr_bridge *bridge;
+	struct prestera_switchdev *switchdev;
+	struct mvsw_pr_router *router;
+	struct prestera_lag *lags;
+	struct notifier_block netdevice_nb;
+	struct device_node *np;
 };
 
-struct prestera_rxtx_params {
-	bool use_sdma;
-	u32 map_addr;
+struct mvsw_pr_router {
+	struct mvsw_pr_switch *sw;
+	struct list_head rif_list;	/* list of mvsw_pr_rif */
+	struct list_head vr_list;	/* list of mvsw_pr_vr */
+	struct rhashtable nh_neigh_ht;
+	struct rhashtable nexthop_group_ht;
+	struct rhashtable fib_ht;
+	struct rhashtable kern_fib_cache_ht;
+	struct rhashtable kern_neigh_cache_ht;
+	struct {
+		struct delayed_work dw;
+		unsigned int interval;	/* ms */
+	} neighs_update;
+	struct notifier_block netevent_nb;
+	struct notifier_block inetaddr_nb;
+	struct notifier_block fib_nb;
+	bool aborted;
 };
 
-#define prestera_dev(sw)		((sw)->dev->dev)
+enum mvsw_pr_fdb_flush_mode {
+	MVSW_PR_FDB_FLUSH_MODE_DYNAMIC = BIT(0),
+	MVSW_PR_FDB_FLUSH_MODE_STATIC = BIT(1),
+	MVSW_PR_FDB_FLUSH_MODE_ALL = MVSW_PR_FDB_FLUSH_MODE_DYNAMIC
+				   | MVSW_PR_FDB_FLUSH_MODE_STATIC,
+};
 
-static inline void prestera_write(const struct prestera_switch *sw,
-				  unsigned int reg, u32 val)
-{
-	writel(val, sw->dev->pp_regs + reg);
-}
+enum prestera_acl_rule_match_entry_type {
+	MVSW_ACL_RULE_MATCH_ENTRY_TYPE_ETH_TYPE = 1,
+	MVSW_ACL_RULE_MATCH_ENTRY_TYPE_ETH_DMAC,
+	MVSW_ACL_RULE_MATCH_ENTRY_TYPE_ETH_SMAC,
+	MVSW_ACL_RULE_MATCH_ENTRY_TYPE_IP_PROTO,
+	MVSW_ACL_RULE_MATCH_ENTRY_TYPE_PORT,
+	MVSW_ACL_RULE_MATCH_ENTRY_TYPE_IP_SRC,
+	MVSW_ACL_RULE_MATCH_ENTRY_TYPE_IP_DST,
+	MVSW_ACL_RULE_MATCH_ENTRY_TYPE_L4_PORT_SRC,
+	MVSW_ACL_RULE_MATCH_ENTRY_TYPE_L4_PORT_DST,
+	MVSW_ACL_RULE_MATCH_ENTRY_TYPE_L4_PORT_RANGE_SRC,
+	MVSW_ACL_RULE_MATCH_ENTRY_TYPE_L4_PORT_RANGE_DST,
+	MVSW_ACL_RULE_MATCH_ENTRY_TYPE_VLAN_ID,
+	MVSW_ACL_RULE_MATCH_ENTRY_TYPE_VLAN_TPID,
+	MVSW_ACL_RULE_MATCH_ENTRY_TYPE_ICMP_TYPE,
+	MVSW_ACL_RULE_MATCH_ENTRY_TYPE_ICMP_CODE
+};
 
-static inline u32 prestera_read(const struct prestera_switch *sw,
-				unsigned int reg)
-{
-	return readl(sw->dev->pp_regs + reg);
-}
+enum prestera_acl_rule_action {
+	MVSW_ACL_RULE_ACTION_ACCEPT,
+	MVSW_ACL_RULE_ACTION_DROP,
+	MVSW_ACL_RULE_ACTION_TRAP,
+	MVSW_ACL_RULE_ACTION_POLICE
+};
 
-int prestera_device_register(struct prestera_device *dev);
-void prestera_device_unregister(struct prestera_device *dev);
+struct prestera_acl_rule_match_entry {
+	struct list_head list;
+	enum prestera_acl_rule_match_entry_type type;
+	union {
+		struct {
+			u8 key, mask;
+		} u8;
+		struct {
+			u16 key, mask;
+		} u16;
+		struct {
+			u32 key, mask;
+		} u32;
+		struct {
+			u64 key, mask;
+		} u64;
+		struct {
+			u8 key[ETH_ALEN];
+			u8 mask[ETH_ALEN];
+		} mac;
+	} keymask;
+};
+
+struct prestera_acl_rule_action_entry {
+	struct list_head list;
+	enum prestera_acl_rule_action id;
+	union {
+		struct {
+			u64 rate, burst;
+		} police;
+	};
+};
+
+struct mvsw_pr_ip_addr {
+	enum {
+		MVSW_PR_IPV4 = 0,
+		MVSW_PR_IPV6
+	} v;
+	union {
+		__be32 ipv4;
+		struct in6_addr ipv6;
+	} u;
+};
 
-struct prestera_port *prestera_port_find_by_hwid(struct prestera_switch *sw,
-						 u32 dev_id, u32 hw_id);
+struct mvsw_pr_fib_key {
+	struct mvsw_pr_ip_addr addr;
+	u32 prefix_len;
+	u32 tb_id;
+};
 
-int prestera_port_autoneg_set(struct prestera_port *port, bool enable,
-			      u64 adver_link_modes, u8 adver_fec);
+struct mvsw_pr_fib_info {
+	struct mvsw_pr_vr *vr;
+	struct list_head vr_node;
+	enum mvsw_pr_fib_type {
+		MVSW_PR_FIB_TYPE_INVALID = 0,
+		/* must be pointer to nh_grp id */
+		MVSW_PR_FIB_TYPE_UC_NH,
+		/* It can be connected route
+		 * and will be overlapped with neighbours
+		 */
+		MVSW_PR_FIB_TYPE_TRAP,
+		MVSW_PR_FIB_TYPE_DROP
+	} type;
+	/* Valid only if type = UC_NH*/
+	struct mvsw_pr_nexthop_group *nh_grp;
+};
 
-struct prestera_port *prestera_find_port(struct prestera_switch *sw, u32 id);
+/* Used for hw call */
+struct mvsw_pr_neigh_info {
+	struct mvsw_pr_iface iface;
+	unsigned char ha[ETH_ALEN];
+	bool connected; /* indicate, if mac/oif valid */
+};
 
-struct prestera_port *prestera_port_dev_lower_find(struct net_device *dev);
+struct mvsw_pr_nh_neigh_key {
+	struct mvsw_pr_ip_addr addr;
+	struct mvsw_pr_rif *rif;
+};
 
-int prestera_port_pvid_set(struct prestera_port *port, u16 vid);
+/* Used to notify nh about neigh change */
+struct mvsw_pr_nh_neigh {
+	struct mvsw_pr_nh_neigh_key key;
+	struct mvsw_pr_neigh_info info;
+	struct rhash_head ht_node; /* node of mvsw_pr_vr */
+	struct list_head nexthop_group_list;
+};
+
+int mvsw_pr_switch_ageing_set(struct mvsw_pr_switch *sw, u32 ageing_time);
+
+int mvsw_pr_port_learning_set(struct mvsw_pr_port *mvsw_pr_port,
+			      bool learn_enable);
+int mvsw_pr_port_uc_flood_set(struct mvsw_pr_port *mvsw_pr_port, bool flood);
+int mvsw_pr_port_mc_flood_set(struct mvsw_pr_port *mvsw_pr_port, bool flood);
+int mvsw_pr_port_pvid_set(struct mvsw_pr_port *mvsw_pr_port, u16 vid);
+int mvsw_pr_port_vid_stp_set(struct mvsw_pr_port *port, u16 vid, u8 state);
+struct mvsw_pr_port_vlan *
+mvsw_pr_port_vlan_create(struct mvsw_pr_port *mvsw_pr_port, u16 vid,
+			 bool untagged);
+void mvsw_pr_port_vlan_destroy(struct mvsw_pr_port_vlan *mvsw_pr_port_vlan);
+int mvsw_pr_port_vlan_set(struct mvsw_pr_port *mvsw_pr_port, u16 vid,
+			  bool is_member, bool untagged);
+
+struct mvsw_pr_bridge_device *
+mvsw_pr_bridge_device_find(const struct mvsw_pr_bridge *bridge,
+			   const struct net_device *br_dev);
+u16 mvsw_pr_vlan_dev_vlan_id(struct mvsw_pr_bridge *bridge,
+			     struct net_device *dev);
+int mvsw_pr_8021d_bridge_create(struct mvsw_pr_switch *sw, u16 *bridge_id);
+int mvsw_pr_8021d_bridge_delete(struct mvsw_pr_switch *sw, u16 bridge_id);
+int mvsw_pr_8021d_bridge_port_add(struct mvsw_pr_port *mvsw_pr_port,
+				  u16 bridge_id);
+int mvsw_pr_8021d_bridge_port_delete(struct mvsw_pr_port *mvsw_pr_port,
+				     u16 bridge_id);
+
+int mvsw_pr_fdb_add(struct mvsw_pr_port *mvsw_pr_port, const unsigned char *mac,
+		    u16 vid, bool dynamic);
+int mvsw_pr_fdb_del(struct mvsw_pr_port *mvsw_pr_port, const unsigned char *mac,
+		    u16 vid);
+int mvsw_pr_fdb_flush_vlan(struct mvsw_pr_switch *sw, u16 vid,
+			   enum mvsw_pr_fdb_flush_mode mode);
+int mvsw_pr_fdb_flush_port_vlan(struct mvsw_pr_port *port, u16 vid,
+				enum mvsw_pr_fdb_flush_mode mode);
+int mvsw_pr_fdb_flush_port(struct mvsw_pr_port *port,
+			   enum mvsw_pr_fdb_flush_mode mode);
+int mvsw_pr_macvlan_add(const struct mvsw_pr_switch *sw, u16 vr_id,
+			const u8 *mac, u16 vid);
+int mvsw_pr_macvlan_del(const struct mvsw_pr_switch *sw, u16 vr_id,
+			const u8 *mac, u16 vid);
+
+int prestera_lag_member_add(struct mvsw_pr_port *port,
+			    struct net_device *lag_dev, u16 lag_id);
+int prestera_lag_member_del(struct mvsw_pr_port *port);
+int prestera_lag_member_enable(struct mvsw_pr_port *port, bool enable);
+bool mvsw_pr_port_is_lag_member(const struct mvsw_pr_port *port);
+int prestera_lag_id_find(struct mvsw_pr_switch *sw, struct net_device *lag_dev,
+			 u16 *lag_id);
+void prestera_lag_member_rif_leave(const struct mvsw_pr_port *port,
+				   u16 lag_id, u16 vr_id);
+
+int mvsw_pr_dev_if_type(const struct net_device *dev);
+
+/* prestera_flower.c */
+int mvsw_pr_flower_replace(struct mvsw_pr_switch *sw,
+			   struct prestera_acl_block *block,
+			   struct flow_cls_offload *f);
+void mvsw_pr_flower_destroy(struct mvsw_pr_switch *sw,
+			    struct prestera_acl_block *block,
+			    struct flow_cls_offload *f);
+int mvsw_pr_flower_stats(struct mvsw_pr_switch *sw,
+			 struct prestera_acl_block *block,
+			 struct flow_cls_offload *f);
+
+/* prestera_acl.c */
+int prestera_acl_init(struct mvsw_pr_switch *sw);
+void prestera_acl_fini(struct mvsw_pr_switch *sw);
+struct prestera_acl_block *
+prestera_acl_block_create(struct mvsw_pr_switch *sw, struct net *net);
+void prestera_acl_block_destroy(struct prestera_acl_block *block);
+struct net *prestera_acl_block_net(struct prestera_acl_block *block);
+struct mvsw_pr_switch *prestera_acl_block_sw(struct prestera_acl_block *block);
+unsigned int prestera_acl_block_rule_count(struct prestera_acl_block *block);
+void prestera_acl_block_disable_inc(struct prestera_acl_block *block);
+void prestera_acl_block_disable_dec(struct prestera_acl_block *block);
+bool prestera_acl_block_disabled(const struct prestera_acl_block *block);
+int prestera_acl_block_bind(struct mvsw_pr_switch *sw,
+			    struct prestera_acl_block *block,
+			    struct mvsw_pr_port *port);
+int prestera_acl_block_unbind(struct mvsw_pr_switch *sw,
+			      struct prestera_acl_block *block,
+			      struct mvsw_pr_port *port);
+struct prestera_acl_ruleset *
+prestera_acl_block_ruleset_get(struct prestera_acl_block *block);
+struct prestera_acl_rule *
+prestera_acl_rule_create(struct prestera_acl_block *block,
+			 unsigned long cookie);
+u32 prestera_acl_rule_priority_get(struct prestera_acl_rule *rule);
+void prestera_acl_rule_priority_set(struct prestera_acl_rule *rule,
+				    u32 priority);
+u8 prestera_acl_rule_hw_tc_get(struct prestera_acl_rule *rule);
+void prestera_acl_rule_hw_tc_set(struct prestera_acl_rule *rule, u8 hw_tc);
+u16 prestera_acl_rule_ruleset_id_get(const struct prestera_acl_rule *rule);
+struct list_head *
+prestera_acl_rule_action_list_get(struct prestera_acl_rule *rule);
+u8 prestera_acl_rule_action_len(struct prestera_acl_rule *rule);
+void prestera_acl_rule_action_add(struct prestera_acl_rule *rule,
+				  struct prestera_acl_rule_action_entry *entry);
+struct list_head *
+prestera_acl_rule_match_list_get(struct prestera_acl_rule *rule);
+void prestera_acl_rule_match_add(struct prestera_acl_rule *rule,
+				 struct prestera_acl_rule_match_entry *entry);
+void prestera_acl_rule_destroy(struct prestera_acl_rule *rule);
+struct prestera_acl_rule *
+prestera_acl_rule_lookup(struct prestera_acl_ruleset *ruleset,
+			 unsigned long cookie);
+int prestera_acl_rule_add(struct mvsw_pr_switch *sw,
+			  struct prestera_acl_rule *rule);
+void prestera_acl_rule_del(struct mvsw_pr_switch *sw,
+			   struct prestera_acl_rule *rule);
+int prestera_acl_rule_get_stats(struct mvsw_pr_switch *sw,
+				struct prestera_acl_rule *rule,
+				u64 *packets, u64 *bytes, u64 *last_use);
+
+/* VLAN API */
+struct mvsw_pr_port_vlan *
+mvsw_pr_port_vlan_find_by_vid(const struct mvsw_pr_port *mvsw_pr_port, u16 vid);
+void
+mvsw_pr_port_vlan_bridge_leave(struct mvsw_pr_port_vlan *mvsw_pr_port_vlan);
+
+int prestera_switchdev_register(struct mvsw_pr_switch *sw);
+void prestera_switchdev_unregister(struct mvsw_pr_switch *sw);
 
-bool prestera_netdev_check(const struct net_device *dev);
+int prestera_device_register(struct prestera_device *dev);
+void prestera_device_unregister(struct prestera_device *dev);
 
-#endif /* _PRESTERA_H_ */
+bool mvsw_pr_netdev_check(const struct net_device *dev);
+struct mvsw_pr_switch *mvsw_pr_switch_get(struct net_device *dev);
+struct mvsw_pr_port *mvsw_pr_port_dev_lower_find(struct net_device *dev);
+
+const struct mvsw_pr_port *mvsw_pr_port_find(u32 dev_hw_id, u32 port_hw_id);
+int mvsw_pr_schedule_dw(struct delayed_work *dwork, unsigned long delay);
+
+/* prestera_router.c */
+int mvsw_pr_router_init(struct mvsw_pr_switch *sw);
+void mvsw_pr_router_fini(struct mvsw_pr_switch *sw);
+int mvsw_pr_netdevice_router_port_event(struct net_device *dev,
+					unsigned long event, void *ptr);
+int mvsw_pr_inetaddr_valid_event(struct notifier_block *unused,
+				 unsigned long event, void *ptr);
+int mvsw_pr_netdevice_vrf_event(struct net_device *dev, unsigned long event,
+				struct netdev_notifier_changeupper_info *info);
+void mvsw_pr_port_router_leave(struct mvsw_pr_port *mvsw_pr_port);
+int mvsw_pr_lpm_add(struct mvsw_pr_switch *sw, u16 hw_vr_id,
+		    struct mvsw_pr_ip_addr *addr, u32 prefix_len, u32 grp_id);
+int mvsw_pr_lpm_del(struct mvsw_pr_switch *sw, u16 hw_vr_id,
+		    struct mvsw_pr_ip_addr *addr, u32 prefix_len);
+int mvsw_pr_nh_entries_set(const struct mvsw_pr_switch *sw, int count,
+			   struct mvsw_pr_neigh_info *nhs, u32 grp_id);
+int mvsw_pr_nh_entries_get(const struct mvsw_pr_switch *sw, int count,
+			   struct mvsw_pr_neigh_info *nhs, u32 grp_id);
+int mvsw_pr_nh_group_create(const struct mvsw_pr_switch *sw, u16 nh_count,
+			    u32 *grp_id);
+int mvsw_pr_nh_group_delete(const struct mvsw_pr_switch *sw, u16 nh_count,
+			    u32 grp_id);
+int mvsw_pr_mp4_hash_set(const struct mvsw_pr_switch *sw, u8 hash_policy);
+
+void mvsw_pr_rif_enable(struct mvsw_pr_switch *sw, struct net_device *dev,
+			bool enable);
+bool mvsw_pr_rif_exists(const struct mvsw_pr_switch *sw,
+			const struct net_device *dev);
+void mvsw_pr_router_lag_member_leave(const struct mvsw_pr_port *port,
+				     const struct net_device *dev);
+void prestera_lag_router_leave(struct mvsw_pr_switch *sw,
+			       struct net_device *lag_dev);
+
+void mvsw_pr_bridge_device_rifs_destroy(struct mvsw_pr_switch *sw,
+					struct net_device *bridge_dev);
+
+#endif /* _MVSW_PRESTERA_H_ */
diff --git a/drivers/net/ethernet/marvell/prestera/prestera_acl.c b/drivers/net/ethernet/marvell/prestera/prestera_acl.c
new file mode 100644
index 000000000..143df723c
--- /dev/null
+++ b/drivers/net/ethernet/marvell/prestera/prestera_acl.c
@@ -0,0 +1,402 @@
+// SPDX-License-Identifier: BSD-3-Clause OR GPL-2.0
+//
+// Copyright (c) 2020 Marvell International Ltd. All rights reserved.
+//
+
+#include <linux/rhashtable.h>
+
+#include "prestera.h"
+#include "prestera_hw.h"
+
+#define MVSW_ACL_RULE_DEF_HW_TC		3
+
+struct prestera_acl {
+	struct mvsw_pr_switch *sw;
+	struct list_head rules;
+};
+
+struct prestera_acl_block_binding {
+	struct list_head list;
+	struct mvsw_pr_port *port;
+};
+
+struct prestera_acl_ruleset {
+	struct rhashtable rule_ht;
+	struct mvsw_pr_switch *sw;
+	u16 id;
+};
+
+struct prestera_acl_block {
+	struct list_head binding_list;
+	struct mvsw_pr_switch *sw;
+	unsigned int rule_count;
+	unsigned int disable_count;
+	struct net *net;
+	struct prestera_acl_ruleset *ruleset;
+};
+
+struct prestera_acl_rule {
+	struct rhash_head ht_node; /* Member of acl HT */
+	struct list_head list;
+	struct list_head match_list;
+	struct list_head action_list;
+	struct prestera_acl_block *block;
+	unsigned long cookie;
+	u32 priority;
+	u8 n_actions;
+	u8 hw_tc;
+	u32 id;
+};
+
+static const struct rhashtable_params prestera_acl_rule_ht_params = {
+	.key_len = sizeof(unsigned long),
+	.key_offset = offsetof(struct prestera_acl_rule, cookie),
+	.head_offset = offsetof(struct prestera_acl_rule, ht_node),
+	.automatic_shrinking = true,
+};
+
+static struct prestera_acl_ruleset *
+prestera_acl_ruleset_create(struct mvsw_pr_switch *sw)
+{
+	int err;
+	struct prestera_acl_ruleset *ruleset;
+
+	ruleset = kzalloc(sizeof(*ruleset), GFP_KERNEL);
+	if (!ruleset)
+		return ERR_PTR(-ENOMEM);
+
+	err = rhashtable_init(&ruleset->rule_ht, &prestera_acl_rule_ht_params);
+	if (err)
+		goto err_rhashtable_init;
+
+	err = mvsw_pr_hw_acl_ruleset_create(sw, &ruleset->id);
+	if (err)
+		goto err_ruleset_create;
+
+	ruleset->sw = sw;
+
+	return ruleset;
+
+err_ruleset_create:
+	rhashtable_destroy(&ruleset->rule_ht);
+err_rhashtable_init:
+	kfree(ruleset);
+	return ERR_PTR(err);
+}
+
+static void prestera_acl_ruleset_destroy(struct prestera_acl_ruleset *ruleset)
+{
+	mvsw_pr_hw_acl_ruleset_del(ruleset->sw, ruleset->id);
+	rhashtable_destroy(&ruleset->rule_ht);
+	kfree(ruleset);
+}
+
+struct prestera_acl_block *
+prestera_acl_block_create(struct mvsw_pr_switch *sw, struct net *net)
+{
+	struct prestera_acl_block *block;
+
+	block = kzalloc(sizeof(*block), GFP_KERNEL);
+	if (!block)
+		return NULL;
+	INIT_LIST_HEAD(&block->binding_list);
+	block->net = net;
+	block->sw = sw;
+
+	block->ruleset = prestera_acl_ruleset_create(sw);
+	if (IS_ERR(block->ruleset)) {
+		kfree(block);
+		return NULL;
+	}
+
+	return block;
+}
+
+void prestera_acl_block_destroy(struct prestera_acl_block *block)
+{
+	prestera_acl_ruleset_destroy(block->ruleset);
+	WARN_ON(!list_empty(&block->binding_list));
+	kfree(block);
+}
+
+static struct prestera_acl_block_binding *
+prestera_acl_block_lookup(struct prestera_acl_block *block,
+			  struct mvsw_pr_port *port)
+{
+	struct prestera_acl_block_binding *binding;
+
+	list_for_each_entry(binding, &block->binding_list, list)
+		if (binding->port == port)
+			return binding;
+
+	return NULL;
+}
+
+unsigned int prestera_acl_block_rule_count(struct prestera_acl_block *block)
+{
+	return block ? block->rule_count : 0;
+}
+
+void prestera_acl_block_disable_inc(struct prestera_acl_block *block)
+{
+	if (block)
+		block->disable_count++;
+}
+
+void prestera_acl_block_disable_dec(struct prestera_acl_block *block)
+{
+	if (block)
+		block->disable_count--;
+}
+
+bool prestera_acl_block_disabled(const struct prestera_acl_block *block)
+{
+	return block->disable_count;
+}
+
+int prestera_acl_block_bind(struct mvsw_pr_switch *sw,
+			    struct prestera_acl_block *block,
+			    struct mvsw_pr_port *port)
+{
+	struct prestera_acl_block_binding *binding;
+	int err;
+
+	if (WARN_ON(prestera_acl_block_lookup(block, port)))
+		return -EEXIST;
+
+	binding = kzalloc(sizeof(*binding), GFP_KERNEL);
+	if (!binding)
+		return -ENOMEM;
+	binding->port = port;
+
+	err = mvsw_pr_hw_acl_port_bind(port, block->ruleset->id);
+	if (err)
+		goto err_rules_bind;
+
+	list_add(&binding->list, &block->binding_list);
+	return 0;
+
+err_rules_bind:
+	kfree(binding);
+	return err;
+}
+
+int prestera_acl_block_unbind(struct mvsw_pr_switch *sw,
+			      struct prestera_acl_block *block,
+			      struct mvsw_pr_port *port)
+{
+	struct prestera_acl_block_binding *binding;
+
+	binding = prestera_acl_block_lookup(block, port);
+	if (!binding)
+		return -ENOENT;
+
+	list_del(&binding->list);
+
+	mvsw_pr_hw_acl_port_unbind(port, block->ruleset->id);
+
+	kfree(binding);
+	return 0;
+}
+
+struct prestera_acl_ruleset *
+prestera_acl_block_ruleset_get(struct prestera_acl_block *block)
+{
+	return block->ruleset;
+}
+
+u16 prestera_acl_rule_ruleset_id_get(const struct prestera_acl_rule *rule)
+{
+	return rule->block->ruleset->id;
+}
+
+struct net *prestera_acl_block_net(struct prestera_acl_block *block)
+{
+	return block->net;
+}
+
+struct mvsw_pr_switch *prestera_acl_block_sw(struct prestera_acl_block *block)
+{
+	return block->sw;
+}
+
+struct prestera_acl_rule *
+prestera_acl_rule_lookup(struct prestera_acl_ruleset *ruleset,
+			 unsigned long cookie)
+{
+	return rhashtable_lookup_fast(&ruleset->rule_ht, &cookie,
+				      prestera_acl_rule_ht_params);
+}
+
+struct prestera_acl_rule *
+prestera_acl_rule_create(struct prestera_acl_block *block,
+			 unsigned long cookie)
+{
+	struct prestera_acl_rule *rule;
+
+	rule = kzalloc(sizeof(*rule), GFP_KERNEL);
+	if (!rule)
+		return ERR_PTR(-ENOMEM);
+
+	INIT_LIST_HEAD(&rule->match_list);
+	INIT_LIST_HEAD(&rule->action_list);
+	rule->cookie = cookie;
+	rule->block = block;
+	rule->hw_tc = MVSW_ACL_RULE_DEF_HW_TC;
+
+	return rule;
+}
+
+struct list_head *
+prestera_acl_rule_match_list_get(struct prestera_acl_rule *rule)
+{
+	return &rule->match_list;
+}
+
+struct list_head *
+prestera_acl_rule_action_list_get(struct prestera_acl_rule *rule)
+{
+	return &rule->action_list;
+}
+
+void prestera_acl_rule_action_add(struct prestera_acl_rule *rule,
+				  struct prestera_acl_rule_action_entry *entry)
+{
+	list_add(&entry->list, &rule->action_list);
+	rule->n_actions++;
+}
+
+u8 prestera_acl_rule_action_len(struct prestera_acl_rule *rule)
+{
+	return rule->n_actions;
+}
+
+u32 prestera_acl_rule_priority_get(struct prestera_acl_rule *rule)
+{
+	return rule->priority;
+}
+
+void prestera_acl_rule_priority_set(struct prestera_acl_rule *rule,
+				    u32 priority)
+{
+	rule->priority = priority;
+}
+
+u8 prestera_acl_rule_hw_tc_get(struct prestera_acl_rule *rule)
+{
+	return rule->hw_tc;
+}
+
+void prestera_acl_rule_hw_tc_set(struct prestera_acl_rule *rule, u8 hw_tc)
+{
+	rule->hw_tc = hw_tc;
+}
+
+void prestera_acl_rule_match_add(struct prestera_acl_rule *rule,
+				 struct prestera_acl_rule_match_entry *entry)
+{
+	list_add(&entry->list, &rule->match_list);
+}
+
+void prestera_acl_rule_destroy(struct prestera_acl_rule *rule)
+{
+	struct prestera_acl_rule_action_entry *a_entry;
+	struct prestera_acl_rule_match_entry *m_entry;
+	struct list_head *pos, *n;
+
+	list_for_each_safe(pos, n, &rule->match_list) {
+		m_entry = list_entry(pos, typeof(*m_entry), list);
+		list_del(pos);
+		kfree(m_entry);
+	}
+	list_for_each_safe(pos, n, &rule->action_list) {
+		a_entry = list_entry(pos, typeof(*a_entry), list);
+		list_del(pos);
+		kfree(a_entry);
+	}
+	kfree(rule);
+}
+
+int prestera_acl_rule_add(struct mvsw_pr_switch *sw,
+			  struct prestera_acl_rule *rule)
+{
+	int err;
+	u32 rule_id;
+
+	/* try to add rule to hash table first */
+	err = rhashtable_insert_fast(&rule->block->ruleset->rule_ht,
+				     &rule->ht_node,
+				     prestera_acl_rule_ht_params);
+	if (err)
+		return err;
+
+	/* add rule to hw */
+	err = mvsw_pr_hw_acl_rule_add(sw, rule, &rule_id);
+	if (err)
+		goto err_rule_add;
+
+	rule->id = rule_id;
+
+	list_add_tail(&rule->list, &sw->acl->rules);
+	rule->block->rule_count++;
+
+	return 0;
+
+err_rule_add:
+	rhashtable_remove_fast(&rule->block->ruleset->rule_ht, &rule->ht_node,
+			       prestera_acl_rule_ht_params);
+	return err;
+}
+
+void prestera_acl_rule_del(struct mvsw_pr_switch *sw,
+			   struct prestera_acl_rule *rule)
+{
+	rhashtable_remove_fast(&rule->block->ruleset->rule_ht, &rule->ht_node,
+			       prestera_acl_rule_ht_params);
+	rule->block->rule_count--;
+	list_del(&rule->list);
+	mvsw_pr_hw_acl_rule_del(sw, rule->id);
+}
+
+int prestera_acl_rule_get_stats(struct mvsw_pr_switch *sw,
+				struct prestera_acl_rule *rule,
+				u64 *packets, u64 *bytes, u64 *last_use)
+{
+	u64 current_packets;
+	u64 current_bytes;
+	int err;
+
+	err = mvsw_pr_hw_acl_rule_stats_get(sw, rule->id, &current_packets,
+					    &current_bytes);
+	if (err)
+		return err;
+
+	*packets = current_packets;
+	*bytes = current_bytes;
+	*last_use = jiffies;
+
+	return 0;
+}
+
+int prestera_acl_init(struct mvsw_pr_switch *sw)
+{
+	struct prestera_acl *acl;
+
+	acl = kzalloc(sizeof(*acl), GFP_KERNEL);
+	if (!acl)
+		return -ENOMEM;
+
+	INIT_LIST_HEAD(&acl->rules);
+	sw->acl = acl;
+	acl->sw = sw;
+
+	return 0;
+}
+
+void prestera_acl_fini(struct mvsw_pr_switch *sw)
+{
+	struct prestera_acl *acl = sw->acl;
+
+	WARN_ON(!list_empty(&acl->rules));
+	kfree(acl);
+}
diff --git a/drivers/net/ethernet/marvell/prestera/prestera_debugfs.c b/drivers/net/ethernet/marvell/prestera/prestera_debugfs.c
new file mode 100644
index 000000000..1b4b8557e
--- /dev/null
+++ b/drivers/net/ethernet/marvell/prestera/prestera_debugfs.c
@@ -0,0 +1,160 @@
+// SPDX-License-Identifier: BSD-3-Clause OR GPL-2.0
+/*
+ * Copyright (c) 2020 Marvell International Ltd. All rights reserved.
+ *
+ */
+#include <linux/kernel.h>
+#include <linux/fs.h>
+#include <linux/device.h>
+#include <linux/debugfs.h>
+
+#include "prestera_debugfs.h"
+#include "prestera.h"
+#include "prestera_log.h"
+#include "prestera_fw_log.h"
+#include "prestera_rxtx.h"
+
+#define DEBUGFS_ROOTDIR	"prestera"
+
+#define CPU_CODE_SUBDIR_NAME	"traps"
+#define CPU_CODE_MAX_BUF_SIZE	(MVSW_PR_RXTX_CPU_CODE_MAX_NUM * 32)
+
+static ssize_t cpu_code_stats_read(struct file *file,
+				   char __user *ubuf,
+				   size_t count, loff_t *ppos);
+
+struct mvsw_pr_debugfs {
+	struct dentry *root_dir;
+	struct dentry *cpu_code_subdir;
+	const struct file_operations cpu_code_stats_fops;
+	char *cpu_code_stats_buf;
+	/* serialize access to cpu_code_stats_buf */
+	struct mutex cpu_code_stats_mtx;
+};
+
+static struct mvsw_pr_debugfs prestera_debugfs = {
+	.cpu_code_stats_fops = {
+		.read = cpu_code_stats_read,
+		.open = simple_open,
+		.llseek = default_llseek,
+	},
+};
+
+int mvsw_pr_debugfs_init(struct mvsw_pr_switch *sw)
+{
+	const struct file_operations *fops =
+		&prestera_debugfs.cpu_code_stats_fops;
+	char file_name[] = "cpu_code_XXX_stats";
+	int err;
+	int i;
+
+	mutex_init(&prestera_debugfs.cpu_code_stats_mtx);
+
+	prestera_debugfs.cpu_code_stats_buf =
+		kzalloc(CPU_CODE_MAX_BUF_SIZE, GFP_KERNEL);
+
+	if (!prestera_debugfs.cpu_code_stats_buf)
+		return -ENOMEM;
+
+	err = mvsw_pr_fw_log_init(sw);
+	if (err)
+		return err;
+
+	prestera_debugfs.root_dir = debugfs_create_dir(DEBUGFS_ROOTDIR, NULL);
+	if (!prestera_debugfs.root_dir) {
+		err = -ENOMEM;
+		goto root_dir_alloc_failed;
+	}
+
+	prestera_debugfs.cpu_code_subdir =
+		debugfs_create_dir(CPU_CODE_SUBDIR_NAME,
+				   prestera_debugfs.root_dir);
+	if (!prestera_debugfs.cpu_code_subdir) {
+		err = -ENOMEM;
+		goto cpu_code_subdir_alloc_failed;
+	}
+
+	for (i = 0; i < MVSW_PR_RXTX_CPU_CODE_MAX_NUM; ++i) {
+		snprintf(file_name, sizeof(file_name), "cpu_code_%d_stats", i);
+		if (!debugfs_create_file(file_name, 0644,
+					 prestera_debugfs.cpu_code_subdir,
+					 (void *)(long)i, fops)) {
+			err = -ENOMEM;
+			goto cpu_code_single_file_creation_failed;
+		}
+	}
+
+	strncpy(file_name, "cpu_code_stats", sizeof(file_name));
+
+	if (!debugfs_create_file(file_name, 0644,
+				 prestera_debugfs.cpu_code_subdir,
+				 (void *)(long)MVSW_PR_RXTX_CPU_CODE_MAX_NUM,
+				 fops)) {
+		err = -ENOMEM;
+		goto cpu_code_single_file_creation_failed;
+	}
+
+	return 0;
+
+cpu_code_single_file_creation_failed:
+	debugfs_remove(prestera_debugfs.cpu_code_subdir);
+cpu_code_subdir_alloc_failed:
+	debugfs_remove(prestera_debugfs.root_dir);
+root_dir_alloc_failed:
+	mvsw_pr_fw_log_fini(sw);
+
+	return err;
+}
+
+void mvsw_pr_debugfs_fini(struct mvsw_pr_switch *sw)
+{
+	mvsw_pr_fw_log_fini(sw);
+
+	debugfs_remove(prestera_debugfs.cpu_code_subdir);
+	debugfs_remove(prestera_debugfs.root_dir);
+
+	mutex_destroy(&prestera_debugfs.cpu_code_stats_mtx);
+
+	kfree(prestera_debugfs.cpu_code_stats_buf);
+}
+
+static ssize_t cpu_code_stats_read(struct file *file,
+				   char __user *ubuf,
+				   size_t count, loff_t *ppos)
+{
+	char *buf = prestera_debugfs.cpu_code_stats_buf;
+	u16 cpu_code = (u16)(long)file->private_data;
+	u64 cpu_code_stats;
+	/* as the snprintf doesn't count for \0, start with 1 */
+	int buf_len = 1;
+	int ret;
+
+	mutex_lock(&prestera_debugfs.cpu_code_stats_mtx);
+
+	if (cpu_code == MVSW_PR_RXTX_CPU_CODE_MAX_NUM) {
+		int i;
+
+		memset(buf, 0, CPU_CODE_MAX_BUF_SIZE);
+
+		for (i = 0; i < MVSW_PR_RXTX_CPU_CODE_MAX_NUM; ++i) {
+			cpu_code_stats = mvsw_pr_rxtx_get_cpu_code_stats(i);
+
+			if (!cpu_code_stats)
+				continue;
+
+			buf_len += snprintf(buf + buf_len,
+					    CPU_CODE_MAX_BUF_SIZE - buf_len,
+					    "%u:%llu\n", i, cpu_code_stats);
+		}
+
+	} else {
+		cpu_code_stats = mvsw_pr_rxtx_get_cpu_code_stats((u8)cpu_code);
+
+		buf_len += sprintf(buf, "%llu\n", cpu_code_stats);
+	}
+
+	ret = simple_read_from_buffer(ubuf, count, ppos, buf, buf_len);
+	mutex_unlock(&prestera_debugfs.cpu_code_stats_mtx);
+
+	return ret;
+}
diff --git a/drivers/net/ethernet/marvell/prestera/prestera_debugfs.h b/drivers/net/ethernet/marvell/prestera/prestera_debugfs.h
new file mode 100644
index 000000000..b3ce1a41b
--- /dev/null
+++ b/drivers/net/ethernet/marvell/prestera/prestera_debugfs.h
@@ -0,0 +1,14 @@
+/* SPDX-License-Identifier: BSD-3-Clause OR GPL-2.0
+ *
+ * Copyright (c) 2020 Marvell International Ltd. All rights reserved.
+ *
+ */
+#ifndef _MVSW_PRESTERA_DEBUGFS_H_
+#define _MVSW_PRESTERA_DEBUGFS_H_
+
+struct mvsw_pr_switch;
+
+int mvsw_pr_debugfs_init(struct mvsw_pr_switch *sw);
+void mvsw_pr_debugfs_fini(struct mvsw_pr_switch *sw);
+
+#endif /* _MVSW_PRESTERA_DEBUGFS_H_ */
diff --git a/drivers/net/ethernet/marvell/prestera/prestera_devlink.c b/drivers/net/ethernet/marvell/prestera/prestera_devlink.c
index 94c185a0e..7a9d5f44e 100644
--- a/drivers/net/ethernet/marvell/prestera/prestera_devlink.c
+++ b/drivers/net/ethernet/marvell/prestera/prestera_devlink.c
@@ -1,5 +1,5 @@
 // SPDX-License-Identifier: BSD-3-Clause OR GPL-2.0
-/* Copyright (c) 2019-2020 Marvell International Ltd. All rights reserved */
+/* Copyright (c) 2020 Marvell International Ltd. All rights reserved */
 
 #include <net/devlink.h>
 
@@ -9,7 +9,7 @@ static int prestera_dl_info_get(struct devlink *dl,
 				struct devlink_info_req *req,
 				struct netlink_ext_ack *extack)
 {
-	struct prestera_switch *sw = devlink_priv(dl);
+	struct mvsw_pr_switch *sw = devlink_priv(dl);
 	char buf[16];
 	int err;
 
@@ -31,44 +31,44 @@ static const struct devlink_ops prestera_dl_ops = {
 	.info_get = prestera_dl_info_get,
 };
 
-struct prestera_switch *prestera_devlink_alloc(void)
+struct mvsw_pr_switch *prestera_devlink_alloc(void)
 {
 	struct devlink *dl;
 
-	dl = devlink_alloc(&prestera_dl_ops, sizeof(struct prestera_switch));
+	dl = devlink_alloc(&prestera_dl_ops, sizeof(struct mvsw_pr_switch));
 
 	return devlink_priv(dl);
 }
 
-void prestera_devlink_free(struct prestera_switch *sw)
+void prestera_devlink_free(struct mvsw_pr_switch *sw)
 {
 	struct devlink *dl = priv_to_devlink(sw);
 
 	devlink_free(dl);
 }
 
-int prestera_devlink_register(struct prestera_switch *sw)
+int prestera_devlink_register(struct mvsw_pr_switch *sw)
 {
 	struct devlink *dl = priv_to_devlink(sw);
 	int err;
 
 	err = devlink_register(dl, sw->dev->dev);
 	if (err)
-		dev_err(prestera_dev(sw), "devlink_register failed: %d\n", err);
+		dev_err(sw->dev->dev, "devlink_register failed: %d\n", err);
 
 	return err;
 }
 
-void prestera_devlink_unregister(struct prestera_switch *sw)
+void prestera_devlink_unregister(struct mvsw_pr_switch *sw)
 {
 	struct devlink *dl = priv_to_devlink(sw);
 
 	devlink_unregister(dl);
 }
 
-int prestera_devlink_port_register(struct prestera_port *port)
+int prestera_devlink_port_register(struct mvsw_pr_port *port)
 {
-	struct prestera_switch *sw = port->sw;
+	struct mvsw_pr_switch *sw = port->sw;
 	struct devlink *dl = priv_to_devlink(sw);
 	struct devlink_port_attrs attrs = {};
 	int err;
@@ -82,31 +82,31 @@ int prestera_devlink_port_register(struct prestera_port *port)
 
 	err = devlink_port_register(dl, &port->dl_port, port->fp_id);
 	if (err) {
-		dev_err(prestera_dev(sw), "devlink_port_register failed: %d\n", err);
+		dev_err(sw->dev->dev, "devlink_port_register failed\n");
 		return err;
 	}
 
 	return 0;
 }
 
-void prestera_devlink_port_unregister(struct prestera_port *port)
+void prestera_devlink_port_unregister(struct mvsw_pr_port *port)
 {
 	devlink_port_unregister(&port->dl_port);
 }
 
-void prestera_devlink_port_set(struct prestera_port *port)
+void prestera_devlink_port_set(struct mvsw_pr_port *port)
 {
-	devlink_port_type_eth_set(&port->dl_port, port->dev);
+	devlink_port_type_eth_set(&port->dl_port, port->net_dev);
 }
 
-void prestera_devlink_port_clear(struct prestera_port *port)
+void prestera_devlink_port_clear(struct mvsw_pr_port *port)
 {
 	devlink_port_type_clear(&port->dl_port);
 }
 
 struct devlink_port *prestera_devlink_get_port(struct net_device *dev)
 {
-	struct prestera_port *port = netdev_priv(dev);
+	struct mvsw_pr_port *port = netdev_priv(dev);
 
 	return &port->dl_port;
 }
diff --git a/drivers/net/ethernet/marvell/prestera/prestera_devlink.h b/drivers/net/ethernet/marvell/prestera/prestera_devlink.h
index 51bee9f75..4d51c4287 100644
--- a/drivers/net/ethernet/marvell/prestera/prestera_devlink.h
+++ b/drivers/net/ethernet/marvell/prestera/prestera_devlink.h
@@ -1,22 +1,22 @@
 /* SPDX-License-Identifier: BSD-3-Clause OR GPL-2.0 */
-/* Copyright (c) 2019-2020 Marvell International Ltd. All rights reserved. */
+/* Copyright (c) 2020 Marvell International Ltd. All rights reserved. */
 
 #ifndef _PRESTERA_DEVLINK_H_
 #define _PRESTERA_DEVLINK_H_
 
 #include "prestera.h"
 
-struct prestera_switch *prestera_devlink_alloc(void);
-void prestera_devlink_free(struct prestera_switch *sw);
+struct mvsw_pr_switch *prestera_devlink_alloc(void);
+void prestera_devlink_free(struct mvsw_pr_switch *sw);
 
-int prestera_devlink_register(struct prestera_switch *sw);
-void prestera_devlink_unregister(struct prestera_switch *sw);
+int prestera_devlink_register(struct mvsw_pr_switch *sw);
+void prestera_devlink_unregister(struct mvsw_pr_switch *sw);
 
-int prestera_devlink_port_register(struct prestera_port *port);
-void prestera_devlink_port_unregister(struct prestera_port *port);
+int prestera_devlink_port_register(struct mvsw_pr_port *port);
+void prestera_devlink_port_unregister(struct mvsw_pr_port *port);
 
-void prestera_devlink_port_set(struct prestera_port *port);
-void prestera_devlink_port_clear(struct prestera_port *port);
+void prestera_devlink_port_set(struct mvsw_pr_port *port);
+void prestera_devlink_port_clear(struct mvsw_pr_port *port);
 
 struct devlink_port *prestera_devlink_get_port(struct net_device *dev);
 
diff --git a/drivers/net/ethernet/marvell/prestera/prestera_drv_ver.h b/drivers/net/ethernet/marvell/prestera/prestera_drv_ver.h
new file mode 100644
index 000000000..29b337586
--- /dev/null
+++ b/drivers/net/ethernet/marvell/prestera/prestera_drv_ver.h
@@ -0,0 +1,23 @@
+/* SPDX-License-Identifier: BSD-3-Clause OR GPL-2.0
+ *
+ * Copyright (c) 2019-2020 Marvell International Ltd. All rights reserved.
+ *
+ */
+#ifndef _PRESTERA_DRV_VER_H_
+#define _PRESTERA_DRV_VER_H_
+
+#include <linux/stringify.h>
+
+/* Prestera driver version */
+#define PRESTERA_DRV_VER_MAJOR	2
+#define PRESTERA_DRV_VER_MINOR	0
+#define PRESTERA_DRV_VER_PATCH	0
+#define PRESTERA_DRV_VER_EXTRA
+
+#define PRESTERA_DRV_VER \
+		__stringify(PRESTERA_DRV_VER_MAJOR)  "." \
+		__stringify(PRESTERA_DRV_VER_MINOR)  "." \
+		__stringify(PRESTERA_DRV_VER_PATCH)  \
+		__stringify(PRESTERA_DRV_VER_EXTRA)
+
+#endif  /* _PRESTERA_DRV_VER_H_ */
diff --git a/drivers/net/ethernet/marvell/prestera/prestera_dsa.c b/drivers/net/ethernet/marvell/prestera/prestera_dsa.c
index a5e01c7a3..3a1b81979 100644
--- a/drivers/net/ethernet/marvell/prestera/prestera_dsa.c
+++ b/drivers/net/ethernet/marvell/prestera/prestera_dsa.c
@@ -1,104 +1,316 @@
 // SPDX-License-Identifier: BSD-3-Clause OR GPL-2.0
-/* Copyright (c) 2020 Marvell International Ltd. All rights reserved */
+/*
+ * Copyright (c) 2020 Marvell International Ltd. All rights reserved.
+ *
+ */
+#include "prestera.h"
+#include "prestera_dsa.h"
 
-#include <linux/bitfield.h>
+#include <linux/string.h>
 #include <linux/bitops.h>
+#include <linux/bitfield.h>
 #include <linux/errno.h>
-#include <linux/string.h>
 
-#include "prestera_dsa.h"
+#define W0_MASK_IS_TAGGED	BIT(29)
+
+/* TrgDev[4:0] = {Word0[28:24]} */
+#define W0_MASK_HW_DEV_NUM	GENMASK(28, 24)
+
+/* SrcPort/TrgPort extended to 8b
+ * SrcPort/TrgPort[7:0] = {Word2[20], Word1[11:10], Word0[23:19]}
+ */
+#define W0_MASK_IFACE_PORT_NUM	GENMASK(23, 19)
+
+/* bits 30:31 - TagCommand 1 = FROM_CPU */
+#define W0_MASK_DSA_CMD		GENMASK(31, 30)
+
+/* bits 13:15 -- UP */
+#define W0_MASK_VPT		GENMASK(15, 13)
+
+#define W0_MASK_EXT_BIT		BIT(12)
+#define W0_MASK_OPCODE		GENMASK(18, 16)
+
+/* bit 16 - CFI */
+#define W0_MASK_CFI_BIT		BIT(16)
+
+/* bits 0:11 -- VID */
+#define W0_MASK_VID		GENMASK(11, 0)
+
+#define W1_MASK_SRC_IS_TARNK	BIT(27)
+
+/* SrcPort/TrgPort extended to 8b
+ * SrcPort/TrgPort[7:0] = {Word2[20], Word1[11:10], Word0[23:19]}
+ */
+#define W1_MASK_IFACE_PORT_NUM	GENMASK(11, 10)
+
+#define W1_MASK_EXT_BIT		BIT(31)
+#define W1_MASK_CFI_BIT		BIT(30)
+
+/* bit 30 -- EgressFilterEn */
+#define W1_MASK_EGR_FILTER_EN	BIT(30)
+
+/* bit 28 -- egrFilterRegistered */
+#define W1_MASK_EGR_FILTER_REG	BIT(28)
 
-#define PRESTERA_DSA_W0_CMD		GENMASK(31, 30)
-#define PRESTERA_DSA_W0_IS_TAGGED	BIT(29)
-#define PRESTERA_DSA_W0_DEV_NUM		GENMASK(28, 24)
-#define PRESTERA_DSA_W0_PORT_NUM	GENMASK(23, 19)
-#define PRESTERA_DSA_W0_VPT		GENMASK(15, 13)
-#define PRESTERA_DSA_W0_EXT_BIT		BIT(12)
-#define PRESTERA_DSA_W0_VID		GENMASK(11, 0)
+/* bits 20-24 -- Src-ID */
+#define W1_MASK_SRC_ID		GENMASK(24, 20)
 
-#define PRESTERA_DSA_W1_EXT_BIT		BIT(31)
-#define PRESTERA_DSA_W1_CFI_BIT		BIT(30)
-#define PRESTERA_DSA_W1_PORT_NUM	GENMASK(11, 10)
+/* bits 15-19 -- SrcDev */
+#define W1_MASK_SRC_DEV		GENMASK(19, 15)
 
-#define PRESTERA_DSA_W2_EXT_BIT		BIT(31)
-#define PRESTERA_DSA_W2_PORT_NUM	BIT(20)
+/* SrcTrunk is extended to 12b
+ * SrcTrunk[11:0] = {Word2[14:3]
+ */
+#define W2_MASK_SRC_TRANK_ID	GENMASK(14, 3)
 
-#define PRESTERA_DSA_W3_VID		GENMASK(30, 27)
-#define PRESTERA_DSA_W3_DST_EPORT	GENMASK(23, 7)
-#define PRESTERA_DSA_W3_DEV_NUM		GENMASK(6, 0)
+/* SRCePort[16:0]/TRGePort[16:0]/ = {Word2[19:3]} */
+#define W2_MASK_IFACE_EPORT	GENMASK(19, 3)
 
-#define PRESTERA_DSA_VID		GENMASK(15, 12)
-#define PRESTERA_DSA_DEV_NUM		GENMASK(11, 5)
+/* SrcPort/TrgPort extended to 8b
+ * SrcPort/TrgPort[7:0] = {Word2[20], Word1[11:10], Word0[23:19]}
+ */
+#define W2_MASK_IFACE_PORT_NUM	BIT(20)
 
-int prestera_dsa_parse(struct prestera_dsa *dsa, const u8 *dsa_buf)
+#define W2_MASK_EXT_BIT		BIT(31)
+
+/* 5b SrcID is extended to 12 bits
+ * SrcID[11:0] = {Word2[27:21], Word1[24:20]}
+ */
+#define W2_MASK_SRC_ID		GENMASK(27, 21)
+
+/* 5b SrcDev is extended to 12b
+ * SrcDev[11:0] = {Word2[20:14], Word1[19:15]}
+ */
+#define W2_MASK_SRC_DEV		GENMASK(20, 14)
+
+/* trgHwDev and trgPort
+ * TrgDev[11:5] = {Word3[6:0]}
+ */
+#define W3_MASK_HW_DEV_NUM	GENMASK(6, 0)
+
+/* bits 0-7 -- CpuCode */
+#define W1_MASK_CPU_CODE	GENMASK(7, 0)
+
+/* VID becomes 16b eVLAN. eVLAN[15:0] = {Word3[30:27], Word0[11:0]} */
+#define W3_MASK_VID		GENMASK(30, 27)
+
+/* TRGePort[16:0] = {Word3[23:7]} */
+#define W3_MASK_DST_EPORT	GENMASK(23, 7)
+
+#define DEV_NUM_MASK		GENMASK(11, 5)
+#define VID_MASK		GENMASK(15, 12)
+
+static int net_if_dsa_to_cpu_parse(const u32 *words_ptr,
+				   struct mvsw_pr_dsa *dsa_info_ptr)
+{
+	u32 get_value;	/* used to get needed bits from the DSA */
+	struct mvsw_pr_dsa_to_cpu *to_cpu_ptr;
+
+	to_cpu_ptr = &dsa_info_ptr->dsa_info.to_cpu;
+	to_cpu_ptr->is_tagged =
+	    (bool)FIELD_GET(W0_MASK_IS_TAGGED, words_ptr[0]);
+	to_cpu_ptr->hw_dev_num = FIELD_GET(W0_MASK_HW_DEV_NUM, words_ptr[0]);
+	to_cpu_ptr->src_is_trunk =
+	    (bool)FIELD_GET(W1_MASK_SRC_IS_TARNK, words_ptr[1]);
+
+	/* set hw dev num */
+	get_value = FIELD_GET(W3_MASK_HW_DEV_NUM, words_ptr[3]);
+	to_cpu_ptr->hw_dev_num &= W3_MASK_HW_DEV_NUM;
+	to_cpu_ptr->hw_dev_num |= FIELD_PREP(DEV_NUM_MASK, get_value);
+
+	get_value = FIELD_GET(W1_MASK_CPU_CODE, words_ptr[1]);
+	to_cpu_ptr->cpu_code = (u8)get_value;
+
+	if (to_cpu_ptr->src_is_trunk) {
+		to_cpu_ptr->iface.src_trunk_id =
+		    (u16)FIELD_GET(W2_MASK_SRC_TRANK_ID, words_ptr[2]);
+	} else {
+		/* When to_cpu_ptr->is_egress_pipe = false:
+		 *   this field indicates the source ePort number assigned by
+		 *   the ingress device.
+		 * When to_cpu_ptr->is_egress_pipe = true:
+		 *   this field indicates the target ePort number assigned by
+		 *   the ingress device.
+		 */
+		to_cpu_ptr->iface.eport =
+		    FIELD_GET(W2_MASK_IFACE_EPORT, words_ptr[2]);
+	}
+	to_cpu_ptr->iface.port_num =
+	    (FIELD_GET(W0_MASK_IFACE_PORT_NUM, words_ptr[0]) << 0) |
+	    (FIELD_GET(W1_MASK_IFACE_PORT_NUM, words_ptr[1]) << 5) |
+	    (FIELD_GET(W2_MASK_IFACE_PORT_NUM, words_ptr[2]) << 7);
+
+	return 0;
+}
+
+int mvsw_pr_dsa_parse(const u8 *dsa_bytes_ptr, struct mvsw_pr_dsa *dsa_info_ptr)
 {
-	__be32 *dsa_words = (__be32 *)dsa_buf;
-	enum prestera_dsa_cmd cmd;
-	u32 words[4];
-	u32 field;
+	u32 get_value;		/* used to get needed bits from the DSA */
+	u32 words_ptr[4] = { 0 };	/* DSA tag can be up to 4 words */
+	u32 *dsa_words_ptr = (u32 *)dsa_bytes_ptr;
 
-	words[0] = ntohl(dsa_words[0]);
-	words[1] = ntohl(dsa_words[1]);
-	words[2] = ntohl(dsa_words[2]);
-	words[3] = ntohl(dsa_words[3]);
+	/* sanity */
+	if (unlikely(!dsa_info_ptr || !dsa_bytes_ptr))
+		return -EINVAL;
+
+	/* zero results */
+	memset(dsa_info_ptr, 0, sizeof(struct mvsw_pr_dsa));
+
+	/* copy the data of the first word */
+	words_ptr[0] = ntohl((__force __be32)dsa_words_ptr[0]);
 
 	/* set the common parameters */
-	cmd = (enum prestera_dsa_cmd)FIELD_GET(PRESTERA_DSA_W0_CMD, words[0]);
+	dsa_info_ptr->dsa_cmd =
+	    (enum mvsw_pr_dsa_cmd)FIELD_GET(W0_MASK_DSA_CMD, words_ptr[0]);
+
+	/* vid & vlan prio */
+	dsa_info_ptr->common_params.vid =
+	    (u16)FIELD_GET(W0_MASK_VID, words_ptr[0]);
+	dsa_info_ptr->common_params.vpt =
+	    (u8)FIELD_GET(W0_MASK_VPT, words_ptr[0]);
 
 	/* only to CPU is supported */
-	if (unlikely(cmd != PRESTERA_DSA_CMD_TO_CPU))
+	if (unlikely(dsa_info_ptr->dsa_cmd != MVSW_NET_DSA_CMD_TO_CPU_E))
 		return -EINVAL;
 
-	if (FIELD_GET(PRESTERA_DSA_W0_EXT_BIT, words[0]) == 0)
+	/* check extended bit */
+	if (FIELD_GET(W0_MASK_EXT_BIT, words_ptr[0]) == 0)
+		/* 1 words DSA tag is not supported */
 		return -EINVAL;
-	if (FIELD_GET(PRESTERA_DSA_W1_EXT_BIT, words[1]) == 0)
+
+	/* check that the "old" cpu opcode is set the 0xF
+	 * (with the extended bit)
+	 */
+	if (FIELD_GET(W0_MASK_OPCODE, words_ptr[0]) != 0x07)
 		return -EINVAL;
-	if (FIELD_GET(PRESTERA_DSA_W2_EXT_BIT, words[2]) == 0)
+
+	/* copy the data of the second word */
+	words_ptr[1] = ntohl((__force __be32)dsa_words_ptr[1]);
+
+	/* check the extended bit */
+	if (FIELD_GET(W1_MASK_EXT_BIT, words_ptr[1]) == 0)
+		/* 2 words DSA tag is not supported */
+		return -EINVAL;
+
+	/* copy the data of the third word */
+	words_ptr[2] = ntohl((__force __be32)dsa_words_ptr[2]);
+
+	/* check the extended bit */
+	if (FIELD_GET(W2_MASK_EXT_BIT, words_ptr[1]) == 0)
+		/* 3 words DSA tag is not supported */
 		return -EINVAL;
 
-	field = FIELD_GET(PRESTERA_DSA_W3_VID, words[3]);
+	/* copy the data of the forth word */
+	words_ptr[3] = ntohl((__force __be32)dsa_words_ptr[3]);
 
-	dsa->vlan.is_tagged = FIELD_GET(PRESTERA_DSA_W0_IS_TAGGED, words[0]);
-	dsa->vlan.cfi_bit = FIELD_GET(PRESTERA_DSA_W1_CFI_BIT, words[1]);
-	dsa->vlan.vpt = FIELD_GET(PRESTERA_DSA_W0_VPT, words[0]);
-	dsa->vlan.vid = FIELD_GET(PRESTERA_DSA_W0_VID, words[0]);
-	dsa->vlan.vid &= ~PRESTERA_DSA_VID;
-	dsa->vlan.vid |= FIELD_PREP(PRESTERA_DSA_VID, field);
+	/* VID */
+	get_value = FIELD_GET(W3_MASK_VID, words_ptr[3]);
+	dsa_info_ptr->common_params.vid &= ~VID_MASK;
+	dsa_info_ptr->common_params.vid |= FIELD_PREP(VID_MASK, get_value);
 
-	field = FIELD_GET(PRESTERA_DSA_W3_DEV_NUM, words[3]);
+	dsa_info_ptr->common_params.cfi_bit =
+	    (u8)FIELD_GET(W1_MASK_CFI_BIT, words_ptr[1]);
 
-	dsa->hw_dev_num = FIELD_GET(PRESTERA_DSA_W0_DEV_NUM, words[0]);
-	dsa->hw_dev_num |= FIELD_PREP(PRESTERA_DSA_DEV_NUM, field);
+	return net_if_dsa_to_cpu_parse(words_ptr, dsa_info_ptr);
+}
+
+static int net_if_dsa_tag_from_cpu_build(const struct mvsw_pr_dsa *dsa_info_ptr,
+					 u32 *words_ptr)
+{
+	u32 trg_hw_dev = 0;
+	u32 trg_port = 0;
+	const struct mvsw_pr_dsa_from_cpu *from_cpu_ptr =
+	    &dsa_info_ptr->dsa_info.from_cpu;
+
+	if (unlikely(from_cpu_ptr->dst_iface.type != MVSW_IF_PORT_E))
+		/* only sending to port interface is supported */
+		return -EINVAL;
 
-	dsa->port_num = (FIELD_GET(PRESTERA_DSA_W0_PORT_NUM, words[0]) << 0) |
-			(FIELD_GET(PRESTERA_DSA_W1_PORT_NUM, words[1]) << 5) |
-			(FIELD_GET(PRESTERA_DSA_W2_PORT_NUM, words[2]) << 7);
+	words_ptr[0] |=
+	    FIELD_PREP(W0_MASK_DSA_CMD, MVSW_NET_DSA_CMD_FROM_CPU_E);
+
+	trg_hw_dev = from_cpu_ptr->dst_iface.dev_port.hw_dev_num;
+	trg_port = from_cpu_ptr->dst_iface.dev_port.port_num;
+
+	if (trg_hw_dev >= BIT(12))
+		return -EINVAL;
+
+	if (trg_port >= BIT(8) || trg_port >= BIT(10))
+		return -EINVAL;
+
+	words_ptr[0] |= FIELD_PREP(W0_MASK_HW_DEV_NUM, trg_hw_dev);
+	words_ptr[3] |= FIELD_PREP(W3_MASK_HW_DEV_NUM, (trg_hw_dev >> 5));
+
+	if (dsa_info_ptr->common_params.cfi_bit == 1)
+		words_ptr[0] |= FIELD_PREP(W0_MASK_CFI_BIT, 1);
+
+	words_ptr[0] |= FIELD_PREP(W0_MASK_VPT,
+				   dsa_info_ptr->common_params.vpt);
+	words_ptr[0] |= FIELD_PREP(W0_MASK_VID,
+				   dsa_info_ptr->common_params.vid);
+
+	/* set extended bits */
+	words_ptr[0] |= FIELD_PREP(W0_MASK_EXT_BIT, 1);
+	words_ptr[1] |= FIELD_PREP(W1_MASK_EXT_BIT, 1);
+	words_ptr[2] |= FIELD_PREP(W2_MASK_EXT_BIT, 1);
+
+	if (from_cpu_ptr->egr_filter_en)
+		words_ptr[1] |= FIELD_PREP(W1_MASK_EGR_FILTER_EN, 1);
+
+	if (from_cpu_ptr->egr_filter_registered)
+		words_ptr[1] |= FIELD_PREP(W1_MASK_EGR_FILTER_REG, 1);
+
+	/* check src_id & src_hw_dev */
+	if (from_cpu_ptr->src_id >= BIT(12) ||
+	    from_cpu_ptr->src_hw_dev >= BIT(12)) {
+		return -EINVAL;
+	}
+
+	words_ptr[1] |= FIELD_PREP(W1_MASK_SRC_ID, from_cpu_ptr->src_id);
+	words_ptr[1] |= FIELD_PREP(W1_MASK_SRC_DEV, from_cpu_ptr->src_hw_dev);
+
+	words_ptr[2] |= FIELD_PREP(W2_MASK_SRC_ID, from_cpu_ptr->src_id >> 5);
+	words_ptr[2] |= FIELD_PREP(W2_MASK_SRC_DEV,
+				   from_cpu_ptr->src_hw_dev >> 5);
+
+	/* bits 0:9 -- reserved with value 0 */
+	if (from_cpu_ptr->dst_eport >= BIT(17))
+		return -EINVAL;
+
+	words_ptr[3] |= FIELD_PREP(W3_MASK_DST_EPORT, from_cpu_ptr->dst_eport);
+	words_ptr[3] |= FIELD_PREP(W3_MASK_VID,
+				   (dsa_info_ptr->common_params.vid >> 12));
 
 	return 0;
 }
 
-int prestera_dsa_build(const struct prestera_dsa *dsa, u8 *dsa_buf)
+int mvsw_pr_dsa_build(const struct mvsw_pr_dsa *dsa_info_ptr,
+		      u8 *dsa_bytes_ptr)
 {
-	__be32 *dsa_words = (__be32 *)dsa_buf;
-	u32 dev_num = dsa->hw_dev_num;
-	u32 words[4] = { 0 };
+	int rc;
+	u32 words_ptr[4] = { 0 };	/* 4 words of DSA tag */
+	__be32 *dsa_words_ptr = (__be32 *)dsa_bytes_ptr;
 
-	words[0] |= FIELD_PREP(PRESTERA_DSA_W0_CMD, PRESTERA_DSA_CMD_FROM_CPU);
+	if (unlikely(!dsa_info_ptr || !dsa_bytes_ptr))
+		return -EINVAL;
 
-	words[0] |= FIELD_PREP(PRESTERA_DSA_W0_DEV_NUM, dev_num);
-	dev_num = FIELD_GET(PRESTERA_DSA_DEV_NUM, dev_num);
-	words[3] |= FIELD_PREP(PRESTERA_DSA_W3_DEV_NUM, dev_num);
+	if (dsa_info_ptr->common_params.cfi_bit >= BIT(1) ||
+	    dsa_info_ptr->common_params.vpt >= BIT(3)) {
+		return -EINVAL;
+	}
 
-	words[3] |= FIELD_PREP(PRESTERA_DSA_W3_DST_EPORT, dsa->port_num);
+	if (unlikely(dsa_info_ptr->dsa_cmd != MVSW_NET_DSA_CMD_FROM_CPU_E))
+		return -EINVAL;
 
-	words[0] |= FIELD_PREP(PRESTERA_DSA_W0_EXT_BIT, 1);
-	words[1] |= FIELD_PREP(PRESTERA_DSA_W1_EXT_BIT, 1);
-	words[2] |= FIELD_PREP(PRESTERA_DSA_W2_EXT_BIT, 1);
+	/* build form CPU DSA tag */
+	rc = net_if_dsa_tag_from_cpu_build(dsa_info_ptr, words_ptr);
+	if (rc != 0)
+		return rc;
 
-	dsa_words[0] = htonl(words[0]);
-	dsa_words[1] = htonl(words[1]);
-	dsa_words[2] = htonl(words[2]);
-	dsa_words[3] = htonl(words[3]);
+	dsa_words_ptr[0] = htonl(words_ptr[0]);
+	dsa_words_ptr[1] = htonl(words_ptr[1]);
+	dsa_words_ptr[2] = htonl(words_ptr[2]);
+	dsa_words_ptr[3] = htonl(words_ptr[3]);
 
 	return 0;
 }
diff --git a/drivers/net/ethernet/marvell/prestera/prestera_dsa.h b/drivers/net/ethernet/marvell/prestera/prestera_dsa.h
index 67018629b..a118a4859 100644
--- a/drivers/net/ethernet/marvell/prestera/prestera_dsa.h
+++ b/drivers/net/ethernet/marvell/prestera/prestera_dsa.h
@@ -1,35 +1,67 @@
-/* SPDX-License-Identifier: BSD-3-Clause OR GPL-2.0 */
-/* Copyright (c) 2020 Marvell International Ltd. All rights reserved. */
-
-#ifndef __PRESTERA_DSA_H_
-#define __PRESTERA_DSA_H_
+/* SPDX-License-Identifier: BSD-3-Clause OR GPL-2.0
+ *
+ * Copyright (c) 2020 Marvell International Ltd. All rights reserved.
+ *
+ */
+#ifndef _MVSW_PRESTERA_DSA_H_
+#define _MVSW_PRESTERA_DSA_H_
 
 #include <linux/types.h>
 
-#define PRESTERA_DSA_HLEN	16
+#define MVSW_PR_DSA_HLEN	16
 
-enum prestera_dsa_cmd {
+enum mvsw_pr_dsa_cmd {
 	/* DSA command is "To CPU" */
-	PRESTERA_DSA_CMD_TO_CPU = 0,
+	MVSW_NET_DSA_CMD_TO_CPU_E = 0,
 
-	/* DSA command is "From CPU" */
-	PRESTERA_DSA_CMD_FROM_CPU,
+	/* DSA command is "FROM CPU" */
+	MVSW_NET_DSA_CMD_FROM_CPU_E,
 };
 
-struct prestera_dsa_vlan {
-	u16 vid;
+struct mvsw_pr_dsa_common {
+	/* the value vlan priority tag (APPLICABLE RANGES: 0..7) */
 	u8 vpt;
+
+	/* CFI bit of the vlan tag (APPLICABLE RANGES: 0..1) */
 	u8 cfi_bit;
-	bool is_tagged;
+
+	/* Vlan id */
+	u16 vid;
 };
 
-struct prestera_dsa {
-	struct prestera_dsa_vlan vlan;
+struct mvsw_pr_dsa_to_cpu {
+	bool is_tagged;
 	u32 hw_dev_num;
-	u32 port_num;
+	bool src_is_trunk;
+	u8 cpu_code;
+	struct {
+		u16 src_trunk_id;
+		u32 port_num;
+		u32 eport;
+	} iface;
+};
+
+struct mvsw_pr_dsa_from_cpu {
+	struct mvsw_pr_iface dst_iface;	/* vid/port */
+	bool egr_filter_en;
+	bool egr_filter_registered;
+	u32 src_id;
+	u32 src_hw_dev;
+	u32 dst_eport;	/* for port but not for vid */
+};
+
+struct mvsw_pr_dsa {
+	struct mvsw_pr_dsa_common common_params;
+	enum mvsw_pr_dsa_cmd dsa_cmd;
+	union {
+		struct mvsw_pr_dsa_to_cpu to_cpu;
+		struct mvsw_pr_dsa_from_cpu from_cpu;
+	} dsa_info;
 };
 
-int prestera_dsa_parse(struct prestera_dsa *dsa, const u8 *dsa_buf);
-int prestera_dsa_build(const struct prestera_dsa *dsa, u8 *dsa_buf);
+int mvsw_pr_dsa_parse(const u8 *dsa_bytes_ptr,
+		      struct mvsw_pr_dsa *dsa_info_ptr);
+int mvsw_pr_dsa_build(const struct mvsw_pr_dsa *dsa_info_ptr,
+		      u8 *dsa_bytes_ptr);
 
-#endif /* _PRESTERA_DSA_H_ */
+#endif /* _MVSW_PRESTERA_DSA_H_ */
diff --git a/drivers/net/ethernet/marvell/prestera/prestera_ethtool.c b/drivers/net/ethernet/marvell/prestera/prestera_ethtool.c
deleted file mode 100644
index 93a5e2baf..000000000
--- a/drivers/net/ethernet/marvell/prestera/prestera_ethtool.c
+++ /dev/null
@@ -1,780 +0,0 @@
-// SPDX-License-Identifier: BSD-3-Clause OR GPL-2.0
-/* Copyright (c) 2019-2020 Marvell International Ltd. All rights reserved */
-
-#include <linux/ethtool.h>
-#include <linux/kernel.h>
-#include <linux/netdevice.h>
-
-#include "prestera_ethtool.h"
-#include "prestera.h"
-#include "prestera_hw.h"
-
-#define PRESTERA_STATS_CNT \
-	(sizeof(struct prestera_port_stats) / sizeof(u64))
-#define PRESTERA_STATS_IDX(name) \
-	(offsetof(struct prestera_port_stats, name) / sizeof(u64))
-#define PRESTERA_STATS_FIELD(name)	\
-	[PRESTERA_STATS_IDX(name)] = __stringify(name)
-
-static const char driver_kind[] = "prestera";
-
-static const struct prestera_link_mode {
-	enum ethtool_link_mode_bit_indices eth_mode;
-	u32 speed;
-	u64 pr_mask;
-	u8 duplex;
-	u8 port_type;
-} port_link_modes[PRESTERA_LINK_MODE_MAX] = {
-	[PRESTERA_LINK_MODE_10baseT_Half] = {
-		.eth_mode =  ETHTOOL_LINK_MODE_10baseT_Half_BIT,
-		.speed = 10,
-		.pr_mask = 1 << PRESTERA_LINK_MODE_10baseT_Half,
-		.duplex = PRESTERA_PORT_DUPLEX_HALF,
-		.port_type = PRESTERA_PORT_TYPE_TP,
-	},
-	[PRESTERA_LINK_MODE_10baseT_Full] = {
-		.eth_mode =  ETHTOOL_LINK_MODE_10baseT_Full_BIT,
-		.speed = 10,
-		.pr_mask = 1 << PRESTERA_LINK_MODE_10baseT_Full,
-		.duplex = PRESTERA_PORT_DUPLEX_FULL,
-		.port_type = PRESTERA_PORT_TYPE_TP,
-	},
-	[PRESTERA_LINK_MODE_100baseT_Half] = {
-		.eth_mode =  ETHTOOL_LINK_MODE_100baseT_Half_BIT,
-		.speed = 100,
-		.pr_mask = 1 << PRESTERA_LINK_MODE_100baseT_Half,
-		.duplex = PRESTERA_PORT_DUPLEX_HALF,
-		.port_type = PRESTERA_PORT_TYPE_TP,
-	},
-	[PRESTERA_LINK_MODE_100baseT_Full] = {
-		.eth_mode =  ETHTOOL_LINK_MODE_100baseT_Full_BIT,
-		.speed = 100,
-		.pr_mask = 1 << PRESTERA_LINK_MODE_100baseT_Full,
-		.duplex = PRESTERA_PORT_DUPLEX_FULL,
-		.port_type = PRESTERA_PORT_TYPE_TP,
-	},
-	[PRESTERA_LINK_MODE_1000baseT_Half] = {
-		.eth_mode =  ETHTOOL_LINK_MODE_1000baseT_Half_BIT,
-		.speed = 1000,
-		.pr_mask = 1 << PRESTERA_LINK_MODE_1000baseT_Half,
-		.duplex = PRESTERA_PORT_DUPLEX_HALF,
-		.port_type = PRESTERA_PORT_TYPE_TP,
-	},
-	[PRESTERA_LINK_MODE_1000baseT_Full] = {
-		.eth_mode =  ETHTOOL_LINK_MODE_1000baseT_Full_BIT,
-		.speed = 1000,
-		.pr_mask = 1 << PRESTERA_LINK_MODE_1000baseT_Full,
-		.duplex = PRESTERA_PORT_DUPLEX_FULL,
-		.port_type = PRESTERA_PORT_TYPE_TP,
-	},
-	[PRESTERA_LINK_MODE_1000baseX_Full] = {
-		.eth_mode = ETHTOOL_LINK_MODE_1000baseX_Full_BIT,
-		.speed = 1000,
-		.pr_mask = 1 << PRESTERA_LINK_MODE_1000baseX_Full,
-		.duplex = PRESTERA_PORT_DUPLEX_FULL,
-		.port_type = PRESTERA_PORT_TYPE_FIBRE,
-	},
-	[PRESTERA_LINK_MODE_1000baseKX_Full] = {
-		.eth_mode = ETHTOOL_LINK_MODE_1000baseKX_Full_BIT,
-		.speed = 1000,
-		.pr_mask = 1 << PRESTERA_LINK_MODE_1000baseKX_Full,
-		.duplex = PRESTERA_PORT_DUPLEX_FULL,
-		.port_type = PRESTERA_PORT_TYPE_TP,
-	},
-	[PRESTERA_LINK_MODE_2500baseX_Full] = {
-		.eth_mode =  ETHTOOL_LINK_MODE_2500baseX_Full_BIT,
-		.speed = 2500,
-		.pr_mask = 1 << PRESTERA_LINK_MODE_2500baseX_Full,
-		.duplex = PRESTERA_PORT_DUPLEX_FULL,
-	},
-	[PRESTERA_LINK_MODE_10GbaseKR_Full] = {
-		.eth_mode = ETHTOOL_LINK_MODE_10000baseKR_Full_BIT,
-		.speed = 10000,
-		.pr_mask = 1 << PRESTERA_LINK_MODE_10GbaseKR_Full,
-		.duplex = PRESTERA_PORT_DUPLEX_FULL,
-		.port_type = PRESTERA_PORT_TYPE_TP,
-	},
-	[PRESTERA_LINK_MODE_10GbaseSR_Full] = {
-		.eth_mode = ETHTOOL_LINK_MODE_10000baseSR_Full_BIT,
-		.speed = 10000,
-		.pr_mask = 1 << PRESTERA_LINK_MODE_10GbaseSR_Full,
-		.duplex = PRESTERA_PORT_DUPLEX_FULL,
-		.port_type = PRESTERA_PORT_TYPE_FIBRE,
-	},
-	[PRESTERA_LINK_MODE_10GbaseLR_Full] = {
-		.eth_mode = ETHTOOL_LINK_MODE_10000baseLR_Full_BIT,
-		.speed = 10000,
-		.pr_mask = 1 << PRESTERA_LINK_MODE_10GbaseLR_Full,
-		.duplex = PRESTERA_PORT_DUPLEX_FULL,
-		.port_type = PRESTERA_PORT_TYPE_FIBRE,
-	},
-	[PRESTERA_LINK_MODE_20GbaseKR2_Full] = {
-		.eth_mode = ETHTOOL_LINK_MODE_20000baseKR2_Full_BIT,
-		.speed = 20000,
-		.pr_mask = 1 << PRESTERA_LINK_MODE_20GbaseKR2_Full,
-		.duplex = PRESTERA_PORT_DUPLEX_FULL,
-		.port_type = PRESTERA_PORT_TYPE_TP,
-	},
-	[PRESTERA_LINK_MODE_25GbaseCR_Full] = {
-		.eth_mode = ETHTOOL_LINK_MODE_25000baseCR_Full_BIT,
-		.speed = 25000,
-		.pr_mask = 1 << PRESTERA_LINK_MODE_25GbaseCR_Full,
-		.duplex = PRESTERA_PORT_DUPLEX_FULL,
-		.port_type = PRESTERA_PORT_TYPE_DA,
-	},
-	[PRESTERA_LINK_MODE_25GbaseKR_Full] = {
-		.eth_mode = ETHTOOL_LINK_MODE_25000baseKR_Full_BIT,
-		.speed = 25000,
-		.pr_mask = 1 << PRESTERA_LINK_MODE_25GbaseKR_Full,
-		.duplex = PRESTERA_PORT_DUPLEX_FULL,
-		.port_type = PRESTERA_PORT_TYPE_TP,
-	},
-	[PRESTERA_LINK_MODE_25GbaseSR_Full] = {
-		.eth_mode = ETHTOOL_LINK_MODE_25000baseSR_Full_BIT,
-		.speed = 25000,
-		.pr_mask = 1 << PRESTERA_LINK_MODE_25GbaseSR_Full,
-		.duplex = PRESTERA_PORT_DUPLEX_FULL,
-		.port_type = PRESTERA_PORT_TYPE_FIBRE,
-	},
-	[PRESTERA_LINK_MODE_40GbaseKR4_Full] = {
-		.eth_mode = ETHTOOL_LINK_MODE_40000baseKR4_Full_BIT,
-		.speed = 40000,
-		.pr_mask = 1 << PRESTERA_LINK_MODE_40GbaseKR4_Full,
-		.duplex = PRESTERA_PORT_DUPLEX_FULL,
-		.port_type = PRESTERA_PORT_TYPE_TP,
-	},
-	[PRESTERA_LINK_MODE_40GbaseCR4_Full] = {
-		.eth_mode = ETHTOOL_LINK_MODE_40000baseCR4_Full_BIT,
-		.speed = 40000,
-		.pr_mask = 1 << PRESTERA_LINK_MODE_40GbaseCR4_Full,
-		.duplex = PRESTERA_PORT_DUPLEX_FULL,
-		.port_type = PRESTERA_PORT_TYPE_DA,
-	},
-	[PRESTERA_LINK_MODE_40GbaseSR4_Full] = {
-		.eth_mode = ETHTOOL_LINK_MODE_40000baseSR4_Full_BIT,
-		.speed = 40000,
-		.pr_mask = 1 << PRESTERA_LINK_MODE_40GbaseSR4_Full,
-		.duplex = PRESTERA_PORT_DUPLEX_FULL,
-		.port_type = PRESTERA_PORT_TYPE_FIBRE,
-	},
-	[PRESTERA_LINK_MODE_50GbaseCR2_Full] = {
-		.eth_mode = ETHTOOL_LINK_MODE_50000baseCR2_Full_BIT,
-		.speed = 50000,
-		.pr_mask = 1 << PRESTERA_LINK_MODE_50GbaseCR2_Full,
-		.duplex = PRESTERA_PORT_DUPLEX_FULL,
-		.port_type = PRESTERA_PORT_TYPE_DA,
-	},
-	[PRESTERA_LINK_MODE_50GbaseKR2_Full] = {
-		.eth_mode = ETHTOOL_LINK_MODE_50000baseKR2_Full_BIT,
-		.speed = 50000,
-		.pr_mask = 1 << PRESTERA_LINK_MODE_50GbaseKR2_Full,
-		.duplex = PRESTERA_PORT_DUPLEX_FULL,
-		.port_type = PRESTERA_PORT_TYPE_TP,
-	},
-	[PRESTERA_LINK_MODE_50GbaseSR2_Full] = {
-		.eth_mode = ETHTOOL_LINK_MODE_50000baseSR2_Full_BIT,
-		.speed = 50000,
-		.pr_mask = 1 << PRESTERA_LINK_MODE_50GbaseSR2_Full,
-		.duplex = PRESTERA_PORT_DUPLEX_FULL,
-		.port_type = PRESTERA_PORT_TYPE_FIBRE,
-	},
-	[PRESTERA_LINK_MODE_100GbaseKR4_Full] = {
-		.eth_mode = ETHTOOL_LINK_MODE_100000baseKR4_Full_BIT,
-		.speed = 100000,
-		.pr_mask = 1 << PRESTERA_LINK_MODE_100GbaseKR4_Full,
-		.duplex = PRESTERA_PORT_DUPLEX_FULL,
-		.port_type = PRESTERA_PORT_TYPE_TP,
-	},
-	[PRESTERA_LINK_MODE_100GbaseSR4_Full] = {
-		.eth_mode = ETHTOOL_LINK_MODE_100000baseSR4_Full_BIT,
-		.speed = 100000,
-		.pr_mask = 1 << PRESTERA_LINK_MODE_100GbaseSR4_Full,
-		.duplex = PRESTERA_PORT_DUPLEX_FULL,
-		.port_type = PRESTERA_PORT_TYPE_FIBRE,
-	},
-	[PRESTERA_LINK_MODE_100GbaseCR4_Full] = {
-		.eth_mode = ETHTOOL_LINK_MODE_100000baseCR4_Full_BIT,
-		.speed = 100000,
-		.pr_mask = 1 << PRESTERA_LINK_MODE_100GbaseCR4_Full,
-		.duplex = PRESTERA_PORT_DUPLEX_FULL,
-		.port_type = PRESTERA_PORT_TYPE_DA,
-	}
-};
-
-static const struct prestera_fec {
-	u32 eth_fec;
-	enum ethtool_link_mode_bit_indices eth_mode;
-	u8 pr_fec;
-} port_fec_caps[PRESTERA_PORT_FEC_MAX] = {
-	[PRESTERA_PORT_FEC_OFF] = {
-		.eth_fec = ETHTOOL_FEC_OFF,
-		.eth_mode = ETHTOOL_LINK_MODE_FEC_NONE_BIT,
-		.pr_fec = 1 << PRESTERA_PORT_FEC_OFF,
-	},
-	[PRESTERA_PORT_FEC_BASER] = {
-		.eth_fec = ETHTOOL_FEC_BASER,
-		.eth_mode = ETHTOOL_LINK_MODE_FEC_BASER_BIT,
-		.pr_fec = 1 << PRESTERA_PORT_FEC_BASER,
-	},
-	[PRESTERA_PORT_FEC_RS] = {
-		.eth_fec = ETHTOOL_FEC_RS,
-		.eth_mode = ETHTOOL_LINK_MODE_FEC_RS_BIT,
-		.pr_fec = 1 << PRESTERA_PORT_FEC_RS,
-	}
-};
-
-static const struct prestera_port_type {
-	enum ethtool_link_mode_bit_indices eth_mode;
-	u8 eth_type;
-} port_types[PRESTERA_PORT_TYPE_MAX] = {
-	[PRESTERA_PORT_TYPE_NONE] = {
-		.eth_mode = __ETHTOOL_LINK_MODE_MASK_NBITS,
-		.eth_type = PORT_NONE,
-	},
-	[PRESTERA_PORT_TYPE_TP] = {
-		.eth_mode = ETHTOOL_LINK_MODE_TP_BIT,
-		.eth_type = PORT_TP,
-	},
-	[PRESTERA_PORT_TYPE_AUI] = {
-		.eth_mode = ETHTOOL_LINK_MODE_AUI_BIT,
-		.eth_type = PORT_AUI,
-	},
-	[PRESTERA_PORT_TYPE_MII] = {
-		.eth_mode = ETHTOOL_LINK_MODE_MII_BIT,
-		.eth_type = PORT_MII,
-	},
-	[PRESTERA_PORT_TYPE_FIBRE] = {
-		.eth_mode = ETHTOOL_LINK_MODE_FIBRE_BIT,
-		.eth_type = PORT_FIBRE,
-	},
-	[PRESTERA_PORT_TYPE_BNC] = {
-		.eth_mode = ETHTOOL_LINK_MODE_BNC_BIT,
-		.eth_type = PORT_BNC,
-	},
-	[PRESTERA_PORT_TYPE_DA] = {
-		.eth_mode = ETHTOOL_LINK_MODE_TP_BIT,
-		.eth_type = PORT_TP,
-	},
-	[PRESTERA_PORT_TYPE_OTHER] = {
-		.eth_mode = __ETHTOOL_LINK_MODE_MASK_NBITS,
-		.eth_type = PORT_OTHER,
-	}
-};
-
-static const char prestera_cnt_name[PRESTERA_STATS_CNT][ETH_GSTRING_LEN] = {
-	PRESTERA_STATS_FIELD(good_octets_received),
-	PRESTERA_STATS_FIELD(bad_octets_received),
-	PRESTERA_STATS_FIELD(mac_trans_error),
-	PRESTERA_STATS_FIELD(broadcast_frames_received),
-	PRESTERA_STATS_FIELD(multicast_frames_received),
-	PRESTERA_STATS_FIELD(frames_64_octets),
-	PRESTERA_STATS_FIELD(frames_65_to_127_octets),
-	PRESTERA_STATS_FIELD(frames_128_to_255_octets),
-	PRESTERA_STATS_FIELD(frames_256_to_511_octets),
-	PRESTERA_STATS_FIELD(frames_512_to_1023_octets),
-	PRESTERA_STATS_FIELD(frames_1024_to_max_octets),
-	PRESTERA_STATS_FIELD(excessive_collision),
-	PRESTERA_STATS_FIELD(multicast_frames_sent),
-	PRESTERA_STATS_FIELD(broadcast_frames_sent),
-	PRESTERA_STATS_FIELD(fc_sent),
-	PRESTERA_STATS_FIELD(fc_received),
-	PRESTERA_STATS_FIELD(buffer_overrun),
-	PRESTERA_STATS_FIELD(undersize),
-	PRESTERA_STATS_FIELD(fragments),
-	PRESTERA_STATS_FIELD(oversize),
-	PRESTERA_STATS_FIELD(jabber),
-	PRESTERA_STATS_FIELD(rx_error_frame_received),
-	PRESTERA_STATS_FIELD(bad_crc),
-	PRESTERA_STATS_FIELD(collisions),
-	PRESTERA_STATS_FIELD(late_collision),
-	PRESTERA_STATS_FIELD(unicast_frames_received),
-	PRESTERA_STATS_FIELD(unicast_frames_sent),
-	PRESTERA_STATS_FIELD(sent_multiple),
-	PRESTERA_STATS_FIELD(sent_deferred),
-	PRESTERA_STATS_FIELD(good_octets_sent),
-};
-
-static void prestera_ethtool_get_drvinfo(struct net_device *dev,
-					 struct ethtool_drvinfo *drvinfo)
-{
-	struct prestera_port *port = netdev_priv(dev);
-	struct prestera_switch *sw = port->sw;
-
-	strlcpy(drvinfo->driver, driver_kind, sizeof(drvinfo->driver));
-	strlcpy(drvinfo->bus_info, dev_name(prestera_dev(sw)),
-		sizeof(drvinfo->bus_info));
-	snprintf(drvinfo->fw_version, sizeof(drvinfo->fw_version),
-		 "%d.%d.%d",
-		 sw->dev->fw_rev.maj,
-		 sw->dev->fw_rev.min,
-		 sw->dev->fw_rev.sub);
-}
-
-static u8 prestera_port_type_get(struct prestera_port *port)
-{
-	if (port->caps.type < PRESTERA_PORT_TYPE_MAX)
-		return port_types[port->caps.type].eth_type;
-
-	return PORT_OTHER;
-}
-
-static int prestera_port_type_set(const struct ethtool_link_ksettings *ecmd,
-				  struct prestera_port *port)
-{
-	u32 new_mode = PRESTERA_LINK_MODE_MAX;
-	u32 type, mode;
-	int err;
-
-	for (type = 0; type < PRESTERA_PORT_TYPE_MAX; type++) {
-		if (port_types[type].eth_type == ecmd->base.port &&
-		    test_bit(port_types[type].eth_mode,
-			     ecmd->link_modes.supported)) {
-			break;
-		}
-	}
-
-	if (type == port->caps.type)
-		return 0;
-	if (type != port->caps.type && ecmd->base.autoneg == AUTONEG_ENABLE)
-		return -EINVAL;
-	if (type == PRESTERA_PORT_TYPE_MAX)
-		return -EOPNOTSUPP;
-
-	for (mode = 0; mode < PRESTERA_LINK_MODE_MAX; mode++) {
-		if ((port_link_modes[mode].pr_mask &
-		    port->caps.supp_link_modes) &&
-		    type == port_link_modes[mode].port_type) {
-			new_mode = mode;
-		}
-	}
-
-	if (new_mode < PRESTERA_LINK_MODE_MAX)
-		err = prestera_hw_port_link_mode_set(port, new_mode);
-	else
-		err = -EINVAL;
-
-	if (err)
-		return err;
-
-	port->caps.type = type;
-	port->autoneg = false;
-
-	return 0;
-}
-
-static void prestera_modes_to_eth(unsigned long *eth_modes, u64 link_modes,
-				  u8 fec, u8 type)
-{
-	u32 mode;
-
-	for (mode = 0; mode < PRESTERA_LINK_MODE_MAX; mode++) {
-		if ((port_link_modes[mode].pr_mask & link_modes) == 0)
-			continue;
-
-		if (type != PRESTERA_PORT_TYPE_NONE &&
-		    port_link_modes[mode].port_type != type)
-			continue;
-
-		__set_bit(port_link_modes[mode].eth_mode, eth_modes);
-	}
-
-	for (mode = 0; mode < PRESTERA_PORT_FEC_MAX; mode++) {
-		if ((port_fec_caps[mode].pr_fec & fec) == 0)
-			continue;
-
-		__set_bit(port_fec_caps[mode].eth_mode, eth_modes);
-	}
-}
-
-static void prestera_modes_from_eth(const unsigned long *eth_modes,
-				    u64 *link_modes, u8 *fec, u8 type)
-{
-	u64 adver_modes = 0;
-	u32 fec_modes = 0;
-	u32 mode;
-
-	for (mode = 0; mode < PRESTERA_LINK_MODE_MAX; mode++) {
-		if (!test_bit(port_link_modes[mode].eth_mode, eth_modes))
-			continue;
-
-		if (port_link_modes[mode].port_type != type)
-			continue;
-
-		adver_modes |= port_link_modes[mode].pr_mask;
-	}
-
-	for (mode = 0; mode < PRESTERA_PORT_FEC_MAX; mode++) {
-		if (!test_bit(port_fec_caps[mode].eth_mode, eth_modes))
-			continue;
-
-		fec_modes |= port_fec_caps[mode].pr_fec;
-	}
-
-	*link_modes = adver_modes;
-	*fec = fec_modes;
-}
-
-static void prestera_port_supp_types_get(struct ethtool_link_ksettings *ecmd,
-					 struct prestera_port *port)
-{
-	u32 mode;
-	u8 ptype;
-
-	for (mode = 0; mode < PRESTERA_LINK_MODE_MAX; mode++) {
-		if ((port_link_modes[mode].pr_mask &
-		    port->caps.supp_link_modes) == 0)
-			continue;
-
-		ptype = port_link_modes[mode].port_type;
-		__set_bit(port_types[ptype].eth_mode,
-			  ecmd->link_modes.supported);
-	}
-}
-
-static void prestera_port_remote_cap_get(struct ethtool_link_ksettings *ecmd,
-					 struct prestera_port *port)
-{
-	bool asym_pause;
-	bool pause;
-	u64 bitmap;
-	int err;
-
-	err = prestera_hw_port_remote_cap_get(port, &bitmap);
-	if (!err) {
-		prestera_modes_to_eth(ecmd->link_modes.lp_advertising,
-				      bitmap, 0, PRESTERA_PORT_TYPE_NONE);
-
-		if (!bitmap_empty(ecmd->link_modes.lp_advertising,
-				  __ETHTOOL_LINK_MODE_MASK_NBITS)) {
-			ethtool_link_ksettings_add_link_mode(ecmd,
-							     lp_advertising,
-							     Autoneg);
-		}
-	}
-
-	err = prestera_hw_port_remote_fc_get(port, &pause, &asym_pause);
-	if (err)
-		return;
-
-	if (pause)
-		ethtool_link_ksettings_add_link_mode(ecmd,
-						     lp_advertising,
-						     Pause);
-	if (asym_pause)
-		ethtool_link_ksettings_add_link_mode(ecmd,
-						     lp_advertising,
-						     Asym_Pause);
-}
-
-static void prestera_port_speed_get(struct ethtool_link_ksettings *ecmd,
-				    struct prestera_port *port)
-{
-	u32 speed;
-	int err;
-
-	err = prestera_hw_port_speed_get(port, &speed);
-	ecmd->base.speed = err ? SPEED_UNKNOWN : speed;
-}
-
-static void prestera_port_duplex_get(struct ethtool_link_ksettings *ecmd,
-				     struct prestera_port *port)
-{
-	u8 duplex;
-	int err;
-
-	err = prestera_hw_port_duplex_get(port, &duplex);
-	if (err) {
-		ecmd->base.duplex = DUPLEX_UNKNOWN;
-		return;
-	}
-
-	ecmd->base.duplex = duplex == PRESTERA_PORT_DUPLEX_FULL ?
-			    DUPLEX_FULL : DUPLEX_HALF;
-}
-
-static int
-prestera_ethtool_get_link_ksettings(struct net_device *dev,
-				    struct ethtool_link_ksettings *ecmd)
-{
-	struct prestera_port *port = netdev_priv(dev);
-
-	ethtool_link_ksettings_zero_link_mode(ecmd, supported);
-	ethtool_link_ksettings_zero_link_mode(ecmd, advertising);
-	ethtool_link_ksettings_zero_link_mode(ecmd, lp_advertising);
-
-	ecmd->base.autoneg = port->autoneg ? AUTONEG_ENABLE : AUTONEG_DISABLE;
-
-	if (port->caps.type == PRESTERA_PORT_TYPE_TP) {
-		ethtool_link_ksettings_add_link_mode(ecmd, supported, Autoneg);
-
-		if (netif_running(dev) &&
-		    (port->autoneg ||
-		     port->caps.transceiver == PRESTERA_PORT_TCVR_COPPER))
-			ethtool_link_ksettings_add_link_mode(ecmd, advertising,
-							     Autoneg);
-	}
-
-	prestera_modes_to_eth(ecmd->link_modes.supported,
-			      port->caps.supp_link_modes,
-			      port->caps.supp_fec,
-			      port->caps.type);
-
-	prestera_port_supp_types_get(ecmd, port);
-
-	if (netif_carrier_ok(dev)) {
-		prestera_port_speed_get(ecmd, port);
-		prestera_port_duplex_get(ecmd, port);
-	} else {
-		ecmd->base.speed = SPEED_UNKNOWN;
-		ecmd->base.duplex = DUPLEX_UNKNOWN;
-	}
-
-	ecmd->base.port = prestera_port_type_get(port);
-
-	if (port->autoneg) {
-		if (netif_running(dev))
-			prestera_modes_to_eth(ecmd->link_modes.advertising,
-					      port->adver_link_modes,
-					      port->adver_fec,
-					      port->caps.type);
-
-		if (netif_carrier_ok(dev) &&
-		    port->caps.transceiver == PRESTERA_PORT_TCVR_COPPER)
-			prestera_port_remote_cap_get(ecmd, port);
-	}
-
-	if (port->caps.type == PRESTERA_PORT_TYPE_TP &&
-	    port->caps.transceiver == PRESTERA_PORT_TCVR_COPPER)
-		prestera_hw_port_mdix_get(port, &ecmd->base.eth_tp_mdix,
-					  &ecmd->base.eth_tp_mdix_ctrl);
-
-	return 0;
-}
-
-static int prestera_port_mdix_set(const struct ethtool_link_ksettings *ecmd,
-				  struct prestera_port *port)
-{
-	if (ecmd->base.eth_tp_mdix_ctrl != ETH_TP_MDI_INVALID &&
-	    port->caps.transceiver == PRESTERA_PORT_TCVR_COPPER &&
-	    port->caps.type == PRESTERA_PORT_TYPE_TP)
-		return prestera_hw_port_mdix_set(port,
-						 ecmd->base.eth_tp_mdix_ctrl);
-
-	return 0;
-}
-
-static int prestera_port_link_mode_set(struct prestera_port *port,
-				       u32 speed, u8 duplex, u8 type)
-{
-	u32 new_mode = PRESTERA_LINK_MODE_MAX;
-	u32 mode;
-
-	for (mode = 0; mode < PRESTERA_LINK_MODE_MAX; mode++) {
-		if (speed != port_link_modes[mode].speed)
-			continue;
-
-		if (duplex != port_link_modes[mode].duplex)
-			continue;
-
-		if (!(port_link_modes[mode].pr_mask &
-		    port->caps.supp_link_modes))
-			continue;
-
-		if (type != port_link_modes[mode].port_type)
-			continue;
-
-		new_mode = mode;
-		break;
-	}
-
-	if (new_mode == PRESTERA_LINK_MODE_MAX)
-		return -EOPNOTSUPP;
-
-	return prestera_hw_port_link_mode_set(port, new_mode);
-}
-
-static int
-prestera_port_speed_duplex_set(const struct ethtool_link_ksettings *ecmd,
-			       struct prestera_port *port)
-{
-	u32 curr_mode;
-	u8 duplex;
-	u32 speed;
-	int err;
-
-	err = prestera_hw_port_link_mode_get(port, &curr_mode);
-	if (err)
-		return err;
-	if (curr_mode >= PRESTERA_LINK_MODE_MAX)
-		return -EINVAL;
-
-	if (ecmd->base.duplex != DUPLEX_UNKNOWN)
-		duplex = ecmd->base.duplex == DUPLEX_FULL ?
-			 PRESTERA_PORT_DUPLEX_FULL : PRESTERA_PORT_DUPLEX_HALF;
-	else
-		duplex = port_link_modes[curr_mode].duplex;
-
-	if (ecmd->base.speed != SPEED_UNKNOWN)
-		speed = ecmd->base.speed;
-	else
-		speed = port_link_modes[curr_mode].speed;
-
-	return prestera_port_link_mode_set(port, speed, duplex,
-					   port->caps.type);
-}
-
-static int
-prestera_ethtool_set_link_ksettings(struct net_device *dev,
-				    const struct ethtool_link_ksettings *ecmd)
-{
-	struct prestera_port *port = netdev_priv(dev);
-	u64 adver_modes;
-	u8 adver_fec;
-	int err;
-
-	err = prestera_port_type_set(ecmd, port);
-	if (err)
-		return err;
-
-	if (port->caps.transceiver == PRESTERA_PORT_TCVR_COPPER) {
-		err = prestera_port_mdix_set(ecmd, port);
-		if (err)
-			return err;
-	}
-
-	prestera_modes_from_eth(ecmd->link_modes.advertising, &adver_modes,
-				&adver_fec, port->caps.type);
-
-	err = prestera_port_autoneg_set(port,
-					ecmd->base.autoneg == AUTONEG_ENABLE,
-					adver_modes, adver_fec);
-	if (err)
-		return err;
-
-	if (ecmd->base.autoneg == AUTONEG_DISABLE) {
-		err = prestera_port_speed_duplex_set(ecmd, port);
-		if (err)
-			return err;
-	}
-
-	return 0;
-}
-
-static int prestera_ethtool_get_fecparam(struct net_device *dev,
-					 struct ethtool_fecparam *fecparam)
-{
-	struct prestera_port *port = netdev_priv(dev);
-	u8 active;
-	u32 mode;
-	int err;
-
-	err = prestera_hw_port_fec_get(port, &active);
-	if (err)
-		return err;
-
-	fecparam->fec = 0;
-
-	for (mode = 0; mode < PRESTERA_PORT_FEC_MAX; mode++) {
-		if ((port_fec_caps[mode].pr_fec & port->caps.supp_fec) == 0)
-			continue;
-
-		fecparam->fec |= port_fec_caps[mode].eth_fec;
-	}
-
-	if (active < PRESTERA_PORT_FEC_MAX)
-		fecparam->active_fec = port_fec_caps[active].eth_fec;
-	else
-		fecparam->active_fec = ETHTOOL_FEC_AUTO;
-
-	return 0;
-}
-
-static int prestera_ethtool_set_fecparam(struct net_device *dev,
-					 struct ethtool_fecparam *fecparam)
-{
-	struct prestera_port *port = netdev_priv(dev);
-	u8 fec, active;
-	u32 mode;
-	int err;
-
-	if (port->autoneg) {
-		netdev_err(dev, "FEC set is not allowed while autoneg is on\n");
-		return -EINVAL;
-	}
-
-	err = prestera_hw_port_fec_get(port, &active);
-	if (err)
-		return err;
-
-	fec = PRESTERA_PORT_FEC_MAX;
-	for (mode = 0; mode < PRESTERA_PORT_FEC_MAX; mode++) {
-		if ((port_fec_caps[mode].eth_fec & fecparam->fec) &&
-		    (port_fec_caps[mode].pr_fec & port->caps.supp_fec)) {
-			fec = mode;
-			break;
-		}
-	}
-
-	if (fec == active)
-		return 0;
-
-	if (fec == PRESTERA_PORT_FEC_MAX)
-		return -EOPNOTSUPP;
-
-	return prestera_hw_port_fec_set(port, fec);
-}
-
-static int prestera_ethtool_get_sset_count(struct net_device *dev, int sset)
-{
-	switch (sset) {
-	case ETH_SS_STATS:
-		return PRESTERA_STATS_CNT;
-	default:
-		return -EOPNOTSUPP;
-	}
-}
-
-static void prestera_ethtool_get_strings(struct net_device *dev,
-					 u32 stringset, u8 *data)
-{
-	if (stringset != ETH_SS_STATS)
-		return;
-
-	memcpy(data, prestera_cnt_name, sizeof(prestera_cnt_name));
-}
-
-static void prestera_ethtool_get_stats(struct net_device *dev,
-				       struct ethtool_stats *stats, u64 *data)
-{
-	struct prestera_port *port = netdev_priv(dev);
-	struct prestera_port_stats *port_stats;
-
-	port_stats = &port->cached_hw_stats.stats;
-
-	memcpy(data, port_stats, sizeof(*port_stats));
-}
-
-static int prestera_ethtool_nway_reset(struct net_device *dev)
-{
-	struct prestera_port *port = netdev_priv(dev);
-
-	if (netif_running(dev) &&
-	    port->caps.transceiver == PRESTERA_PORT_TCVR_COPPER &&
-	    port->caps.type == PRESTERA_PORT_TYPE_TP)
-		return prestera_hw_port_autoneg_restart(port);
-
-	return -EINVAL;
-}
-
-const struct ethtool_ops prestera_ethtool_ops = {
-	.get_drvinfo = prestera_ethtool_get_drvinfo,
-	.get_link_ksettings = prestera_ethtool_get_link_ksettings,
-	.set_link_ksettings = prestera_ethtool_set_link_ksettings,
-	.get_fecparam = prestera_ethtool_get_fecparam,
-	.set_fecparam = prestera_ethtool_set_fecparam,
-	.get_sset_count = prestera_ethtool_get_sset_count,
-	.get_strings = prestera_ethtool_get_strings,
-	.get_ethtool_stats = prestera_ethtool_get_stats,
-	.get_link = ethtool_op_get_link,
-	.nway_reset = prestera_ethtool_nway_reset
-};
diff --git a/drivers/net/ethernet/marvell/prestera/prestera_ethtool.h b/drivers/net/ethernet/marvell/prestera/prestera_ethtool.h
deleted file mode 100644
index 523ef1f59..000000000
--- a/drivers/net/ethernet/marvell/prestera/prestera_ethtool.h
+++ /dev/null
@@ -1,11 +0,0 @@
-/* SPDX-License-Identifier: BSD-3-Clause OR GPL-2.0 */
-/* Copyright (c) 2019-2020 Marvell International Ltd. All rights reserved. */
-
-#ifndef __PRESTERA_ETHTOOL_H_
-#define __PRESTERA_ETHTOOL_H_
-
-#include <linux/ethtool.h>
-
-extern const struct ethtool_ops prestera_ethtool_ops;
-
-#endif /* _PRESTERA_ETHTOOL_H_ */
diff --git a/drivers/net/ethernet/marvell/prestera/prestera_flower.c b/drivers/net/ethernet/marvell/prestera/prestera_flower.c
new file mode 100644
index 000000000..4070def64
--- /dev/null
+++ b/drivers/net/ethernet/marvell/prestera/prestera_flower.c
@@ -0,0 +1,430 @@
+// SPDX-License-Identifier: BSD-3-Clause OR GPL-2.0
+//
+// Copyright (c) 2020 Marvell International Ltd. All rights reserved.
+//
+
+#include "prestera.h"
+#include "prestera_hw.h"
+
+#define PRESTERA_DEFAULT_TC_NUM	8
+
+static int mvsw_pr_flower_parse_actions(struct mvsw_pr_switch *sw,
+					struct prestera_acl_block *block,
+					struct prestera_acl_rule *rule,
+					struct flow_action *flow_action,
+					struct netlink_ext_ack *extack)
+{
+	const struct flow_action_entry *act;
+	struct prestera_acl_rule_action_entry *a_entry;
+	int i;
+
+	if (!flow_action_has_entries(flow_action))
+		return 0;
+
+	flow_action_for_each(i, act, flow_action) {
+		/* allocate action entry */
+		a_entry = kmalloc(sizeof(*a_entry), GFP_KERNEL);
+		if (!a_entry)
+			return -ENOMEM;
+
+		switch (act->id) {
+		case FLOW_ACTION_ACCEPT:
+			a_entry->id = MVSW_ACL_RULE_ACTION_ACCEPT;
+			prestera_acl_rule_action_add(rule, a_entry);
+			break;
+		case FLOW_ACTION_DROP:
+			a_entry->id = MVSW_ACL_RULE_ACTION_DROP;
+			prestera_acl_rule_action_add(rule, a_entry);
+			break;
+		case FLOW_ACTION_TRAP:
+			a_entry->id = MVSW_ACL_RULE_ACTION_TRAP;
+			prestera_acl_rule_action_add(rule, a_entry);
+			break;
+		case FLOW_ACTION_POLICE:
+			a_entry->id = MVSW_ACL_RULE_ACTION_POLICE;
+			a_entry->police.rate = act->police.rate_bytes_ps;
+			a_entry->police.burst =
+				div_u64(a_entry->police.rate *
+					PSCHED_NS2TICKS(act->police.burst),
+					PSCHED_TICKS_PER_SEC);
+			prestera_acl_rule_action_add(rule, a_entry);
+			break;
+		default:
+			kfree(a_entry);
+			NL_SET_ERR_MSG_MOD(extack, "Unsupported action");
+			pr_err("Unsupported action\n");
+			return -EOPNOTSUPP;
+		}
+	}
+
+	return 0;
+}
+
+static int mvsw_pr_flower_parse_meta(struct prestera_acl_rule *rule,
+				     struct flow_cls_offload *f,
+				     struct prestera_acl_block *block)
+{
+	struct flow_rule *f_rule = flow_cls_offload_flow_rule(f);
+	struct prestera_acl_rule_match_entry *m_entry;
+	struct mvsw_pr_port *port;
+	struct net_device *ingress_dev;
+	struct flow_match_meta match;
+
+	flow_rule_match_meta(f_rule, &match);
+	if (match.mask->ingress_ifindex != 0xFFFFFFFF) {
+		NL_SET_ERR_MSG_MOD(f->common.extack,
+				   "Unsupported ingress ifindex mask");
+		return -EINVAL;
+	}
+
+	ingress_dev = __dev_get_by_index(prestera_acl_block_net(block),
+					 match.key->ingress_ifindex);
+	if (!ingress_dev) {
+		NL_SET_ERR_MSG_MOD(f->common.extack,
+				   "Can't find specified ingress port to match on");
+		return -EINVAL;
+	}
+
+	if (!mvsw_pr_netdev_check(ingress_dev)) {
+		NL_SET_ERR_MSG_MOD(f->common.extack,
+				   "Can't match on switchdev ingress port");
+		return -EINVAL;
+	}
+	port = netdev_priv(ingress_dev);
+
+	/* add port key,mask */
+	m_entry = kmalloc(sizeof(*m_entry), GFP_KERNEL);
+	if (!m_entry)
+		return -ENOMEM;
+
+	m_entry->type = MVSW_ACL_RULE_MATCH_ENTRY_TYPE_PORT;
+	m_entry->keymask.u64.key = port->hw_id | ((u64)port->dev_id << 32);
+	m_entry->keymask.u64.mask = ~(u64)0;
+	prestera_acl_rule_match_add(rule, m_entry);
+
+	return 0;
+}
+
+static int mvsw_pr_flower_parse(struct mvsw_pr_switch *sw,
+				struct prestera_acl_block *block,
+				struct prestera_acl_rule *rule,
+				struct flow_cls_offload *f)
+{
+	struct flow_rule *f_rule = flow_cls_offload_flow_rule(f);
+	struct flow_dissector *dissector = f_rule->match.dissector;
+	struct prestera_acl_rule_match_entry *m_entry;
+	u16 n_proto_mask = 0;
+	u16 n_proto_key = 0;
+	u16 addr_type = 0;
+	u8 ip_proto = 0;
+	u32 hwtc = 0;
+	int err;
+
+	if (dissector->used_keys &
+	    ~(BIT(FLOW_DISSECTOR_KEY_META) |
+	      BIT(FLOW_DISSECTOR_KEY_CONTROL) |
+	      BIT(FLOW_DISSECTOR_KEY_BASIC) |
+	      BIT(FLOW_DISSECTOR_KEY_ETH_ADDRS) |
+	      BIT(FLOW_DISSECTOR_KEY_IPV4_ADDRS) |
+	      BIT(FLOW_DISSECTOR_KEY_IPV6_ADDRS) |
+	      BIT(FLOW_DISSECTOR_KEY_ICMP) |
+	      BIT(FLOW_DISSECTOR_KEY_PORTS) |
+	      BIT(FLOW_DISSECTOR_KEY_PORTS_RANGE) |
+	      BIT(FLOW_DISSECTOR_KEY_VLAN))) {
+		NL_SET_ERR_MSG_MOD(f->common.extack, "Unsupported key");
+		return -EOPNOTSUPP;
+	}
+
+	if (f->classid) {
+		/* The classid values of TC_H_MIN_PRIORITY through
+		 * TC_H_MIN_PRIORITY + PRESTERA_DEFAULT_TC_NUM - 1 represents
+		 * the hardware traffic classes.
+		 */
+		hwtc = TC_H_MIN(f->classid) - TC_H_MIN_PRIORITY;
+		if (hwtc >= PRESTERA_DEFAULT_TC_NUM) {
+			NL_SET_ERR_MSG_MOD(f->common.extack,
+					   "Unsupported HW TC");
+			return -EINVAL;
+		}
+		prestera_acl_rule_hw_tc_set(rule, hwtc);
+	}
+
+	prestera_acl_rule_priority_set(rule, f->common.prio);
+
+	if (flow_rule_match_key(f_rule, FLOW_DISSECTOR_KEY_META)) {
+		err = mvsw_pr_flower_parse_meta(rule, f, block);
+		if (err)
+			return err;
+	}
+
+	if (flow_rule_match_key(f_rule, FLOW_DISSECTOR_KEY_CONTROL)) {
+		struct flow_match_control match;
+
+		flow_rule_match_control(f_rule, &match);
+		addr_type = match.key->addr_type;
+	}
+
+	if (flow_rule_match_key(f_rule, FLOW_DISSECTOR_KEY_BASIC)) {
+		struct flow_match_basic match;
+
+		flow_rule_match_basic(f_rule, &match);
+		n_proto_key = ntohs(match.key->n_proto);
+		n_proto_mask = ntohs(match.mask->n_proto);
+
+		if (n_proto_key == ETH_P_ALL) {
+			n_proto_key = 0;
+			n_proto_mask = 0;
+		}
+
+		/* add eth type key,mask */
+		m_entry = kmalloc(sizeof(*m_entry), GFP_KERNEL);
+		if (!m_entry)
+			return -ENOMEM;
+
+		m_entry->type = MVSW_ACL_RULE_MATCH_ENTRY_TYPE_ETH_TYPE;
+		m_entry->keymask.u16.key = n_proto_key;
+		m_entry->keymask.u16.mask = n_proto_mask;
+		prestera_acl_rule_match_add(rule, m_entry);
+
+		/* add ip proto key,mask */
+		m_entry = kmalloc(sizeof(*m_entry), GFP_KERNEL);
+		if (!m_entry)
+			return -ENOMEM;
+
+		m_entry->type = MVSW_ACL_RULE_MATCH_ENTRY_TYPE_IP_PROTO;
+		m_entry->keymask.u8.key = match.key->ip_proto;
+		m_entry->keymask.u8.mask = match.mask->ip_proto;
+		prestera_acl_rule_match_add(rule, m_entry);
+		ip_proto = match.key->ip_proto;
+	}
+
+	if (flow_rule_match_key(f_rule, FLOW_DISSECTOR_KEY_ETH_ADDRS)) {
+		struct flow_match_eth_addrs match;
+
+		flow_rule_match_eth_addrs(f_rule, &match);
+
+		/* add ethernet dst key,mask */
+		m_entry = kmalloc(sizeof(*m_entry), GFP_KERNEL);
+		if (!m_entry)
+			return -ENOMEM;
+
+		m_entry->type = MVSW_ACL_RULE_MATCH_ENTRY_TYPE_ETH_DMAC;
+		memcpy(&m_entry->keymask.mac.key,
+		       &match.key->dst, sizeof(match.key->dst));
+		memcpy(&m_entry->keymask.mac.mask,
+		       &match.mask->dst, sizeof(match.mask->dst));
+		prestera_acl_rule_match_add(rule, m_entry);
+
+		/* add ethernet src key,mask */
+		m_entry = kmalloc(sizeof(*m_entry), GFP_KERNEL);
+		if (!m_entry)
+			return -ENOMEM;
+
+		m_entry->type = MVSW_ACL_RULE_MATCH_ENTRY_TYPE_ETH_SMAC;
+		memcpy(&m_entry->keymask.mac.key,
+		       &match.key->src, sizeof(match.key->src));
+		memcpy(&m_entry->keymask.mac.mask,
+		       &match.mask->src, sizeof(match.mask->src));
+		prestera_acl_rule_match_add(rule, m_entry);
+	}
+
+	if (addr_type == FLOW_DISSECTOR_KEY_IPV4_ADDRS) {
+		struct flow_match_ipv4_addrs match;
+
+		flow_rule_match_ipv4_addrs(f_rule, &match);
+
+		m_entry = kmalloc(sizeof(*m_entry), GFP_KERNEL);
+		if (!m_entry)
+			return -ENOMEM;
+
+		m_entry->type = MVSW_ACL_RULE_MATCH_ENTRY_TYPE_IP_SRC;
+		memcpy(&m_entry->keymask.u32.key,
+		       &match.key->src, sizeof(match.key->src));
+		memcpy(&m_entry->keymask.u32.mask,
+		       &match.mask->src, sizeof(match.mask->src));
+		prestera_acl_rule_match_add(rule, m_entry);
+
+		m_entry = kmalloc(sizeof(*m_entry), GFP_KERNEL);
+		if (!m_entry)
+			return -ENOMEM;
+
+		m_entry->type = MVSW_ACL_RULE_MATCH_ENTRY_TYPE_IP_DST;
+		memcpy(&m_entry->keymask.u32.key,
+		       &match.key->dst, sizeof(match.key->dst));
+		memcpy(&m_entry->keymask.u32.mask,
+		       &match.mask->dst, sizeof(match.mask->dst));
+		prestera_acl_rule_match_add(rule, m_entry);
+	}
+
+	if (flow_rule_match_key(f_rule, FLOW_DISSECTOR_KEY_PORTS)) {
+		struct flow_match_ports match;
+
+		if (ip_proto != IPPROTO_TCP && ip_proto != IPPROTO_UDP) {
+			NL_SET_ERR_MSG_MOD
+			    (f->common.extack,
+			     "Only UDP and TCP keys are supported");
+			return -EINVAL;
+		}
+
+		flow_rule_match_ports(f_rule, &match);
+
+		m_entry = kmalloc(sizeof(*m_entry), GFP_KERNEL);
+		if (!m_entry)
+			return -ENOMEM;
+		m_entry->type = MVSW_ACL_RULE_MATCH_ENTRY_TYPE_L4_PORT_SRC;
+		m_entry->keymask.u16.key = ntohs(match.key->src);
+		m_entry->keymask.u16.mask = ntohs(match.mask->src);
+		prestera_acl_rule_match_add(rule, m_entry);
+
+		m_entry = kmalloc(sizeof(*m_entry), GFP_KERNEL);
+		if (!m_entry)
+			return -ENOMEM;
+		m_entry->type = MVSW_ACL_RULE_MATCH_ENTRY_TYPE_L4_PORT_DST;
+		m_entry->keymask.u16.key = ntohs(match.key->dst);
+		m_entry->keymask.u16.mask = ntohs(match.mask->dst);
+		prestera_acl_rule_match_add(rule, m_entry);
+	}
+
+	if (flow_rule_match_key(f_rule, FLOW_DISSECTOR_KEY_PORTS_RANGE)) {
+		struct flow_match_ports_range match;
+
+		flow_rule_match_ports_range(f_rule, &match);
+
+		m_entry = kmalloc(sizeof(*m_entry), GFP_KERNEL);
+		if (!m_entry)
+			return -ENOMEM;
+		m_entry->type =
+			MVSW_ACL_RULE_MATCH_ENTRY_TYPE_L4_PORT_RANGE_SRC;
+		m_entry->keymask.u32.key = ntohs(match.key->tp_min.src) |
+				(u32)ntohs(match.key->tp_max.src) << 16;
+		m_entry->keymask.u32.mask = ntohs(match.mask->tp_min.src) |
+				(u32)ntohs(match.mask->tp_max.src) << 16;
+		prestera_acl_rule_match_add(rule, m_entry);
+
+		m_entry = kmalloc(sizeof(*m_entry), GFP_KERNEL);
+		if (!m_entry)
+			return -ENOMEM;
+		m_entry->type =
+			MVSW_ACL_RULE_MATCH_ENTRY_TYPE_L4_PORT_RANGE_DST;
+		m_entry->keymask.u32.key = ntohs(match.key->tp_min.dst) |
+				(u32)ntohs(match.key->tp_max.dst) << 16;
+		m_entry->keymask.u32.mask = ntohs(match.mask->tp_min.dst) |
+				(u32)ntohs(match.mask->tp_max.dst) << 16;
+		prestera_acl_rule_match_add(rule, m_entry);
+	}
+
+	if (flow_rule_match_key(f_rule, FLOW_DISSECTOR_KEY_VLAN)) {
+		struct flow_match_vlan match;
+
+		flow_rule_match_vlan(f_rule, &match);
+
+		if (match.mask->vlan_id != 0) {
+			m_entry = kmalloc(sizeof(*m_entry), GFP_KERNEL);
+			if (!m_entry)
+				return -ENOMEM;
+			m_entry->type = MVSW_ACL_RULE_MATCH_ENTRY_TYPE_VLAN_ID;
+			m_entry->keymask.u16.key = match.key->vlan_id;
+			m_entry->keymask.u16.mask = match.mask->vlan_id;
+			prestera_acl_rule_match_add(rule, m_entry);
+		}
+
+		m_entry = kmalloc(sizeof(*m_entry), GFP_KERNEL);
+		if (!m_entry)
+			return -ENOMEM;
+		m_entry->type = MVSW_ACL_RULE_MATCH_ENTRY_TYPE_VLAN_TPID;
+		m_entry->keymask.u16.key = ntohs(match.key->vlan_tpid);
+		m_entry->keymask.u16.mask = ntohs(match.mask->vlan_tpid);
+		prestera_acl_rule_match_add(rule, m_entry);
+	}
+
+	if (flow_rule_match_key(f_rule, FLOW_DISSECTOR_KEY_ICMP)) {
+		struct flow_match_icmp match;
+
+		flow_rule_match_icmp(f_rule, &match);
+
+		m_entry = kmalloc(sizeof(*m_entry), GFP_KERNEL);
+		if (!m_entry)
+			return -ENOMEM;
+		m_entry->type = MVSW_ACL_RULE_MATCH_ENTRY_TYPE_ICMP_TYPE;
+		m_entry->keymask.u8.key = match.key->type;
+		m_entry->keymask.u8.mask = match.mask->type;
+		prestera_acl_rule_match_add(rule, m_entry);
+
+		m_entry = kmalloc(sizeof(*m_entry), GFP_KERNEL);
+		if (!m_entry)
+			return -ENOMEM;
+		m_entry->type = MVSW_ACL_RULE_MATCH_ENTRY_TYPE_ICMP_CODE;
+		m_entry->keymask.u8.key = match.key->code;
+		m_entry->keymask.u8.mask = match.mask->code;
+		prestera_acl_rule_match_add(rule, m_entry);
+	}
+
+	return mvsw_pr_flower_parse_actions(sw, block, rule,
+					    &f->rule->action,
+					    f->common.extack);
+}
+
+int mvsw_pr_flower_replace(struct mvsw_pr_switch *sw,
+			   struct prestera_acl_block *block,
+			   struct flow_cls_offload *f)
+{
+	struct prestera_acl_rule *rule;
+	int err;
+
+	rule = prestera_acl_rule_create(block, f->cookie);
+	if (IS_ERR(rule))
+		return PTR_ERR(rule);
+
+	err = mvsw_pr_flower_parse(sw, block, rule, f);
+	if (err)
+		goto err_flower_parse;
+
+	err = prestera_acl_rule_add(sw, rule);
+	if (err)
+		goto err_rule_add;
+
+	return 0;
+
+err_rule_add:
+err_flower_parse:
+	prestera_acl_rule_destroy(rule);
+	return err;
+}
+
+void mvsw_pr_flower_destroy(struct mvsw_pr_switch *sw,
+			    struct prestera_acl_block *block,
+			    struct flow_cls_offload *f)
+{
+	struct prestera_acl_rule *rule;
+
+	rule = prestera_acl_rule_lookup(prestera_acl_block_ruleset_get(block),
+					f->cookie);
+	if (rule) {
+		prestera_acl_rule_del(sw, rule);
+		prestera_acl_rule_destroy(rule);
+	}
+}
+
+int mvsw_pr_flower_stats(struct mvsw_pr_switch *sw,
+			 struct prestera_acl_block *block,
+			 struct flow_cls_offload *f)
+{
+	struct prestera_acl_rule *rule;
+	u64 packets;
+	u64 lastuse;
+	u64 bytes;
+	int err;
+
+	rule = prestera_acl_rule_lookup(prestera_acl_block_ruleset_get(block),
+					f->cookie);
+	if (!rule)
+		return -EINVAL;
+
+	err = prestera_acl_rule_get_stats(sw, rule, &packets, &bytes, &lastuse);
+	if (err)
+		return err;
+
+	flow_stats_update(&f->stats, bytes, packets, 0, lastuse,
+			  FLOW_ACTION_HW_STATS_IMMEDIATE);
+	return 0;
+}
diff --git a/drivers/net/ethernet/marvell/prestera/prestera_fw_log.c b/drivers/net/ethernet/marvell/prestera/prestera_fw_log.c
new file mode 100644
index 000000000..e1c9f6ae3
--- /dev/null
+++ b/drivers/net/ethernet/marvell/prestera/prestera_fw_log.c
@@ -0,0 +1,422 @@
+/* SPDX-License-Identifier: BSD-3-Clause OR GPL-2.0
+ *
+ * Copyright (c) 2019-2020 Marvell International Ltd. All rights reserved.
+ *
+ */
+#include <linux/sysfs.h>
+#include <linux/fs.h>
+#include <linux/etherdevice.h>
+#include <linux/string.h>
+#include <linux/ctype.h>
+#include <linux/debugfs.h>
+
+#include "prestera.h"
+#include "prestera_hw.h"
+#include "prestera_log.h"
+#include "prestera_fw_log.h"
+
+#define FW_LOG_DBGFS_CFG_DIR	"mvsw_pr_fw_log"
+#define FW_LOG_DBGFS_CFG_NAME	"cfg"
+#define FW_LOG_DBGFS_MAX_STR_LEN	64
+#define FW_LOG_PR_LOG_PREFIX	"[mvsw_pr_fw_log]"
+#define FW_LOG_PR_LIB_SIZE	32
+#define FW_LOG_PR_READ_BUF_SIZE	8192
+#define MVSW_FW_LOG_INFO(fmt, ...)	\
+	pr_info(fmt, ##__VA_ARGS__)
+
+#define FW_LOG_READ_TABLE_FMT	"%-23s"
+
+#define mvsw_dev(sw)		((sw)->dev->dev)
+
+static void mvsw_pr_fw_log_evt_handler(struct mvsw_pr_switch *,
+				       struct mvsw_pr_event *,
+				       void *);
+static ssize_t mvsw_pr_fw_log_debugfs_read(struct file *file,
+					   char __user *ubuf,
+					   size_t count, loff_t *ppos);
+static ssize_t mvsw_pr_fw_log_debugfs_write(struct file *file,
+					    const char __user *ubuf,
+					    size_t count, loff_t *ppos);
+static inline int mvsw_pr_fw_log_get_type_from_str(const char *str);
+static inline int mvsw_pr_fw_log_get_lib_from_str(const char *str);
+
+static int mvsw_pr_fw_log_event_handler_register(struct mvsw_pr_switch *sw);
+static void mvsw_pr_fw_log_event_handler_unregister(struct mvsw_pr_switch *sw);
+
+struct mvsw_pr_fw_log_prv_debugfs {
+	struct dentry *cfg_dir;
+	struct dentry *cfg;
+	const struct file_operations cfg_fops;
+	char *read_buf;
+};
+
+static u8 fw_log_lib_type_config[MVSW_FW_LOG_LIB_MAX] = { 0 };
+
+static struct mvsw_pr_fw_log_prv_debugfs fw_log_debugfs_handle = {
+	.cfg_dir = NULL,
+	.cfg_fops = {
+		.read = mvsw_pr_fw_log_debugfs_read,
+		.write = mvsw_pr_fw_log_debugfs_write,
+		.open = simple_open,
+		.llseek = default_llseek,
+	}
+};
+
+static const char *mvsw_pr_fw_log_lib_id2name[MVSW_FW_LOG_LIB_MAX] = {
+	[MVSW_FW_LOG_LIB_ALL] =  "all",
+	[MVSW_FW_LOG_LIB_BRIDGE] =  "bridge",
+	[MVSW_FW_LOG_LIB_CNC] =  "cnc",
+	[MVSW_FW_LOG_LIB_CONFIG] =  "config",
+	[MVSW_FW_LOG_LIB_COS] =  "cos",
+	[MVSW_FW_LOG_LIB_CSCD] =  "cscd",
+	[MVSW_FW_LOG_LIB_CUT_THROUGH] =  "cut-through",
+	[MVSW_FW_LOG_LIB_DIAG] =  "diag",
+	[MVSW_FW_LOG_LIB_DRAGONITE] =  "dragonite",
+	[MVSW_FW_LOG_LIB_EGRESS] =  "egress",
+	[MVSW_FW_LOG_LIB_EXACT_MATCH] =  "exact-match",
+	[MVSW_FW_LOG_LIB_FABRIC] =  "fabric",
+	[MVSW_FW_LOG_LIB_BRIDGE_FDB_MANAGER] =  "fdb-manager",
+	[MVSW_FW_LOG_LIB_FLOW_MANAGER] =  "flow-manager",
+	[MVSW_FW_LOG_LIB_HW_INIT] =  "hw-init",
+	[MVSW_FW_LOG_LIB_I2C] =  "i2c",
+	[MVSW_FW_LOG_LIB_INGRESS] =  "ingress",
+	[MVSW_FW_LOG_LIB_INIT] =  "init",
+	[MVSW_FW_LOG_LIB_IPFIX] =  "ipfix",
+	[MVSW_FW_LOG_LIB_IP] =  "ip",
+	[MVSW_FW_LOG_LIB_IP_LPM] =  "ip-lpm",
+	[MVSW_FW_LOG_LIB_L2_MLL] =  "l2-mll",
+	[MVSW_FW_LOG_LIB_LATENCY_MONITORING] =  "latency-monitoring",
+	[MVSW_FW_LOG_LIB_LOGICAL_TARGET] =  "logical-target",
+	[MVSW_FW_LOG_LIB_LPM] =  "lpm",
+	[MVSW_FW_LOG_LIB_MIRROR] =  "mirror",
+	[MVSW_FW_LOG_LIB_MULTI_PORT_GROUP] =  "multi-port-group",
+	[MVSW_FW_LOG_LIB_NETWORK_IF] =  "network-if",
+	[MVSW_FW_LOG_LIB_NST] =  "nst",
+	[MVSW_FW_LOG_LIB_OAM] =  "oam",
+	[MVSW_FW_LOG_LIB_PACKET_ANALYZER] =  "packet-analyzer",
+	[MVSW_FW_LOG_LIB_PCL] =  "pcl",
+	[MVSW_FW_LOG_LIB_PHA] =  "pha",
+	[MVSW_FW_LOG_LIB_PHY] =  "phy",
+	[MVSW_FW_LOG_LIB_POLICER] =  "policer",
+	[MVSW_FW_LOG_LIB_PROTECTION] =  "protection",
+	[MVSW_FW_LOG_LIB_PTP] =  "ptp",
+	[MVSW_FW_LOG_LIB_RESOURCE_MANAGER] =  "resource-manager",
+	[MVSW_FW_LOG_LIB_SMI] =  "smi",
+	[MVSW_FW_LOG_LIB_SYSTEM_RECOVERY] =  "system-recovery",
+	[MVSW_FW_LOG_LIB_TAM] =  "tam",
+	[MVSW_FW_LOG_LIB_TCAM] =  "tcam",
+	[MVSW_FW_LOG_LIB_TM] =  "tm",
+	[MVSW_FW_LOG_LIB_TM_GLUE] =  "tm-glue",
+	[MVSW_FW_LOG_LIB_TRUNK] =  "trunk",
+	[MVSW_FW_LOG_LIB_TTI] =  "tti",
+	[MVSW_FW_LOG_LIB_TUNNEL] =  "tunnel",
+	[MVSW_FW_LOG_LIB_VERSION] =  "version",
+	[MVSW_FW_LOG_LIB_VIRTUAL_TCAM] =  "virtual-tcam",
+	[MVSW_FW_LOG_LIB_VNT] =  "vnt",
+	[MVSW_FW_LOG_LIB_PPU] = "ppu",
+	[MVSW_FW_LOG_LIB_EXACT_MATCH_MANAGER] = "exact-match-manager",
+	[MVSW_FW_LOG_LIB_MAC_SEC] = "mac-sec",
+};
+
+static const char *mvsw_pr_fw_log_prv_type_id2name[MVSW_FW_LOG_TYPE_MAX] = {
+	[MVSW_FW_LOG_TYPE_INFO] = "info",
+	[MVSW_FW_LOG_TYPE_ENTRY_LEVEL_FUNCTION] = "entry-level-function",
+	[MVSW_FW_LOG_TYPE_ERROR] = "error",
+	[MVSW_FW_LOG_TYPE_ALL] = "all",
+	[MVSW_FW_LOG_TYPE_NONE]  = "none",
+};
+
+static void mvsw_pr_fw_log_evt_handler(struct mvsw_pr_switch *sw,
+				       struct mvsw_pr_event *evt, void *arg)
+{
+	u32 log_len = evt->fw_log_evt.log_len;
+	u8 *buf = evt->fw_log_evt.data;
+
+	buf[log_len] = '\0';
+
+	MVSW_FW_LOG_INFO(FW_LOG_PR_LOG_PREFIX "%s\n", buf);
+}
+
+static ssize_t mvsw_pr_fw_log_format_str(void)
+{
+	char *buf = fw_log_debugfs_handle.read_buf;
+	int chars_written = 0;
+	int lib, type;
+	int ret;
+
+	memset(buf, 0, FW_LOG_PR_READ_BUF_SIZE);
+
+	ret = snprintf(buf, FW_LOG_PR_READ_BUF_SIZE, FW_LOG_READ_TABLE_FMT,
+		       " ");
+	if (ret < 0)
+		return ret;
+
+	chars_written += ret;
+
+	for (type = 0; type < MVSW_FW_LOG_TYPE_MAX; ++type) {
+		if (type == MVSW_FW_LOG_TYPE_NONE ||
+		    type == MVSW_FW_LOG_TYPE_ALL)
+			continue;
+
+		ret = snprintf(buf + chars_written,
+			       FW_LOG_PR_READ_BUF_SIZE - chars_written,
+			       FW_LOG_READ_TABLE_FMT,
+			       mvsw_pr_fw_log_prv_type_id2name[type]);
+		if (ret < 0)
+			return ret;
+
+		chars_written += ret;
+	}
+
+	strcat(buf, "\n");
+	++chars_written;
+
+	for (lib = 0; lib < MVSW_FW_LOG_LIB_MAX; ++lib) {
+		if (lib == MVSW_FW_LOG_LIB_ALL ||
+		    !mvsw_pr_fw_log_lib_id2name[lib])
+			continue;
+
+		ret = snprintf(buf + chars_written,
+			       FW_LOG_PR_READ_BUF_SIZE - chars_written,
+			       FW_LOG_READ_TABLE_FMT,
+			       mvsw_pr_fw_log_lib_id2name[lib]);
+		if (ret < 0)
+			return ret;
+
+		chars_written += ret;
+
+		for (type = 0; type < MVSW_FW_LOG_TYPE_MAX; ++type) {
+			if (type == MVSW_FW_LOG_TYPE_NONE ||
+			    type == MVSW_FW_LOG_TYPE_ALL)
+				continue;
+
+			ret = snprintf(buf + chars_written,
+				       FW_LOG_PR_READ_BUF_SIZE - chars_written,
+				       FW_LOG_READ_TABLE_FMT,
+				       fw_log_lib_type_config[lib] & BIT(type)
+						? "+" : "-");
+			if (ret < 0)
+				return ret;
+
+			chars_written += ret;
+		}
+		strlcat(buf, "\n", FW_LOG_PR_READ_BUF_SIZE);
+		++chars_written;
+	}
+
+	return chars_written;
+}
+
+static ssize_t mvsw_pr_fw_log_debugfs_read(struct file *file,
+					   char __user *ubuf,
+					   size_t count, loff_t *ppos)
+{
+	char *buf = fw_log_debugfs_handle.read_buf;
+
+	return simple_read_from_buffer(ubuf, count, ppos, buf,
+				       FW_LOG_PR_READ_BUF_SIZE);
+}
+
+static int mvsw_pr_fw_log_parse_usr_input(int *name, int *type,
+					  const char __user *ubuf, size_t count)
+{
+	u8 tmp_buf[FW_LOG_DBGFS_MAX_STR_LEN] = { 0 };
+	u8 lib_str[FW_LOG_PR_LIB_SIZE] = { 0 };
+	u8 type_str[FW_LOG_PR_LIB_SIZE] = { 0 };
+	ssize_t len_to_copy = count - 1;
+	u8 *ppos_lib, *ppos_type;
+	char *end = tmp_buf;
+	int err;
+
+	if (len_to_copy > FW_LOG_DBGFS_MAX_STR_LEN) {
+		MVSW_LOG_ERROR("Len is > than max(%zu vs max possible %d)\n",
+			       count, FW_LOG_DBGFS_MAX_STR_LEN);
+		return -EMSGSIZE;
+	}
+
+	err = copy_from_user(tmp_buf, ubuf, len_to_copy);
+	if (err)
+		return -EINVAL;
+
+	ppos_lib  = strsep(&end, " \t");
+	ppos_type = strsep(&end, " \t\0");
+
+	if (!ppos_lib || !ppos_type)
+		return -EINVAL;
+
+	strcpy(lib_str, ppos_lib);
+
+	strcpy(type_str, ppos_type);
+
+	if (iscntrl(lib_str[0]) || isspace(lib_str[0]) || lib_str[0] == '\0' ||
+	    iscntrl(type_str[0]) || isspace(type_str[0]) ||
+	    type_str[0] == '\0') {
+		return -EINVAL;
+	}
+
+	*name = mvsw_pr_fw_log_get_lib_from_str(lib_str);
+	*type = mvsw_pr_fw_log_get_type_from_str(type_str);
+
+	if (*name >= MVSW_FW_LOG_LIB_MAX ||
+	    *type >= MVSW_FW_LOG_TYPE_MAX ||
+	    (*name != MVSW_FW_LOG_LIB_ALL && *type == MVSW_FW_LOG_TYPE_NONE))
+		return -EINVAL;
+
+	return 0;
+}
+
+static ssize_t mvsw_pr_fw_log_debugfs_write(struct file *file,
+					    const char __user *ubuf,
+					    size_t count, loff_t *ppos)
+{
+	struct mvsw_pr_switch *sw = file->private_data;
+	int lib, type;
+	int i, j;
+	int err;
+
+	err = mvsw_pr_fw_log_parse_usr_input(&lib, &type, ubuf, count);
+	if (err)
+		goto error;
+
+	err = mvsw_pr_hw_fw_log_level_set(sw, lib, type);
+	if (err) {
+		dev_err(mvsw_dev(sw), "Failed to send request to firmware\n");
+		return err;
+	}
+
+	/* specific lib and specific type */
+	if (lib != MVSW_FW_LOG_LIB_ALL && type != MVSW_FW_LOG_TYPE_ALL) {
+		/* special type 'NONE' to disable feature */
+		if (type == MVSW_FW_LOG_TYPE_NONE)
+			memset(fw_log_lib_type_config, 0,
+			       sizeof(fw_log_lib_type_config));
+		/* Actual type should be switched */
+		else
+			fw_log_lib_type_config[lib] ^= (1 << type);
+	/* specific lib but all types */
+	} else if (lib != MVSW_FW_LOG_LIB_ALL && type == MVSW_FW_LOG_TYPE_ALL) {
+		for (j = 0; j < MVSW_FW_LOG_TYPE_ALL; ++j)
+			fw_log_lib_type_config[lib] ^= (1 << j);
+	/* specific type but all libs */
+	} else if (lib == MVSW_FW_LOG_LIB_ALL && type != MVSW_FW_LOG_TYPE_ALL) {
+		for (i = 0; i < MVSW_FW_LOG_LIB_ALL; ++i)
+			fw_log_lib_type_config[i] |= (1 << type);
+	/* all libs and all types */
+	} else {
+		for (i = 0; i < MVSW_FW_LOG_LIB_ALL; ++i) {
+			for (j = 0; j < MVSW_FW_LOG_TYPE_ALL; ++j)
+				fw_log_lib_type_config[i] |= (1 << j);
+		}
+	}
+
+	err = mvsw_pr_fw_log_format_str();
+	if (err <= 0) {
+		dev_err(mvsw_dev(sw), "Failed to form output string\n");
+		return err;
+	}
+
+	return count;
+
+error:
+	dev_warn(mvsw_dev(sw),
+		 "Invalid str received, make sure request is valid\n");
+	dev_warn(mvsw_dev(sw),
+		 "Valid fmt consists of: \"lib type\" string, e.g:\n");
+	dev_warn(mvsw_dev(sw),
+		 "\"phy error\" for 'phy' lib 'error' logs enabled\n");
+
+	return err;
+}
+
+static inline int mvsw_pr_fw_log_get_type_from_str(const char *str)
+{
+	int i;
+
+	for (i = 0; i < MVSW_FW_LOG_TYPE_MAX; ++i) {
+		if (!mvsw_pr_fw_log_prv_type_id2name[i])
+			continue;
+
+		if (strcmp(mvsw_pr_fw_log_prv_type_id2name[i], str) == 0)
+			return i;
+	}
+
+	return MVSW_FW_LOG_TYPE_MAX;
+}
+
+static inline int mvsw_pr_fw_log_get_lib_from_str(const char *str)
+{
+	int i;
+
+	for (i = 0; i < MVSW_FW_LOG_LIB_MAX; ++i) {
+		if (!mvsw_pr_fw_log_lib_id2name[i])
+			continue;
+
+		if (strcmp(mvsw_pr_fw_log_lib_id2name[i], str) == 0)
+			return i;
+	}
+
+	return MVSW_FW_LOG_LIB_MAX;
+}
+
+static int mvsw_pr_fw_log_event_handler_register(struct mvsw_pr_switch *sw)
+{
+	return mvsw_pr_hw_event_handler_register(sw, MVSW_EVENT_TYPE_FW_LOG,
+						 mvsw_pr_fw_log_evt_handler,
+						 NULL);
+}
+
+static void mvsw_pr_fw_log_event_handler_unregister(struct mvsw_pr_switch *sw)
+{
+	mvsw_pr_hw_event_handler_unregister(sw, MVSW_EVENT_TYPE_FW_LOG);
+}
+
+int mvsw_pr_fw_log_init(struct mvsw_pr_switch *sw)
+{
+	fw_log_debugfs_handle.cfg_dir =
+		debugfs_create_dir(FW_LOG_DBGFS_CFG_DIR, NULL);
+
+	if (!fw_log_debugfs_handle.cfg_dir) {
+		MVSW_LOG_ERROR("Failed to create debugfs dir entry");
+		return -1;
+	}
+
+	fw_log_debugfs_handle.cfg =
+		debugfs_create_file(FW_LOG_DBGFS_CFG_NAME, 0644,
+				    fw_log_debugfs_handle.cfg_dir, sw,
+				    &fw_log_debugfs_handle.cfg_fops);
+
+	if (!fw_log_debugfs_handle.cfg) {
+		MVSW_LOG_ERROR("Failed to create debugfs dir entry");
+		debugfs_remove(fw_log_debugfs_handle.cfg_dir);
+		return -1;
+	}
+
+	if (mvsw_pr_fw_log_event_handler_register(sw))
+		goto error;
+
+	fw_log_debugfs_handle.read_buf =
+		kzalloc(FW_LOG_PR_READ_BUF_SIZE, GFP_KERNEL);
+
+	if (!fw_log_debugfs_handle.read_buf)
+		goto error;
+
+	mvsw_pr_hw_fw_log_level_set(sw, MVSW_FW_LOG_LIB_ALL,
+				    MVSW_FW_LOG_TYPE_NONE);
+	mvsw_pr_fw_log_format_str();
+
+	return 0;
+error:
+	debugfs_remove(fw_log_debugfs_handle.cfg);
+	debugfs_remove(fw_log_debugfs_handle.cfg_dir);
+	return -1;
+}
+
+void mvsw_pr_fw_log_fini(struct mvsw_pr_switch *sw)
+{
+	mvsw_pr_fw_log_event_handler_unregister(sw);
+
+	kfree(fw_log_debugfs_handle.read_buf);
+
+	debugfs_remove(fw_log_debugfs_handle.cfg);
+	debugfs_remove(fw_log_debugfs_handle.cfg_dir);
+}
diff --git a/drivers/net/ethernet/marvell/prestera/prestera_fw_log.h b/drivers/net/ethernet/marvell/prestera/prestera_fw_log.h
new file mode 100644
index 000000000..ccd551439
--- /dev/null
+++ b/drivers/net/ethernet/marvell/prestera/prestera_fw_log.h
@@ -0,0 +1,15 @@
+/* SPDX-License-Identifier: BSD-3-Clause OR GPL-2.0
+ *
+ * Copyright (c) 2019-2020 Marvell International Ltd. All rights reserved.
+ *
+ */
+
+#ifndef _MVSW_PRESTERA_FW_LOG_H_
+#define _MVSW_PRESTERA_FW_LOG_H_
+
+#include "prestera.h"
+
+int  mvsw_pr_fw_log_init(struct mvsw_pr_switch *sw);
+void mvsw_pr_fw_log_fini(struct mvsw_pr_switch *sw);
+
+#endif /* _MVSW_PRESTERA_FW_LOG_H_ */
diff --git a/drivers/net/ethernet/marvell/prestera/prestera_hw.c b/drivers/net/ethernet/marvell/prestera/prestera_hw.c
index 0424718d5..eb0a544ec 100644
--- a/drivers/net/ethernet/marvell/prestera/prestera_hw.c
+++ b/drivers/net/ethernet/marvell/prestera/prestera_hw.c
@@ -1,411 +1,643 @@
-// SPDX-License-Identifier: BSD-3-Clause OR GPL-2.0
-/* Copyright (c) 2019-2020 Marvell International Ltd. All rights reserved */
-
+/* SPDX-License-Identifier: BSD-3-Clause OR GPL-2.0
+ *
+ * Copyright (c) 2019-2020 Marvell International Ltd. All rights reserved.
+ *
+ */
 #include <linux/etherdevice.h>
 #include <linux/ethtool.h>
+#include <linux/netdevice.h>
 #include <linux/list.h>
 
-#include "prestera.h"
 #include "prestera_hw.h"
+#include "prestera.h"
+#include "prestera_log.h"
+#include "prestera_fw_log.h"
+#include "prestera_rxtx.h"
+
+#define MVSW_PR_INIT_TIMEOUT 30000000	/* 30sec */
+#define MVSW_PR_MIN_MTU 64
+#define MVSW_PR_MSG_BUFF_CHUNK_SIZE	32	/* bytes */
+
+enum mvsw_msg_type {
+	MVSW_MSG_TYPE_SWITCH_INIT = 0x1,
+	MVSW_MSG_TYPE_SWITCH_ATTR_SET = 0x2,
+
+	MVSW_MSG_TYPE_PORT_ATTR_SET = 0x100,
+	MVSW_MSG_TYPE_PORT_ATTR_GET = 0x101,
+	MVSW_MSG_TYPE_PORT_INFO_GET = 0x110,
+
+	MVSW_MSG_TYPE_VLAN_CREATE = 0x200,
+	MVSW_MSG_TYPE_VLAN_DELETE = 0x201,
+	MVSW_MSG_TYPE_VLAN_PORT_SET = 0x202,
+	MVSW_MSG_TYPE_VLAN_PVID_SET = 0x203,
+
+	MVSW_MSG_TYPE_FDB_ADD = 0x300,
+	MVSW_MSG_TYPE_FDB_DELETE = 0x301,
+	MVSW_MSG_TYPE_FDB_FLUSH_PORT = 0x310,
+	MVSW_MSG_TYPE_FDB_FLUSH_VLAN = 0x311,
+	MVSW_MSG_TYPE_FDB_FLUSH_PORT_VLAN = 0x312,
+	MVSW_MSG_TYPE_FDB_MACVLAN_ADD = 0x320,
+	MVSW_MSG_TYPE_FDB_MACVLAN_DEL = 0x321,
+
+	MVSW_MSG_TYPE_LOG_LEVEL_SET,
+
+	MVSW_MSG_TYPE_BRIDGE_CREATE = 0x400,
+	MVSW_MSG_TYPE_BRIDGE_DELETE = 0x401,
+	MVSW_MSG_TYPE_BRIDGE_PORT_ADD = 0x402,
+	MVSW_MSG_TYPE_BRIDGE_PORT_DELETE = 0x403,
+
+	MVSW_MSG_TYPE_ACL_RULE_ADD = 0x500,
+	MVSW_MSG_TYPE_ACL_RULE_DELETE = 0x501,
+	MVSW_MSG_TYPE_ACL_RULE_STATS_GET = 0x510,
+	MVSW_MSG_TYPE_ACL_RULESET_CREATE = 0x520,
+	MVSW_MSG_TYPE_ACL_RULESET_DELETE = 0x521,
+	MVSW_MSG_TYPE_ACL_PORT_BIND = 0x530,
+	MVSW_MSG_TYPE_ACL_PORT_UNBIND = 0x531,
+
+	MVSW_MSG_TYPE_ROUTER_RIF_CREATE = 0x600,
+	MVSW_MSG_TYPE_ROUTER_RIF_DELETE = 0x601,
+	MVSW_MSG_TYPE_ROUTER_RIF_SET = 0x602,
+	MVSW_MSG_TYPE_ROUTER_LPM_ADD = 0x610,
+	MVSW_MSG_TYPE_ROUTER_LPM_DELETE = 0x611,
+	MVSW_MSG_TYPE_ROUTER_NH_GRP_SET = 0x622,
+	MVSW_MSG_TYPE_ROUTER_NH_GRP_GET = 0x644,
+	MVSW_MSG_TYPE_ROUTER_NH_GRP_ADD = 0x623,
+	MVSW_MSG_TYPE_ROUTER_NH_GRP_DELETE = 0x624,
+	MVSW_MSG_TYPE_ROUTER_VR_CREATE = 0x630,
+	MVSW_MSG_TYPE_ROUTER_VR_DELETE = 0x631,
+	MVSW_MSG_TYPE_ROUTER_VR_ABORT = 0x632,
+	MVSW_MSG_TYPE_ROUTER_MP_HASH_SET = 0x650,
+
+	MVSW_MSG_TYPE_RXTX_INIT = 0x800,
+
+	MVSW_MSG_TYPE_LAG_ADD = 0x900,
+	MVSW_MSG_TYPE_LAG_DELETE = 0x901,
+	MVSW_MSG_TYPE_LAG_ENABLE = 0x902,
+	MVSW_MSG_TYPE_LAG_DISABLE = 0x903,
+	MVSW_MSG_TYPE_LAG_ROUTER_LEAVE = 0x904,
+
+	MVSW_MSG_TYPE_STP_PORT_SET = 0x1000,
+
+	MVSW_MSG_TYPE_ACK = 0x10000,
+	MVSW_MSG_TYPE_MAX
+};
 
-#define PRESTERA_SWITCH_INIT_TIMEOUT_MS (30 * 1000)
-
-#define PRESTERA_MIN_MTU 64
-
-enum prestera_cmd_type_t {
-	PRESTERA_CMD_TYPE_SWITCH_INIT = 0x1,
-	PRESTERA_CMD_TYPE_SWITCH_ATTR_SET = 0x2,
-
-	PRESTERA_CMD_TYPE_PORT_ATTR_SET = 0x100,
-	PRESTERA_CMD_TYPE_PORT_ATTR_GET = 0x101,
-	PRESTERA_CMD_TYPE_PORT_INFO_GET = 0x110,
-
-	PRESTERA_CMD_TYPE_VLAN_CREATE = 0x200,
-	PRESTERA_CMD_TYPE_VLAN_DELETE = 0x201,
-	PRESTERA_CMD_TYPE_VLAN_PORT_SET = 0x202,
-	PRESTERA_CMD_TYPE_VLAN_PVID_SET = 0x203,
-
-	PRESTERA_CMD_TYPE_FDB_ADD = 0x300,
-	PRESTERA_CMD_TYPE_FDB_DELETE = 0x301,
-	PRESTERA_CMD_TYPE_FDB_FLUSH_PORT = 0x310,
-	PRESTERA_CMD_TYPE_FDB_FLUSH_VLAN = 0x311,
-	PRESTERA_CMD_TYPE_FDB_FLUSH_PORT_VLAN = 0x312,
-
-	PRESTERA_CMD_TYPE_BRIDGE_CREATE = 0x400,
-	PRESTERA_CMD_TYPE_BRIDGE_DELETE = 0x401,
-	PRESTERA_CMD_TYPE_BRIDGE_PORT_ADD = 0x402,
-	PRESTERA_CMD_TYPE_BRIDGE_PORT_DELETE = 0x403,
-
-	PRESTERA_CMD_TYPE_RXTX_INIT = 0x800,
-	PRESTERA_CMD_TYPE_RXTX_PORT_INIT = 0x801,
-
-	PRESTERA_CMD_TYPE_STP_PORT_SET = 0x1000,
+enum mvsw_msg_port_attr {
+	MVSW_MSG_PORT_ATTR_ADMIN_STATE = 1,
+	MVSW_MSG_PORT_ATTR_OPER_STATE = 2,
+	MVSW_MSG_PORT_ATTR_MTU = 3,
+	MVSW_MSG_PORT_ATTR_MAC = 4,
+	MVSW_MSG_PORT_ATTR_SPEED = 5,
+	MVSW_MSG_PORT_ATTR_ACCEPT_FRAME_TYPE = 6,
+	MVSW_MSG_PORT_ATTR_LEARNING = 7,
+	MVSW_MSG_PORT_ATTR_FLOOD = 8,
+	MVSW_MSG_PORT_ATTR_CAPABILITY = 9,
+	MVSW_MSG_PORT_ATTR_REMOTE_CAPABILITY = 10,
+	MVSW_MSG_PORT_ATTR_REMOTE_FC = 11,
+	MVSW_MSG_PORT_ATTR_LINK_MODE = 12,
+	MVSW_MSG_PORT_ATTR_TYPE = 13,
+	MVSW_MSG_PORT_ATTR_FEC = 14,
+	MVSW_MSG_PORT_ATTR_AUTONEG = 15,
+	MVSW_MSG_PORT_ATTR_DUPLEX = 16,
+	MVSW_MSG_PORT_ATTR_STATS = 17,
+	MVSW_MSG_PORT_ATTR_MDIX = 18,
+	MVSW_MSG_PORT_ATTR_AUTONEG_RESTART = 19,
+	MVSW_MSG_PORT_ATTR_MAX
+};
 
-	PRESTERA_CMD_TYPE_ACK = 0x10000,
-	PRESTERA_CMD_TYPE_MAX
+enum mvsw_msg_switch_attr {
+	MVSW_MSG_SWITCH_ATTR_MAC = 1,
+	MVSW_MSG_SWITCH_ATTR_AGEING = 2,
+	MVSW_MSG_SWITCH_ATTR_TRAP_POLICER = 3,
 };
 
 enum {
-	PRESTERA_CMD_PORT_ATTR_ADMIN_STATE = 1,
-	PRESTERA_CMD_PORT_ATTR_MTU = 3,
-	PRESTERA_CMD_PORT_ATTR_MAC = 4,
-	PRESTERA_CMD_PORT_ATTR_SPEED = 5,
-	PRESTERA_CMD_PORT_ATTR_ACCEPT_FRAME_TYPE = 6,
-	PRESTERA_CMD_PORT_ATTR_LEARNING = 7,
-	PRESTERA_CMD_PORT_ATTR_FLOOD = 8,
-	PRESTERA_CMD_PORT_ATTR_CAPABILITY = 9,
-	PRESTERA_CMD_PORT_ATTR_REMOTE_CAPABILITY = 10,
-	PRESTERA_CMD_PORT_ATTR_REMOTE_FC = 11,
-	PRESTERA_CMD_PORT_ATTR_LINK_MODE = 12,
-	PRESTERA_CMD_PORT_ATTR_TYPE = 13,
-	PRESTERA_CMD_PORT_ATTR_FEC = 14,
-	PRESTERA_CMD_PORT_ATTR_AUTONEG = 15,
-	PRESTERA_CMD_PORT_ATTR_DUPLEX = 16,
-	PRESTERA_CMD_PORT_ATTR_STATS = 17,
-	PRESTERA_CMD_PORT_ATTR_MDIX = 18,
-	PRESTERA_CMD_PORT_ATTR_AUTONEG_RESTART = 19,
+	MVSW_MSG_ACK_OK,
+	MVSW_MSG_ACK_FAILED,
+	MVSW_MSG_ACK_MAX
 };
 
 enum {
-	PRESTERA_CMD_SWITCH_ATTR_MAC = 1,
-	PRESTERA_CMD_SWITCH_ATTR_AGEING = 2,
+	MVSW_PORT_TP_NA,
+	MVSW_PORT_TP_MDI,
+	MVSW_PORT_TP_MDIX,
+	MVSW_PORT_TP_AUTO
 };
 
 enum {
-	PRESTERA_CMD_ACK_OK,
-	PRESTERA_CMD_ACK_FAILED,
-
-	PRESTERA_CMD_ACK_MAX
+	MVSW_PORT_GOOD_OCTETS_RCV_CNT,
+	MVSW_PORT_BAD_OCTETS_RCV_CNT,
+	MVSW_PORT_MAC_TRANSMIT_ERR_CNT,
+	MVSW_PORT_BRDC_PKTS_RCV_CNT,
+	MVSW_PORT_MC_PKTS_RCV_CNT,
+	MVSW_PORT_PKTS_64_OCTETS_CNT,
+	MVSW_PORT_PKTS_65TO127_OCTETS_CNT,
+	MVSW_PORT_PKTS_128TO255_OCTETS_CNT,
+	MVSW_PORT_PKTS_256TO511_OCTETS_CNT,
+	MVSW_PORT_PKTS_512TO1023_OCTETS_CNT,
+	MVSW_PORT_PKTS_1024TOMAX_OCTETS_CNT,
+	MVSW_PORT_EXCESSIVE_COLLISIONS_CNT,
+	MVSW_PORT_MC_PKTS_SENT_CNT,
+	MVSW_PORT_BRDC_PKTS_SENT_CNT,
+	MVSW_PORT_FC_SENT_CNT,
+	MVSW_PORT_GOOD_FC_RCV_CNT,
+	MVSW_PORT_DROP_EVENTS_CNT,
+	MVSW_PORT_UNDERSIZE_PKTS_CNT,
+	MVSW_PORT_FRAGMENTS_PKTS_CNT,
+	MVSW_PORT_OVERSIZE_PKTS_CNT,
+	MVSW_PORT_JABBER_PKTS_CNT,
+	MVSW_PORT_MAC_RCV_ERROR_CNT,
+	MVSW_PORT_BAD_CRC_CNT,
+	MVSW_PORT_COLLISIONS_CNT,
+	MVSW_PORT_LATE_COLLISIONS_CNT,
+	MVSW_PORT_GOOD_UC_PKTS_RCV_CNT,
+	MVSW_PORT_GOOD_UC_PKTS_SENT_CNT,
+	MVSW_PORT_MULTIPLE_PKTS_SENT_CNT,
+	MVSW_PORT_DEFERRED_PKTS_SENT_CNT,
+	MVSW_PORT_GOOD_OCTETS_SENT_CNT,
+	MVSW_PORT_CNT_MAX,
 };
 
 enum {
-	PRESTERA_PORT_TP_NA,
-	PRESTERA_PORT_TP_MDI,
-	PRESTERA_PORT_TP_MDIX,
-	PRESTERA_PORT_TP_AUTO,
+	MVSW_FC_NONE,
+	MVSW_FC_SYMMETRIC,
+	MVSW_FC_ASYMMETRIC,
+	MVSW_FC_SYMM_ASYMM,
 };
 
 enum {
-	PRESTERA_PORT_GOOD_OCTETS_RCV_CNT,
-	PRESTERA_PORT_BAD_OCTETS_RCV_CNT,
-	PRESTERA_PORT_MAC_TRANSMIT_ERR_CNT,
-	PRESTERA_PORT_BRDC_PKTS_RCV_CNT,
-	PRESTERA_PORT_MC_PKTS_RCV_CNT,
-	PRESTERA_PORT_PKTS_64L_CNT,
-	PRESTERA_PORT_PKTS_65TO127L_CNT,
-	PRESTERA_PORT_PKTS_128TO255L_CNT,
-	PRESTERA_PORT_PKTS_256TO511L_CNT,
-	PRESTERA_PORT_PKTS_512TO1023L_CNT,
-	PRESTERA_PORT_PKTS_1024TOMAXL_CNT,
-	PRESTERA_PORT_EXCESSIVE_COLLISIONS_CNT,
-	PRESTERA_PORT_MC_PKTS_SENT_CNT,
-	PRESTERA_PORT_BRDC_PKTS_SENT_CNT,
-	PRESTERA_PORT_FC_SENT_CNT,
-	PRESTERA_PORT_GOOD_FC_RCV_CNT,
-	PRESTERA_PORT_DROP_EVENTS_CNT,
-	PRESTERA_PORT_UNDERSIZE_PKTS_CNT,
-	PRESTERA_PORT_FRAGMENTS_PKTS_CNT,
-	PRESTERA_PORT_OVERSIZE_PKTS_CNT,
-	PRESTERA_PORT_JABBER_PKTS_CNT,
-	PRESTERA_PORT_MAC_RCV_ERROR_CNT,
-	PRESTERA_PORT_BAD_CRC_CNT,
-	PRESTERA_PORT_COLLISIONS_CNT,
-	PRESTERA_PORT_LATE_COLLISIONS_CNT,
-	PRESTERA_PORT_GOOD_UC_PKTS_RCV_CNT,
-	PRESTERA_PORT_GOOD_UC_PKTS_SENT_CNT,
-	PRESTERA_PORT_MULTIPLE_PKTS_SENT_CNT,
-	PRESTERA_PORT_DEFERRED_PKTS_SENT_CNT,
-	PRESTERA_PORT_GOOD_OCTETS_SENT_CNT,
-
-	PRESTERA_PORT_CNT_MAX
+	MVSW_HW_FDB_ENTRY_TYPE_REG_PORT = 0,
+	MVSW_HW_FDB_ENTRY_TYPE_LAG = 1,
+	MVSW_HW_FDB_ENTRY_TYPE_MAX = 2,
 };
 
 enum {
-	PRESTERA_FC_NONE,
-	PRESTERA_FC_SYMMETRIC,
-	PRESTERA_FC_ASYMMETRIC,
-	PRESTERA_FC_SYMM_ASYMM,
+	MVSW_PORT_FLOOD_TYPE_UC = 0,
+	MVSW_PORT_FLOOD_TYPE_MC = 1,
+	MVSW_PORT_FLOOD_TYPE_BC = 2,
 };
 
-struct prestera_fw_event_handler {
-	struct list_head list;
-	struct rcu_head rcu;
-	enum prestera_event_type type;
-	prestera_event_cb_t func;
-	void *arg;
+struct mvsw_msg_buff {
+	u32 free;
+	u32 total;
+	u32 used;
+	void *data;
 };
 
-struct prestera_msg_cmd {
+struct mvsw_msg_cmd {
 	u32 type;
-};
+} __packed __aligned(4);
 
-struct prestera_msg_ret {
-	struct prestera_msg_cmd cmd;
+struct mvsw_msg_ret {
+	struct mvsw_msg_cmd cmd;
 	u32 status;
-};
+} __packed __aligned(4);
 
-struct prestera_msg_common_req {
-	struct prestera_msg_cmd cmd;
-};
+struct mvsw_msg_common_request {
+	struct mvsw_msg_cmd cmd;
+} __packed __aligned(4);
 
-struct prestera_msg_common_resp {
-	struct prestera_msg_ret ret;
-};
+struct mvsw_msg_common_response {
+	struct mvsw_msg_ret ret;
+} __packed __aligned(4);
 
-union prestera_msg_switch_param {
-	u8 mac[ETH_ALEN];
-	u32 ageing_timeout_ms;
+union mvsw_msg_switch_param {
+	u32 ageing_timeout;
+	u8  mac[ETH_ALEN];
+	u32 trap_policer_profile;
 };
 
-struct prestera_msg_switch_attr_req {
-	struct prestera_msg_cmd cmd;
+struct mvsw_msg_switch_attr_cmd {
+	struct mvsw_msg_cmd cmd;
 	u32 attr;
-	union prestera_msg_switch_param param;
-};
+	union mvsw_msg_switch_param param;
+} __packed __aligned(4);
 
-struct prestera_msg_switch_init_resp {
-	struct prestera_msg_ret ret;
+struct mvsw_msg_switch_init_ret {
+	struct mvsw_msg_ret ret;
 	u32 port_count;
 	u32 mtu_max;
 	u8  switch_id;
-};
+	u8  lag_max;
+	u8  lag_member_max;
+} __packed __aligned(4);
 
-struct prestera_msg_port_autoneg_param {
+struct mvsw_msg_port_autoneg_param {
 	u64 link_mode;
 	u8  enable;
 	u8  fec;
 };
 
-struct prestera_msg_port_cap_param {
+struct mvsw_msg_port_cap_param {
 	u64 link_mode;
 	u8  type;
 	u8  fec;
 	u8  transceiver;
 };
 
-struct prestera_msg_port_mdix_param {
+struct mvsw_msg_port_mdix_param {
 	u8 status;
 	u8 admin_mode;
 };
 
-union prestera_msg_port_param {
+struct mvsw_msg_port_flood_param {
+	u8 type;
+	u8 enable;
+};
+
+union mvsw_msg_port_param {
 	u8  admin_state;
 	u8  oper_state;
 	u32 mtu;
 	u8  mac[ETH_ALEN];
 	u8  accept_frm_type;
+	u8  learning;
 	u32 speed;
-	u8 learning;
-	u8 flood;
 	u32 link_mode;
 	u8  type;
 	u8  duplex;
 	u8  fec;
 	u8  fc;
-	struct prestera_msg_port_mdix_param mdix;
-	struct prestera_msg_port_autoneg_param autoneg;
-	struct prestera_msg_port_cap_param cap;
+	struct mvsw_msg_port_mdix_param mdix;
+	struct mvsw_msg_port_autoneg_param autoneg;
+	struct mvsw_msg_port_cap_param cap;
+	struct mvsw_msg_port_flood_param flood;
 };
 
-struct prestera_msg_port_attr_req {
-	struct prestera_msg_cmd cmd;
+struct mvsw_msg_port_attr_cmd {
+	struct mvsw_msg_cmd cmd;
 	u32 attr;
 	u32 port;
 	u32 dev;
-	union prestera_msg_port_param param;
-};
+	union mvsw_msg_port_param param;
+} __packed __aligned(4);
 
-struct prestera_msg_port_attr_resp {
-	struct prestera_msg_ret ret;
-	union prestera_msg_port_param param;
-};
+struct mvsw_msg_port_attr_ret {
+	struct mvsw_msg_ret ret;
+	union mvsw_msg_port_param param;
+} __packed __aligned(4);
 
-struct prestera_msg_port_stats_resp {
-	struct prestera_msg_ret ret;
-	u64 stats[PRESTERA_PORT_CNT_MAX];
-};
+struct mvsw_msg_port_stats_ret {
+	struct mvsw_msg_ret ret;
+	u64 stats[MVSW_PORT_CNT_MAX];
+} __packed __aligned(4);
 
-struct prestera_msg_port_info_req {
-	struct prestera_msg_cmd cmd;
+struct mvsw_msg_port_info_cmd {
+	struct mvsw_msg_cmd cmd;
 	u32 port;
-};
+} __packed __aligned(4);
 
-struct prestera_msg_port_info_resp {
-	struct prestera_msg_ret ret;
+struct mvsw_msg_port_info_ret {
+	struct mvsw_msg_ret ret;
 	u32 hw_id;
 	u32 dev_id;
 	u16 fp_id;
-};
+} __packed __aligned(4);
 
-struct prestera_msg_vlan_req {
-	struct prestera_msg_cmd cmd;
+struct mvsw_msg_vlan_cmd {
+	struct mvsw_msg_cmd cmd;
 	u32 port;
 	u32 dev;
 	u16 vid;
 	u8  is_member;
 	u8  is_tagged;
-};
+} __packed __aligned(4);
 
-struct prestera_msg_fdb_req {
-	struct prestera_msg_cmd cmd;
+struct mvsw_msg_fdb_cmd {
+	struct mvsw_msg_cmd cmd;
 	u8 dest_type;
-	u32 port;
-	u32 dev;
+	union {
+		struct {
+			u32 port;
+			u32 dev;
+		};
+		u16 lag_id;
+	} dest;
 	u8  mac[ETH_ALEN];
 	u16 vid;
 	u8  dynamic;
 	u32 flush_mode;
-};
+} __packed __aligned(4);
 
-struct prestera_msg_bridge_req {
-	struct prestera_msg_cmd cmd;
+struct mvsw_msg_log_lvl_set_cmd {
+	struct mvsw_msg_cmd cmd;
+	u32 lib;
+	u32 type;
+} __packed __aligned(4);
+
+struct mvsw_msg_acl_rule_cmd {
+	struct mvsw_msg_cmd cmd;
+	u32 id;
+	u16 ruleset_id;
+} __packed __aligned(4);
+
+struct mvsw_msg_acl_rule_ret {
+	struct mvsw_msg_ret ret;
+	u32 id;
+} __packed __aligned(4);
+
+struct mvsw_msg_acl_rule_stats_ret {
+	struct mvsw_msg_ret ret;
+	u64 packets;
+	u64 bytes;
+} __packed __aligned(4);
+
+struct mvsw_msg_acl_ruleset_bind_cmd {
+	struct mvsw_msg_cmd cmd;
 	u32 port;
 	u32 dev;
-	u16 bridge;
-};
+	u16 ruleset_id;
+} __packed __aligned(4);
 
-struct prestera_msg_bridge_resp {
-	struct prestera_msg_ret ret;
-	u16 bridge;
-};
+struct mvsw_msg_acl_ruleset_cmd {
+	struct mvsw_msg_cmd cmd;
+	u16 id;
+} __packed __aligned(4);
 
-struct prestera_msg_stp_req {
-	struct prestera_msg_cmd cmd;
-	u32 port;
-	u32 dev;
-	u16 vid;
-	u8  state;
-};
+struct mvsw_msg_acl_ruleset_ret {
+	struct mvsw_msg_ret ret;
+	u16 id;
+} __packed __aligned(4);
 
-struct prestera_msg_rxtx_req {
-	struct prestera_msg_cmd cmd;
-	u8 use_sdma;
-};
+struct mvsw_msg_event {
+	u16 type;
+	u16 id;
+} __packed __aligned(4);
 
-struct prestera_msg_rxtx_resp {
-	struct prestera_msg_ret ret;
-	u32 map_addr;
-};
+struct mvsw_msg_event_log {
+	struct mvsw_msg_event id;
+	u32 log_string_size;
+	u8 log_string[0];
+} __packed __aligned(4);
 
-struct prestera_msg_rxtx_port_req {
-	struct prestera_msg_cmd cmd;
-	u32 port;
-	u32 dev;
+union mvsw_msg_event_fdb_param {
+	u8 mac[ETH_ALEN];
 };
 
-struct prestera_msg_event {
-	u16 type;
-	u16 id;
-};
+struct mvsw_msg_event_fdb {
+	struct mvsw_msg_event id;
+	u8 dest_type;
+	union {
+		u32 port_id;
+		u16 lag_id;
+	} dest;
+	u32 vid;
+	union mvsw_msg_event_fdb_param param;
+} __packed __aligned(4);
 
-union prestera_msg_event_port_param {
-	u32 oper_state;
-};
+struct mvsw_msg_event_port_param {
+	u8 oper_state;
+	u8 duplex;
+	u32 speed;
+} __packed __aligned(4);
 
-struct prestera_msg_event_port {
-	struct prestera_msg_event id;
+struct mvsw_msg_event_port {
+	struct mvsw_msg_event id;
 	u32 port_id;
-	union prestera_msg_event_port_param param;
-};
+	struct mvsw_msg_event_port_param param;
+} __packed __aligned(4);
 
-union prestera_msg_event_fdb_param {
+struct mvsw_msg_bridge_cmd {
+	struct mvsw_msg_cmd cmd;
+	u32 port;
+	u32 dev;
+	u16 bridge;
+} __packed __aligned(4);
+
+struct mvsw_msg_bridge_ret {
+	struct mvsw_msg_ret ret;
+	u16 bridge;
+} __packed __aligned(4);
+
+struct mvsw_msg_macvlan_cmd {
+	struct mvsw_msg_cmd cmd;
+	u16 vr_id;
 	u8 mac[ETH_ALEN];
-};
+	u16 vid;
+} __packed __aligned(4);
 
-struct prestera_msg_event_fdb {
-	struct prestera_msg_event id;
-	u8 dest_type;
-	u32 port_id;
-	u32 vid;
-	union prestera_msg_event_fdb_param param;
-};
+struct mvsw_msg_stp_cmd {
+	struct mvsw_msg_cmd cmd;
+	u32 port;
+	u32 dev;
+	u16 vid;
+	u8  state;
+} __packed __aligned(4);
 
-static int __prestera_cmd_ret(struct prestera_switch *sw,
-			      enum prestera_cmd_type_t type,
-			      struct prestera_msg_cmd *cmd, size_t clen,
-			      struct prestera_msg_ret *ret, size_t rlen,
-			      int waitms)
-{
-	struct prestera_device *dev = sw->dev;
-	int err;
+struct mvsw_msg_iface {
+	u8 type;
+	u16 vid;
+	u16 vr_id;
+	union {
+		struct {
+			u32 dev;
+			u32 port;
+		};
+		u16 lag_id;
+	};
+} __packed __aligned(4);
 
-	cmd->type = type;
+struct mvsw_msg_rif_cmd {
+	struct mvsw_msg_cmd cmd;
+	struct mvsw_msg_iface iif;
+	u16 rif_id;
+	u8 mac[ETH_ALEN];
+	u32 mtu;
+} __packed __aligned(4);
+
+struct mvsw_msg_rif_ret {
+	struct mvsw_msg_ret ret;
+	u16 rif_id;
+} __packed __aligned(4);
+
+struct mvsw_msg_lpm_cmd {
+	struct mvsw_msg_cmd cmd;
+	u32 grp_id;
+	__be32 dst;
+	u32 dst_len;
+	u16 vr_id;
+} __packed __aligned(4);
+
+struct mvsw_msg_nh {
+	struct mvsw_msg_iface oif;
+	u8 is_active;
+	u32 hw_id;
+	u8 mac[ETH_ALEN];
+} __packed __aligned(4);
+
+struct mvsw_msg_nh_cmd {
+	struct mvsw_msg_cmd cmd;
+	u32 size;
+	u32 grp_id;
+	struct mvsw_msg_nh nh[MVSW_PR_NHGR_SIZE_MAX];
+} __packed __aligned(4);
+
+struct mvsw_msg_nh_ret {
+	struct mvsw_msg_ret ret;
+	struct mvsw_msg_nh nh[MVSW_PR_NHGR_SIZE_MAX];
+} __packed __aligned(4);
+
+struct mvsw_msg_nh_grp_cmd {
+	struct mvsw_msg_cmd cmd;
+	u32 grp_id;
+	u32 size;
+} __packed __aligned(4);
+
+struct mvsw_msg_nh_grp_ret {
+	struct mvsw_msg_ret ret;
+	u32 grp_id;
+} __packed __aligned(4);
+
+struct mvsw_msg_mp_cmd {
+	struct mvsw_msg_cmd cmd;
+	u8 hash_policy;
+} __packed __aligned(4);
+
+struct mvsw_msg_rxtx_cmd {
+	struct mvsw_msg_cmd cmd;
+	u8 use_sdma;
+} __packed __aligned(4);
 
-	err = dev->send_req(dev, cmd, clen, ret, rlen, waitms);
-	if (err)
-		return err;
+struct mvsw_msg_rxtx_ret {
+	struct mvsw_msg_ret ret;
+	u32 map_addr;
+} __packed __aligned(4);
 
-	if (ret->cmd.type != PRESTERA_CMD_TYPE_ACK)
-		return -EBADE;
-	if (ret->status != PRESTERA_CMD_ACK_OK)
-		return -EINVAL;
+struct mvsw_msg_vr_cmd {
+	struct mvsw_msg_cmd cmd;
+	u16 vr_id;
+} __packed __aligned(4);
 
-	return 0;
-}
+struct mvsw_msg_vr_ret {
+	struct mvsw_msg_ret ret;
+	u16 vr_id;
+} __packed __aligned(4);
 
-static int prestera_cmd_ret(struct prestera_switch *sw,
-			    enum prestera_cmd_type_t type,
-			    struct prestera_msg_cmd *cmd, size_t clen,
-			    struct prestera_msg_ret *ret, size_t rlen)
-{
-	return __prestera_cmd_ret(sw, type, cmd, clen, ret, rlen, 0);
-}
+struct mvsw_msg_lag_cmd {
+	struct mvsw_msg_cmd cmd;
+	u32 port;
+	u32 dev;
+	u16 lag_id;
+	u16 vr_id;
+} __packed __aligned(4);
+
+#define fw_check_resp(_response)	\
+({								\
+	int __er = 0;						\
+	typeof(_response) __r = (_response);			\
+	if (__r->ret.cmd.type != MVSW_MSG_TYPE_ACK)		\
+		__er = -EBADE;					\
+	else if (__r->ret.status != MVSW_MSG_ACK_OK)		\
+		__er = -EINVAL;					\
+	(__er);							\
+})
+
+#define __fw_send_req_resp(_switch, _type, _req, _req_size,	\
+_response, _wait)						\
+({								\
+	int __e;						\
+	typeof(_switch) __sw = (_switch);			\
+	typeof(_req) __req = (_req);				\
+	typeof(_response) __resp = (_response);			\
+	__req->cmd.type = (_type);				\
+	__e = __sw->dev->send_req(__sw->dev,			\
+		(u8 *)__req, _req_size,				\
+		(u8 *)__resp, sizeof(*__resp),			\
+		_wait);						\
+	if (!__e)						\
+		__e = fw_check_resp(__resp);			\
+	(__e);							\
+})
+
+#define fw_send_nreq_resp(_sw, _t, _req, _req_size, _resp)	\
+	__fw_send_req_resp(_sw, _t, _req, _req_size, _resp, 0)
+
+#define fw_send_req_resp(_sw, _t, _req, _resp)	\
+	__fw_send_req_resp(_sw, _t, _req, sizeof(*_req), _resp, 0)
+
+#define fw_send_req_resp_wait(_sw, _t, _req, _resp, _wait)	\
+	__fw_send_req_resp(_sw, _t, _req, sizeof(*_req), _resp, _wait)
+
+#define fw_send_req(_sw, _t, _req)	\
+({							\
+	struct mvsw_msg_common_response __re;		\
+	(fw_send_req_resp(_sw, _t, _req, &__re));	\
+})
+
+struct mvsw_fw_event_handler {
+	struct list_head list;
+	enum mvsw_pr_event_type type;
+	void (*func)(struct mvsw_pr_switch *sw,
+		     struct mvsw_pr_event *evt,
+		     void *arg);
+	void *arg;
+};
 
-static int prestera_cmd_ret_wait(struct prestera_switch *sw,
-				 enum prestera_cmd_type_t type,
-				 struct prestera_msg_cmd *cmd, size_t clen,
-				 struct prestera_msg_ret *ret, size_t rlen,
-				 int waitms)
+static int fw_parse_port_evt(u8 *msg, struct mvsw_pr_event *evt)
 {
-	return __prestera_cmd_ret(sw, type, cmd, clen, ret, rlen, waitms);
-}
+	struct mvsw_msg_event_port *hw_evt = (struct mvsw_msg_event_port *)msg;
 
-static int prestera_cmd(struct prestera_switch *sw,
-			enum prestera_cmd_type_t type,
-			struct prestera_msg_cmd *cmd, size_t clen)
-{
-	struct prestera_msg_common_resp resp;
+	evt->port_evt.port_id = hw_evt->port_id;
 
-	return prestera_cmd_ret(sw, type, cmd, clen, &resp.ret, sizeof(resp));
+	if (evt->id == MVSW_PORT_EVENT_STATE_CHANGED) {
+		evt->port_evt.data.oper_state = hw_evt->param.oper_state;
+		evt->port_evt.data.duplex = hw_evt->param.duplex;
+		evt->port_evt.data.speed = hw_evt->param.speed;
+	} else {
+		return -EINVAL;
+	}
+	return 0;
 }
 
-static int prestera_fw_parse_port_evt(void *msg, struct prestera_event *evt)
+static int fw_parse_fdb_evt(u8 *msg, struct mvsw_pr_event *evt)
 {
-	struct prestera_msg_event_port *hw_evt = msg;
+	struct mvsw_msg_event_fdb *hw_evt = (struct mvsw_msg_event_fdb *)msg;
 
-	if (evt->id != PRESTERA_PORT_EVENT_STATE_CHANGED)
+	switch (hw_evt->dest_type) {
+	case MVSW_HW_FDB_ENTRY_TYPE_REG_PORT:
+		evt->fdb_evt.type = MVSW_PR_FDB_ENTRY_TYPE_REG_PORT;
+		evt->fdb_evt.dest.port_id = hw_evt->dest.port_id;
+		break;
+	case MVSW_HW_FDB_ENTRY_TYPE_LAG:
+		evt->fdb_evt.type = MVSW_PR_FDB_ENTRY_TYPE_LAG;
+		evt->fdb_evt.dest.lag_id = hw_evt->dest.lag_id;
+		break;
+	default:
 		return -EINVAL;
+	}
 
-	evt->port_evt.data.oper_state = hw_evt->param.oper_state;
-	evt->port_evt.port_id = hw_evt->port_id;
+	evt->fdb_evt.vid = hw_evt->vid;
+
+	memcpy(&evt->fdb_evt.data, &hw_evt->param, sizeof(u8) * ETH_ALEN);
 
 	return 0;
 }
 
-static int prestera_fw_parse_fdb_evt(void *msg, struct prestera_event *evt)
+static int fw_parse_log_evt(u8 *msg, struct mvsw_pr_event *evt)
 {
-	struct prestera_msg_event_fdb *hw_evt = msg;
-
-	evt->fdb_evt.port_id = hw_evt->port_id;
-	evt->fdb_evt.vid = hw_evt->vid;
+	struct mvsw_msg_event_log *hw_evt = (struct mvsw_msg_event_log *)msg;
 
-	ether_addr_copy(evt->fdb_evt.data.mac, hw_evt->param.mac);
+	evt->fw_log_evt.log_len	= hw_evt->log_string_size;
+	evt->fw_log_evt.data	= hw_evt->log_string;
 
 	return 0;
 }
 
-static struct prestera_fw_evt_parser {
-	int (*func)(void *msg, struct prestera_event *evt);
-} fw_event_parsers[PRESTERA_EVENT_TYPE_MAX] = {
-	[PRESTERA_EVENT_TYPE_PORT] = { .func = prestera_fw_parse_port_evt },
-	[PRESTERA_EVENT_TYPE_FDB] = { .func = prestera_fw_parse_fdb_evt },
+struct mvsw_fw_evt_parser {
+	int (*func)(u8 *msg, struct mvsw_pr_event *evt);
+};
+
+static struct mvsw_fw_evt_parser fw_event_parsers[MVSW_EVENT_TYPE_MAX] = {
+	[MVSW_EVENT_TYPE_PORT] = {.func = fw_parse_port_evt},
+	[MVSW_EVENT_TYPE_FDB] = {.func = fw_parse_fdb_evt},
+	[MVSW_EVENT_TYPE_FW_LOG] = {.func = fw_parse_log_evt}
 };
 
-static struct prestera_fw_event_handler *
-__find_event_handler(const struct prestera_switch *sw,
-		     enum prestera_event_type type)
+static struct mvsw_fw_event_handler *
+__find_event_handler(const struct mvsw_pr_switch *sw,
+		     enum mvsw_pr_event_type type)
 {
-	struct prestera_fw_event_handler *eh;
+	struct mvsw_fw_event_handler *eh;
 
 	list_for_each_entry_rcu(eh, &sw->event_handlers, list) {
 		if (eh->type == type)
@@ -415,11 +647,11 @@ __find_event_handler(const struct prestera_switch *sw,
 	return NULL;
 }
 
-static int prestera_find_event_handler(const struct prestera_switch *sw,
-				       enum prestera_event_type type,
-				       struct prestera_fw_event_handler *eh)
+static int mvsw_find_event_handler(const struct mvsw_pr_switch *sw,
+				   enum mvsw_pr_event_type type,
+				   struct mvsw_fw_event_handler *eh)
 {
-	struct prestera_fw_event_handler *tmp;
+	struct mvsw_fw_event_handler *tmp;
 	int err = 0;
 
 	rcu_read_lock();
@@ -427,827 +659,1592 @@ static int prestera_find_event_handler(const struct prestera_switch *sw,
 	if (tmp)
 		*eh = *tmp;
 	else
-		err = -ENOENT;
+		err = -EEXIST;
 	rcu_read_unlock();
 
 	return err;
 }
 
-static int prestera_evt_recv(struct prestera_device *dev, void *buf, size_t size)
+static int fw_event_recv(struct prestera_device *dev, u8 *buf, size_t size)
 {
-	struct prestera_switch *sw = dev->priv;
-	struct prestera_msg_event *msg = buf;
-	struct prestera_fw_event_handler eh;
-	struct prestera_event evt;
+	struct mvsw_msg_event *msg = (struct mvsw_msg_event *)buf;
+	struct mvsw_pr_switch *sw = dev->priv;
+	struct mvsw_fw_event_handler eh;
+	struct mvsw_pr_event evt;
 	int err;
 
-	if (msg->type >= PRESTERA_EVENT_TYPE_MAX)
+	if (msg->type >= MVSW_EVENT_TYPE_MAX)
 		return -EINVAL;
-	if (!fw_event_parsers[msg->type].func)
-		return -ENOENT;
 
-	err = prestera_find_event_handler(sw, msg->type, &eh);
-	if (err)
-		return err;
+	err = mvsw_find_event_handler(sw, msg->type, &eh);
+
+	if (err || !fw_event_parsers[msg->type].func)
+		return 0;
 
 	evt.id = msg->id;
 
 	err = fw_event_parsers[msg->type].func(buf, &evt);
-	if (err)
-		return err;
+	if (!err)
+		eh.func(sw, &evt, eh.arg);
 
-	eh.func(sw, &evt, eh.arg);
-
-	return 0;
+	return err;
 }
 
-static void prestera_pkt_recv(struct prestera_device *dev)
+static void fw_pkt_recv(struct prestera_device *dev)
 {
-	struct prestera_switch *sw = dev->priv;
-	struct prestera_fw_event_handler eh;
-	struct prestera_event ev;
+	struct mvsw_pr_switch *sw = dev->priv;
+	struct mvsw_fw_event_handler eh;
+	struct mvsw_pr_event ev;
 	int err;
 
-	ev.id = PRESTERA_RXTX_EVENT_RCV_PKT;
+	ev.id = MVSW_RXTX_EVENT_RCV_PKT;
 
-	err = prestera_find_event_handler(sw, PRESTERA_EVENT_TYPE_RXTX, &eh);
+	err = mvsw_find_event_handler(sw, MVSW_EVENT_TYPE_RXTX, &eh);
 	if (err)
 		return;
 
 	eh.func(sw, &ev, eh.arg);
 }
 
-int prestera_hw_port_info_get(const struct prestera_port *port,
-			      u32 *dev_id, u32 *hw_id, u16 *fp_id)
+static struct mvsw_msg_buff *mvsw_msg_buff_create(u16 head_size)
 {
-	struct prestera_msg_port_info_req req = {
-		.port = port->id,
-	};
-	struct prestera_msg_port_info_resp resp;
+	struct mvsw_msg_buff *msg_buff;
+
+	msg_buff = kzalloc(sizeof(*msg_buff), GFP_KERNEL);
+	if (!msg_buff)
+		return NULL;
+
+	msg_buff->data = kzalloc(MVSW_PR_MSG_BUFF_CHUNK_SIZE + head_size,
+				 GFP_KERNEL);
+	if (!msg_buff->data) {
+		kfree(msg_buff);
+		return NULL;
+	}
+
+	msg_buff->total = MVSW_PR_MSG_BUFF_CHUNK_SIZE + head_size;
+	msg_buff->free = MVSW_PR_MSG_BUFF_CHUNK_SIZE;
+	msg_buff->used = head_size;
+
+	return msg_buff;
+}
+
+static void *mvsw_msg_buff_data(struct mvsw_msg_buff *msg_buff)
+{
+	return msg_buff->data;
+}
+
+static u32 mvsw_msg_buff_size(const struct mvsw_msg_buff *msg_buff)
+{
+	return msg_buff->used;
+}
+
+static int mvsw_msg_buff_resize(struct mvsw_msg_buff *msg_buff)
+{
+	void *data;
+
+	data = krealloc(msg_buff->data,
+			msg_buff->total + MVSW_PR_MSG_BUFF_CHUNK_SIZE,
+			GFP_KERNEL);
+	if (!data)
+		return -ENOMEM;
+
+	msg_buff->total += MVSW_PR_MSG_BUFF_CHUNK_SIZE;
+	msg_buff->free += MVSW_PR_MSG_BUFF_CHUNK_SIZE;
+	msg_buff->data = data;
+
+	return 0;
+}
+
+static int mvsw_msg_buff_put(struct mvsw_msg_buff *msg_buff,
+			     void *data, u16 size)
+{
+	void *data_ptr;
 	int err;
 
-	err = prestera_cmd_ret(port->sw, PRESTERA_CMD_TYPE_PORT_INFO_GET,
-			       &req.cmd, sizeof(req), &resp.ret, sizeof(resp));
-	if (err)
-		return err;
+	if (size > msg_buff->free) {
+		err = mvsw_msg_buff_resize(msg_buff);
+		if (err)
+			return err;
+	}
+	/* point to unused data */
+	data_ptr = msg_buff->data + msg_buff->used;
 
-	*dev_id = resp.dev_id;
-	*hw_id = resp.hw_id;
-	*fp_id = resp.fp_id;
+	/* set the data */
+	memcpy(data_ptr, data, size);
+	msg_buff->used += size;
+	msg_buff->free -= size;
 
 	return 0;
 }
 
-int prestera_hw_switch_mac_set(struct prestera_switch *sw, const char *mac)
+static int mvsw_msg_buff_terminate(struct mvsw_msg_buff *msg_buff)
 {
-	struct prestera_msg_switch_attr_req req = {
-		.attr = PRESTERA_CMD_SWITCH_ATTR_MAC,
-	};
+	u16 padding_size;
+	void *data_ptr;
+	int err;
+
+	/* the data should be aligned to 4 byte, so calculate
+	 * the padding leaving at least one byte for termination
+	 */
+	padding_size = ALIGN(msg_buff->used + sizeof(u8),
+			     sizeof(u32)) - msg_buff->used;
+	if (msg_buff->free < padding_size) {
+		err = mvsw_msg_buff_resize(msg_buff);
+		if (err)
+			return err;
+	}
+	/* point to unused data */
+	data_ptr = msg_buff->data + msg_buff->used;
 
-	ether_addr_copy(req.param.mac, mac);
+	/* terminate buffer by zero byte */
+	memset(data_ptr, 0, padding_size);
+	msg_buff->used += padding_size;
+	msg_buff->free -= padding_size;
+	data_ptr += padding_size;
 
-	return prestera_cmd(sw, PRESTERA_CMD_TYPE_SWITCH_ATTR_SET,
-			    &req.cmd, sizeof(req));
+	return 0;
+}
+
+static void mvsw_msg_buff_destroy(struct mvsw_msg_buff *msg_buff)
+{
+	kfree(msg_buff->data);
+	kfree(msg_buff);
 }
 
-int prestera_hw_switch_init(struct prestera_switch *sw)
+int mvsw_pr_hw_port_info_get(const struct mvsw_pr_port *port,
+			     u16 *fp_id, u32 *hw_id, u32 *dev_id)
 {
-	struct prestera_msg_switch_init_resp resp;
-	struct prestera_msg_common_req req;
+	struct mvsw_msg_port_info_ret resp;
+	struct mvsw_msg_port_info_cmd req = {
+		.port = port->id
+	};
 	int err;
 
+	err = fw_send_req_resp(port->sw, MVSW_MSG_TYPE_PORT_INFO_GET,
+			       &req, &resp);
+	if (err)
+		return err;
+
+	*hw_id = resp.hw_id;
+	*dev_id = resp.dev_id;
+	*fp_id = resp.fp_id;
+
+	return 0;
+}
+
+int mvsw_pr_hw_switch_init(struct mvsw_pr_switch *sw)
+{
+	struct mvsw_msg_switch_init_ret resp;
+	struct mvsw_msg_common_request req;
+	int err = 0;
+
 	INIT_LIST_HEAD(&sw->event_handlers);
 
-	err = prestera_cmd_ret_wait(sw, PRESTERA_CMD_TYPE_SWITCH_INIT,
-				    &req.cmd, sizeof(req),
-				    &resp.ret, sizeof(resp),
-				    PRESTERA_SWITCH_INIT_TIMEOUT_MS);
+	err = fw_send_req_resp_wait(sw, MVSW_MSG_TYPE_SWITCH_INIT, &req, &resp,
+				    MVSW_PR_INIT_TIMEOUT);
 	if (err)
 		return err;
 
-	sw->dev->recv_msg = prestera_evt_recv;
-	sw->dev->recv_pkt = prestera_pkt_recv;
+	sw->id = resp.switch_id;
 	sw->port_count = resp.port_count;
-	sw->mtu_min = PRESTERA_MIN_MTU;
+	sw->mtu_min = MVSW_PR_MIN_MTU;
 	sw->mtu_max = resp.mtu_max;
-	sw->id = resp.switch_id;
+	sw->lag_max = resp.lag_max;
+	sw->lag_member_max = resp.lag_member_max;
+	sw->dev->recv_msg = fw_event_recv;
+	sw->dev->recv_pkt = fw_pkt_recv;
 
-	return 0;
+	return err;
 }
 
-void prestera_hw_switch_fini(struct prestera_switch *sw)
+int mvsw_pr_hw_switch_ageing_set(const struct mvsw_pr_switch *sw,
+				 u32 ageing_time)
 {
-	WARN_ON(!list_empty(&sw->event_handlers));
+	struct mvsw_msg_switch_attr_cmd req = {
+		.param = {.ageing_timeout = ageing_time},
+		.attr = MVSW_MSG_SWITCH_ATTR_AGEING,
+	};
+
+	return fw_send_req(sw, MVSW_MSG_TYPE_SWITCH_ATTR_SET, &req);
 }
 
-int prestera_hw_switch_ageing_set(struct prestera_switch *sw, u32 ageing_ms)
+int mvsw_pr_hw_switch_mac_set(const struct mvsw_pr_switch *sw, const u8 *mac)
 {
-	struct prestera_msg_switch_attr_req req = {
-		.attr = PRESTERA_CMD_SWITCH_ATTR_AGEING,
-		.param = {
-			.ageing_timeout_ms = ageing_ms,
-		},
+	struct mvsw_msg_switch_attr_cmd req = {
+		.attr = MVSW_MSG_SWITCH_ATTR_MAC,
 	};
 
-	return prestera_cmd(sw, PRESTERA_CMD_TYPE_SWITCH_ATTR_SET,
-			    &req.cmd, sizeof(req));
+	memcpy(req.param.mac, mac, sizeof(req.param.mac));
+
+	return fw_send_req(sw, MVSW_MSG_TYPE_SWITCH_ATTR_SET, &req);
 }
 
-int prestera_hw_port_state_set(const struct prestera_port *port,
-			       bool admin_state)
+int mvsw_pr_hw_switch_trap_policer_set(const struct mvsw_pr_switch *sw,
+				       u8 profile)
 {
-	struct prestera_msg_port_attr_req req = {
-		.attr = PRESTERA_CMD_PORT_ATTR_ADMIN_STATE,
-		.port = port->hw_id,
-		.dev = port->dev_id,
-		.param = {
-			.admin_state = admin_state,
-		}
+	struct mvsw_msg_switch_attr_cmd req = {
+		.param = {.trap_policer_profile = profile},
+		.attr = MVSW_MSG_SWITCH_ATTR_TRAP_POLICER,
 	};
 
-	return prestera_cmd(port->sw, PRESTERA_CMD_TYPE_PORT_ATTR_SET,
-			    &req.cmd, sizeof(req));
+	return fw_send_req(sw, MVSW_MSG_TYPE_SWITCH_ATTR_SET, &req);
 }
 
-int prestera_hw_port_mtu_set(const struct prestera_port *port, u32 mtu)
+int mvsw_pr_hw_port_state_set(const struct mvsw_pr_port *port,
+			      bool admin_state)
 {
-	struct prestera_msg_port_attr_req req = {
-		.attr = PRESTERA_CMD_PORT_ATTR_MTU,
+	struct mvsw_msg_port_attr_cmd req = {
+		.attr = MVSW_MSG_PORT_ATTR_ADMIN_STATE,
 		.port = port->hw_id,
 		.dev = port->dev_id,
-		.param = {
-			.mtu = mtu,
-		}
+		.param = {.admin_state = admin_state ? 1 : 0}
 	};
 
-	return prestera_cmd(port->sw, PRESTERA_CMD_TYPE_PORT_ATTR_SET,
-			    &req.cmd, sizeof(req));
+	return fw_send_req(port->sw, MVSW_MSG_TYPE_PORT_ATTR_SET, &req);
 }
 
-int prestera_hw_port_mac_set(const struct prestera_port *port, const char *mac)
+int mvsw_pr_hw_port_state_get(const struct mvsw_pr_port *port,
+			      bool *admin_state, bool *oper_state)
 {
-	struct prestera_msg_port_attr_req req = {
-		.attr = PRESTERA_CMD_PORT_ATTR_MAC,
+	struct mvsw_msg_port_attr_ret resp;
+	struct mvsw_msg_port_attr_cmd req = {
 		.port = port->hw_id,
-		.dev = port->dev_id,
+		.dev = port->dev_id
 	};
+	int err;
+
+	if (admin_state) {
+		req.attr = MVSW_MSG_PORT_ATTR_ADMIN_STATE;
+		err = fw_send_req_resp(port->sw, MVSW_MSG_TYPE_PORT_ATTR_GET,
+				       &req, &resp);
+		if (err)
+			return err;
+		*admin_state = resp.param.admin_state != 0;
+	}
 
-	ether_addr_copy(req.param.mac, mac);
+	if (oper_state) {
+		req.attr = MVSW_MSG_PORT_ATTR_OPER_STATE;
+		err = fw_send_req_resp(port->sw, MVSW_MSG_TYPE_PORT_ATTR_GET,
+				       &req, &resp);
+		if (err)
+			return err;
+		*oper_state = resp.param.oper_state != 0;
+	}
 
-	return prestera_cmd(port->sw, PRESTERA_CMD_TYPE_PORT_ATTR_SET,
-			    &req.cmd, sizeof(req));
+	return 0;
 }
 
-int prestera_hw_port_accept_frm_type(struct prestera_port *port,
-				     enum prestera_accept_frm_type type)
+int mvsw_pr_hw_port_mtu_set(const struct mvsw_pr_port *port, u32 mtu)
 {
-	struct prestera_msg_port_attr_req req = {
-		.attr = PRESTERA_CMD_PORT_ATTR_ACCEPT_FRAME_TYPE,
+	struct mvsw_msg_port_attr_cmd req = {
+		.attr = MVSW_MSG_PORT_ATTR_MTU,
 		.port = port->hw_id,
 		.dev = port->dev_id,
-		.param = {
-			.accept_frm_type = type,
-		}
+		.param = {.mtu = mtu}
 	};
 
-	return prestera_cmd(port->sw, PRESTERA_CMD_TYPE_PORT_ATTR_SET,
-			    &req.cmd, sizeof(req));
+	return fw_send_req(port->sw, MVSW_MSG_TYPE_PORT_ATTR_SET, &req);
 }
 
-int prestera_hw_port_cap_get(const struct prestera_port *port,
-			     struct prestera_port_caps *caps)
+int mvsw_pr_hw_port_mtu_get(const struct mvsw_pr_port *port, u32 *mtu)
 {
-	struct prestera_msg_port_attr_req req = {
-		.attr = PRESTERA_CMD_PORT_ATTR_CAPABILITY,
+	struct mvsw_msg_port_attr_ret resp;
+	struct mvsw_msg_port_attr_cmd req = {
+		.attr = MVSW_MSG_PORT_ATTR_MTU,
 		.port = port->hw_id,
-		.dev = port->dev_id,
+		.dev = port->dev_id
 	};
-	struct prestera_msg_port_attr_resp resp;
 	int err;
 
-	err = prestera_cmd_ret(port->sw, PRESTERA_CMD_TYPE_PORT_ATTR_GET,
-			       &req.cmd, sizeof(req), &resp.ret, sizeof(resp));
+	err = fw_send_req_resp(port->sw, MVSW_MSG_TYPE_PORT_ATTR_GET,
+			       &req, &resp);
 	if (err)
 		return err;
 
-	caps->supp_link_modes = resp.param.cap.link_mode;
-	caps->transceiver = resp.param.cap.transceiver;
-	caps->supp_fec = resp.param.cap.fec;
-	caps->type = resp.param.cap.type;
+	*mtu = resp.param.mtu;
 
 	return err;
 }
 
-int prestera_hw_port_remote_cap_get(const struct prestera_port *port,
-				    u64 *link_mode_bitmap)
+int mvsw_pr_hw_port_mac_set(const struct mvsw_pr_port *port, char *mac)
 {
-	struct prestera_msg_port_attr_req req = {
-		.attr = PRESTERA_CMD_PORT_ATTR_REMOTE_CAPABILITY,
+	struct mvsw_msg_port_attr_cmd req = {
+		.attr = MVSW_MSG_PORT_ATTR_MAC,
 		.port = port->hw_id,
-		.dev = port->dev_id,
+		.dev = port->dev_id
 	};
-	struct prestera_msg_port_attr_resp resp;
-	int err;
+	memcpy(&req.param.mac, mac, sizeof(req.param.mac));
 
-	err = prestera_cmd_ret(port->sw, PRESTERA_CMD_TYPE_PORT_ATTR_GET,
-			       &req.cmd, sizeof(req), &resp.ret, sizeof(resp));
-	if (err)
-		return err;
-
-	*link_mode_bitmap = resp.param.cap.link_mode;
-
-	return 0;
+	return fw_send_req(port->sw, MVSW_MSG_TYPE_PORT_ATTR_SET, &req);
 }
 
-int prestera_hw_port_remote_fc_get(const struct prestera_port *port,
-				   bool *pause, bool *asym_pause)
+int mvsw_pr_hw_port_mac_get(const struct mvsw_pr_port *port, char *mac)
 {
-	struct prestera_msg_port_attr_req req = {
-		.attr = PRESTERA_CMD_PORT_ATTR_REMOTE_FC,
+	struct mvsw_msg_port_attr_ret resp;
+	struct mvsw_msg_port_attr_cmd req = {
+		.attr = MVSW_MSG_PORT_ATTR_MAC,
 		.port = port->hw_id,
-		.dev = port->dev_id,
+		.dev = port->dev_id
 	};
-	struct prestera_msg_port_attr_resp resp;
 	int err;
 
-	err = prestera_cmd_ret(port->sw, PRESTERA_CMD_TYPE_PORT_ATTR_GET,
-			       &req.cmd, sizeof(req), &resp.ret, sizeof(resp));
+	err = fw_send_req_resp(port->sw, MVSW_MSG_TYPE_PORT_ATTR_GET,
+			       &req, &resp);
 	if (err)
 		return err;
 
-	switch (resp.param.fc) {
-	case PRESTERA_FC_SYMMETRIC:
-		*pause = true;
-		*asym_pause = false;
-		break;
-	case PRESTERA_FC_ASYMMETRIC:
-		*pause = false;
-		*asym_pause = true;
-		break;
-	case PRESTERA_FC_SYMM_ASYMM:
-		*pause = true;
-		*asym_pause = true;
-		break;
-	default:
-		*pause = false;
-		*asym_pause = false;
-	}
+	memcpy(mac, resp.param.mac, sizeof(resp.param.mac));
 
-	return 0;
+	return err;
 }
 
-int prestera_hw_port_type_get(const struct prestera_port *port, u8 *type)
+int mvsw_pr_hw_port_accept_frame_type_set(const struct mvsw_pr_port *port,
+					  enum mvsw_pr_accept_frame_type type)
 {
-	struct prestera_msg_port_attr_req req = {
-		.attr = PRESTERA_CMD_PORT_ATTR_TYPE,
+	struct mvsw_msg_port_attr_cmd req = {
+		.attr = MVSW_MSG_PORT_ATTR_ACCEPT_FRAME_TYPE,
 		.port = port->hw_id,
 		.dev = port->dev_id,
+		.param = {.accept_frm_type = type}
 	};
-	struct prestera_msg_port_attr_resp resp;
-	int err;
 
-	err = prestera_cmd_ret(port->sw, PRESTERA_CMD_TYPE_PORT_ATTR_GET,
-			       &req.cmd, sizeof(req), &resp.ret, sizeof(resp));
-	if (err)
-		return err;
-
-	*type = resp.param.type;
-
-	return 0;
+	return fw_send_req(port->sw, MVSW_MSG_TYPE_PORT_ATTR_SET, &req);
 }
 
-int prestera_hw_port_fec_get(const struct prestera_port *port, u8 *fec)
+int mvsw_pr_hw_port_learning_set(const struct mvsw_pr_port *port, bool enable)
 {
-	struct prestera_msg_port_attr_req req = {
-		.attr = PRESTERA_CMD_PORT_ATTR_FEC,
+	struct mvsw_msg_port_attr_cmd req = {
+		.attr = MVSW_MSG_PORT_ATTR_LEARNING,
 		.port = port->hw_id,
 		.dev = port->dev_id,
+		.param = {.learning = enable ? 1 : 0}
 	};
-	struct prestera_msg_port_attr_resp resp;
-	int err;
-
-	err = prestera_cmd_ret(port->sw, PRESTERA_CMD_TYPE_PORT_ATTR_GET,
-			       &req.cmd, sizeof(req), &resp.ret, sizeof(resp));
-	if (err)
-		return err;
-
-	*fec = resp.param.fec;
 
-	return 0;
+	return fw_send_req(port->sw, MVSW_MSG_TYPE_PORT_ATTR_SET, &req);
 }
 
-int prestera_hw_port_fec_set(const struct prestera_port *port, u8 fec)
+int mvsw_pr_hw_event_handler_register(struct mvsw_pr_switch *sw,
+				      enum mvsw_pr_event_type type,
+				      void (*cb)(struct mvsw_pr_switch *sw,
+						 struct mvsw_pr_event *evt,
+						 void *arg),
+				      void *arg)
 {
-	struct prestera_msg_port_attr_req req = {
-		.attr = PRESTERA_CMD_PORT_ATTR_FEC,
-		.port = port->hw_id,
-		.dev = port->dev_id,
-		.param = {
-			.fec = fec,
-		}
-	};
-
-	return prestera_cmd(port->sw, PRESTERA_CMD_TYPE_PORT_ATTR_SET,
-			    &req.cmd, sizeof(req));
-}
+	struct mvsw_fw_event_handler *eh;
 
-static u8 prestera_hw_mdix_to_eth(u8 mode)
-{
-	switch (mode) {
-	case PRESTERA_PORT_TP_MDI:
-		return ETH_TP_MDI;
-	case PRESTERA_PORT_TP_MDIX:
-		return ETH_TP_MDI_X;
-	case PRESTERA_PORT_TP_AUTO:
-		return ETH_TP_MDI_AUTO;
-	default:
-		return ETH_TP_MDI_INVALID;
-	}
+	eh = __find_event_handler(sw, type);
+	if (eh)
+		return -EEXIST;
+	eh = kmalloc(sizeof(*eh), GFP_KERNEL);
+	if (!eh)
+		return -ENOMEM;
+
+	eh->type = type;
+	eh->func = cb;
+	eh->arg = arg;
+
+	INIT_LIST_HEAD(&eh->list);
+
+	list_add_rcu(&eh->list, &sw->event_handlers);
+
+	return 0;
 }
 
-static u8 prestera_hw_mdix_from_eth(u8 mode)
+void mvsw_pr_hw_event_handler_unregister(struct mvsw_pr_switch *sw,
+					 enum mvsw_pr_event_type type)
 {
-	switch (mode) {
-	case ETH_TP_MDI:
-		return PRESTERA_PORT_TP_MDI;
-	case ETH_TP_MDI_X:
-		return PRESTERA_PORT_TP_MDIX;
-	case ETH_TP_MDI_AUTO:
-		return PRESTERA_PORT_TP_AUTO;
-	default:
-		return PRESTERA_PORT_TP_NA;
-	}
+	struct mvsw_fw_event_handler *eh;
+
+	eh = __find_event_handler(sw, type);
+	if (!eh)
+		return;
+
+	list_del_rcu(&eh->list);
+	synchronize_rcu();
+	kfree(eh);
 }
 
-int prestera_hw_port_mdix_get(const struct prestera_port *port, u8 *status,
-			      u8 *admin_mode)
+int mvsw_pr_hw_vlan_create(const struct mvsw_pr_switch *sw, u16 vid)
 {
-	struct prestera_msg_port_attr_req req = {
-		.attr = PRESTERA_CMD_PORT_ATTR_MDIX,
+	struct mvsw_msg_vlan_cmd req = {
+		.vid = vid,
+	};
+
+	return fw_send_req(sw, MVSW_MSG_TYPE_VLAN_CREATE, &req);
+}
+
+int mvsw_pr_hw_vlan_delete(const struct mvsw_pr_switch *sw, u16 vid)
+{
+	struct mvsw_msg_vlan_cmd req = {
+		.vid = vid,
+	};
+
+	return fw_send_req(sw, MVSW_MSG_TYPE_VLAN_DELETE, &req);
+}
+
+int mvsw_pr_hw_vlan_port_set(const struct mvsw_pr_port *port,
+			     u16 vid, bool is_member, bool untagged)
+{
+	struct mvsw_msg_vlan_cmd req = {
 		.port = port->hw_id,
 		.dev = port->dev_id,
+		.vid = vid,
+		.is_member = is_member ? 1 : 0,
+		.is_tagged = untagged ? 0 : 1
+	};
+
+	return fw_send_req(port->sw, MVSW_MSG_TYPE_VLAN_PORT_SET, &req);
+}
+
+int mvsw_pr_hw_vlan_port_vid_set(const struct mvsw_pr_port *port, u16 vid)
+{
+	struct mvsw_msg_vlan_cmd req = {
+		.port = port->hw_id,
+		.dev = port->dev_id,
+		.vid = vid
+	};
+
+	return fw_send_req(port->sw, MVSW_MSG_TYPE_VLAN_PVID_SET, &req);
+}
+
+int mvsw_pr_hw_port_vid_stp_set(struct mvsw_pr_port *port, u16 vid, u8 state)
+{
+	struct mvsw_msg_stp_cmd req = {
+		.port = port->hw_id,
+		.dev = port->dev_id,
+		.vid = vid,
+		.state = state
+	};
+
+	return fw_send_req(port->sw, MVSW_MSG_TYPE_STP_PORT_SET, &req);
+}
+
+int mvsw_pr_hw_port_speed_get(const struct mvsw_pr_port *port, u32 *speed)
+{
+	struct mvsw_msg_port_attr_ret resp;
+	struct mvsw_msg_port_attr_cmd req = {
+		.attr = MVSW_MSG_PORT_ATTR_SPEED,
+		.port = port->hw_id,
+		.dev = port->dev_id
 	};
-	struct prestera_msg_port_attr_resp resp;
 	int err;
 
-	err = prestera_cmd_ret(port->sw, PRESTERA_CMD_TYPE_PORT_ATTR_GET,
-			       &req.cmd, sizeof(req), &resp.ret, sizeof(resp));
+	err = fw_send_req_resp(port->sw, MVSW_MSG_TYPE_PORT_ATTR_GET,
+			       &req, &resp);
 	if (err)
 		return err;
 
-	*status = prestera_hw_mdix_to_eth(resp.param.mdix.status);
-	*admin_mode = prestera_hw_mdix_to_eth(resp.param.mdix.admin_mode);
+	*speed = resp.param.speed;
 
-	return 0;
+	return err;
 }
 
-int prestera_hw_port_mdix_set(const struct prestera_port *port, u8 mode)
+int mvsw_pr_hw_port_uc_flood_set(const struct mvsw_pr_port *port, bool flood)
 {
-	struct prestera_msg_port_attr_req req = {
-		.attr = PRESTERA_CMD_PORT_ATTR_MDIX,
+	struct mvsw_msg_port_attr_cmd req = {
+		.attr = MVSW_MSG_PORT_ATTR_FLOOD,
 		.port = port->hw_id,
 		.dev = port->dev_id,
+		.param = {
+			.flood = {
+				.type = MVSW_PORT_FLOOD_TYPE_UC,
+				.enable = flood ? 1 : 0,
+			}
+		}
 	};
 
-	req.param.mdix.admin_mode = prestera_hw_mdix_from_eth(mode);
-
-	return prestera_cmd(port->sw, PRESTERA_CMD_TYPE_PORT_ATTR_SET,
-			    &req.cmd, sizeof(req));
+	return fw_send_req(port->sw, MVSW_MSG_TYPE_PORT_ATTR_SET, &req);
 }
 
-int prestera_hw_port_link_mode_set(const struct prestera_port *port, u32 mode)
+int mvsw_pr_hw_port_mc_flood_set(const struct mvsw_pr_port *port, bool flood)
 {
-	struct prestera_msg_port_attr_req req = {
-		.attr = PRESTERA_CMD_PORT_ATTR_LINK_MODE,
+	struct mvsw_msg_port_attr_cmd req = {
+		.attr = MVSW_MSG_PORT_ATTR_FLOOD,
 		.port = port->hw_id,
 		.dev = port->dev_id,
 		.param = {
-			.link_mode = mode,
+			.flood = {
+				.type = MVSW_PORT_FLOOD_TYPE_MC,
+				.enable = flood ? 1 : 0,
+			}
 		}
 	};
 
-	return prestera_cmd(port->sw, PRESTERA_CMD_TYPE_PORT_ATTR_SET,
-			    &req.cmd, sizeof(req));
+	return fw_send_req(port->sw, MVSW_MSG_TYPE_PORT_ATTR_SET, &req);
+}
+
+int mvsw_pr_hw_fdb_add(const struct mvsw_pr_port *port,
+		       const unsigned char *mac, u16 vid, bool dynamic)
+{
+	struct mvsw_msg_fdb_cmd req = {
+		.dest_type = MVSW_HW_FDB_ENTRY_TYPE_REG_PORT,
+		.dest = {
+			.dev = port->dev_id,
+			.port = port->hw_id,
+		},
+		.vid = vid,
+		.dynamic = dynamic ? 1 : 0
+	};
+
+	memcpy(req.mac, mac, sizeof(req.mac));
+
+	return fw_send_req(port->sw, MVSW_MSG_TYPE_FDB_ADD, &req);
 }
 
-int prestera_hw_port_link_mode_get(const struct prestera_port *port, u32 *mode)
+int mvsw_pr_hw_lag_fdb_add(const struct mvsw_pr_switch *sw, u16 lag_id,
+			   const unsigned char *mac, u16 vid, bool dynamic)
 {
-	struct prestera_msg_port_attr_req req = {
-		.attr = PRESTERA_CMD_PORT_ATTR_LINK_MODE,
+	struct mvsw_msg_fdb_cmd req = {
+		.dest_type = MVSW_HW_FDB_ENTRY_TYPE_LAG,
+		.dest = { .lag_id = lag_id },
+		.vid = vid,
+		.dynamic = dynamic
+	};
+
+	memcpy(req.mac, mac, sizeof(req.mac));
+
+	return fw_send_req(sw, MVSW_MSG_TYPE_FDB_ADD, &req);
+}
+
+int mvsw_pr_hw_fdb_del(const struct mvsw_pr_port *port,
+		       const unsigned char *mac, u16 vid)
+{
+	struct mvsw_msg_fdb_cmd req = {
+		.dest_type = MVSW_HW_FDB_ENTRY_TYPE_REG_PORT,
+		.dest = {
+			.dev = port->dev_id,
+			.port = port->hw_id,
+		},
+		.vid = vid
+	};
+
+	memcpy(req.mac, mac, sizeof(req.mac));
+
+	return fw_send_req(port->sw, MVSW_MSG_TYPE_FDB_DELETE, &req);
+}
+
+int mvsw_pr_hw_lag_fdb_del(const struct mvsw_pr_switch *sw, u16 lag_id,
+			   const unsigned char *mac, u16 vid)
+{
+	struct mvsw_msg_fdb_cmd req = {
+		.dest_type = MVSW_HW_FDB_ENTRY_TYPE_LAG,
+		.dest = { .lag_id = lag_id },
+		.vid = vid
+	};
+
+	memcpy(req.mac, mac, sizeof(req.mac));
+
+	return fw_send_req(sw, MVSW_MSG_TYPE_FDB_DELETE, &req);
+}
+
+int mvsw_pr_hw_port_cap_get(const struct mvsw_pr_port *port,
+			    struct mvsw_pr_port_caps *caps)
+{
+	struct mvsw_msg_port_attr_ret resp;
+	struct mvsw_msg_port_attr_cmd req = {
+		.attr = MVSW_MSG_PORT_ATTR_CAPABILITY,
 		.port = port->hw_id,
-		.dev = port->dev_id,
+		.dev = port->dev_id
 	};
-	struct prestera_msg_port_attr_resp resp;
 	int err;
 
-	err = prestera_cmd_ret(port->sw, PRESTERA_CMD_TYPE_PORT_ATTR_GET,
-			       &req.cmd, sizeof(req), &resp.ret, sizeof(resp));
+	err = fw_send_req_resp(port->sw, MVSW_MSG_TYPE_PORT_ATTR_GET,
+			       &req, &resp);
 	if (err)
 		return err;
 
-	*mode = resp.param.link_mode;
+	caps->supp_link_modes = resp.param.cap.link_mode;
+	caps->supp_fec = resp.param.cap.fec;
+	caps->type = resp.param.cap.type;
+	caps->transceiver = resp.param.cap.transceiver;
 
-	return 0;
+	return err;
 }
 
-int prestera_hw_port_speed_get(const struct prestera_port *port, u32 *speed)
+int mvsw_pr_hw_port_remote_cap_get(const struct mvsw_pr_port *port,
+				   u64 *link_mode_bitmap)
 {
-	struct prestera_msg_port_attr_req req = {
-		.attr = PRESTERA_CMD_PORT_ATTR_SPEED,
+	struct mvsw_msg_port_attr_ret resp;
+	struct mvsw_msg_port_attr_cmd req = {
+		.attr = MVSW_MSG_PORT_ATTR_REMOTE_CAPABILITY,
 		.port = port->hw_id,
-		.dev = port->dev_id,
+		.dev = port->dev_id
 	};
-	struct prestera_msg_port_attr_resp resp;
 	int err;
 
-	err = prestera_cmd_ret(port->sw, PRESTERA_CMD_TYPE_PORT_ATTR_GET,
-			       &req.cmd, sizeof(req), &resp.ret, sizeof(resp));
+	err = fw_send_req_resp(port->sw, MVSW_MSG_TYPE_PORT_ATTR_GET,
+			       &req, &resp);
 	if (err)
 		return err;
 
-	*speed = resp.param.speed;
+	*link_mode_bitmap = resp.param.cap.link_mode;
+
+	return err;
+}
+
+static u8 mvsw_mdix_to_eth(u8 mode)
+{
+	switch (mode) {
+	case MVSW_PORT_TP_MDI:
+		return ETH_TP_MDI;
+	case MVSW_PORT_TP_MDIX:
+		return ETH_TP_MDI_X;
+	case MVSW_PORT_TP_AUTO:
+		return ETH_TP_MDI_AUTO;
+	}
+
+	return ETH_TP_MDI_INVALID;
+}
+
+static u8 mvsw_mdix_from_eth(u8 mode)
+{
+	switch (mode) {
+	case ETH_TP_MDI:
+		return MVSW_PORT_TP_MDI;
+	case ETH_TP_MDI_X:
+		return MVSW_PORT_TP_MDIX;
+	case ETH_TP_MDI_AUTO:
+		return MVSW_PORT_TP_AUTO;
+	}
+
+	return MVSW_PORT_TP_NA;
+}
+
+int mvsw_pr_hw_port_mdix_get(const struct mvsw_pr_port *port, u8 *status,
+			     u8 *admin_mode)
+{
+	struct mvsw_msg_port_attr_ret resp;
+	struct mvsw_msg_port_attr_cmd req = {
+		.attr = MVSW_MSG_PORT_ATTR_MDIX,
+		.port = port->hw_id,
+		.dev = port->dev_id
+	};
+	int err;
+
+	err = fw_send_req_resp(port->sw, MVSW_MSG_TYPE_PORT_ATTR_GET,
+			       &req, &resp);
+	if (err)
+		return err;
+
+	*status = mvsw_mdix_to_eth(resp.param.mdix.status);
+	*admin_mode = mvsw_mdix_to_eth(resp.param.mdix.admin_mode);
 
 	return 0;
 }
 
-int prestera_hw_port_autoneg_set(const struct prestera_port *port,
-				 bool autoneg, u64 link_modes, u8 fec)
+int mvsw_pr_hw_port_mdix_set(const struct mvsw_pr_port *port, u8 mode)
 {
-	struct prestera_msg_port_attr_req req = {
-		.attr = PRESTERA_CMD_PORT_ATTR_AUTONEG,
+	struct mvsw_msg_port_attr_cmd req = {
+		.attr = MVSW_MSG_PORT_ATTR_MDIX,
 		.port = port->hw_id,
-		.dev = port->dev_id,
-		.param = {
-			.autoneg = {
-				.link_mode = link_modes,
-				.enable = autoneg,
-				.fec = fec,
-			}
-		}
+		.dev = port->dev_id
 	};
 
-	return prestera_cmd(port->sw, PRESTERA_CMD_TYPE_PORT_ATTR_SET,
-			    &req.cmd, sizeof(req));
+	req.param.mdix.admin_mode = mvsw_mdix_from_eth(mode);
+
+	return fw_send_req(port->sw, MVSW_MSG_TYPE_PORT_ATTR_SET, &req);
 }
 
-int prestera_hw_port_autoneg_restart(struct prestera_port *port)
+int mvsw_pr_hw_port_type_get(const struct mvsw_pr_port *port, u8 *type)
 {
-	struct prestera_msg_port_attr_req req = {
-		.attr = PRESTERA_CMD_PORT_ATTR_AUTONEG_RESTART,
+	struct mvsw_msg_port_attr_ret resp;
+	struct mvsw_msg_port_attr_cmd req = {
+		.attr = MVSW_MSG_PORT_ATTR_TYPE,
 		.port = port->hw_id,
-		.dev = port->dev_id,
+		.dev = port->dev_id
 	};
+	int err;
 
-	return prestera_cmd(port->sw, PRESTERA_CMD_TYPE_PORT_ATTR_SET,
-			    &req.cmd, sizeof(req));
+	err = fw_send_req_resp(port->sw, MVSW_MSG_TYPE_PORT_ATTR_GET,
+			       &req, &resp);
+	if (err)
+		return err;
+
+	*type = resp.param.type;
+
+	return err;
 }
 
-int prestera_hw_port_duplex_get(const struct prestera_port *port, u8 *duplex)
+int mvsw_pr_hw_port_fec_get(const struct mvsw_pr_port *port, u8 *fec)
 {
-	struct prestera_msg_port_attr_req req = {
-		.attr = PRESTERA_CMD_PORT_ATTR_DUPLEX,
+	struct mvsw_msg_port_attr_ret resp;
+	struct mvsw_msg_port_attr_cmd req = {
+		.attr = MVSW_MSG_PORT_ATTR_FEC,
 		.port = port->hw_id,
-		.dev = port->dev_id,
+		.dev = port->dev_id
 	};
-	struct prestera_msg_port_attr_resp resp;
 	int err;
 
-	err = prestera_cmd_ret(port->sw, PRESTERA_CMD_TYPE_PORT_ATTR_GET,
-			       &req.cmd, sizeof(req), &resp.ret, sizeof(resp));
+	err = fw_send_req_resp(port->sw, MVSW_MSG_TYPE_PORT_ATTR_GET,
+			       &req, &resp);
 	if (err)
 		return err;
 
-	*duplex = resp.param.duplex;
+	*fec = resp.param.fec;
 
-	return 0;
+	return err;
 }
 
-int prestera_hw_port_stats_get(const struct prestera_port *port,
-			       struct prestera_port_stats *st)
+int mvsw_pr_hw_port_fec_set(const struct mvsw_pr_port *port, u8 fec)
 {
-	struct prestera_msg_port_attr_req req = {
-		.attr = PRESTERA_CMD_PORT_ATTR_STATS,
+	struct mvsw_msg_port_attr_cmd req = {
+		.attr = MVSW_MSG_PORT_ATTR_FEC,
 		.port = port->hw_id,
 		.dev = port->dev_id,
+		.param = {.fec = fec}
+	};
+
+	return fw_send_req(port->sw, MVSW_MSG_TYPE_PORT_ATTR_SET, &req);
+}
+
+int mvsw_pr_hw_fw_log_level_set(const struct mvsw_pr_switch *sw,
+				u32 lib, u32 type)
+{
+	struct mvsw_msg_log_lvl_set_cmd req = {
+		.lib = lib,
+		.type = type
 	};
-	struct prestera_msg_port_stats_resp resp;
-	u64 *hw = resp.stats;
 	int err;
 
-	err = prestera_cmd_ret(port->sw, PRESTERA_CMD_TYPE_PORT_ATTR_GET,
-			       &req.cmd, sizeof(req), &resp.ret, sizeof(resp));
+	err = fw_send_req(sw, MVSW_MSG_TYPE_LOG_LEVEL_SET, &req);
 	if (err)
 		return err;
 
-	st->good_octets_received = hw[PRESTERA_PORT_GOOD_OCTETS_RCV_CNT];
-	st->bad_octets_received = hw[PRESTERA_PORT_BAD_OCTETS_RCV_CNT];
-	st->mac_trans_error = hw[PRESTERA_PORT_MAC_TRANSMIT_ERR_CNT];
-	st->broadcast_frames_received = hw[PRESTERA_PORT_BRDC_PKTS_RCV_CNT];
-	st->multicast_frames_received = hw[PRESTERA_PORT_MC_PKTS_RCV_CNT];
-	st->frames_64_octets = hw[PRESTERA_PORT_PKTS_64L_CNT];
-	st->frames_65_to_127_octets = hw[PRESTERA_PORT_PKTS_65TO127L_CNT];
-	st->frames_128_to_255_octets = hw[PRESTERA_PORT_PKTS_128TO255L_CNT];
-	st->frames_256_to_511_octets = hw[PRESTERA_PORT_PKTS_256TO511L_CNT];
-	st->frames_512_to_1023_octets = hw[PRESTERA_PORT_PKTS_512TO1023L_CNT];
-	st->frames_1024_to_max_octets = hw[PRESTERA_PORT_PKTS_1024TOMAXL_CNT];
-	st->excessive_collision = hw[PRESTERA_PORT_EXCESSIVE_COLLISIONS_CNT];
-	st->multicast_frames_sent = hw[PRESTERA_PORT_MC_PKTS_SENT_CNT];
-	st->broadcast_frames_sent = hw[PRESTERA_PORT_BRDC_PKTS_SENT_CNT];
-	st->fc_sent = hw[PRESTERA_PORT_FC_SENT_CNT];
-	st->fc_received = hw[PRESTERA_PORT_GOOD_FC_RCV_CNT];
-	st->buffer_overrun = hw[PRESTERA_PORT_DROP_EVENTS_CNT];
-	st->undersize = hw[PRESTERA_PORT_UNDERSIZE_PKTS_CNT];
-	st->fragments = hw[PRESTERA_PORT_FRAGMENTS_PKTS_CNT];
-	st->oversize = hw[PRESTERA_PORT_OVERSIZE_PKTS_CNT];
-	st->jabber = hw[PRESTERA_PORT_JABBER_PKTS_CNT];
-	st->rx_error_frame_received = hw[PRESTERA_PORT_MAC_RCV_ERROR_CNT];
-	st->bad_crc = hw[PRESTERA_PORT_BAD_CRC_CNT];
-	st->collisions = hw[PRESTERA_PORT_COLLISIONS_CNT];
-	st->late_collision = hw[PRESTERA_PORT_LATE_COLLISIONS_CNT];
-	st->unicast_frames_received = hw[PRESTERA_PORT_GOOD_UC_PKTS_RCV_CNT];
-	st->unicast_frames_sent = hw[PRESTERA_PORT_GOOD_UC_PKTS_SENT_CNT];
-	st->sent_multiple = hw[PRESTERA_PORT_MULTIPLE_PKTS_SENT_CNT];
-	st->sent_deferred = hw[PRESTERA_PORT_DEFERRED_PKTS_SENT_CNT];
-	st->good_octets_sent = hw[PRESTERA_PORT_GOOD_OCTETS_SENT_CNT];
-
 	return 0;
 }
 
-int prestera_hw_port_learning_set(struct prestera_port *port, bool enable)
+int mvsw_pr_hw_port_autoneg_set(const struct mvsw_pr_port *port,
+				bool autoneg, u64 link_modes, u8 fec)
 {
-	struct prestera_msg_port_attr_req req = {
-		.attr = PRESTERA_CMD_PORT_ATTR_LEARNING,
+	struct mvsw_msg_port_attr_cmd req = {
+		.attr = MVSW_MSG_PORT_ATTR_AUTONEG,
 		.port = port->hw_id,
 		.dev = port->dev_id,
-		.param = {
-			.learning = enable,
+		.param = {.autoneg = {.link_mode = link_modes,
+				      .enable = autoneg ? 1 : 0,
+				      .fec = fec}
 		}
 	};
 
-	return prestera_cmd(port->sw, PRESTERA_CMD_TYPE_PORT_ATTR_SET,
-			    &req.cmd, sizeof(req));
+	return fw_send_req(port->sw, MVSW_MSG_TYPE_PORT_ATTR_SET, &req);
 }
 
-int prestera_hw_port_flood_set(struct prestera_port *port, bool flood)
+int mvsw_pr_hw_port_duplex_get(const struct mvsw_pr_port *port, u8 *duplex)
 {
-	struct prestera_msg_port_attr_req req = {
-		.attr = PRESTERA_CMD_PORT_ATTR_FLOOD,
+	struct mvsw_msg_port_attr_ret resp;
+	struct mvsw_msg_port_attr_cmd req = {
+		.attr = MVSW_MSG_PORT_ATTR_DUPLEX,
 		.port = port->hw_id,
-		.dev = port->dev_id,
-		.param = {
-			.flood = flood,
-		}
+		.dev = port->dev_id
 	};
+	int err;
 
-	return prestera_cmd(port->sw, PRESTERA_CMD_TYPE_PORT_ATTR_SET,
-			    &req.cmd, sizeof(req));
+	err = fw_send_req_resp(port->sw, MVSW_MSG_TYPE_PORT_ATTR_GET,
+			       &req, &resp);
+	if (err)
+		return err;
+
+	*duplex = resp.param.duplex;
+
+	return err;
 }
 
-int prestera_hw_vlan_create(struct prestera_switch *sw, u16 vid)
+int mvsw_pr_hw_port_stats_get(const struct mvsw_pr_port *port,
+			      struct mvsw_pr_port_stats *stats)
 {
-	struct prestera_msg_vlan_req req = {
-		.vid = vid,
+	struct mvsw_msg_port_stats_ret resp;
+	struct mvsw_msg_port_attr_cmd req = {
+		.attr = MVSW_MSG_PORT_ATTR_STATS,
+		.port = port->hw_id,
+		.dev = port->dev_id
 	};
+	u64 *hw_val = resp.stats;
+	int err;
+
+	err = fw_send_req_resp(port->sw, MVSW_MSG_TYPE_PORT_ATTR_GET,
+			       &req, &resp);
+	if (err)
+		return err;
+
+	stats->good_octets_received = hw_val[MVSW_PORT_GOOD_OCTETS_RCV_CNT];
+	stats->bad_octets_received = hw_val[MVSW_PORT_BAD_OCTETS_RCV_CNT];
+	stats->mac_trans_error = hw_val[MVSW_PORT_MAC_TRANSMIT_ERR_CNT];
+	stats->broadcast_frames_received = hw_val[MVSW_PORT_BRDC_PKTS_RCV_CNT];
+	stats->multicast_frames_received = hw_val[MVSW_PORT_MC_PKTS_RCV_CNT];
+	stats->frames_64_octets = hw_val[MVSW_PORT_PKTS_64_OCTETS_CNT];
+	stats->frames_65_to_127_octets =
+		hw_val[MVSW_PORT_PKTS_65TO127_OCTETS_CNT];
+	stats->frames_128_to_255_octets =
+		hw_val[MVSW_PORT_PKTS_128TO255_OCTETS_CNT];
+	stats->frames_256_to_511_octets =
+		hw_val[MVSW_PORT_PKTS_256TO511_OCTETS_CNT];
+	stats->frames_512_to_1023_octets =
+		hw_val[MVSW_PORT_PKTS_512TO1023_OCTETS_CNT];
+	stats->frames_1024_to_max_octets =
+		hw_val[MVSW_PORT_PKTS_1024TOMAX_OCTETS_CNT];
+	stats->excessive_collision = hw_val[MVSW_PORT_EXCESSIVE_COLLISIONS_CNT];
+	stats->multicast_frames_sent = hw_val[MVSW_PORT_MC_PKTS_SENT_CNT];
+	stats->broadcast_frames_sent = hw_val[MVSW_PORT_BRDC_PKTS_SENT_CNT];
+	stats->fc_sent = hw_val[MVSW_PORT_FC_SENT_CNT];
+	stats->fc_received = hw_val[MVSW_PORT_GOOD_FC_RCV_CNT];
+	stats->buffer_overrun = hw_val[MVSW_PORT_DROP_EVENTS_CNT];
+	stats->undersize = hw_val[MVSW_PORT_UNDERSIZE_PKTS_CNT];
+	stats->fragments = hw_val[MVSW_PORT_FRAGMENTS_PKTS_CNT];
+	stats->oversize = hw_val[MVSW_PORT_OVERSIZE_PKTS_CNT];
+	stats->jabber = hw_val[MVSW_PORT_JABBER_PKTS_CNT];
+	stats->rx_error_frame_received = hw_val[MVSW_PORT_MAC_RCV_ERROR_CNT];
+	stats->bad_crc = hw_val[MVSW_PORT_BAD_CRC_CNT];
+	stats->collisions = hw_val[MVSW_PORT_COLLISIONS_CNT];
+	stats->late_collision = hw_val[MVSW_PORT_LATE_COLLISIONS_CNT];
+	stats->unicast_frames_received = hw_val[MVSW_PORT_GOOD_UC_PKTS_RCV_CNT];
+	stats->unicast_frames_sent = hw_val[MVSW_PORT_GOOD_UC_PKTS_SENT_CNT];
+	stats->sent_multiple = hw_val[MVSW_PORT_MULTIPLE_PKTS_SENT_CNT];
+	stats->sent_deferred = hw_val[MVSW_PORT_DEFERRED_PKTS_SENT_CNT];
+	stats->good_octets_sent = hw_val[MVSW_PORT_GOOD_OCTETS_SENT_CNT];
 
-	return prestera_cmd(sw, PRESTERA_CMD_TYPE_VLAN_CREATE,
-			    &req.cmd, sizeof(req));
+	return 0;
 }
 
-int prestera_hw_vlan_delete(struct prestera_switch *sw, u16 vid)
+int mvsw_pr_hw_bridge_create(const struct mvsw_pr_switch *sw, u16 *bridge_id)
 {
-	struct prestera_msg_vlan_req req = {
-		.vid = vid,
+	struct mvsw_msg_bridge_cmd req;
+	struct mvsw_msg_bridge_ret resp;
+	int err;
+
+	err = fw_send_req_resp(sw, MVSW_MSG_TYPE_BRIDGE_CREATE, &req, &resp);
+	if (err)
+		return err;
+
+	*bridge_id = resp.bridge;
+	return err;
+}
+
+int mvsw_pr_hw_bridge_delete(const struct mvsw_pr_switch *sw, u16 bridge_id)
+{
+	struct mvsw_msg_bridge_cmd req = {
+		.bridge = bridge_id
 	};
 
-	return prestera_cmd(sw, PRESTERA_CMD_TYPE_VLAN_DELETE,
-			    &req.cmd, sizeof(req));
+	return fw_send_req(sw, MVSW_MSG_TYPE_BRIDGE_DELETE, &req);
 }
 
-int prestera_hw_vlan_port_set(struct prestera_port *port, u16 vid,
-			      bool is_member, bool untagged)
+int mvsw_pr_hw_bridge_port_add(const struct mvsw_pr_port *port, u16 bridge_id)
 {
-	struct prestera_msg_vlan_req req = {
+	struct mvsw_msg_bridge_cmd req = {
+		.bridge = bridge_id,
 		.port = port->hw_id,
-		.dev = port->dev_id,
-		.vid = vid,
-		.is_member = is_member,
-		.is_tagged = !untagged,
+		.dev = port->dev_id
 	};
 
-	return prestera_cmd(port->sw, PRESTERA_CMD_TYPE_VLAN_PORT_SET,
-			    &req.cmd, sizeof(req));
+	return fw_send_req(port->sw, MVSW_MSG_TYPE_BRIDGE_PORT_ADD, &req);
 }
 
-int prestera_hw_vlan_port_vid_set(struct prestera_port *port, u16 vid)
+int mvsw_pr_hw_bridge_port_delete(const struct mvsw_pr_port *port,
+				  u16 bridge_id)
 {
-	struct prestera_msg_vlan_req req = {
+	struct mvsw_msg_bridge_cmd req = {
+		.bridge = bridge_id,
 		.port = port->hw_id,
-		.dev = port->dev_id,
-		.vid = vid,
+		.dev = port->dev_id
 	};
 
-	return prestera_cmd(port->sw, PRESTERA_CMD_TYPE_VLAN_PVID_SET,
-			    &req.cmd, sizeof(req));
+	return fw_send_req(port->sw, MVSW_MSG_TYPE_BRIDGE_PORT_DELETE, &req);
 }
 
-int prestera_hw_vlan_port_stp_set(struct prestera_port *port, u16 vid, u8 state)
+int mvsw_pr_hw_macvlan_add(const struct mvsw_pr_switch *sw, u16 vr_id,
+			   const u8 *mac, u16 vid)
 {
-	struct prestera_msg_stp_req req = {
-		.port = port->hw_id,
-		.dev = port->dev_id,
-		.vid = vid,
-		.state = state,
+	struct mvsw_msg_macvlan_cmd req = {
+		.vr_id = vr_id,
+		.vid = vid
 	};
 
-	return prestera_cmd(port->sw, PRESTERA_CMD_TYPE_STP_PORT_SET,
-			    &req.cmd, sizeof(req));
+	memcpy(req.mac, mac, ETH_ALEN);
+
+	return fw_send_req(sw, MVSW_MSG_TYPE_FDB_MACVLAN_ADD, &req);
 }
 
-int prestera_hw_fdb_add(struct prestera_port *port, const unsigned char *mac,
-			u16 vid, bool dynamic)
+int mvsw_pr_hw_macvlan_del(const struct mvsw_pr_switch *sw, u16 vr_id,
+			   const u8 *mac, u16 vid)
 {
-	struct prestera_msg_fdb_req req = {
-		.port = port->hw_id,
-		.dev = port->dev_id,
-		.vid = vid,
-		.dynamic = dynamic,
+	struct mvsw_msg_macvlan_cmd req = {
+		.vr_id = vr_id,
+		.vid = vid
 	};
 
-	ether_addr_copy(req.mac, mac);
+	memcpy(req.mac, mac, ETH_ALEN);
 
-	return prestera_cmd(port->sw, PRESTERA_CMD_TYPE_FDB_ADD,
-			    &req.cmd, sizeof(req));
+	return fw_send_req(sw, MVSW_MSG_TYPE_FDB_MACVLAN_DEL, &req);
 }
 
-int prestera_hw_fdb_del(struct prestera_port *port, const unsigned char *mac,
-			u16 vid)
+int mvsw_pr_hw_fdb_flush_port(const struct mvsw_pr_port *port, u32 mode)
 {
-	struct prestera_msg_fdb_req req = {
-		.port = port->hw_id,
-		.dev = port->dev_id,
-		.vid = vid,
+	struct mvsw_msg_fdb_cmd req = {
+		.dest_type = MVSW_HW_FDB_ENTRY_TYPE_REG_PORT,
+		.dest = {
+			.dev = port->dev_id,
+			.port = port->hw_id,
+		},
+		.flush_mode = mode,
 	};
 
-	ether_addr_copy(req.mac, mac);
+	return fw_send_req(port->sw, MVSW_MSG_TYPE_FDB_FLUSH_PORT, &req);
+}
 
-	return prestera_cmd(port->sw, PRESTERA_CMD_TYPE_FDB_DELETE,
-			    &req.cmd, sizeof(req));
+int mvsw_pr_hw_fdb_flush_lag(const struct mvsw_pr_switch *sw, u16 lag_id,
+			     u32 mode)
+{
+	struct mvsw_msg_fdb_cmd req = {
+		.dest_type = MVSW_HW_FDB_ENTRY_TYPE_LAG,
+		.dest = { .lag_id = lag_id },
+		.flush_mode = mode,
+	};
+
+	return fw_send_req(sw, MVSW_MSG_TYPE_FDB_FLUSH_PORT, &req);
 }
 
-int prestera_hw_fdb_flush_port(struct prestera_port *port, u32 mode)
+int mvsw_pr_hw_fdb_flush_vlan(const struct mvsw_pr_switch *sw, u16 vid,
+			      u32 mode)
 {
-	struct prestera_msg_fdb_req req = {
-		.port = port->hw_id,
-		.dev = port->dev_id,
+	struct mvsw_msg_fdb_cmd req = {
+		.vid = vid,
 		.flush_mode = mode,
 	};
 
-	return prestera_cmd(port->sw, PRESTERA_CMD_TYPE_FDB_FLUSH_PORT,
-			    &req.cmd, sizeof(req));
+	return fw_send_req(sw, MVSW_MSG_TYPE_FDB_FLUSH_VLAN, &req);
 }
 
-int prestera_hw_fdb_flush_vlan(struct prestera_switch *sw, u16 vid, u32 mode)
+int mvsw_pr_hw_fdb_flush_port_vlan(const struct mvsw_pr_port *port, u16 vid,
+				   u32 mode)
 {
-	struct prestera_msg_fdb_req req = {
+	struct mvsw_msg_fdb_cmd req = {
+		.dest_type = MVSW_HW_FDB_ENTRY_TYPE_REG_PORT,
+		.dest = {
+			.dev = port->dev_id,
+			.port = port->hw_id,
+		},
 		.vid = vid,
 		.flush_mode = mode,
 	};
 
-	return prestera_cmd(sw, PRESTERA_CMD_TYPE_FDB_FLUSH_VLAN,
-			    &req.cmd, sizeof(req));
+	return fw_send_req(port->sw, MVSW_MSG_TYPE_FDB_FLUSH_PORT_VLAN, &req);
 }
 
-int prestera_hw_fdb_flush_port_vlan(struct prestera_port *port, u16 vid,
-				    u32 mode)
+int mvsw_pr_hw_fdb_flush_lag_vlan(const struct mvsw_pr_switch *sw,
+				  u16 lag_id, u16 vid, u32 mode)
 {
-	struct prestera_msg_fdb_req req = {
-		.port = port->hw_id,
-		.dev = port->dev_id,
+	struct mvsw_msg_fdb_cmd req = {
+		.dest_type = MVSW_HW_FDB_ENTRY_TYPE_LAG,
+		.dest = { .lag_id = lag_id },
 		.vid = vid,
 		.flush_mode = mode,
 	};
 
-	return prestera_cmd(port->sw, PRESTERA_CMD_TYPE_FDB_FLUSH_PORT_VLAN,
-			    &req.cmd, sizeof(req));
+	return fw_send_req(sw, MVSW_MSG_TYPE_FDB_FLUSH_PORT_VLAN, &req);
 }
 
-int prestera_hw_bridge_create(struct prestera_switch *sw, u16 *bridge_id)
+int mvsw_pr_hw_port_link_mode_get(const struct mvsw_pr_port *port,
+				  u32 *mode)
 {
-	struct prestera_msg_bridge_resp resp;
-	struct prestera_msg_bridge_req req;
+	struct mvsw_msg_port_attr_ret resp;
+	struct mvsw_msg_port_attr_cmd req = {
+		.attr = MVSW_MSG_PORT_ATTR_LINK_MODE,
+		.port = port->hw_id,
+		.dev = port->dev_id
+	};
 	int err;
 
-	err = prestera_cmd_ret(sw, PRESTERA_CMD_TYPE_BRIDGE_CREATE,
-			       &req.cmd, sizeof(req),
-			       &resp.ret, sizeof(resp));
+	err = fw_send_req_resp(port->sw, MVSW_MSG_TYPE_PORT_ATTR_GET,
+			       &req, &resp);
 	if (err)
 		return err;
 
-	*bridge_id = resp.bridge;
+	*mode = resp.param.link_mode;
 
+	return err;
+}
+
+int mvsw_pr_hw_port_link_mode_set(const struct mvsw_pr_port *port,
+				  u32 mode)
+{
+	struct mvsw_msg_port_attr_cmd req = {
+		.attr = MVSW_MSG_PORT_ATTR_LINK_MODE,
+		.port = port->hw_id,
+		.dev = port->dev_id,
+		.param = {.link_mode = mode}
+	};
+
+	return fw_send_req(port->sw, MVSW_MSG_TYPE_PORT_ATTR_SET, &req);
+}
+
+static int mvsw_pr_iface_to_msg(struct mvsw_pr_iface *iface,
+				struct mvsw_msg_iface *msg_if)
+{
+	switch (iface->type) {
+	case MVSW_IF_PORT_E:
+	case MVSW_IF_VID_E:
+		msg_if->port = iface->dev_port.port_num;
+		msg_if->dev = iface->dev_port.hw_dev_num;
+		break;
+	case MVSW_IF_LAG_E:
+		msg_if->lag_id = iface->lag_id;
+		break;
+	default:
+		return -ENOTSUPP;
+	}
+
+	msg_if->vr_id = iface->vr_id;
+	msg_if->vid = iface->vlan_id;
+	msg_if->type = iface->type;
 	return 0;
 }
 
-int prestera_hw_bridge_delete(struct prestera_switch *sw, u16 bridge_id)
+int mvsw_pr_hw_rif_create(const struct mvsw_pr_switch *sw,
+			  struct mvsw_pr_iface *iif, u8 *mac, u16 *rif_id)
 {
-	struct prestera_msg_bridge_req req = {
-		.bridge = bridge_id,
+	struct mvsw_msg_rif_cmd req;
+	struct mvsw_msg_rif_ret resp;
+	int err;
+
+	memcpy(req.mac, mac, ETH_ALEN);
+
+	err = mvsw_pr_iface_to_msg(iif, &req.iif);
+	if (err)
+		return err;
+
+	err = fw_send_req_resp(sw, MVSW_MSG_TYPE_ROUTER_RIF_CREATE,
+			       &req, &resp);
+	if (err)
+		return err;
+
+	*rif_id = resp.rif_id;
+	return err;
+}
+
+int mvsw_pr_hw_rif_delete(const struct mvsw_pr_switch *sw, u16 rif_id,
+			  struct mvsw_pr_iface *iif)
+{
+	struct mvsw_msg_rif_cmd req = {
+		.rif_id = rif_id,
 	};
+	int err;
+
+	err = mvsw_pr_iface_to_msg(iif, &req.iif);
+	if (err)
+		return err;
 
-	return prestera_cmd(sw, PRESTERA_CMD_TYPE_BRIDGE_DELETE,
-			    &req.cmd, sizeof(req));
+	return fw_send_req(sw, MVSW_MSG_TYPE_ROUTER_RIF_DELETE, &req);
 }
 
-int prestera_hw_bridge_port_add(struct prestera_port *port, u16 bridge_id)
+int mvsw_pr_hw_rif_set(const struct mvsw_pr_switch *sw, u16 *rif_id,
+		       struct mvsw_pr_iface *iif, u8 *mac)
 {
-	struct prestera_msg_bridge_req req = {
-		.bridge = bridge_id,
-		.port = port->hw_id,
-		.dev = port->dev_id,
+	struct mvsw_msg_rif_ret resp;
+	struct mvsw_msg_rif_cmd req = {
+		.rif_id = *rif_id,
 	};
+	int err;
+
+	memcpy(req.mac, mac, ETH_ALEN);
+
+	err = mvsw_pr_iface_to_msg(iif, &req.iif);
+	if (err)
+		return err;
+
+	err = fw_send_req_resp(sw, MVSW_MSG_TYPE_ROUTER_RIF_SET, &req, &resp);
+	if (err)
+		return err;
 
-	return prestera_cmd(port->sw, PRESTERA_CMD_TYPE_BRIDGE_PORT_ADD,
-			    &req.cmd, sizeof(req));
+	*rif_id = resp.rif_id;
+	return err;
 }
 
-int prestera_hw_bridge_port_delete(struct prestera_port *port, u16 bridge_id)
+int mvsw_pr_hw_vr_create(const struct mvsw_pr_switch *sw, u16 *vr_id)
 {
-	struct prestera_msg_bridge_req req = {
-		.bridge = bridge_id,
-		.port = port->hw_id,
-		.dev = port->dev_id,
+	int err;
+	struct mvsw_msg_vr_ret resp;
+	struct mvsw_msg_vr_cmd req;
+
+	err = fw_send_req_resp(sw, MVSW_MSG_TYPE_ROUTER_VR_CREATE, &req, &resp);
+	if (err)
+		return err;
+
+	*vr_id = resp.vr_id;
+	return err;
+}
+
+int mvsw_pr_hw_vr_delete(const struct mvsw_pr_switch *sw, u16 vr_id)
+{
+	struct mvsw_msg_vr_cmd req = {
+		.vr_id = vr_id,
+	};
+
+	return fw_send_req(sw, MVSW_MSG_TYPE_ROUTER_VR_DELETE, &req);
+}
+
+int mvsw_pr_hw_vr_abort(const struct mvsw_pr_switch *sw, u16 vr_id)
+{
+	struct mvsw_msg_vr_cmd req = {
+		.vr_id = vr_id,
 	};
 
-	return prestera_cmd(port->sw, PRESTERA_CMD_TYPE_BRIDGE_PORT_DELETE,
-			    &req.cmd, sizeof(req));
+	return fw_send_req(sw, MVSW_MSG_TYPE_ROUTER_VR_ABORT, &req);
 }
 
-int prestera_hw_rxtx_init(struct prestera_switch *sw,
-			  struct prestera_rxtx_params *params)
+int mvsw_pr_hw_lpm_add(const struct mvsw_pr_switch *sw, u16 vr_id,
+		       __be32 dst, u32 dst_len, u32 grp_id)
 {
-	struct prestera_msg_rxtx_resp resp;
-	struct prestera_msg_rxtx_req req;
+	struct mvsw_msg_lpm_cmd req = {
+		.dst = dst,
+		.dst_len = dst_len,
+		.vr_id = vr_id,
+		.grp_id = grp_id
+	};
+
+	return fw_send_req(sw, MVSW_MSG_TYPE_ROUTER_LPM_ADD, &req);
+}
+
+int mvsw_pr_hw_lpm_del(const struct mvsw_pr_switch *sw, u16 vr_id, __be32 dst,
+		       u32 dst_len)
+{
+	struct mvsw_msg_lpm_cmd req = {
+		.dst = dst,
+		.dst_len = dst_len,
+		.vr_id = vr_id,
+	};
+
+	return fw_send_req(sw, MVSW_MSG_TYPE_ROUTER_LPM_DELETE, &req);
+}
+
+int mvsw_pr_hw_nh_entries_set(const struct mvsw_pr_switch *sw, int count,
+			      struct mvsw_pr_neigh_info *nhs, u32 grp_id)
+{
+	struct mvsw_msg_nh_cmd req = { .size = count, .grp_id = grp_id };
+	int i, err;
+
+	for (i = 0; i < count; i++) {
+		req.nh[i].is_active = nhs[i].connected;
+		memcpy(&req.nh[i].mac, nhs[i].ha, ETH_ALEN);
+		err = mvsw_pr_iface_to_msg(&nhs[i].iface, &req.nh[i].oif);
+		if (err)
+			return err;
+	}
+
+	return fw_send_req(sw, MVSW_MSG_TYPE_ROUTER_NH_GRP_SET, &req);
+}
+
+/* TODO: more than one nh */
+/* For now "count = 1" supported only */
+int mvsw_pr_hw_nh_entries_get(const struct mvsw_pr_switch *sw, int count,
+			      struct mvsw_pr_neigh_info *nhs, u32 grp_id)
+{
+	struct mvsw_msg_nh_cmd req = { .size = count, .grp_id = grp_id };
+	struct mvsw_msg_nh_ret resp;
+	int err, i;
+
+	err = fw_send_req_resp(sw, MVSW_MSG_TYPE_ROUTER_NH_GRP_GET,
+			       &req, &resp);
+	if (err)
+		return err;
+
+	for (i = 0; i < count; i++)
+		nhs[i].connected = resp.nh[i].is_active;
+
+	return err;
+}
+
+int mvsw_pr_hw_nh_group_create(const struct mvsw_pr_switch *sw, u16 nh_count,
+			       u32 *grp_id)
+{
+	struct mvsw_msg_nh_grp_cmd req = { .size = nh_count };
+	struct mvsw_msg_nh_grp_ret resp;
+	int err;
+
+	err = fw_send_req_resp(sw, MVSW_MSG_TYPE_ROUTER_NH_GRP_ADD, &req,
+			       &resp);
+	if (err)
+		return err;
+
+	*grp_id = resp.grp_id;
+	return err;
+}
+
+int mvsw_pr_hw_nh_group_delete(const struct mvsw_pr_switch *sw, u16 nh_count,
+			       u32 grp_id)
+{
+	struct mvsw_msg_nh_grp_cmd req = {
+	    .grp_id = grp_id,
+	    .size = nh_count
+	};
+
+	return fw_send_req(sw, MVSW_MSG_TYPE_ROUTER_NH_GRP_DELETE, &req);
+}
+
+int mvsw_pr_hw_mp4_hash_set(const struct mvsw_pr_switch *sw, u8 hash_policy)
+{
+	struct mvsw_msg_mp_cmd req = { .hash_policy = hash_policy};
+
+	return fw_send_req(sw, MVSW_MSG_TYPE_ROUTER_MP_HASH_SET, &req);
+}
+
+int mvsw_pr_hw_rxtx_init(const struct mvsw_pr_switch *sw, bool use_sdma,
+			 u32 *map_addr)
+{
+	struct mvsw_msg_rxtx_ret resp;
+	struct mvsw_msg_rxtx_cmd req;
 	int err;
 
-	req.use_sdma = params->use_sdma;
+	req.use_sdma = use_sdma;
 
-	err = prestera_cmd_ret(sw, PRESTERA_CMD_TYPE_RXTX_INIT,
-			       &req.cmd, sizeof(req), &resp.ret, sizeof(resp));
+	err = fw_send_req_resp(sw, MVSW_MSG_TYPE_RXTX_INIT, &req, &resp);
 	if (err)
 		return err;
 
-	params->map_addr = resp.map_addr;
+	if (map_addr)
+		*map_addr = resp.map_addr;
 
 	return 0;
 }
 
-int prestera_hw_rxtx_port_init(struct prestera_port *port)
+int mvsw_pr_hw_port_autoneg_restart(struct mvsw_pr_port *port)
 {
-	struct prestera_msg_rxtx_port_req req = {
+	struct mvsw_msg_port_attr_cmd req = {
+		.attr = MVSW_MSG_PORT_ATTR_AUTONEG_RESTART,
 		.port = port->hw_id,
 		.dev = port->dev_id,
 	};
 
-	return prestera_cmd(port->sw, PRESTERA_CMD_TYPE_RXTX_PORT_INIT,
-			    &req.cmd, sizeof(req));
+	return fw_send_req(port->sw, MVSW_MSG_TYPE_PORT_ATTR_SET, &req);
 }
 
-int prestera_hw_event_handler_register(struct prestera_switch *sw,
-				       enum prestera_event_type type,
-				       prestera_event_cb_t fn,
-				       void *arg)
+int mvsw_pr_hw_port_remote_fc_get(const struct mvsw_pr_port *port,
+				  bool *pause, bool *asym_pause)
 {
-	struct prestera_fw_event_handler *eh;
+	struct mvsw_msg_port_attr_ret resp;
+	struct mvsw_msg_port_attr_cmd req = {
+		.attr = MVSW_MSG_PORT_ATTR_REMOTE_FC,
+		.port = port->hw_id,
+		.dev = port->dev_id
+	};
+	int err;
 
-	eh = __find_event_handler(sw, type);
-	if (eh)
-		return -EEXIST;
+	err = fw_send_req_resp(port->sw, MVSW_MSG_TYPE_PORT_ATTR_GET,
+			       &req, &resp);
+	if (err)
+		return err;
 
-	eh = kmalloc(sizeof(*eh), GFP_KERNEL);
-	if (!eh)
+	switch (resp.param.fc) {
+	case MVSW_FC_SYMMETRIC:
+		*pause = true;
+		*asym_pause = false;
+		break;
+	case MVSW_FC_ASYMMETRIC:
+		*pause = false;
+		*asym_pause = true;
+		break;
+	case MVSW_FC_SYMM_ASYMM:
+		*pause = true;
+		*asym_pause = true;
+		break;
+	default:
+		*pause = false;
+		*asym_pause = false;
+	};
+
+	return err;
+}
+
+/* ACL API */
+int mvsw_pr_hw_acl_ruleset_create(const struct mvsw_pr_switch *sw,
+				  u16 *ruleset_id)
+{
+	int err;
+	struct mvsw_msg_acl_ruleset_ret resp;
+	struct mvsw_msg_acl_ruleset_cmd req;
+
+	err = fw_send_req_resp(sw, MVSW_MSG_TYPE_ACL_RULESET_CREATE,
+			       &req, &resp);
+	if (err)
+		return err;
+
+	*ruleset_id = resp.id;
+	return 0;
+}
+
+int mvsw_pr_hw_acl_ruleset_del(const struct mvsw_pr_switch *sw,
+			       u16 ruleset_id)
+{
+	struct mvsw_msg_acl_ruleset_cmd req = {
+		.id = ruleset_id,
+	};
+
+	return fw_send_req(sw, MVSW_MSG_TYPE_ACL_RULESET_DELETE, &req);
+}
+
+static int acl_rule_add_put_actions(struct mvsw_msg_buff *msg,
+				    struct prestera_acl_rule *rule)
+{
+	struct list_head *a_list = prestera_acl_rule_action_list_get(rule);
+	u8 n_actions = prestera_acl_rule_action_len(rule);
+	struct prestera_acl_rule_action_entry *a_entry;
+	__be64 be64;
+	int err;
+
+	err = mvsw_msg_buff_put(msg, &n_actions, sizeof(n_actions));
+	if (err)
+		return err;
+
+	list_for_each_entry(a_entry, a_list, list) {
+		err = mvsw_msg_buff_put(msg, (u8 *)&a_entry->id, sizeof(u8));
+		if (err)
+			return err;
+
+		switch (a_entry->id) {
+		case MVSW_ACL_RULE_ACTION_ACCEPT:
+		case MVSW_ACL_RULE_ACTION_DROP:
+		case MVSW_ACL_RULE_ACTION_TRAP:
+			/* just rule action id, no specific data */
+			break;
+		case MVSW_ACL_RULE_ACTION_POLICE:
+			be64 = cpu_to_be64(a_entry->police.rate);
+			err = mvsw_msg_buff_put(msg, &be64, sizeof(be64));
+			be64 = cpu_to_be64(a_entry->police.burst);
+			err = mvsw_msg_buff_put(msg, &be64, sizeof(be64));
+			break;
+		default:
+			err = -EINVAL;
+		}
+		if (err)
+			return err;
+	}
+
+	return 0;
+}
+
+static int acl_rule_add_put_matches(struct mvsw_msg_buff *msg,
+				    struct prestera_acl_rule *rule)
+{
+	struct list_head *m_list = prestera_acl_rule_match_list_get(rule);
+	struct prestera_acl_rule_match_entry *m_entry;
+	int err;
+
+	list_for_each_entry(m_entry, m_list, list) {
+		err = mvsw_msg_buff_put(msg, (u8 *)&m_entry->type, sizeof(u8));
+		if (err)
+			return err;
+
+		switch (m_entry->type) {
+		case MVSW_ACL_RULE_MATCH_ENTRY_TYPE_ETH_TYPE:
+		case MVSW_ACL_RULE_MATCH_ENTRY_TYPE_L4_PORT_SRC:
+		case MVSW_ACL_RULE_MATCH_ENTRY_TYPE_L4_PORT_DST:
+		case MVSW_ACL_RULE_MATCH_ENTRY_TYPE_VLAN_ID:
+		case MVSW_ACL_RULE_MATCH_ENTRY_TYPE_VLAN_TPID:
+			err = mvsw_msg_buff_put
+				(msg, &m_entry->keymask.u16.key,
+				 sizeof(m_entry->keymask.u16.key));
+			err |= mvsw_msg_buff_put
+				(msg, &m_entry->keymask.u16.mask,
+				 sizeof(m_entry->keymask.u16.mask));
+			break;
+		case MVSW_ACL_RULE_MATCH_ENTRY_TYPE_ICMP_TYPE:
+		case MVSW_ACL_RULE_MATCH_ENTRY_TYPE_ICMP_CODE:
+		case MVSW_ACL_RULE_MATCH_ENTRY_TYPE_IP_PROTO:
+			err = mvsw_msg_buff_put
+				(msg, &m_entry->keymask.u8.key,
+				 sizeof(m_entry->keymask.u8.key));
+			err |= mvsw_msg_buff_put
+				(msg, &m_entry->keymask.u8.mask,
+				 sizeof(m_entry->keymask.u8.mask));
+			break;
+		case MVSW_ACL_RULE_MATCH_ENTRY_TYPE_ETH_SMAC:
+		case MVSW_ACL_RULE_MATCH_ENTRY_TYPE_ETH_DMAC:
+			err = mvsw_msg_buff_put
+				(msg, &m_entry->keymask.mac.key,
+				 sizeof(m_entry->keymask.mac.key));
+			err |= mvsw_msg_buff_put
+				(msg, &m_entry->keymask.mac.mask,
+				 sizeof(m_entry->keymask.mac.mask));
+			break;
+		case MVSW_ACL_RULE_MATCH_ENTRY_TYPE_IP_SRC:
+		case MVSW_ACL_RULE_MATCH_ENTRY_TYPE_IP_DST:
+		case MVSW_ACL_RULE_MATCH_ENTRY_TYPE_L4_PORT_RANGE_SRC:
+		case MVSW_ACL_RULE_MATCH_ENTRY_TYPE_L4_PORT_RANGE_DST:
+			err = mvsw_msg_buff_put
+				(msg, &m_entry->keymask.u32.key,
+				 sizeof(m_entry->keymask.u32.key));
+			err |= mvsw_msg_buff_put
+				(msg, &m_entry->keymask.u32.mask,
+				 sizeof(m_entry->keymask.u32.mask));
+			break;
+		case MVSW_ACL_RULE_MATCH_ENTRY_TYPE_PORT:
+			err = mvsw_msg_buff_put
+				(msg, &m_entry->keymask.u64.key,
+				 sizeof(m_entry->keymask.u64.key));
+			err |= mvsw_msg_buff_put
+				(msg, &m_entry->keymask.u64.mask,
+				 sizeof(m_entry->keymask.u64.mask));
+			break;
+		default:
+			err = -EINVAL;
+		}
+		if (err)
+			return err;
+	}
+
+	return 0;
+}
+
+int mvsw_pr_hw_acl_rule_add(const struct mvsw_pr_switch *sw,
+			    struct prestera_acl_rule *rule,
+			    u32 *rule_id)
+{
+	int err;
+	struct mvsw_msg_acl_rule_ret resp;
+	u8 hw_tc = prestera_acl_rule_hw_tc_get(rule);
+	u32 priority = prestera_acl_rule_priority_get(rule);
+	u16 ruleset_id = prestera_acl_rule_ruleset_id_get(rule);
+	struct mvsw_msg_acl_rule_cmd *req;
+	struct mvsw_msg_buff *msg;
+
+	msg = mvsw_msg_buff_create(sizeof(*req));
+	if (!msg)
 		return -ENOMEM;
 
-	eh->type = type;
-	eh->func = fn;
-	eh->arg = arg;
+	/* put priority first */
+	err = mvsw_msg_buff_put(msg, &priority, sizeof(priority));
+	if (err)
+		goto free_msg;
 
-	INIT_LIST_HEAD(&eh->list);
+	/* put hw_tc into the message */
+	err = mvsw_msg_buff_put(msg, &hw_tc, sizeof(hw_tc));
+	if (err)
+		goto free_msg;
 
-	list_add_rcu(&eh->list, &sw->event_handlers);
+	/* put acl actions into the message */
+	err = acl_rule_add_put_actions(msg, rule);
+	if (err)
+		goto free_msg;
+
+	/* put acl matches into the message */
+	err = acl_rule_add_put_matches(msg, rule);
+	if (err)
+		goto free_msg;
+
+	/* terminate message */
+	err = mvsw_msg_buff_terminate(msg);
+	if (err)
+		goto free_msg;
+
+	req = (struct mvsw_msg_acl_rule_cmd *)mvsw_msg_buff_data(msg);
+
+	req->ruleset_id = ruleset_id;
 
+	err = fw_send_nreq_resp(sw, MVSW_MSG_TYPE_ACL_RULE_ADD, req,
+				mvsw_msg_buff_size(msg), &resp);
+	if (err)
+		goto free_msg;
+
+	*rule_id = resp.id;
+free_msg:
+	mvsw_msg_buff_destroy(msg);
+	return err;
+}
+
+int mvsw_pr_hw_acl_rule_del(const struct mvsw_pr_switch *sw, u32 rule_id)
+{
+	struct mvsw_msg_acl_rule_cmd req = {
+		.id = rule_id
+	};
+
+	return fw_send_req(sw, MVSW_MSG_TYPE_ACL_RULE_DELETE, &req);
+}
+
+int mvsw_pr_hw_acl_rule_stats_get(const struct mvsw_pr_switch *sw, u32 rule_id,
+				  u64 *packets, u64 *bytes)
+{
+	int err;
+	struct mvsw_msg_acl_rule_stats_ret resp;
+	struct mvsw_msg_acl_rule_cmd req = {
+		.id = rule_id
+	};
+
+	err = fw_send_req_resp(sw, MVSW_MSG_TYPE_ACL_RULE_STATS_GET,
+			       &req, &resp);
+	if (err)
+		return err;
+
+	*packets = resp.packets;
+	*bytes = resp.bytes;
 	return 0;
 }
 
-void prestera_hw_event_handler_unregister(struct prestera_switch *sw,
-					  enum prestera_event_type type,
-					  prestera_event_cb_t fn)
+int mvsw_pr_hw_acl_port_bind(const struct mvsw_pr_port *port, u16 ruleset_id)
 {
-	struct prestera_fw_event_handler *eh;
+	struct mvsw_msg_acl_ruleset_bind_cmd req = {
+		.port = port->hw_id,
+		.dev = port->dev_id,
+		.ruleset_id = ruleset_id,
+	};
 
-	eh = __find_event_handler(sw, type);
-	if (!eh)
-		return;
+	return fw_send_req(port->sw, MVSW_MSG_TYPE_ACL_PORT_BIND, &req);
+}
 
-	list_del_rcu(&eh->list);
-	kfree_rcu(eh, rcu);
+int mvsw_pr_hw_acl_port_unbind(const struct mvsw_pr_port *port, u16 ruleset_id)
+{
+	struct mvsw_msg_acl_ruleset_bind_cmd req = {
+		.port = port->hw_id,
+		.dev = port->dev_id,
+		.ruleset_id = ruleset_id,
+	};
+
+	return fw_send_req(port->sw, MVSW_MSG_TYPE_ACL_PORT_UNBIND, &req);
+}
+
+int mvsw_pr_hw_lag_member_add(struct mvsw_pr_port *port, u16 lag_id)
+{
+	struct mvsw_msg_lag_cmd req = {
+		.port = port->hw_id,
+		.dev = port->dev_id,
+		.lag_id = lag_id
+	};
+
+	return fw_send_req(port->sw, MVSW_MSG_TYPE_LAG_ADD, &req);
+}
+
+int mvsw_pr_hw_lag_member_del(struct mvsw_pr_port *port, u16 lag_id)
+{
+	struct mvsw_msg_lag_cmd req = {
+		.port = port->hw_id,
+		.dev = port->dev_id,
+		.lag_id = lag_id
+	};
+
+	return fw_send_req(port->sw, MVSW_MSG_TYPE_LAG_DELETE, &req);
+}
+
+int mvsw_pr_hw_lag_member_enable(struct mvsw_pr_port *port, u16 lag_id,
+				 bool enable)
+{
+	u32 cmd;
+	struct mvsw_msg_lag_cmd req = {
+		.port = port->hw_id,
+		.dev = port->dev_id,
+		.lag_id = lag_id
+	};
+
+	cmd = enable ? MVSW_MSG_TYPE_LAG_ENABLE : MVSW_MSG_TYPE_LAG_DISABLE;
+	return fw_send_req(port->sw, cmd, &req);
+}
+
+int mvsw_pr_hw_lag_member_rif_leave(const struct mvsw_pr_port *port,
+				    u16 lag_id, u16 vr_id)
+{
+	struct mvsw_msg_lag_cmd req = {
+		.port = port->hw_id,
+		.dev = port->dev_id,
+		.lag_id = lag_id,
+		.vr_id = vr_id
+	};
+
+	return fw_send_req(port->sw, MVSW_MSG_TYPE_LAG_ROUTER_LEAVE, &req);
 }
diff --git a/drivers/net/ethernet/marvell/prestera/prestera_hw.h b/drivers/net/ethernet/marvell/prestera/prestera_hw.h
index b2b5ac95b..ad6c4dadd 100644
--- a/drivers/net/ethernet/marvell/prestera/prestera_hw.h
+++ b/drivers/net/ethernet/marvell/prestera/prestera_hw.h
@@ -1,182 +1,324 @@
-/* SPDX-License-Identifier: BSD-3-Clause OR GPL-2.0 */
-/* Copyright (c) 2019-2020 Marvell International Ltd. All rights reserved. */
+/* SPDX-License-Identifier: BSD-3-Clause OR GPL-2.0
+ *
+ * Copyright (c) 2019-2020 Marvell International Ltd. All rights reserved.
+ *
+ */
 
-#ifndef _PRESTERA_HW_H_
-#define _PRESTERA_HW_H_
+#ifndef _MVSW_PRESTERA_HW_H_
+#define _MVSW_PRESTERA_HW_H_
 
 #include <linux/types.h>
 
-enum prestera_accept_frm_type {
-	PRESTERA_ACCEPT_FRAME_TYPE_TAGGED,
-	PRESTERA_ACCEPT_FRAME_TYPE_UNTAGGED,
-	PRESTERA_ACCEPT_FRAME_TYPE_ALL,
+enum mvsw_pr_accept_frame_type {
+	MVSW_ACCEPT_FRAME_TYPE_TAGGED,
+	MVSW_ACCEPT_FRAME_TYPE_UNTAGGED,
+	MVSW_ACCEPT_FRAME_TYPE_ALL
 };
 
-enum prestera_fdb_flush_mode {
-	PRESTERA_FDB_FLUSH_MODE_DYNAMIC = BIT(0),
-	PRESTERA_FDB_FLUSH_MODE_STATIC = BIT(1),
-	PRESTERA_FDB_FLUSH_MODE_ALL = PRESTERA_FDB_FLUSH_MODE_DYNAMIC
-					| PRESTERA_FDB_FLUSH_MODE_STATIC,
+enum {
+	MVSW_LINK_MODE_10baseT_Half_BIT,
+	MVSW_LINK_MODE_10baseT_Full_BIT,
+	MVSW_LINK_MODE_100baseT_Half_BIT,
+	MVSW_LINK_MODE_100baseT_Full_BIT,
+	MVSW_LINK_MODE_1000baseT_Half_BIT,
+	MVSW_LINK_MODE_1000baseT_Full_BIT,
+	MVSW_LINK_MODE_1000baseX_Full_BIT,
+	MVSW_LINK_MODE_1000baseKX_Full_BIT,
+	MVSW_LINK_MODE_2500baseX_Full_BIT,
+	MVSW_LINK_MODE_10GbaseKR_Full_BIT,
+	MVSW_LINK_MODE_10GbaseSR_Full_BIT,
+	MVSW_LINK_MODE_10GbaseLR_Full_BIT,
+	MVSW_LINK_MODE_20GbaseKR2_Full_BIT,
+	MVSW_LINK_MODE_25GbaseCR_Full_BIT,
+	MVSW_LINK_MODE_25GbaseKR_Full_BIT,
+	MVSW_LINK_MODE_25GbaseSR_Full_BIT,
+	MVSW_LINK_MODE_40GbaseKR4_Full_BIT,
+	MVSW_LINK_MODE_40GbaseCR4_Full_BIT,
+	MVSW_LINK_MODE_40GbaseSR4_Full_BIT,
+	MVSW_LINK_MODE_50GbaseCR2_Full_BIT,
+	MVSW_LINK_MODE_50GbaseKR2_Full_BIT,
+	MVSW_LINK_MODE_50GbaseSR2_Full_BIT,
+	MVSW_LINK_MODE_100GbaseKR4_Full_BIT,
+	MVSW_LINK_MODE_100GbaseSR4_Full_BIT,
+	MVSW_LINK_MODE_100GbaseCR4_Full_BIT,
+	MVSW_LINK_MODE_MAX,
 };
 
 enum {
-	PRESTERA_LINK_MODE_10baseT_Half,
-	PRESTERA_LINK_MODE_10baseT_Full,
-	PRESTERA_LINK_MODE_100baseT_Half,
-	PRESTERA_LINK_MODE_100baseT_Full,
-	PRESTERA_LINK_MODE_1000baseT_Half,
-	PRESTERA_LINK_MODE_1000baseT_Full,
-	PRESTERA_LINK_MODE_1000baseX_Full,
-	PRESTERA_LINK_MODE_1000baseKX_Full,
-	PRESTERA_LINK_MODE_2500baseX_Full,
-	PRESTERA_LINK_MODE_10GbaseKR_Full,
-	PRESTERA_LINK_MODE_10GbaseSR_Full,
-	PRESTERA_LINK_MODE_10GbaseLR_Full,
-	PRESTERA_LINK_MODE_20GbaseKR2_Full,
-	PRESTERA_LINK_MODE_25GbaseCR_Full,
-	PRESTERA_LINK_MODE_25GbaseKR_Full,
-	PRESTERA_LINK_MODE_25GbaseSR_Full,
-	PRESTERA_LINK_MODE_40GbaseKR4_Full,
-	PRESTERA_LINK_MODE_40GbaseCR4_Full,
-	PRESTERA_LINK_MODE_40GbaseSR4_Full,
-	PRESTERA_LINK_MODE_50GbaseCR2_Full,
-	PRESTERA_LINK_MODE_50GbaseKR2_Full,
-	PRESTERA_LINK_MODE_50GbaseSR2_Full,
-	PRESTERA_LINK_MODE_100GbaseKR4_Full,
-	PRESTERA_LINK_MODE_100GbaseSR4_Full,
-	PRESTERA_LINK_MODE_100GbaseCR4_Full,
-
-	PRESTERA_LINK_MODE_MAX
+	MVSW_PORT_TYPE_NONE,
+	MVSW_PORT_TYPE_TP,
+	MVSW_PORT_TYPE_AUI,
+	MVSW_PORT_TYPE_MII,
+	MVSW_PORT_TYPE_FIBRE,
+	MVSW_PORT_TYPE_BNC,
+	MVSW_PORT_TYPE_DA,
+	MVSW_PORT_TYPE_OTHER,
+	MVSW_PORT_TYPE_MAX,
 };
 
 enum {
-	PRESTERA_PORT_TYPE_NONE,
-	PRESTERA_PORT_TYPE_TP,
-	PRESTERA_PORT_TYPE_AUI,
-	PRESTERA_PORT_TYPE_MII,
-	PRESTERA_PORT_TYPE_FIBRE,
-	PRESTERA_PORT_TYPE_BNC,
-	PRESTERA_PORT_TYPE_DA,
-	PRESTERA_PORT_TYPE_OTHER,
-
-	PRESTERA_PORT_TYPE_MAX
+	MVSW_PORT_TRANSCEIVER_COPPER,
+	MVSW_PORT_TRANSCEIVER_SFP,
+	MVSW_PORT_TRANSCEIVER_MAX,
 };
 
 enum {
-	PRESTERA_PORT_TCVR_COPPER,
-	PRESTERA_PORT_TCVR_SFP,
-
-	PRESTERA_PORT_TCVR_MAX
+	MVSW_PORT_FEC_OFF_BIT,
+	MVSW_PORT_FEC_BASER_BIT,
+	MVSW_PORT_FEC_RS_BIT,
+	MVSW_PORT_FEC_MAX,
 };
 
 enum {
-	PRESTERA_PORT_FEC_OFF,
-	PRESTERA_PORT_FEC_BASER,
-	PRESTERA_PORT_FEC_RS,
-
-	PRESTERA_PORT_FEC_MAX
+	MVSW_PORT_DUPLEX_HALF,
+	MVSW_PORT_DUPLEX_FULL
 };
 
 enum {
-	PRESTERA_PORT_DUPLEX_HALF,
-	PRESTERA_PORT_DUPLEX_FULL,
+	MVSW_STP_DISABLED,
+	MVSW_STP_BLOCK_LISTEN,
+	MVSW_STP_LEARN,
+	MVSW_STP_FORWARD
 };
 
 enum {
-	PRESTERA_STP_DISABLED,
-	PRESTERA_STP_BLOCK_LISTEN,
-	PRESTERA_STP_LEARN,
-	PRESTERA_STP_FORWARD,
+	MVSW_FW_LOG_LIB_BRIDGE = 0,
+	MVSW_FW_LOG_LIB_CNC,
+	MVSW_FW_LOG_LIB_CONFIG,
+	MVSW_FW_LOG_LIB_COS,
+	MVSW_FW_LOG_LIB_HW_INIT,
+	MVSW_FW_LOG_LIB_CSCD,
+	MVSW_FW_LOG_LIB_CUT_THROUGH,
+	MVSW_FW_LOG_LIB_DIAG,
+	MVSW_FW_LOG_LIB_FABRIC,
+	MVSW_FW_LOG_LIB_IP,
+	MVSW_FW_LOG_LIB_IPFIX,
+	MVSW_FW_LOG_LIB_IP_LPM,
+	MVSW_FW_LOG_LIB_L2_MLL,
+	MVSW_FW_LOG_LIB_LOGICAL_TARGET,
+	MVSW_FW_LOG_LIB_LPM,
+	MVSW_FW_LOG_LIB_MIRROR,
+	MVSW_FW_LOG_LIB_MULTI_PORT_GROUP,
+	MVSW_FW_LOG_LIB_NETWORK_IF,
+	MVSW_FW_LOG_LIB_NST,
+	MVSW_FW_LOG_LIB_OAM,
+	MVSW_FW_LOG_LIB_PCL,
+	MVSW_FW_LOG_LIB_PHY,
+	MVSW_FW_LOG_LIB_POLICER,
+	MVSW_FW_LOG_LIB_PORT,
+	MVSW_FW_LOG_LIB_PROTECTION,
+	MVSW_FW_LOG_LIB_PTP,
+	MVSW_FW_LOG_LIB_SYSTEM_RECOVERY,
+	MVSW_FW_LOG_LIB_TCAM,
+	MVSW_FW_LOG_LIB_TM_GLUE,
+	MVSW_FW_LOG_LIB_TRUNK,
+	MVSW_FW_LOG_LIB_TTI,
+	MVSW_FW_LOG_LIB_TUNNEL,
+	MVSW_FW_LOG_LIB_VNT,
+	MVSW_FW_LOG_LIB_RESOURCE_MANAGER,
+	MVSW_FW_LOG_LIB_VERSION,
+	MVSW_FW_LOG_LIB_TM,
+	MVSW_FW_LOG_LIB_SMI,
+	MVSW_FW_LOG_LIB_INIT,
+	MVSW_FW_LOG_LIB_DRAGONITE,
+	MVSW_FW_LOG_LIB_VIRTUAL_TCAM,
+	MVSW_FW_LOG_LIB_INGRESS,
+	MVSW_FW_LOG_LIB_EGRESS,
+	MVSW_FW_LOG_LIB_LATENCY_MONITORING,
+	MVSW_FW_LOG_LIB_TAM,
+	MVSW_FW_LOG_LIB_EXACT_MATCH,
+	MVSW_FW_LOG_LIB_PHA,
+	MVSW_FW_LOG_LIB_PACKET_ANALYZER,
+	MVSW_FW_LOG_LIB_FLOW_MANAGER,
+	MVSW_FW_LOG_LIB_BRIDGE_FDB_MANAGER,
+	MVSW_FW_LOG_LIB_I2C,
+	MVSW_FW_LOG_LIB_PPU,
+	MVSW_FW_LOG_LIB_EXACT_MATCH_MANAGER,
+	MVSW_FW_LOG_LIB_MAC_SEC,
+	MVSW_FW_LOG_LIB_ALL,
+
+	MVSW_FW_LOG_LIB_MAX
 };
 
-struct prestera_switch;
-struct prestera_port;
-struct prestera_port_stats;
-struct prestera_port_caps;
-enum prestera_event_type;
-struct prestera_event;
+enum {
+	MVSW_FW_LOG_TYPE_INFO = 0,
+	MVSW_FW_LOG_TYPE_ENTRY_LEVEL_FUNCTION,
+	MVSW_FW_LOG_TYPE_ERROR,
+	MVSW_FW_LOG_TYPE_ALL,
+	MVSW_FW_LOG_TYPE_NONE,
+
+	MVSW_FW_LOG_TYPE_MAX
+};
 
-typedef void (*prestera_event_cb_t)
-	(struct prestera_switch *sw, struct prestera_event *evt, void *arg);
+struct mvsw_pr_switch;
+struct mvsw_pr_port;
+struct mvsw_pr_port_stats;
+struct mvsw_pr_port_caps;
+struct prestera_acl_rule;
+struct mvsw_pr_iface;
+struct mvsw_pr_neigh_info;
 
-struct prestera_rxtx_params;
+enum mvsw_pr_event_type;
+struct mvsw_pr_event;
 
 /* Switch API */
-int prestera_hw_switch_init(struct prestera_switch *sw);
-void prestera_hw_switch_fini(struct prestera_switch *sw);
-int prestera_hw_switch_ageing_set(struct prestera_switch *sw, u32 ageing_ms);
-int prestera_hw_switch_mac_set(struct prestera_switch *sw, const char *mac);
+int mvsw_pr_hw_switch_init(struct mvsw_pr_switch *sw);
+int mvsw_pr_hw_switch_ageing_set(const struct mvsw_pr_switch *sw,
+				 u32 ageing_time);
+int mvsw_pr_hw_switch_mac_set(const struct mvsw_pr_switch *sw, const u8 *mac);
+int mvsw_pr_hw_switch_trap_policer_set(const struct mvsw_pr_switch *sw,
+				       u8 profile);
 
 /* Port API */
-int prestera_hw_port_info_get(const struct prestera_port *port,
-			      u32 *dev_id, u32 *hw_id, u16 *fp_id);
-int prestera_hw_port_state_set(const struct prestera_port *port,
-			       bool admin_state);
-int prestera_hw_port_mtu_set(const struct prestera_port *port, u32 mtu);
-int prestera_hw_port_mtu_get(const struct prestera_port *port, u32 *mtu);
-int prestera_hw_port_mac_set(const struct prestera_port *port, const char *mac);
-int prestera_hw_port_mac_get(const struct prestera_port *port, char *mac);
-int prestera_hw_port_cap_get(const struct prestera_port *port,
-			     struct prestera_port_caps *caps);
-int prestera_hw_port_remote_cap_get(const struct prestera_port *port,
-				    u64 *link_mode_bitmap);
-int prestera_hw_port_remote_fc_get(const struct prestera_port *port,
-				   bool *pause, bool *asym_pause);
-int prestera_hw_port_type_get(const struct prestera_port *port, u8 *type);
-int prestera_hw_port_fec_get(const struct prestera_port *port, u8 *fec);
-int prestera_hw_port_fec_set(const struct prestera_port *port, u8 fec);
-int prestera_hw_port_autoneg_set(const struct prestera_port *port,
-				 bool autoneg, u64 link_modes, u8 fec);
-int prestera_hw_port_autoneg_restart(struct prestera_port *port);
-int prestera_hw_port_duplex_get(const struct prestera_port *port, u8 *duplex);
-int prestera_hw_port_stats_get(const struct prestera_port *port,
-			       struct prestera_port_stats *stats);
-int prestera_hw_port_link_mode_set(const struct prestera_port *port, u32 mode);
-int prestera_hw_port_link_mode_get(const struct prestera_port *port, u32 *mode);
-int prestera_hw_port_mdix_get(const struct prestera_port *port, u8 *status,
-			      u8 *admin_mode);
-int prestera_hw_port_mdix_set(const struct prestera_port *port, u8 mode);
-int prestera_hw_port_speed_get(const struct prestera_port *port, u32 *speed);
-int prestera_hw_port_learning_set(struct prestera_port *port, bool enable);
-int prestera_hw_port_flood_set(struct prestera_port *port, bool flood);
-int prestera_hw_port_accept_frm_type(struct prestera_port *port,
-				     enum prestera_accept_frm_type type);
+int mvsw_pr_hw_port_info_get(const struct mvsw_pr_port *port,
+			     u16 *fp_id, u32 *hw_id, u32 *dev_id);
+int mvsw_pr_hw_port_state_set(const struct mvsw_pr_port *port,
+			      bool admin_state);
+int mvsw_pr_hw_port_state_get(const struct mvsw_pr_port *port,
+			      bool *admin_state, bool *oper_state);
+int mvsw_pr_hw_port_mtu_set(const struct mvsw_pr_port *port, u32 mtu);
+int mvsw_pr_hw_port_mtu_get(const struct mvsw_pr_port *port, u32 *mtu);
+int mvsw_pr_hw_port_mac_set(const struct mvsw_pr_port *port, char *mac);
+int mvsw_pr_hw_port_mac_get(const struct mvsw_pr_port *port, char *mac);
+int mvsw_pr_hw_port_accept_frame_type_set(const struct mvsw_pr_port *port,
+					  enum mvsw_pr_accept_frame_type type);
+int mvsw_pr_hw_port_learning_set(const struct mvsw_pr_port *port, bool enable);
+int mvsw_pr_hw_port_speed_get(const struct mvsw_pr_port *port, u32 *speed);
+int mvsw_pr_hw_port_uc_flood_set(const struct mvsw_pr_port *port, bool flood);
+int mvsw_pr_hw_port_mc_flood_set(const struct mvsw_pr_port *port, bool flood);
+int mvsw_pr_hw_port_cap_get(const struct mvsw_pr_port *port,
+			    struct mvsw_pr_port_caps *caps);
+int mvsw_pr_hw_port_remote_cap_get(const struct mvsw_pr_port *port,
+				   u64 *link_mode_bitmap);
+int mvsw_pr_hw_port_remote_fc_get(const struct mvsw_pr_port *port,
+				  bool *pause, bool *asym_pause);
+int mvsw_pr_hw_port_type_get(const struct mvsw_pr_port *port, u8 *type);
+int mvsw_pr_hw_port_fec_get(const struct mvsw_pr_port *port, u8 *fec);
+int mvsw_pr_hw_port_fec_set(const struct mvsw_pr_port *port, u8 fec);
+int mvsw_pr_hw_port_autoneg_set(const struct mvsw_pr_port *port,
+				bool autoneg, u64 link_modes, u8 fec);
+int mvsw_pr_hw_port_duplex_get(const struct mvsw_pr_port *port, u8 *duplex);
+int mvsw_pr_hw_port_stats_get(const struct mvsw_pr_port *port,
+			      struct mvsw_pr_port_stats *stats);
+int mvsw_pr_hw_port_link_mode_get(const struct mvsw_pr_port *port,
+				  u32 *mode);
+int mvsw_pr_hw_port_link_mode_set(const struct mvsw_pr_port *port,
+				  u32 mode);
+int mvsw_pr_hw_port_mdix_get(const struct mvsw_pr_port *port, u8 *status,
+			     u8 *admin_mode);
+int mvsw_pr_hw_port_mdix_set(const struct mvsw_pr_port *port, u8 mode);
+int mvsw_pr_hw_port_autoneg_restart(struct mvsw_pr_port *port);
+
 /* Vlan API */
-int prestera_hw_vlan_create(struct prestera_switch *sw, u16 vid);
-int prestera_hw_vlan_delete(struct prestera_switch *sw, u16 vid);
-int prestera_hw_vlan_port_set(struct prestera_port *port, u16 vid,
-			      bool is_member, bool untagged);
-int prestera_hw_vlan_port_vid_set(struct prestera_port *port, u16 vid);
-int prestera_hw_vlan_port_stp_set(struct prestera_port *port, u16 vid, u8 state);
+int mvsw_pr_hw_vlan_create(const struct mvsw_pr_switch *sw, u16 vid);
+int mvsw_pr_hw_vlan_delete(const struct mvsw_pr_switch *sw, u16 vid);
+int mvsw_pr_hw_vlan_port_set(const struct mvsw_pr_port *port,
+			     u16 vid, bool is_member, bool untagged);
+int mvsw_pr_hw_vlan_port_vid_set(const struct mvsw_pr_port *port, u16 vid);
 
 /* FDB API */
-int prestera_hw_fdb_add(struct prestera_port *port, const unsigned char *mac,
-			u16 vid, bool dynamic);
-int prestera_hw_fdb_del(struct prestera_port *port, const unsigned char *mac,
-			u16 vid);
-int prestera_hw_fdb_flush_port(struct prestera_port *port, u32 mode);
-int prestera_hw_fdb_flush_vlan(struct prestera_switch *sw, u16 vid, u32 mode);
-int prestera_hw_fdb_flush_port_vlan(struct prestera_port *port, u16 vid,
-				    u32 mode);
+int mvsw_pr_hw_fdb_add(const struct mvsw_pr_port *port,
+		       const unsigned char *mac, u16 vid, bool dynamic);
+int mvsw_pr_hw_fdb_del(const struct mvsw_pr_port *port,
+		       const unsigned char *mac, u16 vid);
+int mvsw_pr_hw_fdb_flush_port(const struct mvsw_pr_port *port, u32 mode);
+int mvsw_pr_hw_fdb_flush_vlan(const struct mvsw_pr_switch *sw, u16 vid,
+			      u32 mode);
+int mvsw_pr_hw_fdb_flush_port_vlan(const struct mvsw_pr_port *port, u16 vid,
+				   u32 mode);
+int mvsw_pr_hw_macvlan_add(const struct mvsw_pr_switch *sw, u16 vr_id,
+			   const u8 *mac, u16 vid);
+int mvsw_pr_hw_macvlan_del(const struct mvsw_pr_switch *sw, u16 vr_id,
+			   const u8 *mac, u16 vid);
 
 /* Bridge API */
-int prestera_hw_bridge_create(struct prestera_switch *sw, u16 *bridge_id);
-int prestera_hw_bridge_delete(struct prestera_switch *sw, u16 bridge_id);
-int prestera_hw_bridge_port_add(struct prestera_port *port, u16 bridge_id);
-int prestera_hw_bridge_port_delete(struct prestera_port *port, u16 bridge_id);
+int mvsw_pr_hw_bridge_create(const struct mvsw_pr_switch *sw, u16 *bridge_id);
+int mvsw_pr_hw_bridge_delete(const struct mvsw_pr_switch *sw, u16 bridge_id);
+int mvsw_pr_hw_bridge_port_add(const struct mvsw_pr_port *port, u16 bridge_id);
+int mvsw_pr_hw_bridge_port_delete(const struct mvsw_pr_port *port,
+				  u16 bridge_id);
+
+/* STP API */
+int mvsw_pr_hw_port_vid_stp_set(struct mvsw_pr_port *port, u16 vid, u8 state);
+
+/* ACL API */
+int mvsw_pr_hw_acl_ruleset_create(const struct mvsw_pr_switch *sw,
+				  u16 *ruleset_id);
+int mvsw_pr_hw_acl_ruleset_del(const struct mvsw_pr_switch *sw,
+			       u16 ruleset_id);
+int mvsw_pr_hw_acl_rule_add(const struct mvsw_pr_switch *sw,
+			    struct prestera_acl_rule *rule,
+			    u32 *rule_id);
+int mvsw_pr_hw_acl_rule_del(const struct mvsw_pr_switch *sw, u32 rule_id);
+int mvsw_pr_hw_acl_rule_stats_get(const struct mvsw_pr_switch *sw, u32 rule_id,
+				  u64 *packets, u64 *bytes);
+int mvsw_pr_hw_acl_port_bind(const struct mvsw_pr_port *port, u16 ruleset_id);
+int mvsw_pr_hw_acl_port_unbind(const struct mvsw_pr_port *port, u16 ruleset_id);
+
+/* Router API */
+int mvsw_pr_hw_rif_create(const struct mvsw_pr_switch *sw,
+			  struct mvsw_pr_iface *iif, u8 *mac, u16 *rif_id);
+int mvsw_pr_hw_rif_delete(const struct mvsw_pr_switch *sw, u16 rif_id,
+			  struct mvsw_pr_iface *iif);
+int mvsw_pr_hw_rif_set(const struct mvsw_pr_switch *sw, u16 *rif_id,
+		       struct mvsw_pr_iface *iif, u8 *mac);
+
+/* Virtual Router API */
+int mvsw_pr_hw_vr_create(const struct mvsw_pr_switch *sw, u16 *vr_id);
+int mvsw_pr_hw_vr_delete(const struct mvsw_pr_switch *sw, u16 vr_id);
+int mvsw_pr_hw_vr_abort(const struct mvsw_pr_switch *sw, u16 vr_id);
+
+/* LPM API */
+int mvsw_pr_hw_lpm_add(const struct mvsw_pr_switch *sw, u16 vr_id,
+		       __be32 dst, u32 dst_len, u32 grp_id);
+int mvsw_pr_hw_lpm_del(const struct mvsw_pr_switch *sw, u16 vr_id, __be32 dst,
+		       u32 dst_len);
+
+/* NH API */
+int mvsw_pr_hw_nh_entries_set(const struct mvsw_pr_switch *sw, int count,
+			      struct mvsw_pr_neigh_info *nhs, u32 grp_id);
+int mvsw_pr_hw_nh_entries_get(const struct mvsw_pr_switch *sw, int count,
+			      struct mvsw_pr_neigh_info *nhs, u32 grp_id);
+int mvsw_pr_hw_nh_group_create(const struct mvsw_pr_switch *sw, u16 nh_count,
+			       u32 *grp_id);
+int mvsw_pr_hw_nh_group_delete(const struct mvsw_pr_switch *sw, u16 nh_count,
+			       u32 grp_id);
+
+/* MP API */
+int mvsw_pr_hw_mp4_hash_set(const struct mvsw_pr_switch *sw, u8 hash_policy);
+
+/* LAG API */
+int mvsw_pr_hw_lag_member_add(struct mvsw_pr_port *port, u16 lag_id);
+int mvsw_pr_hw_lag_member_del(struct mvsw_pr_port *port, u16 lag_id);
+int mvsw_pr_hw_lag_member_enable(struct mvsw_pr_port *port, u16 lag_id,
+				 bool enable);
+int mvsw_pr_hw_lag_fdb_add(const struct mvsw_pr_switch *sw, u16 lag_id,
+			   const unsigned char *mac, u16 vid, bool dynamic);
+int mvsw_pr_hw_lag_fdb_del(const struct mvsw_pr_switch *sw, u16 lag_id,
+			   const unsigned char *mac, u16 vid);
+int mvsw_pr_hw_fdb_flush_lag(const struct mvsw_pr_switch *sw, u16 lag_id,
+			     u32 mode);
+int mvsw_pr_hw_fdb_flush_lag_vlan(const struct mvsw_pr_switch *sw,
+				  u16 lag_id, u16 vid, u32 mode);
+int mvsw_pr_hw_lag_member_rif_leave(const struct mvsw_pr_port *port,
+				    u16 lag_id, u16 vr_id);
 
 /* Event handlers */
-int prestera_hw_event_handler_register(struct prestera_switch *sw,
-				       enum prestera_event_type type,
-				       prestera_event_cb_t fn,
-				       void *arg);
-void prestera_hw_event_handler_unregister(struct prestera_switch *sw,
-					  enum prestera_event_type type,
-					  prestera_event_cb_t fn);
-
-/* RX/TX */
-int prestera_hw_rxtx_init(struct prestera_switch *sw,
-			  struct prestera_rxtx_params *params);
-int prestera_hw_rxtx_port_init(struct prestera_port *port);
-
-#endif /* _PRESTERA_HW_H_ */
+int mvsw_pr_hw_event_handler_register(struct mvsw_pr_switch *sw,
+				      enum mvsw_pr_event_type type,
+				      void (*cb)(struct mvsw_pr_switch *sw,
+						 struct mvsw_pr_event *evt,
+						 void *arg),
+				      void *arg);
+
+void mvsw_pr_hw_event_handler_unregister(struct mvsw_pr_switch *sw,
+					 enum mvsw_pr_event_type type);
+
+/* FW Log API */
+int mvsw_pr_hw_fw_log_level_set(const struct mvsw_pr_switch *sw, u32 lib,
+				u32 type);
+
+int mvsw_pr_hw_rxtx_init(const struct mvsw_pr_switch *sw, bool use_sdma,
+			 u32 *map_addr);
+
+#endif /* _MVSW_PRESTERA_HW_H_ */
diff --git a/drivers/net/ethernet/marvell/prestera/prestera_log.c b/drivers/net/ethernet/marvell/prestera/prestera_log.c
new file mode 100644
index 000000000..bf2fb60a2
--- /dev/null
+++ b/drivers/net/ethernet/marvell/prestera/prestera_log.c
@@ -0,0 +1,203 @@
+/* SPDX-License-Identifier: BSD-3-Clause OR GPL-2.0
+ *
+ * Copyright (c) 2019-2020 Marvell International Ltd. All rights reserved.
+ *
+ */
+#include "prestera_log.h"
+
+static const char unknown[] = "UNKNOWN";
+
+DEF_ENUM_MAP(netdev_cmd) = {
+	[NETDEV_UP] = "NETDEV_UP",
+	[NETDEV_DOWN] = "NETDEV_DOWN",
+	[NETDEV_REBOOT] = "NETDEV_REBOOT",
+	[NETDEV_CHANGE] = "NETDEV_CHANGE",
+	[NETDEV_REGISTER] = "NETDEV_REGISTER",
+	[NETDEV_UNREGISTER] = "NETDEV_UNREGISTER",
+	[NETDEV_CHANGEMTU] = "NETDEV_CHANGEMTU",
+	[NETDEV_CHANGEADDR] = "NETDEV_CHANGEADDR",
+	[NETDEV_PRE_CHANGEADDR] = "NETDEV_PRE_CHANGEADDR",
+	[NETDEV_GOING_DOWN] = "NETDEV_GOING_DOWN",
+	[NETDEV_CHANGENAME] = "NETDEV_CHANGENAME",
+	[NETDEV_FEAT_CHANGE] = "NETDEV_FEAT_CHANGE",
+	[NETDEV_BONDING_FAILOVER] = "NETDEV_BONDING_FAILOVER",
+	[NETDEV_PRE_UP] = "NETDEV_PRE_UP",
+	[NETDEV_PRE_TYPE_CHANGE] = "NETDEV_PRE_TYPE_CHANGE",
+	[NETDEV_POST_TYPE_CHANGE] = "NETDEV_POST_TYPE_CHANGE",
+	[NETDEV_POST_INIT] = "NETDEV_POST_INIT",
+	[NETDEV_RELEASE] = "NETDEV_RELEASE",
+	[NETDEV_NOTIFY_PEERS] = "NETDEV_NOTIFY_PEERS",
+	[NETDEV_JOIN] = "NETDEV_JOIN",
+	[NETDEV_CHANGEUPPER] = "NETDEV_CHANGEUPPER",
+	[NETDEV_RESEND_IGMP] = "NETDEV_RESEND_IGMP",
+	[NETDEV_PRECHANGEMTU] = "NETDEV_PRECHANGEMTU",
+	[NETDEV_CHANGEINFODATA] = "NETDEV_CHANGEINFODATA",
+	[NETDEV_BONDING_INFO] = "NETDEV_BONDING_INFO",
+	[NETDEV_PRECHANGEUPPER] = "NETDEV_PRECHANGEUPPER",
+	[NETDEV_CHANGELOWERSTATE] = "NETDEV_CHANGELOWERSTATE",
+	[NETDEV_UDP_TUNNEL_PUSH_INFO] = "NETDEV_UDP_TUNNEL_PUSH_INFO",
+	[NETDEV_UDP_TUNNEL_DROP_INFO] = "NETDEV_UDP_TUNNEL_DROP_INFO",
+	[NETDEV_CHANGE_TX_QUEUE_LEN] = "NETDEV_CHANGE_TX_QUEUE_LEN",
+	[NETDEV_CVLAN_FILTER_PUSH_INFO] = "NETDEV_CVLAN_FILTER_PUSH_INFO",
+	[NETDEV_CVLAN_FILTER_DROP_INFO] = "NETDEV_CVLAN_FILTER_DROP_INFO",
+	[NETDEV_SVLAN_FILTER_PUSH_INFO] = "NETDEV_SVLAN_FILTER_PUSH_INFO",
+	[NETDEV_SVLAN_FILTER_DROP_INFO] = "NETDEV_SVLAN_FILTER_DROP_INFO"
+};
+
+DEF_ENUM_MAP(switchdev_notifier_type) = {
+	[SWITCHDEV_FDB_ADD_TO_BRIDGE] = "SWITCHDEV_FDB_ADD_TO_BRIDGE",
+	[SWITCHDEV_FDB_DEL_TO_BRIDGE] = "SWITCHDEV_FDB_DEL_TO_BRIDGE",
+	[SWITCHDEV_FDB_ADD_TO_DEVICE] = "SWITCHDEV_FDB_ADD_TO_DEVICE",
+	[SWITCHDEV_FDB_DEL_TO_DEVICE] = "SWITCHDEV_FDB_DEL_TO_DEVICE",
+	[SWITCHDEV_FDB_OFFLOADED] = "SWITCHDEV_FDB_OFFLOADED",
+	[SWITCHDEV_PORT_OBJ_ADD] = "SWITCHDEV_PORT_OBJ_ADD",
+	[SWITCHDEV_PORT_OBJ_DEL] = "SWITCHDEV_PORT_OBJ_DEL",
+	[SWITCHDEV_PORT_ATTR_SET] = "SWITCHDEV_PORT_ATTR_SET",
+	[SWITCHDEV_VXLAN_FDB_ADD_TO_BRIDGE] =
+		"SWITCHDEV_VXLAN_FDB_ADD_TO_BRIDGE",
+	[SWITCHDEV_VXLAN_FDB_DEL_TO_BRIDGE] =
+		"SWITCHDEV_VXLAN_FDB_DEL_TO_BRIDGE",
+	[SWITCHDEV_VXLAN_FDB_ADD_TO_DEVICE] =
+		"SWITCHDEV_VXLAN_FDB_ADD_TO_DEVICE",
+	[SWITCHDEV_VXLAN_FDB_DEL_TO_DEVICE] =
+		"SWITCHDEV_VXLAN_FDB_DEL_TO_DEVICE",
+	[SWITCHDEV_VXLAN_FDB_OFFLOADED] = "SWITCHDEV_VXLAN_FDB_OFFLOADED"
+};
+
+DEF_ENUM_MAP(switchdev_attr_id) = {
+	[SWITCHDEV_ATTR_ID_UNDEFINED] =
+		"SWITCHDEV_ATTR_ID_UNDEFINED",
+	[SWITCHDEV_ATTR_ID_PORT_STP_STATE] =
+		"SWITCHDEV_ATTR_ID_PORT_STP_STATE",
+	[SWITCHDEV_ATTR_ID_PORT_BRIDGE_FLAGS] =
+		"SWITCHDEV_ATTR_ID_PORT_BRIDGE_FLAGS",
+	[SWITCHDEV_ATTR_ID_PORT_PRE_BRIDGE_FLAGS] =
+		"SWITCHDEV_ATTR_ID_PORT_PRE_BRIDGE_FLAGS",
+	[SWITCHDEV_ATTR_ID_PORT_MROUTER] =
+		"SWITCHDEV_ATTR_ID_PORT_MROUTER",
+	[SWITCHDEV_ATTR_ID_BRIDGE_AGEING_TIME] =
+		"SWITCHDEV_ATTR_ID_BRIDGE_AGEING_TIME",
+	[SWITCHDEV_ATTR_ID_BRIDGE_VLAN_FILTERING] =
+		"SWITCHDEV_ATTR_ID_BRIDGE_VLAN_FILTERING",
+	[SWITCHDEV_ATTR_ID_BRIDGE_MC_DISABLED] =
+		"SWITCHDEV_ATTR_ID_BRIDGE_MC_DISABLED",
+	[SWITCHDEV_ATTR_ID_BRIDGE_MROUTER] =
+		"SWITCHDEV_ATTR_ID_BRIDGE_MROUTER"
+};
+
+DEF_ENUM_MAP(switchdev_obj_id) = {
+	[SWITCHDEV_OBJ_ID_UNDEFINED] = "SWITCHDEV_OBJ_ID_UNDEFINED",
+	[SWITCHDEV_OBJ_ID_PORT_VLAN] = "SWITCHDEV_OBJ_ID_PORT_VLAN",
+	[SWITCHDEV_OBJ_ID_PORT_MDB] = "SWITCHDEV_OBJ_ID_PORT_MDB",
+	[SWITCHDEV_OBJ_ID_HOST_MDB] = "SWITCHDEV_OBJ_ID_HOST_MDB",
+};
+
+DEF_ENUM_MAP(fib_event_type) = {
+	[FIB_EVENT_ENTRY_REPLACE] = "FIB_EVENT_ENTRY_REPLACE",
+	[FIB_EVENT_ENTRY_APPEND] = "FIB_EVENT_ENTRY_APPEND",
+	[FIB_EVENT_ENTRY_ADD] = "FIB_EVENT_ENTRY_ADD",
+	[FIB_EVENT_ENTRY_DEL] = "FIB_EVENT_ENTRY_DEL",
+	[FIB_EVENT_RULE_ADD] = "FIB_EVENT_RULE_ADD",
+	[FIB_EVENT_RULE_DEL] = "FIB_EVENT_RULE_DEL",
+	[FIB_EVENT_NH_ADD] = "FIB_EVENT_NH_ADD",
+	[FIB_EVENT_NH_DEL] = "FIB_EVENT_NH_DEL",
+	[FIB_EVENT_VIF_ADD] = "FIB_EVENT_VIF_ADD",
+	[FIB_EVENT_VIF_DEL] = "FIB_EVENT_VIF_DEL",
+};
+
+DEF_ENUM_MAP(netevent_notif_type) = {
+	[NETEVENT_NEIGH_UPDATE] = "NETEVENT_NEIGH_UPDATE",
+	[NETEVENT_REDIRECT] = "NETEVENT_REDIRECT",
+	[NETEVENT_DELAY_PROBE_TIME_UPDATE] =
+		"NETEVENT_DELAY_PROBE_TIME_UPDATE",
+	[NETEVENT_IPV4_MPATH_HASH_UPDATE] =
+		"NETEVENT_IPV4_MPATH_HASH_UPDATE",
+	[NETEVENT_IPV6_MPATH_HASH_UPDATE] =
+		"NETEVENT_IPV6_MPATH_HASH_UPDATE",
+	[NETEVENT_IPV4_FWD_UPDATE_PRIORITY_UPDATE] =
+		"NETEVENT_IPV4_FWD_UPDATE_PRIORITY_UPDATE",
+};
+
+DEF_ENUM_MAP(tc_setup_type) = {
+	[TC_SETUP_QDISC_MQPRIO] = "TC_SETUP_QDISC_MQPRIO",
+	[TC_SETUP_CLSU32] = "TC_SETUP_CLSU32",
+	[TC_SETUP_CLSFLOWER] = "TC_SETUP_CLSFLOWER",
+	[TC_SETUP_CLSMATCHALL] = "TC_SETUP_CLSMATCHALL",
+	[TC_SETUP_CLSBPF] = "TC_SETUP_CLSBPF",
+	[TC_SETUP_BLOCK] = "TC_SETUP_BLOCK",
+	[TC_SETUP_QDISC_CBS] = "TC_SETUP_QDISC_CBS",
+	[TC_SETUP_QDISC_RED] = "TC_SETUP_QDISC_RED",
+	[TC_SETUP_QDISC_PRIO] = "TC_SETUP_QDISC_PRIO",
+	[TC_SETUP_QDISC_MQ] = "TC_SETUP_QDISC_MQ",
+	[TC_SETUP_QDISC_ETF] = "TC_SETUP_QDISC_ETF",
+	[TC_SETUP_ROOT_QDISC] = "TC_SETUP_ROOT_QDISC",
+	[TC_SETUP_QDISC_GRED] = "TC_SETUP_QDISC_GRED",
+};
+
+DEF_ENUM_MAP(flow_block_binder_type) = {
+	[FLOW_BLOCK_BINDER_TYPE_UNSPEC] =
+		"FLOW_BLOCK_BINDER_TYPE_UNSPEC",
+	[FLOW_BLOCK_BINDER_TYPE_CLSACT_INGRESS] =
+		"FLOW_BLOCK_BINDER_TYPE_CLSACT_INGRESS",
+	[FLOW_BLOCK_BINDER_TYPE_CLSACT_EGRESS] =
+		"FLOW_BLOCK_BINDER_TYPE_CLSACT_EGRESS",
+};
+
+DEF_ENUM_MAP(tc_matchall_command) = {
+	[TC_CLSMATCHALL_REPLACE] = "TC_CLSMATCHALL_REPLACE",
+	[TC_CLSMATCHALL_DESTROY] = "TC_CLSMATCHALL_DESTROY",
+	[TC_CLSMATCHALL_STATS] = "TC_CLSMATCHALL_STATS",
+};
+
+DEF_ENUM_MAP(flow_cls_command) = {
+	[FLOW_CLS_REPLACE] = "FLOW_CLS_REPLACE",
+	[FLOW_CLS_DESTROY] = "FLOW_CLS_DESTROY",
+	[FLOW_CLS_STATS] = "FLOW_CLS_STATS",
+	[FLOW_CLS_TMPLT_CREATE] = "FLOW_CLS_TMPLT_CREATE",
+	[FLOW_CLS_TMPLT_DESTROY] = "FLOW_CLS_TMPLT_DESTROY",
+};
+
+DEF_ENUM_MAP(flow_action_id) = {
+	[FLOW_ACTION_ACCEPT] = "FLOW_ACTION_ACCEPT",
+	[FLOW_ACTION_DROP] = "FLOW_ACTION_DROP",
+	[FLOW_ACTION_TRAP] = "FLOW_ACTION_TRAP",
+	[FLOW_ACTION_GOTO] = "FLOW_ACTION_GOTO",
+	[FLOW_ACTION_REDIRECT] = "FLOW_ACTION_REDIRECT",
+	[FLOW_ACTION_MIRRED] = "FLOW_ACTION_MIRRED",
+	[FLOW_ACTION_VLAN_PUSH] = "FLOW_ACTION_VLAN_PUSH",
+	[FLOW_ACTION_VLAN_POP] = "FLOW_ACTION_VLAN_POP",
+	[FLOW_ACTION_VLAN_MANGLE] = "FLOW_ACTION_VLAN_MANGLE",
+	[FLOW_ACTION_TUNNEL_ENCAP] = "FLOW_ACTION_TUNNEL_ENCAP",
+	[FLOW_ACTION_TUNNEL_DECAP] = "FLOW_ACTION_TUNNEL_DECAP",
+	[FLOW_ACTION_MANGLE] = "FLOW_ACTION_MANGLE",
+	[FLOW_ACTION_ADD] = "FLOW_ACTION_ADD",
+	[FLOW_ACTION_CSUM] = "FLOW_ACTION_CSUM",
+	[FLOW_ACTION_MARK] = "FLOW_ACTION_MARK",
+	[FLOW_ACTION_WAKE] = "FLOW_ACTION_WAKE",
+	[FLOW_ACTION_QUEUE] = "FLOW_ACTION_QUEUE",
+	[FLOW_ACTION_SAMPLE] = "FLOW_ACTION_SAMPLE",
+	[FLOW_ACTION_POLICE] = "FLOW_ACTION_POLICE",
+	[FLOW_ACTION_CT] = "FLOW_ACTION_CT",
+};
+
+DEF_ENUM_FUNC(netdev_cmd, NETDEV_UP, NETDEV_SVLAN_FILTER_DROP_INFO)
+
+DEF_ENUM_FUNC(switchdev_notifier_type, SWITCHDEV_FDB_ADD_TO_BRIDGE,
+	      SWITCHDEV_VXLAN_FDB_OFFLOADED)
+DEF_ENUM_FUNC(switchdev_attr_id, SWITCHDEV_ATTR_ID_UNDEFINED,
+	      SWITCHDEV_ATTR_ID_BRIDGE_MROUTER)
+DEF_ENUM_FUNC(switchdev_obj_id, SWITCHDEV_OBJ_ID_UNDEFINED,
+	      SWITCHDEV_OBJ_ID_HOST_MDB)
+
+DEF_ENUM_FUNC(fib_event_type, FIB_EVENT_ENTRY_REPLACE, FIB_EVENT_VIF_DEL)
+
+DEF_ENUM_FUNC(netevent_notif_type, NETEVENT_NEIGH_UPDATE,
+	      NETEVENT_IPV4_FWD_UPDATE_PRIORITY_UPDATE)
+
+/* TC traffic control */
+DEF_ENUM_FUNC(tc_setup_type, TC_SETUP_QDISC_MQPRIO, TC_SETUP_QDISC_GRED)
+DEF_ENUM_FUNC(flow_block_binder_type, FLOW_BLOCK_BINDER_TYPE_UNSPEC,
+	      FLOW_BLOCK_BINDER_TYPE_CLSACT_EGRESS)
+DEF_ENUM_FUNC(tc_matchall_command, TC_CLSMATCHALL_REPLACE, TC_CLSMATCHALL_STATS)
+DEF_ENUM_FUNC(flow_cls_command, FLOW_CLS_REPLACE, FLOW_CLS_TMPLT_DESTROY)
+DEF_ENUM_FUNC(flow_action_id, FLOW_ACTION_ACCEPT, FLOW_ACTION_CT)
diff --git a/drivers/net/ethernet/marvell/prestera/prestera_log.h b/drivers/net/ethernet/marvell/prestera/prestera_log.h
new file mode 100644
index 000000000..aacb296c7
--- /dev/null
+++ b/drivers/net/ethernet/marvell/prestera/prestera_log.h
@@ -0,0 +1,58 @@
+/* SPDX-License-Identifier: BSD-3-Clause OR GPL-2.0
+ *
+ * Copyright (c) 2019-2020 Marvell International Ltd. All rights reserved.
+ *
+ */
+
+#ifndef _MVSW_PRESTERA_LOG_H_
+#define _MVSW_PRESTERA_LOG_H_
+
+#ifdef CONFIG_MRVL_PRESTERA_DEBUG
+
+#include <linux/netdevice.h>
+#include <linux/version.h>
+#include <net/switchdev.h>
+#include <net/fib_notifier.h>
+#include <net/netevent.h>
+#include <net/pkt_cls.h>
+
+#define DEF_ENUM_MAP(enum_name) \
+static const char *enum_name##_map[]
+
+#define DEF_ENUM_FUNC(enum_name, enum_min, enum_max) \
+const char *enum_name##_to_name(enum enum_name val) \
+{ \
+	if (val < enum_min || val > enum_max) \
+		return unknown; \
+	return enum_name##_map[val]; \
+}
+
+#define DEC_ENUM_FUNC(enum_name) \
+const char *enum_name##_to_name(enum enum_name)
+
+#define ENUM_TO_NAME(enum_name, val) enum_name##_to_name(val)
+
+#define MVSW_LOG_INFO(fmt, ...) \
+	pr_info("%s:%d: " fmt "\n", __func__, __LINE__, ##__VA_ARGS__)
+
+#define MVSW_LOG_ERROR(fmt, ...) \
+	pr_err("%s:%d: " fmt "\n", __func__, __LINE__, ##__VA_ARGS__)
+
+DEC_ENUM_FUNC(netdev_cmd);
+DEC_ENUM_FUNC(switchdev_notifier_type);
+DEC_ENUM_FUNC(switchdev_attr_id);
+DEC_ENUM_FUNC(switchdev_obj_id);
+DEC_ENUM_FUNC(fib_event_type);
+DEC_ENUM_FUNC(netevent_notif_type);
+DEC_ENUM_FUNC(tc_setup_type);
+DEC_ENUM_FUNC(flow_block_binder_type);
+DEC_ENUM_FUNC(tc_matchall_command);
+DEC_ENUM_FUNC(flow_cls_command);
+DEC_ENUM_FUNC(flow_action_id);
+
+#else /* CONFIG_MRVL_PRESTERA_DEBUG */
+#define MVSW_LOG_INFO(...)
+#define MVSW_LOG_ERROR(...)
+#endif /* CONFIG_MRVL_PRESTERA_DEBUG */
+
+#endif /* _MVSW_PRESTERA_LOG_H_ */
diff --git a/drivers/net/ethernet/marvell/prestera/prestera_main.c b/drivers/net/ethernet/marvell/prestera/prestera_main.c
index da4b286d1..05c9dc8f2 100644
--- a/drivers/net/ethernet/marvell/prestera/prestera_main.c
+++ b/drivers/net/ethernet/marvell/prestera/prestera_main.c
@@ -1,625 +1,2502 @@
-// SPDX-License-Identifier: BSD-3-Clause OR GPL-2.0
-/* Copyright (c) 2019-2020 Marvell International Ltd. All rights reserved */
-
-#include <linux/etherdevice.h>
-#include <linux/jiffies.h>
-#include <linux/list.h>
+/* SPDX-License-Identifier: BSD-3-Clause OR GPL-2.0
+ *
+ * Copyright (c) 2019-2020 Marvell International Ltd. All rights reserved.
+ *
+ */
+#include <linux/kernel.h>
 #include <linux/module.h>
+#include <linux/list.h>
+#include <linux/netdevice.h>
 #include <linux/netdev_features.h>
+#include <linux/inetdevice.h>
+#include <linux/etherdevice.h>
+#include <linux/ethtool.h>
+#include <linux/jiffies.h>
+#include <linux/if_bridge.h>
+#include <linux/phylink.h>
+#include <linux/of.h>
+#include <net/switchdev.h>
 #include <linux/of.h>
 #include <linux/of_net.h>
 
 #include "prestera.h"
+#include "prestera_log.h"
 #include "prestera_hw.h"
-#include "prestera_rxtx.h"
+#include "prestera_debugfs.h"
 #include "prestera_devlink.h"
-#include "prestera_ethtool.h"
-#include "prestera_switchdev.h"
+#include "prestera_dsa.h"
+#include "prestera_rxtx.h"
+#include "prestera_drv_ver.h"
+
+static u8 trap_policer_profile = 1;
+
+#define MVSW_PR_MTU_DEFAULT 1536
+#define MVSW_PR_MAC_ADDR_OFFSET 4
+
+#define PORT_STATS_CACHE_TIMEOUT_MS	(msecs_to_jiffies(1000))
+#define PORT_STATS_CNT	(sizeof(struct mvsw_pr_port_stats) / sizeof(u64))
+#define PORT_STATS_IDX(name) \
+	(offsetof(struct mvsw_pr_port_stats, name) / sizeof(u64))
+#define PORT_STATS_FIELD(name)	\
+	[PORT_STATS_IDX(name)] = __stringify(name)
+
+static struct list_head switches_registered;
+
+static const char mvsw_driver_kind[] = "prestera_sw";
+static const char mvsw_driver_name[] = "mvsw_switchdev";
+static const char mvsw_driver_version[] = PRESTERA_DRV_VER;
+
+#define mvsw_dev(sw)		((sw)->dev->dev)
+#define mvsw_dev_name(sw)	dev_name((sw)->dev->dev)
 
-#define PRESTERA_MTU_DEFAULT	1536
+static struct workqueue_struct *mvsw_pr_wq;
 
-#define PRESTERA_STATS_DELAY_MS	1000
+struct mvsw_pr_link_mode {
+	enum ethtool_link_mode_bit_indices eth_mode;
+	u32 speed;
+	u64 pr_mask;
+	u8 duplex;
+	u8 port_type;
+};
+
+static const struct mvsw_pr_link_mode
+mvsw_pr_link_modes[MVSW_LINK_MODE_MAX] = {
+	[MVSW_LINK_MODE_10baseT_Half_BIT] = {
+		.eth_mode =  ETHTOOL_LINK_MODE_10baseT_Half_BIT,
+		.speed = 10,
+		.pr_mask = 1 << MVSW_LINK_MODE_10baseT_Half_BIT,
+		.duplex = MVSW_PORT_DUPLEX_HALF,
+		.port_type = MVSW_PORT_TYPE_TP,
+	},
+	[MVSW_LINK_MODE_10baseT_Full_BIT] = {
+		.eth_mode =  ETHTOOL_LINK_MODE_10baseT_Full_BIT,
+		.speed = 10,
+		.pr_mask = 1 << MVSW_LINK_MODE_10baseT_Full_BIT,
+		.duplex = MVSW_PORT_DUPLEX_FULL,
+		.port_type = MVSW_PORT_TYPE_TP,
+	},
+	[MVSW_LINK_MODE_100baseT_Half_BIT] = {
+		.eth_mode =  ETHTOOL_LINK_MODE_100baseT_Half_BIT,
+		.speed = 100,
+		.pr_mask = 1 << MVSW_LINK_MODE_100baseT_Half_BIT,
+		.duplex = MVSW_PORT_DUPLEX_HALF,
+		.port_type = MVSW_PORT_TYPE_TP,
+	},
+	[MVSW_LINK_MODE_100baseT_Full_BIT] = {
+		.eth_mode =  ETHTOOL_LINK_MODE_100baseT_Full_BIT,
+		.speed = 100,
+		.pr_mask = 1 << MVSW_LINK_MODE_100baseT_Full_BIT,
+		.duplex = MVSW_PORT_DUPLEX_FULL,
+		.port_type = MVSW_PORT_TYPE_TP,
+	},
+	[MVSW_LINK_MODE_1000baseT_Half_BIT] = {
+		.eth_mode =  ETHTOOL_LINK_MODE_1000baseT_Half_BIT,
+		.speed = 1000,
+		.pr_mask = 1 << MVSW_LINK_MODE_1000baseT_Half_BIT,
+		.duplex = MVSW_PORT_DUPLEX_HALF,
+		.port_type = MVSW_PORT_TYPE_TP,
+	},
+	[MVSW_LINK_MODE_1000baseT_Full_BIT] = {
+		.eth_mode =  ETHTOOL_LINK_MODE_1000baseT_Full_BIT,
+		.speed = 1000,
+		.pr_mask = 1 << MVSW_LINK_MODE_1000baseT_Full_BIT,
+		.duplex = MVSW_PORT_DUPLEX_FULL,
+		.port_type = MVSW_PORT_TYPE_TP,
+	},
+	[MVSW_LINK_MODE_1000baseX_Full_BIT] = {
+		.eth_mode = ETHTOOL_LINK_MODE_1000baseX_Full_BIT,
+		.speed = 1000,
+		.pr_mask = 1 << MVSW_LINK_MODE_1000baseX_Full_BIT,
+		.duplex = MVSW_PORT_DUPLEX_FULL,
+		.port_type = MVSW_PORT_TYPE_FIBRE,
+	},
+	[MVSW_LINK_MODE_1000baseKX_Full_BIT] = {
+		.eth_mode = ETHTOOL_LINK_MODE_1000baseKX_Full_BIT,
+		.speed = 1000,
+		.pr_mask = 1 << MVSW_LINK_MODE_1000baseKX_Full_BIT,
+		.duplex = MVSW_PORT_DUPLEX_FULL,
+		.port_type = MVSW_PORT_TYPE_TP,
+	},
+	[MVSW_LINK_MODE_2500baseX_Full_BIT] = {
+		.eth_mode =  ETHTOOL_LINK_MODE_2500baseX_Full_BIT,
+		.speed = 2500,
+		.pr_mask = 1 << MVSW_LINK_MODE_2500baseX_Full_BIT,
+		.duplex = MVSW_PORT_DUPLEX_FULL,
+	},
+	[MVSW_LINK_MODE_10GbaseKR_Full_BIT] = {
+		.eth_mode = ETHTOOL_LINK_MODE_10000baseKR_Full_BIT,
+		.speed = 10000,
+		.pr_mask = 1 << MVSW_LINK_MODE_10GbaseKR_Full_BIT,
+		.duplex = MVSW_PORT_DUPLEX_FULL,
+		.port_type = MVSW_PORT_TYPE_TP,
+	},
+	[MVSW_LINK_MODE_10GbaseSR_Full_BIT] = {
+		.eth_mode = ETHTOOL_LINK_MODE_10000baseSR_Full_BIT,
+		.speed = 10000,
+		.pr_mask = 1 << MVSW_LINK_MODE_10GbaseSR_Full_BIT,
+		.duplex = MVSW_PORT_DUPLEX_FULL,
+		.port_type = MVSW_PORT_TYPE_FIBRE,
+	},
+	[MVSW_LINK_MODE_10GbaseLR_Full_BIT] = {
+		.eth_mode = ETHTOOL_LINK_MODE_10000baseLR_Full_BIT,
+		.speed = 10000,
+		.pr_mask = 1 << MVSW_LINK_MODE_10GbaseLR_Full_BIT,
+		.duplex = MVSW_PORT_DUPLEX_FULL,
+		.port_type = MVSW_PORT_TYPE_FIBRE,
+	},
+	[MVSW_LINK_MODE_20GbaseKR2_Full_BIT] = {
+		.eth_mode = ETHTOOL_LINK_MODE_20000baseKR2_Full_BIT,
+		.speed = 20000,
+		.pr_mask = 1 << MVSW_LINK_MODE_20GbaseKR2_Full_BIT,
+		.duplex = MVSW_PORT_DUPLEX_FULL,
+		.port_type = MVSW_PORT_TYPE_TP,
+	},
+	[MVSW_LINK_MODE_25GbaseCR_Full_BIT] = {
+		.eth_mode = ETHTOOL_LINK_MODE_25000baseCR_Full_BIT,
+		.speed = 25000,
+		.pr_mask = 1 << MVSW_LINK_MODE_25GbaseCR_Full_BIT,
+		.duplex = MVSW_PORT_DUPLEX_FULL,
+		.port_type = MVSW_PORT_TYPE_DA,
+	},
+	[MVSW_LINK_MODE_25GbaseKR_Full_BIT] = {
+		.eth_mode = ETHTOOL_LINK_MODE_25000baseKR_Full_BIT,
+		.speed = 25000,
+		.pr_mask = 1 << MVSW_LINK_MODE_25GbaseKR_Full_BIT,
+		.duplex = MVSW_PORT_DUPLEX_FULL,
+		.port_type = MVSW_PORT_TYPE_TP,
+	},
+	[MVSW_LINK_MODE_25GbaseSR_Full_BIT] = {
+		.eth_mode = ETHTOOL_LINK_MODE_25000baseSR_Full_BIT,
+		.speed = 25000,
+		.pr_mask = 1 << MVSW_LINK_MODE_25GbaseSR_Full_BIT,
+		.duplex = MVSW_PORT_DUPLEX_FULL,
+		.port_type = MVSW_PORT_TYPE_FIBRE,
+	},
+	[MVSW_LINK_MODE_40GbaseKR4_Full_BIT] = {
+		.eth_mode = ETHTOOL_LINK_MODE_40000baseKR4_Full_BIT,
+		.speed = 40000,
+		.pr_mask = 1 << MVSW_LINK_MODE_40GbaseKR4_Full_BIT,
+		.duplex = MVSW_PORT_DUPLEX_FULL,
+		.port_type = MVSW_PORT_TYPE_TP,
+	},
+	[MVSW_LINK_MODE_40GbaseCR4_Full_BIT] = {
+		.eth_mode = ETHTOOL_LINK_MODE_40000baseCR4_Full_BIT,
+		.speed = 40000,
+		.pr_mask = 1 << MVSW_LINK_MODE_40GbaseCR4_Full_BIT,
+		.duplex = MVSW_PORT_DUPLEX_FULL,
+		.port_type = MVSW_PORT_TYPE_DA,
+	},
+	[MVSW_LINK_MODE_40GbaseSR4_Full_BIT] = {
+		.eth_mode = ETHTOOL_LINK_MODE_40000baseSR4_Full_BIT,
+		.speed = 40000,
+		.pr_mask = 1 << MVSW_LINK_MODE_40GbaseSR4_Full_BIT,
+		.duplex = MVSW_PORT_DUPLEX_FULL,
+		.port_type = MVSW_PORT_TYPE_FIBRE,
+	},
+	[MVSW_LINK_MODE_50GbaseCR2_Full_BIT] = {
+		.eth_mode = ETHTOOL_LINK_MODE_50000baseCR2_Full_BIT,
+		.speed = 50000,
+		.pr_mask = 1 << MVSW_LINK_MODE_50GbaseCR2_Full_BIT,
+		.duplex = MVSW_PORT_DUPLEX_FULL,
+		.port_type = MVSW_PORT_TYPE_DA,
+	},
+	[MVSW_LINK_MODE_50GbaseKR2_Full_BIT] = {
+		.eth_mode = ETHTOOL_LINK_MODE_50000baseKR2_Full_BIT,
+		.speed = 50000,
+		.pr_mask = 1 << MVSW_LINK_MODE_50GbaseKR2_Full_BIT,
+		.duplex = MVSW_PORT_DUPLEX_FULL,
+		.port_type = MVSW_PORT_TYPE_TP,
+	},
+	[MVSW_LINK_MODE_50GbaseSR2_Full_BIT] = {
+		.eth_mode = ETHTOOL_LINK_MODE_50000baseSR2_Full_BIT,
+		.speed = 50000,
+		.pr_mask = 1 << MVSW_LINK_MODE_50GbaseSR2_Full_BIT,
+		.duplex = MVSW_PORT_DUPLEX_FULL,
+		.port_type = MVSW_PORT_TYPE_FIBRE,
+	},
+	[MVSW_LINK_MODE_100GbaseKR4_Full_BIT] = {
+		.eth_mode = ETHTOOL_LINK_MODE_100000baseKR4_Full_BIT,
+		.speed = 100000,
+		.pr_mask = 1 << MVSW_LINK_MODE_100GbaseKR4_Full_BIT,
+		.duplex = MVSW_PORT_DUPLEX_FULL,
+		.port_type = MVSW_PORT_TYPE_TP,
+	},
+	[MVSW_LINK_MODE_100GbaseSR4_Full_BIT] = {
+		.eth_mode = ETHTOOL_LINK_MODE_100000baseSR4_Full_BIT,
+		.speed = 100000,
+		.pr_mask = 1 << MVSW_LINK_MODE_100GbaseSR4_Full_BIT,
+		.duplex = MVSW_PORT_DUPLEX_FULL,
+		.port_type = MVSW_PORT_TYPE_FIBRE,
+	},
+	[MVSW_LINK_MODE_100GbaseCR4_Full_BIT] = {
+		.eth_mode = ETHTOOL_LINK_MODE_100000baseCR4_Full_BIT,
+		.speed = 100000,
+		.pr_mask = 1 << MVSW_LINK_MODE_100GbaseCR4_Full_BIT,
+		.duplex = MVSW_PORT_DUPLEX_FULL,
+		.port_type = MVSW_PORT_TYPE_DA,
+	}
+};
+
+struct mvsw_pr_fec {
+	u32 eth_fec;
+	enum ethtool_link_mode_bit_indices eth_mode;
+	u8 pr_fec;
+};
+
+static const struct mvsw_pr_fec mvsw_pr_fec_caps[MVSW_PORT_FEC_MAX] = {
+	[MVSW_PORT_FEC_OFF_BIT] = {
+		.eth_fec = ETHTOOL_FEC_OFF,
+		.eth_mode = ETHTOOL_LINK_MODE_FEC_NONE_BIT,
+		.pr_fec = 1 << MVSW_PORT_FEC_OFF_BIT,
+	},
+	[MVSW_PORT_FEC_BASER_BIT] = {
+		.eth_fec = ETHTOOL_FEC_BASER,
+		.eth_mode = ETHTOOL_LINK_MODE_FEC_BASER_BIT,
+		.pr_fec = 1 << MVSW_PORT_FEC_BASER_BIT,
+	},
+	[MVSW_PORT_FEC_RS_BIT] = {
+		.eth_fec = ETHTOOL_FEC_RS,
+		.eth_mode = ETHTOOL_LINK_MODE_FEC_RS_BIT,
+		.pr_fec = 1 << MVSW_PORT_FEC_RS_BIT,
+	}
+};
+
+struct mvsw_pr_port_type {
+	enum ethtool_link_mode_bit_indices eth_mode;
+	u8 eth_type;
+};
+
+static const struct mvsw_pr_port_type
+mvsw_pr_port_types[MVSW_PORT_TYPE_MAX] = {
+	[MVSW_PORT_TYPE_NONE] = {
+		.eth_mode = __ETHTOOL_LINK_MODE_MASK_NBITS,
+		.eth_type = PORT_NONE,
+	},
+	[MVSW_PORT_TYPE_TP] = {
+		.eth_mode = ETHTOOL_LINK_MODE_TP_BIT,
+		.eth_type = PORT_TP,
+	},
+	[MVSW_PORT_TYPE_AUI] = {
+		.eth_mode = ETHTOOL_LINK_MODE_AUI_BIT,
+		.eth_type = PORT_AUI,
+	},
+	[MVSW_PORT_TYPE_MII] = {
+		.eth_mode = ETHTOOL_LINK_MODE_MII_BIT,
+		.eth_type = PORT_MII,
+	},
+	[MVSW_PORT_TYPE_FIBRE] = {
+		.eth_mode = ETHTOOL_LINK_MODE_FIBRE_BIT,
+		.eth_type = PORT_FIBRE,
+	},
+	[MVSW_PORT_TYPE_BNC] = {
+		.eth_mode = ETHTOOL_LINK_MODE_BNC_BIT,
+		.eth_type = PORT_BNC,
+	},
+	[MVSW_PORT_TYPE_DA] = {
+		.eth_mode = ETHTOOL_LINK_MODE_TP_BIT,
+		.eth_type = PORT_TP,
+	},
+	[MVSW_PORT_TYPE_OTHER] = {
+		.eth_mode = __ETHTOOL_LINK_MODE_MASK_NBITS,
+		.eth_type = PORT_OTHER,
+	}
+};
 
-#define PRESTERA_MAC_ADDR_NUM_MAX	255
+static const char mvsw_pr_port_cnt_name[PORT_STATS_CNT][ETH_GSTRING_LEN] = {
+	PORT_STATS_FIELD(good_octets_received),
+	PORT_STATS_FIELD(bad_octets_received),
+	PORT_STATS_FIELD(mac_trans_error),
+	PORT_STATS_FIELD(broadcast_frames_received),
+	PORT_STATS_FIELD(multicast_frames_received),
+	PORT_STATS_FIELD(frames_64_octets),
+	PORT_STATS_FIELD(frames_65_to_127_octets),
+	PORT_STATS_FIELD(frames_128_to_255_octets),
+	PORT_STATS_FIELD(frames_256_to_511_octets),
+	PORT_STATS_FIELD(frames_512_to_1023_octets),
+	PORT_STATS_FIELD(frames_1024_to_max_octets),
+	PORT_STATS_FIELD(excessive_collision),
+	PORT_STATS_FIELD(multicast_frames_sent),
+	PORT_STATS_FIELD(broadcast_frames_sent),
+	PORT_STATS_FIELD(fc_sent),
+	PORT_STATS_FIELD(fc_received),
+	PORT_STATS_FIELD(buffer_overrun),
+	PORT_STATS_FIELD(undersize),
+	PORT_STATS_FIELD(fragments),
+	PORT_STATS_FIELD(oversize),
+	PORT_STATS_FIELD(jabber),
+	PORT_STATS_FIELD(rx_error_frame_received),
+	PORT_STATS_FIELD(bad_crc),
+	PORT_STATS_FIELD(collisions),
+	PORT_STATS_FIELD(late_collision),
+	PORT_STATS_FIELD(unicast_frames_received),
+	PORT_STATS_FIELD(unicast_frames_sent),
+	PORT_STATS_FIELD(sent_multiple),
+	PORT_STATS_FIELD(sent_deferred),
+	PORT_STATS_FIELD(good_octets_sent),
+};
 
-static struct workqueue_struct *prestera_wq;
+static LIST_HEAD(mvsw_pr_block_cb_list);
 
-int prestera_port_pvid_set(struct prestera_port *port, u16 vid)
+static struct mvsw_pr_port *__find_pr_port(const struct mvsw_pr_switch *sw,
+					   u32 port_id)
 {
-	enum prestera_accept_frm_type frm_type;
+	struct mvsw_pr_port *port;
+
+	list_for_each_entry(port, &sw->port_list, list) {
+		if (port->id == port_id)
+			return port;
+	}
+
+	return NULL;
+}
+
+static int mvsw_pr_port_state_set(struct net_device *dev, bool is_up)
+{
+	struct mvsw_pr_port *port = netdev_priv(dev);
 	int err;
 
-	frm_type = PRESTERA_ACCEPT_FRAME_TYPE_TAGGED;
+	if (!is_up)
+		netif_stop_queue(dev);
 
-	if (vid) {
-		err = prestera_hw_vlan_port_vid_set(port, vid);
-		if (err)
-			return err;
+	err = mvsw_pr_hw_port_state_set(port, is_up);
+
+	if (is_up && !err)
+		netif_start_queue(dev);
+
+	return err;
+}
+
+static int mvsw_pr_port_open(struct net_device *dev)
+{
+#ifdef CONFIG_PHYLINK
+	struct mvsw_pr_port *port = netdev_priv(dev);
+
+	if (port->caps.transceiver == MVSW_PORT_TRANSCEIVER_SFP)
+		phylink_start(port->phy_link);
+#endif
+	return mvsw_pr_port_state_set(dev, true);
+}
+
+static int mvsw_pr_port_close(struct net_device *dev)
+{
+	struct mvsw_pr_port *port = netdev_priv(dev);
+
+	mvsw_pr_fdb_flush_port(port, MVSW_PR_FDB_FLUSH_MODE_DYNAMIC);
+
+#ifdef CONFIG_PHYLINK
+	if (port->caps.transceiver == MVSW_PORT_TRANSCEIVER_SFP)
+		phylink_stop(port->phy_link);
+#endif
+	return mvsw_pr_port_state_set(dev, false);
+}
+
+static netdev_tx_t mvsw_pr_port_xmit(struct sk_buff *skb,
+				     struct net_device *dev)
+{
+	struct mvsw_pr_port *port = netdev_priv(dev);
+	struct mvsw_pr_rxtx_info rxtx_info = {
+		.port_id = port->id
+	};
+
+	return mvsw_pr_rxtx_xmit(skb, &rxtx_info);
+}
+
+/* TC flower */
+static int
+mvsw_pr_setup_tc_cls_flower(struct prestera_acl_block *acl_block,
+			    struct flow_cls_offload *f)
+{
+	struct mvsw_pr_switch *sw = prestera_acl_block_sw(acl_block);
+
+	if (f->common.chain_index != 0)
+		return -EOPNOTSUPP;
+
+	switch (f->command) {
+	case FLOW_CLS_REPLACE:
+		return mvsw_pr_flower_replace(sw, acl_block, f);
+	case FLOW_CLS_DESTROY:
+		mvsw_pr_flower_destroy(sw, acl_block, f);
+		return 0;
+	case FLOW_CLS_STATS:
+		return mvsw_pr_flower_stats(sw, acl_block, f);
+	default:
+		return -EOPNOTSUPP;
+	}
+}
+
+static int mvsw_pr_setup_tc_block_cb_flower(enum tc_setup_type type,
+					    void *type_data, void *cb_priv)
+{
+	struct prestera_acl_block *acl_block = cb_priv;
 
-		frm_type = PRESTERA_ACCEPT_FRAME_TYPE_ALL;
+	switch (type) {
+	case TC_SETUP_CLSFLOWER:
+		if (prestera_acl_block_disabled(acl_block))
+			return -EOPNOTSUPP;
+		return mvsw_pr_setup_tc_cls_flower(acl_block, type_data);
+	default:
+		return -EOPNOTSUPP;
 	}
+}
 
-	err = prestera_hw_port_accept_frm_type(port, frm_type);
-	if (err && frm_type == PRESTERA_ACCEPT_FRAME_TYPE_ALL)
-		prestera_hw_vlan_port_vid_set(port, port->pvid);
+static void mvsw_pr_tc_block_flower_release(void *cb_priv)
+{
+	struct prestera_acl_block *acl_block = cb_priv;
 
-	port->pvid = vid;
+	prestera_acl_block_destroy(acl_block);
+}
+
+static int
+mvsw_pr_setup_tc_block_flower_bind(struct mvsw_pr_port *port,
+				   struct flow_block_offload *f)
+{
+	struct mvsw_pr_switch *sw = port->sw;
+	struct prestera_acl_block *acl_block;
+	struct flow_block_cb *block_cb;
+	bool register_block = false;
+	bool disable_block = false;
+	int err;
+
+	block_cb = flow_block_cb_lookup(f->block,
+					mvsw_pr_setup_tc_block_cb_flower, sw);
+	if (!block_cb) {
+		acl_block = prestera_acl_block_create(sw, f->net);
+		if (!acl_block)
+			return -ENOMEM;
+		block_cb = flow_block_cb_alloc(mvsw_pr_setup_tc_block_cb_flower,
+					       sw, acl_block,
+					       mvsw_pr_tc_block_flower_release);
+		if (IS_ERR(block_cb)) {
+			prestera_acl_block_destroy(acl_block);
+			err = PTR_ERR(block_cb);
+			goto err_cb_register;
+		}
+		register_block = true;
+	} else {
+		acl_block = flow_block_cb_priv(block_cb);
+	}
+	flow_block_cb_incref(block_cb);
+
+	if (!tc_can_offload(port->net_dev)) {
+		if (prestera_acl_block_rule_count(acl_block)) {
+			err = -EOPNOTSUPP;
+			goto err_block_bind;
+		}
+
+		disable_block = true;
+	}
+
+	err = prestera_acl_block_bind(sw, acl_block, port);
+	if (err)
+		goto err_block_bind;
+
+	if (register_block) {
+		flow_block_cb_add(block_cb, f);
+		list_add_tail(&block_cb->driver_list, &mvsw_pr_block_cb_list);
+	}
+
+	if (disable_block)
+		prestera_acl_block_disable_inc(acl_block);
+
+	port->acl_block = acl_block;
 	return 0;
+
+err_block_bind:
+	if (!flow_block_cb_decref(block_cb))
+		flow_block_cb_free(block_cb);
+err_cb_register:
+	return err;
 }
 
-struct prestera_port *prestera_port_find_by_hwid(struct prestera_switch *sw,
-						 u32 dev_id, u32 hw_id)
+static void
+mvsw_pr_setup_tc_block_flower_unbind(struct mvsw_pr_port *port,
+				     struct flow_block_offload *f)
 {
-	struct prestera_port *port = NULL;
+	struct mvsw_pr_switch *sw = port->sw;
+	struct prestera_acl_block *acl_block;
+	struct flow_block_cb *block_cb;
+	int err;
 
-	read_lock(&sw->port_list_lock);
-	list_for_each_entry(port, &sw->port_list, list) {
-		if (port->dev_id == dev_id && port->hw_id == hw_id)
-			break;
+	block_cb = flow_block_cb_lookup(f->block,
+					mvsw_pr_setup_tc_block_cb_flower, sw);
+	if (!block_cb)
+		return;
+
+	acl_block = flow_block_cb_priv(block_cb);
+
+	if (!tc_can_offload(port->net_dev))
+		prestera_acl_block_disable_dec(acl_block);
+
+	err = prestera_acl_block_unbind(sw, acl_block, port);
+	if (!err && !flow_block_cb_decref(block_cb)) {
+		flow_block_cb_remove(block_cb, f);
+		list_del(&block_cb->driver_list);
 	}
-	read_unlock(&sw->port_list_lock);
+	port->acl_block = NULL;
+}
 
-	return port;
+static int mvsw_sp_setup_tc_block(struct mvsw_pr_port *port,
+				  struct flow_block_offload *f)
+{
+	if (f->binder_type != FLOW_BLOCK_BINDER_TYPE_CLSACT_INGRESS)
+		return -EOPNOTSUPP;
+
+	f->driver_block_list = &mvsw_pr_block_cb_list;
+
+	switch (f->command) {
+	case FLOW_BLOCK_BIND:
+		return mvsw_pr_setup_tc_block_flower_bind(port, f);
+	case FLOW_BLOCK_UNBIND:
+		mvsw_pr_setup_tc_block_flower_unbind(port, f);
+		return 0;
+	default:
+		return -EOPNOTSUPP;
+	}
 }
 
-struct prestera_port *prestera_find_port(struct prestera_switch *sw, u32 id)
+static int mvsw_pr_setup_tc(struct net_device *dev, enum tc_setup_type type,
+			    void *type_data)
 {
-	struct prestera_port *port = NULL;
+	struct mvsw_pr_port *port = netdev_priv(dev);
 
-	read_lock(&sw->port_list_lock);
-	list_for_each_entry(port, &sw->port_list, list) {
-		if (port->id == id)
-			break;
+	switch (type) {
+	case TC_SETUP_BLOCK:
+		return mvsw_sp_setup_tc_block(port, type_data);
+	default:
+		return -EOPNOTSUPP;
 	}
-	read_unlock(&sw->port_list_lock);
+}
 
-	return port;
+static void mvsw_pr_set_rx_mode(struct net_device *dev)
+{
+	/* TO DO: add implementation */
 }
 
-static int prestera_port_open(struct net_device *dev)
+static int mvsw_is_valid_mac_addr(struct mvsw_pr_port *port, u8 *addr)
 {
-	struct prestera_port *port = netdev_priv(dev);
 	int err;
 
-	err = prestera_hw_port_state_set(port, true);
+	if (!is_valid_ether_addr(addr))
+		return -EADDRNOTAVAIL;
+
+	err = memcmp(port->sw->base_mac, addr, ETH_ALEN - 1);
+	if (err)
+		return -EINVAL;
+
+	return 0;
+}
+
+static int mvsw_pr_port_set_mac_address(struct net_device *dev, void *p)
+{
+	struct mvsw_pr_port *port = netdev_priv(dev);
+	struct sockaddr *addr = p;
+	int err;
+
+	err = mvsw_is_valid_mac_addr(port, addr->sa_data);
 	if (err)
 		return err;
 
-	netif_start_queue(dev);
+	err = mvsw_pr_hw_port_mac_set(port, addr->sa_data);
+	if (!err)
+		memcpy(dev->dev_addr, addr->sa_data, dev->addr_len);
+
+	return err;
+}
+
+static int mvsw_pr_port_change_mtu(struct net_device *dev, int mtu)
+{
+	struct mvsw_pr_port *port = netdev_priv(dev);
+	int err;
+
+	if (port->sw->mtu_min <= mtu && mtu <= port->sw->mtu_max)
+		err = mvsw_pr_hw_port_mtu_set(port, mtu);
+	else
+		err = -EINVAL;
+
+	if (!err)
+		dev->mtu = mtu;
+
+	return err;
+}
+
+static void mvsw_pr_port_get_stats64(struct net_device *dev,
+				     struct rtnl_link_stats64 *stats)
+{
+	struct mvsw_pr_port *port = netdev_priv(dev);
+	struct mvsw_pr_port_stats *port_stats = &port->cached_hw_stats.stats;
+
+	stats->rx_packets =	port_stats->broadcast_frames_received +
+				port_stats->multicast_frames_received +
+				port_stats->unicast_frames_received;
+
+	stats->tx_packets =	port_stats->broadcast_frames_sent +
+				port_stats->multicast_frames_sent +
+				port_stats->unicast_frames_sent;
+
+	stats->rx_bytes = port_stats->good_octets_received;
+
+	stats->tx_bytes = port_stats->good_octets_sent;
+
+	stats->rx_errors = port_stats->rx_error_frame_received;
+	stats->tx_errors = port_stats->mac_trans_error;
 
+	stats->rx_dropped = port_stats->buffer_overrun;
+	stats->tx_dropped = 0;
+
+	stats->multicast = port_stats->multicast_frames_received;
+	stats->collisions = port_stats->excessive_collision;
+
+	stats->rx_crc_errors = port_stats->bad_crc;
+}
+
+static void mvsw_pr_port_get_hw_stats(struct mvsw_pr_port *port)
+{
+	mvsw_pr_hw_port_stats_get(port, &port->cached_hw_stats.stats);
+}
+
+static void update_stats_cache(struct work_struct *work)
+{
+	struct mvsw_pr_port *port =
+		container_of(work, struct mvsw_pr_port,
+			     cached_hw_stats.caching_dw.work);
+
+	rtnl_lock();
+	mvsw_pr_port_get_hw_stats(port);
+	rtnl_unlock();
+
+	queue_delayed_work(mvsw_pr_wq, &port->cached_hw_stats.caching_dw,
+			   PORT_STATS_CACHE_TIMEOUT_MS);
+}
+
+static bool mvsw_pr_port_has_offload_stats(const struct net_device *dev,
+					   int attr_id)
+{
+	/* TO DO: add implementation */
+	return false;
+}
+
+static int mvsw_pr_port_get_offload_stats(int attr_id,
+					  const struct net_device *dev,
+					  void *sp)
+{
+	/* TO DO: add implementation */
 	return 0;
 }
 
-static int prestera_port_close(struct net_device *dev)
+static int mvsw_pr_feature_hw_tc(struct net_device *dev, bool enable)
 {
-	struct prestera_port *port = netdev_priv(dev);
+	struct mvsw_pr_port *port = netdev_priv(dev);
+
+	if (!enable) {
+		if (prestera_acl_block_rule_count(port->acl_block)) {
+			netdev_err(dev, "Active offloaded tc filters, can't turn hw_tc_offload off\n");
+			return -EINVAL;
+		}
+		prestera_acl_block_disable_inc(port->acl_block);
+	} else {
+		prestera_acl_block_disable_dec(port->acl_block);
+	}
+	return 0;
+}
+
+static int
+mvsw_pr_handle_feature(struct net_device *dev,
+		       netdev_features_t wanted_features,
+		       netdev_features_t feature,
+		       int (*feature_handler)(struct net_device *dev,
+					      bool enable))
+{
+	netdev_features_t changes = wanted_features ^ dev->features;
+	bool enable = !!(wanted_features & feature);
 	int err;
 
-	netif_stop_queue(dev);
+	if (!(changes & feature))
+		return 0;
 
-	err = prestera_hw_port_state_set(port, false);
-	if (err)
+	err = feature_handler(dev, enable);
+	if (err) {
+		netdev_err(dev, "%s feature %pNF failed, err %d\n",
+			   enable ? "Enable" : "Disable", &feature, err);
 		return err;
+	}
+
+	if (enable)
+		dev->features |= feature;
+	else
+		dev->features &= ~feature;
 
 	return 0;
 }
 
-static netdev_tx_t prestera_port_xmit(struct sk_buff *skb,
-				      struct net_device *dev)
+static int mvsw_pr_set_features(struct net_device *dev,
+				netdev_features_t features)
 {
-	return prestera_rxtx_xmit(netdev_priv(dev), skb);
+	netdev_features_t oper_features = dev->features;
+	int err = 0;
+
+	err |= mvsw_pr_handle_feature(dev, features, NETIF_F_HW_TC,
+				       mvsw_pr_feature_hw_tc);
+
+	if (err) {
+		dev->features = oper_features;
+		return -EINVAL;
+	}
+
+	return 0;
 }
 
-static int prestera_is_valid_mac_addr(struct prestera_port *port, u8 *addr)
+static void mvsw_pr_port_get_drvinfo(struct net_device *dev,
+				     struct ethtool_drvinfo *drvinfo)
 {
-	if (!is_valid_ether_addr(addr))
-		return -EADDRNOTAVAIL;
+	struct mvsw_pr_port *port = netdev_priv(dev);
+	struct mvsw_pr_switch *sw = port->sw;
+
+	strlcpy(drvinfo->driver, mvsw_driver_kind, sizeof(drvinfo->driver));
+	strlcpy(drvinfo->bus_info, mvsw_dev_name(sw), sizeof(drvinfo->bus_info));
+	snprintf(drvinfo->fw_version, sizeof(drvinfo->fw_version),
+		 "%d.%d.%d",
+		 sw->dev->fw_rev.maj,
+		 sw->dev->fw_rev.min,
+		 sw->dev->fw_rev.sub);
+}
 
-	/* firmware requires that port's MAC address contains first 5 bytes
-	 * of the base MAC address
-	 */
-	if (memcmp(port->sw->base_mac, addr, ETH_ALEN - 1))
+static const struct net_device_ops mvsw_pr_netdev_ops = {
+	.ndo_open = mvsw_pr_port_open,
+	.ndo_stop = mvsw_pr_port_close,
+	.ndo_start_xmit = mvsw_pr_port_xmit,
+	.ndo_setup_tc = mvsw_pr_setup_tc,
+	.ndo_change_mtu = mvsw_pr_port_change_mtu,
+	.ndo_set_rx_mode = mvsw_pr_set_rx_mode,
+	.ndo_get_stats64 = mvsw_pr_port_get_stats64,
+	.ndo_set_features = mvsw_pr_set_features,
+	.ndo_set_mac_address = mvsw_pr_port_set_mac_address,
+	.ndo_has_offload_stats = mvsw_pr_port_has_offload_stats,
+	.ndo_get_offload_stats = mvsw_pr_port_get_offload_stats,
+	.ndo_get_devlink_port = prestera_devlink_get_port,
+};
+
+bool mvsw_pr_netdev_check(const struct net_device *dev)
+{
+	return dev->netdev_ops == &mvsw_pr_netdev_ops;
+}
+
+static int mvsw_pr_lower_dev_walk(struct net_device *lower_dev,
+	struct netdev_nested_priv  *priv)
+{
+	struct mvsw_pr_port **pport = (struct mvsw_pr_port **)priv->data;
+
+	if (mvsw_pr_netdev_check(lower_dev)) {
+		*pport = netdev_priv(lower_dev);
+		return 1;
+	}
+
+	return 0;
+}
+
+struct mvsw_pr_port *mvsw_pr_port_dev_lower_find(struct net_device *dev)
+{
+	struct mvsw_pr_port *port = NULL;
+	struct netdev_nested_priv priv = {
+		.data = (void *)&port,
+	};
+
+	if (!dev)
+		return NULL;
+
+	if (mvsw_pr_netdev_check(dev))
+		return netdev_priv(dev);
+
+	port = NULL;
+	netdev_walk_all_lower_dev(dev, mvsw_pr_lower_dev_walk, &priv);
+
+	return port;
+}
+
+struct mvsw_pr_switch *mvsw_pr_switch_get(struct net_device *dev)
+{
+	struct mvsw_pr_port *port;
+
+	port = mvsw_pr_port_dev_lower_find(dev);
+	return port ? port->sw : NULL;
+}
+
+static void mvsw_modes_to_eth(unsigned long *eth_modes, u64 link_modes, u8 fec,
+			      u8 type)
+{
+	u32 mode;
+
+	for (mode = 0; mode < MVSW_LINK_MODE_MAX; mode++) {
+		if ((mvsw_pr_link_modes[mode].pr_mask & link_modes) == 0)
+			continue;
+		if (type != MVSW_PORT_TYPE_NONE &&
+		    mvsw_pr_link_modes[mode].port_type != type)
+			continue;
+		__set_bit(mvsw_pr_link_modes[mode].eth_mode, eth_modes);
+	}
+
+	for (mode = 0; mode < MVSW_PORT_FEC_MAX; mode++) {
+		if ((mvsw_pr_fec_caps[mode].pr_fec & fec) == 0)
+			continue;
+		__set_bit(mvsw_pr_fec_caps[mode].eth_mode, eth_modes);
+	}
+}
+
+static void mvsw_pr_port_autoneg_get(struct ethtool_link_ksettings *ecmd,
+				     struct mvsw_pr_port *port)
+{
+	ecmd->base.autoneg = port->autoneg ? AUTONEG_ENABLE : AUTONEG_DISABLE;
+
+	mvsw_modes_to_eth(ecmd->link_modes.supported,
+			  port->caps.supp_link_modes,
+			  port->caps.supp_fec,
+			  port->caps.type);
+
+	if (port->caps.type != MVSW_PORT_TYPE_TP)
+		return;
+
+	ethtool_link_ksettings_add_link_mode(ecmd, supported, Autoneg);
+
+	if (!netif_running(port->net_dev))
+		return;
+
+	if (port->autoneg) {
+		mvsw_modes_to_eth(ecmd->link_modes.advertising,
+				  port->adver_link_modes,
+				  port->adver_fec,
+				  port->caps.type);
+		ethtool_link_ksettings_add_link_mode(ecmd, advertising,
+						     Autoneg);
+	} else if (port->caps.transceiver == MVSW_PORT_TRANSCEIVER_COPPER)
+		ethtool_link_ksettings_add_link_mode(ecmd, advertising,
+						     Autoneg);
+}
+
+static int mvsw_modes_from_eth(struct mvsw_pr_port *port,
+			       const unsigned long *advertising,
+			       const unsigned long *supported,
+			       u64 *link_modes, u8 *fec)
+{
+	struct ethtool_link_ksettings curr = {};
+	u32 mode;
+
+	ethtool_link_ksettings_zero_link_mode(&curr, supported);
+	ethtool_link_ksettings_zero_link_mode(&curr, advertising);
+
+	mvsw_pr_port_autoneg_get(&curr, port);
+
+	if (linkmode_equal(advertising, curr.link_modes.advertising)) {
+		*link_modes = port->adver_link_modes;
+		*fec = port->adver_fec;
+		return 0;
+	}
+
+	if (!linkmode_subset(advertising, supported)) {
+		netdev_err(port->net_dev, "Unsupported link mode requested");
+		return -EINVAL;
+	}
+
+	*link_modes  = 0;
+	*fec = 0;
+	for (mode = 0; mode < MVSW_LINK_MODE_MAX; mode++) {
+		if (!test_bit(mvsw_pr_link_modes[mode].eth_mode, advertising))
+			continue;
+		if (mvsw_pr_link_modes[mode].port_type != port->caps.type)
+			continue;
+		*link_modes |= mvsw_pr_link_modes[mode].pr_mask;
+	}
+
+	for (mode = 0; mode < MVSW_PORT_FEC_MAX; mode++) {
+		if (!test_bit(mvsw_pr_fec_caps[mode].eth_mode, advertising))
+			continue;
+		*fec |= mvsw_pr_fec_caps[mode].pr_fec;
+	}
+
+	if (*link_modes == 0 && *fec == 0) {
+		netdev_err(port->net_dev, "No link modes requested");
 		return -EINVAL;
+	}
+	if (*link_modes == 0)
+		*link_modes = port->adver_link_modes;
+	if (*fec == 0)
+		*fec = port->adver_fec ? port->adver_fec :
+					 BIT(MVSW_PORT_FEC_OFF_BIT);
 
 	return 0;
 }
 
-static int prestera_port_set_mac_address(struct net_device *dev, void *p)
+static void mvsw_pr_port_supp_types_get(struct ethtool_link_ksettings *ecmd,
+					struct mvsw_pr_port *port)
 {
-	struct prestera_port *port = netdev_priv(dev);
-	struct sockaddr *addr = p;
+	u32 mode;
+	u8 ptype;
+
+	for (mode = 0; mode < MVSW_LINK_MODE_MAX; mode++) {
+		if ((mvsw_pr_link_modes[mode].pr_mask &
+		    port->caps.supp_link_modes) == 0)
+			continue;
+		ptype = mvsw_pr_link_modes[mode].port_type;
+		__set_bit(mvsw_pr_port_types[ptype].eth_mode,
+			  ecmd->link_modes.supported);
+	}
+}
+
+static void mvsw_pr_port_speed_get(struct ethtool_link_ksettings *ecmd,
+				   struct mvsw_pr_port *port)
+{
+	u32 speed;
+	int err;
+
+	err = mvsw_pr_hw_port_speed_get(port, &speed);
+	ecmd->base.speed = !err ? speed : SPEED_UNKNOWN;
+}
+
+static int mvsw_pr_port_link_mode_set(struct mvsw_pr_port *port,
+				      u32 speed, u8 duplex, u8 type)
+{
+	u32 new_mode = MVSW_LINK_MODE_MAX;
+	u32 mode;
+
+	for (mode = 0; mode < MVSW_LINK_MODE_MAX; mode++) {
+		if (speed != mvsw_pr_link_modes[mode].speed)
+			continue;
+		if (duplex != mvsw_pr_link_modes[mode].duplex)
+			continue;
+		if (!(mvsw_pr_link_modes[mode].pr_mask &
+		    port->caps.supp_link_modes))
+			continue;
+		if (type != mvsw_pr_link_modes[mode].port_type)
+			continue;
+
+		new_mode = mode;
+		break;
+	}
+
+	if (new_mode == MVSW_LINK_MODE_MAX) {
+		netdev_err(port->net_dev, "Unsupported speed/duplex requested");
+		return -EINVAL;
+	}
+
+	return mvsw_pr_hw_port_link_mode_set(port, new_mode);
+}
+
+static int mvsw_pr_port_speed_duplex_set(const struct ethtool_link_ksettings
+					 *ecmd, struct mvsw_pr_port *port)
+{
+	int err;
+	u8 duplex;
+	u32 speed;
+	u32 curr_mode;
+
+	err = mvsw_pr_hw_port_link_mode_get(port, &curr_mode);
+	if (err || curr_mode >= MVSW_LINK_MODE_MAX)
+		return -EINVAL;
+
+	if (ecmd->base.duplex != DUPLEX_UNKNOWN)
+		duplex = ecmd->base.duplex == DUPLEX_FULL ?
+			 MVSW_PORT_DUPLEX_FULL : MVSW_PORT_DUPLEX_HALF;
+	else
+		duplex = mvsw_pr_link_modes[curr_mode].duplex;
+
+	if (ecmd->base.speed != SPEED_UNKNOWN)
+		speed = ecmd->base.speed;
+	else
+		speed = mvsw_pr_link_modes[curr_mode].speed;
+
+	return mvsw_pr_port_link_mode_set(port, speed, duplex, port->caps.type);
+}
+
+static u8 mvsw_pr_port_type_get(struct mvsw_pr_port *port)
+{
+	if (port->caps.type < MVSW_PORT_TYPE_MAX)
+		return mvsw_pr_port_types[port->caps.type].eth_type;
+	return PORT_OTHER;
+}
+
+static int mvsw_pr_port_type_set(const struct ethtool_link_ksettings *ecmd,
+				 struct mvsw_pr_port *port)
+{
+	int err;
+	u32 type, mode;
+	u32 new_mode = MVSW_LINK_MODE_MAX;
+
+	for (type = 0; type < MVSW_PORT_TYPE_MAX; type++) {
+		if (mvsw_pr_port_types[type].eth_type == ecmd->base.port &&
+		    test_bit(mvsw_pr_port_types[type].eth_mode,
+			     ecmd->link_modes.supported)) {
+			break;
+		}
+	}
+
+	if (type == port->caps.type)
+		return 0;
+
+	if (type != port->caps.type && ecmd->base.autoneg == AUTONEG_ENABLE)
+		return -EINVAL;
+
+	if (type == MVSW_PORT_TYPE_MAX) {
+		pr_err("Unsupported port type requested\n");
+		return -EINVAL;
+	}
+
+	for (mode = 0; mode < MVSW_LINK_MODE_MAX; mode++) {
+		if ((mvsw_pr_link_modes[mode].pr_mask &
+		    port->caps.supp_link_modes) &&
+		    type == mvsw_pr_link_modes[mode].port_type) {
+			new_mode = mode;
+		}
+	}
+
+	if (new_mode < MVSW_LINK_MODE_MAX)
+		err = mvsw_pr_hw_port_link_mode_set(port, new_mode);
+	else
+		err = -EINVAL;
+
+	if (!err) {
+		port->caps.type = type;
+		port->autoneg = false;
+	}
+
+	return err;
+}
+
+static void mvsw_pr_port_remote_cap_get(struct ethtool_link_ksettings *ecmd,
+					struct mvsw_pr_port *port)
+{
+	u64 bitmap;
+	bool pause;
+	bool asym_pause;
+
+	if (!mvsw_pr_hw_port_remote_cap_get(port, &bitmap)) {
+		mvsw_modes_to_eth(ecmd->link_modes.lp_advertising,
+				  bitmap, 0, MVSW_PORT_TYPE_NONE);
+
+		if (!bitmap_empty(ecmd->link_modes.lp_advertising,
+				  __ETHTOOL_LINK_MODE_MASK_NBITS)) {
+			ethtool_link_ksettings_add_link_mode(ecmd,
+							     lp_advertising,
+							     Autoneg);
+		}
+	}
+
+	if (mvsw_pr_hw_port_remote_fc_get(port, &pause, &asym_pause))
+		return;
+	if (pause)
+		ethtool_link_ksettings_add_link_mode(ecmd,
+						     lp_advertising,
+						     Pause);
+	if (asym_pause)
+		ethtool_link_ksettings_add_link_mode(ecmd,
+						     lp_advertising,
+						     Asym_Pause);
+}
+
+static void mvsw_pr_port_duplex_get(struct ethtool_link_ksettings *ecmd,
+				    struct mvsw_pr_port *port)
+{
+	u8 duplex;
+
+	if (!mvsw_pr_hw_port_duplex_get(port, &duplex)) {
+		ecmd->base.duplex = duplex == MVSW_PORT_DUPLEX_FULL ?
+				    DUPLEX_FULL : DUPLEX_HALF;
+	} else {
+		ecmd->base.duplex = DUPLEX_UNKNOWN;
+	}
+}
+
+static int mvsw_pr_port_autoneg_set(struct mvsw_pr_port *port, bool enable,
+				    u64 link_modes, u8 fec)
+{
+	if (port->caps.type != MVSW_PORT_TYPE_TP)
+		return enable ? -EINVAL : 0;
+
+	if (port->autoneg == enable && port->adver_link_modes == link_modes &&
+	    port->adver_fec == fec)
+		return 0;
+
+	if (mvsw_pr_hw_port_autoneg_set(port, enable, link_modes, fec))
+		return -EINVAL;
+
+	port->autoneg = enable;
+	port->adver_link_modes = link_modes;
+	port->adver_fec = fec;
+	return 0;
+}
+
+static int mvsw_pr_port_nway_reset(struct net_device *dev)
+{
+	struct mvsw_pr_port *port = netdev_priv(dev);
+
+	if (netif_running(dev) &&
+	    port->caps.transceiver == MVSW_PORT_TRANSCEIVER_COPPER &&
+	    port->caps.type == MVSW_PORT_TYPE_TP)
+		return mvsw_pr_hw_port_autoneg_restart(port);
+
+	return -EINVAL;
+}
+
+static int mvsw_pr_port_mdix_set(const struct ethtool_link_ksettings *ecmd,
+				 struct mvsw_pr_port *port)
+{
+	if (ecmd->base.eth_tp_mdix_ctrl != ETH_TP_MDI_INVALID &&
+	    port->caps.transceiver == MVSW_PORT_TRANSCEIVER_COPPER &&
+	    port->caps.type == MVSW_PORT_TYPE_TP)
+		return mvsw_pr_hw_port_mdix_set(port,
+						ecmd->base.eth_tp_mdix_ctrl);
+	return 0;
+}
+
+static int mvsw_pr_port_get_link_ksettings(struct net_device *dev,
+					   struct ethtool_link_ksettings *ecmd)
+{
+	struct mvsw_pr_port *port = netdev_priv(dev);
+
+	/* Dirty hook: Deinit ecmd.
+	 * It caused by suspicious phylink_ethtool_ksettings_get()
+	 * implementation, which can left "kset" uninitialized, when there is no
+	 * SFP plugged
+	 */
+	ethtool_link_ksettings_zero_link_mode(ecmd, supported);
+	ethtool_link_ksettings_zero_link_mode(ecmd, advertising);
+	ethtool_link_ksettings_zero_link_mode(ecmd, lp_advertising);
+	ecmd->base.speed = SPEED_UNKNOWN;
+	ecmd->base.duplex = DUPLEX_UNKNOWN;
+	ecmd->base.autoneg = AUTONEG_DISABLE;
+#ifdef CONFIG_PHYLINK
+	if (port->caps.transceiver == MVSW_PORT_TRANSCEIVER_SFP)
+		return phylink_ethtool_ksettings_get(port->phy_link, ecmd);
+#endif /* CONFIG_PHYLINK */
+
+	mvsw_pr_port_supp_types_get(ecmd, port);
+
+	mvsw_pr_port_autoneg_get(ecmd, port);
+
+	if (port->autoneg && netif_carrier_ok(dev) &&
+	    port->caps.transceiver == MVSW_PORT_TRANSCEIVER_COPPER)
+		mvsw_pr_port_remote_cap_get(ecmd, port);
+
+	if (netif_carrier_ok(dev)) {
+		mvsw_pr_port_speed_get(ecmd, port);
+		mvsw_pr_port_duplex_get(ecmd, port);
+	} else {
+		ecmd->base.speed = SPEED_UNKNOWN;
+		ecmd->base.duplex = DUPLEX_UNKNOWN;
+	}
+
+	ecmd->base.port = mvsw_pr_port_type_get(port);
+
+	if (port->caps.type == MVSW_PORT_TYPE_TP &&
+	    port->caps.transceiver == MVSW_PORT_TRANSCEIVER_COPPER)
+		mvsw_pr_hw_port_mdix_get(port, &ecmd->base.eth_tp_mdix,
+					 &ecmd->base.eth_tp_mdix_ctrl);
+
+	return 0;
+}
+
+static int mvsw_pr_port_set_link_ksettings(struct net_device *dev,
+					   const struct ethtool_link_ksettings
+					   *ecmd)
+{
+	struct mvsw_pr_port *port = netdev_priv(dev);
+	u64 adver_modes = 0;
+	u8 adver_fec = 0;
 	int err;
 
-	err = prestera_is_valid_mac_addr(port, addr->sa_data);
+#ifdef CONFIG_PHYLINK
+	if (port->caps.transceiver == MVSW_PORT_TRANSCEIVER_SFP)
+		return phylink_ethtool_ksettings_set(port->phy_link, ecmd);
+#endif /* CONFIG_PHYLINK */
+
+	err = mvsw_pr_port_type_set(ecmd, port);
 	if (err)
 		return err;
 
-	err = prestera_hw_port_mac_set(port, addr->sa_data);
+	if (port->caps.transceiver == MVSW_PORT_TRANSCEIVER_COPPER) {
+		err = mvsw_pr_port_mdix_set(ecmd, port);
+		if (err)
+			return err;
+	}
+
+	if (ecmd->base.autoneg == AUTONEG_ENABLE) {
+		if (mvsw_modes_from_eth(port, ecmd->link_modes.advertising,
+					ecmd->link_modes.supported,
+					&adver_modes, &adver_fec))
+			return -EINVAL;
+		if (!port->autoneg && !adver_modes)
+			adver_modes = port->caps.supp_link_modes;
+	} else {
+		adver_modes = port->adver_link_modes;
+		adver_fec = port->adver_fec;
+	}
+
+	err = mvsw_pr_port_autoneg_set(port,
+				       ecmd->base.autoneg == AUTONEG_ENABLE,
+				       adver_modes, adver_fec);
 	if (err)
 		return err;
 
-	ether_addr_copy(dev->dev_addr, addr->sa_data);
+	if (ecmd->base.autoneg == AUTONEG_DISABLE) {
+		err = mvsw_pr_port_speed_duplex_set(ecmd, port);
+		if (err)
+			return err;
+	}
 
 	return 0;
 }
 
-static int prestera_port_change_mtu(struct net_device *dev, int mtu)
+static int mvsw_pr_port_get_fecparam(struct net_device *dev,
+				     struct ethtool_fecparam *fecparam)
 {
-	struct prestera_port *port = netdev_priv(dev);
+	struct mvsw_pr_port *port = netdev_priv(dev);
+	u32 mode;
+	u8 active;
 	int err;
 
-	err = prestera_hw_port_mtu_set(port, mtu);
+	err = mvsw_pr_hw_port_fec_get(port, &active);
 	if (err)
 		return err;
 
-	dev->mtu = mtu;
+	fecparam->fec = 0;
+	for (mode = 0; mode < MVSW_PORT_FEC_MAX; mode++) {
+		if ((mvsw_pr_fec_caps[mode].pr_fec & port->caps.supp_fec) == 0)
+			continue;
+		fecparam->fec |= mvsw_pr_fec_caps[mode].eth_fec;
+	}
+
+	if (active < MVSW_PORT_FEC_MAX)
+		fecparam->active_fec = mvsw_pr_fec_caps[active].eth_fec;
+	else
+		fecparam->active_fec = ETHTOOL_FEC_AUTO;
 
 	return 0;
 }
 
-static void prestera_port_get_stats64(struct net_device *dev,
-				      struct rtnl_link_stats64 *stats)
+static int mvsw_pr_port_set_fecparam(struct net_device *dev,
+				     struct ethtool_fecparam *fecparam)
 {
-	struct prestera_port *port = netdev_priv(dev);
-	struct prestera_port_stats *port_stats = &port->cached_hw_stats.stats;
+	struct mvsw_pr_port *port = netdev_priv(dev);
+	u8 fec, active;
+	u32 mode;
+	int err;
 
-	stats->rx_packets = port_stats->broadcast_frames_received +
-				port_stats->multicast_frames_received +
-				port_stats->unicast_frames_received;
+	if (port->autoneg) {
+		netdev_err(dev, "FEC set is not allowed while autoneg is on\n");
+		return -EINVAL;
+	}
 
-	stats->tx_packets = port_stats->broadcast_frames_sent +
-				port_stats->multicast_frames_sent +
-				port_stats->unicast_frames_sent;
+	err = mvsw_pr_hw_port_fec_get(port, &active);
+	if (err)
+		return err;
 
-	stats->rx_bytes = port_stats->good_octets_received;
+	fec = MVSW_PORT_FEC_MAX;
+	for (mode = 0; mode < MVSW_PORT_FEC_MAX; mode++) {
+		if ((mvsw_pr_fec_caps[mode].eth_fec & fecparam->fec) &&
+		    (mvsw_pr_fec_caps[mode].pr_fec & port->caps.supp_fec)) {
+			fec = mode;
+			break;
+		}
+	}
 
-	stats->tx_bytes = port_stats->good_octets_sent;
+	if (fec == active)
+		return 0;
 
-	stats->rx_errors = port_stats->rx_error_frame_received;
-	stats->tx_errors = port_stats->mac_trans_error;
+	if (fec == MVSW_PORT_FEC_MAX) {
+		netdev_err(dev, "Unsupported FEC requested");
+		return -EINVAL;
+	}
+
+	return mvsw_pr_hw_port_fec_set(port, fec);
+}
+
+static void mvsw_pr_port_get_ethtool_stats(struct net_device *dev,
+					   struct ethtool_stats *stats,
+					   u64 *data)
+{
+	struct mvsw_pr_port *port = netdev_priv(dev);
+	struct mvsw_pr_port_stats *port_stats = &port->cached_hw_stats.stats;
+
+	memcpy((u8 *)data, port_stats, sizeof(*port_stats));
+}
+
+static void mvsw_pr_port_get_strings(struct net_device *dev,
+				     u32 stringset, u8 *data)
+{
+	if (stringset != ETH_SS_STATS)
+		return;
+
+	memcpy(data, *mvsw_pr_port_cnt_name, sizeof(mvsw_pr_port_cnt_name));
+}
+
+static int mvsw_pr_port_get_sset_count(struct net_device *dev, int sset)
+{
+	switch (sset) {
+	case ETH_SS_STATS:
+		return PORT_STATS_CNT;
+	default:
+		return -EOPNOTSUPP;
+	}
+}
+
+static const struct ethtool_ops mvsw_pr_ethtool_ops = {
+	.get_drvinfo = mvsw_pr_port_get_drvinfo,
+	.get_link_ksettings = mvsw_pr_port_get_link_ksettings,
+	.set_link_ksettings = mvsw_pr_port_set_link_ksettings,
+	.get_fecparam = mvsw_pr_port_get_fecparam,
+	.set_fecparam = mvsw_pr_port_set_fecparam,
+	.get_sset_count = mvsw_pr_port_get_sset_count,
+	.get_strings = mvsw_pr_port_get_strings,
+	.get_ethtool_stats = mvsw_pr_port_get_ethtool_stats,
+	.get_link = ethtool_op_get_link,
+	.nway_reset = mvsw_pr_port_nway_reset
+};
+
+int mvsw_pr_port_learning_set(struct mvsw_pr_port *port, bool learn)
+{
+	return mvsw_pr_hw_port_learning_set(port, learn);
+}
+
+int mvsw_pr_port_uc_flood_set(struct mvsw_pr_port *port, bool flood)
+{
+	return mvsw_pr_hw_port_uc_flood_set(port, flood);
+}
+
+int mvsw_pr_port_mc_flood_set(struct mvsw_pr_port *port, bool flood)
+{
+	return mvsw_pr_hw_port_mc_flood_set(port, flood);
+}
+
+int mvsw_pr_port_pvid_set(struct mvsw_pr_port *port, u16 vid)
+{
+	int err;
+
+	if (!vid) {
+		err = mvsw_pr_hw_port_accept_frame_type_set
+		    (port, MVSW_ACCEPT_FRAME_TYPE_TAGGED);
+		if (err)
+			return err;
+	} else {
+		err = mvsw_pr_hw_vlan_port_vid_set(port, vid);
+		if (err)
+			return err;
+		err = mvsw_pr_hw_port_accept_frame_type_set
+		    (port, MVSW_ACCEPT_FRAME_TYPE_ALL);
+		if (err)
+			goto err_port_allow_untagged_set;
+	}
+
+	port->pvid = vid;
+	return 0;
+
+err_port_allow_untagged_set:
+	mvsw_pr_hw_vlan_port_vid_set(port, port->pvid);
+	return err;
+}
+
+int mvsw_pr_port_vid_stp_set(struct mvsw_pr_port *port, u16 vid, u8 state)
+{
+	u8 hw_state = state;
+
+	switch (state) {
+	case BR_STATE_DISABLED:
+		hw_state = MVSW_STP_DISABLED;
+		break;
+
+	case BR_STATE_BLOCKING:
+	case BR_STATE_LISTENING:
+		hw_state = MVSW_STP_BLOCK_LISTEN;
+		break;
+
+	case BR_STATE_LEARNING:
+		hw_state = MVSW_STP_LEARN;
+		break;
+
+	case BR_STATE_FORWARDING:
+		hw_state = MVSW_STP_FORWARD;
+		break;
+
+	default:
+		return -EINVAL;
+	}
+
+	return mvsw_pr_hw_port_vid_stp_set(port, vid, hw_state);
+}
+
+struct mvsw_pr_port_vlan*
+mvsw_pr_port_vlan_find_by_vid(const struct mvsw_pr_port *port, u16 vid)
+{
+	struct mvsw_pr_port_vlan *port_vlan;
+
+	list_for_each_entry(port_vlan, &port->vlans_list, list) {
+		if (port_vlan->vid == vid)
+			return port_vlan;
+	}
+
+	return NULL;
+}
+
+struct mvsw_pr_port_vlan*
+mvsw_pr_port_vlan_create(struct mvsw_pr_port *port, u16 vid, bool untagged)
+{
+	struct mvsw_pr_port_vlan *port_vlan;
+	int err;
+
+	port_vlan = mvsw_pr_port_vlan_find_by_vid(port, vid);
+	if (port_vlan)
+		return ERR_PTR(-EEXIST);
+
+	err = mvsw_pr_port_vlan_set(port, vid, true, untagged);
+	if (err)
+		return ERR_PTR(err);
+
+	port_vlan = kzalloc(sizeof(*port_vlan), GFP_KERNEL);
+	if (!port_vlan) {
+		err = -ENOMEM;
+		goto err_port_vlan_alloc;
+	}
+
+	port_vlan->mvsw_pr_port = port;
+	port_vlan->vid = vid;
+
+	list_add(&port_vlan->list, &port->vlans_list);
+
+	return port_vlan;
+
+err_port_vlan_alloc:
+	mvsw_pr_port_vlan_set(port, vid, false, false);
+	return ERR_PTR(err);
+}
+
+static void
+mvsw_pr_port_vlan_cleanup(struct mvsw_pr_port_vlan *port_vlan)
+{
+	if (port_vlan->bridge_port)
+		mvsw_pr_port_vlan_bridge_leave(port_vlan);
+}
+
+void mvsw_pr_port_vlan_destroy(struct mvsw_pr_port_vlan *port_vlan)
+{
+	struct mvsw_pr_port *port = port_vlan->mvsw_pr_port;
+	u16 vid = port_vlan->vid;
+
+	mvsw_pr_port_vlan_cleanup(port_vlan);
+	list_del(&port_vlan->list);
+	kfree(port_vlan);
+	mvsw_pr_hw_vlan_port_set(port, vid, false, false);
+}
+
+int mvsw_pr_port_vlan_set(struct mvsw_pr_port *port, u16 vid,
+			  bool is_member, bool untagged)
+{
+	return mvsw_pr_hw_vlan_port_set(port, vid, is_member, untagged);
+}
+
+#ifdef CONFIG_PHYLINK
+static void mvsw_pr_link_validate(struct phylink_config *config,
+	unsigned long *supported,
+	struct phylink_link_state *state)
+{
+	__ETHTOOL_DECLARE_LINK_MODE_MASK(mask) = {0,};
+
+	if (state->interface != PHY_INTERFACE_MODE_NA &&
+	    state->interface != PHY_INTERFACE_MODE_10GBASER &&
+	    state->interface != PHY_INTERFACE_MODE_SGMII &&
+	    !phy_interface_mode_is_8023z(state->interface)) {
+		bitmap_zero(supported, __ETHTOOL_LINK_MODE_MASK_NBITS);
+		return;
+	}
+
+	switch (state->interface) {
+	case PHY_INTERFACE_MODE_10GBASER:
+	case PHY_INTERFACE_MODE_NA:
+		phylink_set(mask, 10000baseT_Full);
+		phylink_set(mask, 10000baseCR_Full);
+		phylink_set(mask, 10000baseSR_Full);
+		phylink_set(mask, 10000baseLR_Full);
+		phylink_set(mask, 10000baseLRM_Full);
+		phylink_set(mask, 10000baseER_Full);
+		phylink_set(mask, 10000baseKR_Full);
+		phylink_set(mask, 10000baseKX4_Full);
+		phylink_set(mask, 10000baseR_FEC);
+
+		if (state->interface != PHY_INTERFACE_MODE_NA)
+			break;
+		/* Fall-through */
+	case PHY_INTERFACE_MODE_SGMII:
+		phylink_set(mask, 10baseT_Full);
+		phylink_set(mask, 100baseT_Full);
+		phylink_set(mask, 1000baseT_Full);
+		if (state->interface != PHY_INTERFACE_MODE_NA) {
+			phylink_set(mask, Autoneg);
+			break;
+		}
+		/* Fall-through */
+	case PHY_INTERFACE_MODE_2500BASEX:
+		phylink_set(mask, 2500baseT_Full);
+		phylink_set(mask, 2500baseX_Full);
+		if (state->interface != PHY_INTERFACE_MODE_NA)
+			break;
+		/* Fall-through */
+	case PHY_INTERFACE_MODE_1000BASEX:
+		phylink_set(mask, 1000baseT_Full);
+		phylink_set(mask, 1000baseX_Full);
+		break;
+	default:
+		goto empty_set;
+	}
+
+	phylink_set_port_modes(mask);
+
+	bitmap_and(supported, supported, mask,
+		__ETHTOOL_LINK_MODE_MASK_NBITS);
+	bitmap_and(state->advertising, state->advertising, mask,
+		__ETHTOOL_LINK_MODE_MASK_NBITS);
+
+	phylink_helper_basex_speed(state);
+
+	return;
+
+empty_set:
+	bitmap_zero(supported, __ETHTOOL_LINK_MODE_MASK_NBITS);
+
+}
+
+static void mvsw_pr_mac_pcs_get_state(struct phylink_config *config,
+				      struct phylink_link_state *state)
+{
+	struct net_device *ndev = to_net_dev(config->dev);
+	struct mvsw_pr_port *port = netdev_priv(ndev);
+
+	state->link = !!(port->hw_oper_state);
+	state->pause = 0;
+
+	if (port->hw_oper_state) {
+		/* AN is completed, when port is up */
+		state->an_complete = port->autoneg;
+
+		state->speed = port->hw_speed;
+		state->duplex = (port->hw_duplex == MVSW_PORT_DUPLEX_FULL) ?
+					DUPLEX_FULL : DUPLEX_HALF;
+	} else {
+		state->an_complete = false;
+		state->speed = SPEED_UNKNOWN;
+		state->duplex = DUPLEX_UNKNOWN;
+	}
+}
+
+static void mvsw_pr_mac_config(struct phylink_config *config,
+			       unsigned int an_mode,
+			       const struct phylink_link_state *state)
+{
+	struct net_device *ndev = to_net_dev(config->dev);
+	struct mvsw_pr_port *port = netdev_priv(ndev);
+	u32 mode;
+
+	/* See sfp_select_interface... fIt */
+	switch (state->interface) {
+	case PHY_INTERFACE_MODE_10GBASER:
+		/* Or SR... doesn't matter */
+		mode = MVSW_LINK_MODE_10GbaseLR_Full_BIT;
+		if (state->speed == SPEED_1000)
+			mode = MVSW_LINK_MODE_1000baseX_Full_BIT;
+		if (state->speed == SPEED_2500)
+			mode = MVSW_LINK_MODE_2500baseX_Full_BIT;
+		break;
+	case PHY_INTERFACE_MODE_2500BASEX:
+		/* But it seems to be not supported in HW */
+		mode = MVSW_LINK_MODE_2500baseX_Full_BIT;
+		break;
+	case PHY_INTERFACE_MODE_SGMII:
+		/* Can be T or X. But for HW is no difference. */
+		mode = MVSW_LINK_MODE_1000baseT_Full_BIT;
+		break;
+	case PHY_INTERFACE_MODE_1000BASEX:
+		mode = MVSW_LINK_MODE_1000baseX_Full_BIT;
+		break;
+	default:
+		mode = MVSW_LINK_MODE_1000baseX_Full_BIT;
+	}
 
-	stats->rx_dropped = port_stats->buffer_overrun;
-	stats->tx_dropped = 0;
+	mvsw_pr_hw_port_link_mode_set(port, mode);
 
-	stats->multicast = port_stats->multicast_frames_received;
-	stats->collisions = port_stats->excessive_collision;
+	/* NOTE: we suppose, that inband autoneg is primary used for
+	 * modes, which support only FC autonegotiation. But we don't support
+	 * FC. So inband autoneg always be disabled.
+	 */
 
-	stats->rx_crc_errors = port_stats->bad_crc;
+	if (phylink_autoneg_inband(an_mode))
+		mvsw_pr_port_autoneg_set(port, false, 0, 0);
+	else
+		mvsw_pr_port_autoneg_set(port, false, 0, 0);
 }
 
-static void prestera_port_get_hw_stats(struct prestera_port *port)
+static void mvsw_pr_mac_an_restart(struct phylink_config *config)
 {
-	prestera_hw_port_stats_get(port, &port->cached_hw_stats.stats);
+	/* No need to restart autoneg as it is always with the same parameters,
+	 * because e.g. as for 1000baseX FC isn't supported. And for 1000baseT
+	 * autoneg provided by external tranciever
+	 */
 }
 
-static void prestera_port_stats_update(struct work_struct *work)
+static void mvsw_pr_mac_link_down(struct phylink_config *config,
+				  unsigned int mode, phy_interface_t interface)
 {
-	struct prestera_port *port =
-		container_of(work, struct prestera_port,
-			     cached_hw_stats.caching_dw.work);
-
-	prestera_port_get_hw_stats(port);
+}
 
-	queue_delayed_work(prestera_wq, &port->cached_hw_stats.caching_dw,
-			   msecs_to_jiffies(PRESTERA_STATS_DELAY_MS));
+static void mvsw_pr_mac_link_up(struct phylink_config *config,
+				struct phy_device *phy,
+				unsigned int mode, phy_interface_t interface,
+				int speed, int duplex,
+				bool tx_pause, bool rx_pause)
+{
 }
 
-static const struct net_device_ops prestera_netdev_ops = {
-	.ndo_open = prestera_port_open,
-	.ndo_stop = prestera_port_close,
-	.ndo_start_xmit = prestera_port_xmit,
-	.ndo_change_mtu = prestera_port_change_mtu,
-	.ndo_get_stats64 = prestera_port_get_stats64,
-	.ndo_set_mac_address = prestera_port_set_mac_address,
-	.ndo_get_devlink_port = prestera_devlink_get_port,
+static const struct phylink_mac_ops mvsw_pr_mac_ops = {
+	.validate = mvsw_pr_link_validate,
+	.mac_pcs_get_state = mvsw_pr_mac_pcs_get_state,
+	.mac_config = mvsw_pr_mac_config,
+	.mac_an_restart = mvsw_pr_mac_an_restart,
+	.mac_link_down = mvsw_pr_mac_link_down,
+	.mac_link_up = mvsw_pr_mac_link_up,
 };
 
-int prestera_port_autoneg_set(struct prestera_port *port, bool enable,
-			      u64 adver_link_modes, u8 adver_fec)
+static int mvsw_pr_port_sfp_bind(struct mvsw_pr_port *port)
 {
-	bool refresh = false;
-	u64 link_modes;
+	struct mvsw_pr_switch *sw = port->sw;
+	struct device_node *ports, *node;
+	struct fwnode_handle *fwnode;
+	struct phylink *phy_link;
 	int err;
-	u8 fec;
 
-	if (port->caps.type != PRESTERA_PORT_TYPE_TP)
-		return enable ? -EINVAL : 0;
+	if (!sw->np)
+		return 0;
 
-	if (!enable)
-		goto set_autoneg;
+	of_node_get(sw->np);
 
-	link_modes = port->caps.supp_link_modes & adver_link_modes;
-	fec = port->caps.supp_fec & adver_fec;
+	ports = of_find_node_by_name(sw->np, "ports");
 
-	if (!link_modes && !fec)
-		return -EOPNOTSUPP;
+	for_each_child_of_node(ports, node) {
+		int num;
 
-	if (link_modes && port->adver_link_modes != link_modes) {
-		port->adver_link_modes = link_modes;
-		refresh = true;
-	}
+		err = of_property_read_u32(node, "prestera,port-num", &num);
+		if (err) {
+			dev_err(sw->dev->dev,
+				"device node %pOF has no valid reg property: %d\n",
+				node, err);
+			return err;
+		}
 
-	if (fec && port->adver_fec != fec) {
-		port->adver_fec = fec;
-		refresh = true;
-	}
+		if (port->fp_id != num)
+			continue;
 
-set_autoneg:
-	if (port->autoneg == enable && !refresh)
-		return 0;
+		port->phy_config.dev = &port->net_dev->dev;
+		port->phy_config.type = PHYLINK_NETDEV;
+		port->phy_config.pcs_poll = false;
 
-	err = prestera_hw_port_autoneg_set(port, enable, port->adver_link_modes,
-					   port->adver_fec);
-	if (err)
-		return err;
+		fwnode = of_fwnode_handle(node);
 
-	port->autoneg = enable;
+		phy_link = phylink_create(&port->phy_config, fwnode,
+					  PHY_INTERFACE_MODE_INTERNAL,
+					  &mvsw_pr_mac_ops);
+		if (IS_ERR(phy_link)) {
+			netdev_err(port->net_dev, "failed to create phylink\n");
+			return PTR_ERR(phy_link);
+		}
 
-	return 0;
-}
+		port->phy_link = phy_link;
+		break;
+	}
 
-static void prestera_port_list_add(struct prestera_port *port)
-{
-	write_lock(&port->sw->port_list_lock);
-	list_add(&port->list, &port->sw->port_list);
-	write_unlock(&port->sw->port_list_lock);
+	return 0;
 }
-
-static void prestera_port_list_del(struct prestera_port *port)
+#else
+static int mvsw_pr_port_sfp_bind(struct mvsw_pr_port *port)
 {
-	write_lock(&port->sw->port_list_lock);
-	list_del(&port->list);
-	write_unlock(&port->sw->port_list_lock);
+	return 0;
 }
+#endif
 
-static int prestera_port_create(struct prestera_switch *sw, u32 id)
+static int mvsw_pr_port_create(struct mvsw_pr_switch *sw, u32 id)
 {
-	struct prestera_port *port;
-	struct net_device *dev;
+	struct net_device *net_dev;
+	struct mvsw_pr_port *port;
+	char *mac;
 	int err;
 
-	dev = alloc_etherdev(sizeof(*port));
-	if (!dev)
+	net_dev = alloc_etherdev(sizeof(*port));
+	if (!net_dev)
 		return -ENOMEM;
 
-	port = netdev_priv(dev);
+	port = netdev_priv(net_dev);
 
 	INIT_LIST_HEAD(&port->vlans_list);
-	port->pvid = PRESTERA_DEFAULT_VID;
-	port->dev = dev;
+	port->pvid = MVSW_PR_DEFAULT_VID;
+	port->net_dev = net_dev;
 	port->id = id;
 	port->sw = sw;
+	port->lag_id = sw->lag_max;
 
-	err = prestera_hw_port_info_get(port, &port->dev_id, &port->hw_id,
-					&port->fp_id);
+	err = mvsw_pr_hw_port_info_get(port, &port->fp_id,
+				       &port->hw_id, &port->dev_id);
 	if (err) {
-		dev_err(prestera_dev(sw), "Failed to get port(%u) info\n", id);
-		goto err_port_info_get;
+		dev_err(mvsw_dev(sw), "Failed to get port(%u) info\n", id);
+		goto err_free_netdev;
 	}
 
 	err = prestera_devlink_port_register(port);
 	if (err)
-		goto err_dl_port_register;
+		goto err_devl_port_reg;
 
-	dev->features |= NETIF_F_NETNS_LOCAL;
-	dev->netdev_ops = &prestera_netdev_ops;
-	dev->ethtool_ops = &prestera_ethtool_ops;
+	net_dev->needed_headroom = MVSW_PR_DSA_HLEN + 4;
 
-	netif_carrier_off(dev);
+	net_dev->netdev_ops = &mvsw_pr_netdev_ops;
+	net_dev->ethtool_ops = &mvsw_pr_ethtool_ops;
+	net_dev->features |= NETIF_F_NETNS_LOCAL | NETIF_F_HW_TC;
+	net_dev->hw_features |= NETIF_F_HW_TC;
+	net_dev->ethtool_ops = &mvsw_pr_ethtool_ops;
+	net_dev->netdev_ops = &mvsw_pr_netdev_ops;
+	SET_NETDEV_DEV(net_dev, sw->dev->dev);
 
-	dev->mtu = min_t(unsigned int, sw->mtu_max, PRESTERA_MTU_DEFAULT);
-	dev->min_mtu = sw->mtu_min;
-	dev->max_mtu = sw->mtu_max;
+	net_dev->mtu = min_t(unsigned int, sw->mtu_max, MVSW_PR_MTU_DEFAULT);
+	net_dev->min_mtu = sw->mtu_min;
+	net_dev->max_mtu = sw->mtu_max;
 
-	err = prestera_hw_port_mtu_set(port, dev->mtu);
+	err = mvsw_pr_hw_port_mtu_set(port, net_dev->mtu);
 	if (err) {
-		dev_err(prestera_dev(sw), "Failed to set port(%u) mtu(%d)\n",
-			id, dev->mtu);
+		dev_err(mvsw_dev(sw), "Failed to set port(%u) mtu\n", id);
 		goto err_port_init;
 	}
 
-	if (port->fp_id >= PRESTERA_MAC_ADDR_NUM_MAX) {
-		err = -EINVAL;
+	/* Only 0xFF mac addrs are supported */
+	if (port->fp_id >= 0xFF)
 		goto err_port_init;
-	}
 
-	/* firmware requires that port's MAC address consist of the first
-	 * 5 bytes of the base MAC address
-	 */
-	memcpy(dev->dev_addr, sw->base_mac, dev->addr_len - 1);
-	dev->dev_addr[dev->addr_len - 1] = port->fp_id;
+	mac = net_dev->dev_addr;
+	memcpy(mac, sw->base_mac, net_dev->addr_len);
+	mac[net_dev->addr_len - 1] += port->fp_id + MVSW_PR_MAC_ADDR_OFFSET;
 
-	err = prestera_hw_port_mac_set(port, dev->dev_addr);
+	err = mvsw_pr_hw_port_mac_set(port, mac);
 	if (err) {
-		dev_err(prestera_dev(sw), "Failed to set port(%u) mac addr\n", id);
+		dev_err(mvsw_dev(sw), "Failed to set port(%u) mac addr\n", id);
 		goto err_port_init;
 	}
 
-	err = prestera_hw_port_cap_get(port, &port->caps);
+	err = mvsw_pr_hw_port_cap_get(port, &port->caps);
 	if (err) {
-		dev_err(prestera_dev(sw), "Failed to get port(%u) caps\n", id);
+		dev_err(mvsw_dev(sw), "Failed to get port(%u) caps\n", id);
 		goto err_port_init;
 	}
 
-	port->adver_fec = BIT(PRESTERA_PORT_FEC_OFF);
-	prestera_port_autoneg_set(port, true, port->caps.supp_link_modes,
-				  port->caps.supp_fec);
+#ifdef CONFIG_PHYLINK
+	if (port->caps.transceiver != MVSW_PORT_TRANSCEIVER_SFP)
+		netif_carrier_off(net_dev);
+#else
+	netif_carrier_off(net_dev);
+#endif
 
-	err = prestera_hw_port_state_set(port, false);
+	port->adver_link_modes = 0;
+	port->adver_fec = 0;
+	port->autoneg = false;
+	mvsw_pr_port_autoneg_set(port, true, port->caps.supp_link_modes,
+				 BIT(MVSW_PORT_FEC_OFF_BIT));
+
+	err = mvsw_pr_hw_port_state_set(port, false);
 	if (err) {
-		dev_err(prestera_dev(sw), "Failed to set port(%u) down\n", id);
+		dev_err(mvsw_dev(sw), "Failed to set port(%u) down\n", id);
 		goto err_port_init;
 	}
 
-	err = prestera_rxtx_port_init(port);
+	INIT_DELAYED_WORK(&port->cached_hw_stats.caching_dw,
+			  &update_stats_cache);
+
+	err = register_netdev(net_dev);
 	if (err)
 		goto err_port_init;
 
-	INIT_DELAYED_WORK(&port->cached_hw_stats.caching_dw,
-			  &prestera_port_stats_update);
+	if (port->caps.transceiver == MVSW_PORT_TRANSCEIVER_SFP) {
+		err = mvsw_pr_port_sfp_bind(port);
+		if (err)
+			goto err_sfp_bind;
+	}
 
-	prestera_port_list_add(port);
+	list_add(&port->list, &sw->port_list);
 
-	err = register_netdev(dev);
-	if (err)
-		goto err_register_netdev;
+	mvsw_pr_port_uc_flood_set(port, false);
+	mvsw_pr_port_mc_flood_set(port, false);
 
 	prestera_devlink_port_set(port);
 
 	return 0;
 
-err_register_netdev:
-	prestera_port_list_del(port);
-err_port_init:
+err_sfp_bind:
+	unregister_netdev(net_dev);
 	prestera_devlink_port_unregister(port);
-err_dl_port_register:
-err_port_info_get:
-	free_netdev(dev);
+err_port_init:
+err_devl_port_reg:
+err_free_netdev:
+	free_netdev(net_dev);
 	return err;
 }
 
-static void prestera_port_destroy(struct prestera_port *port)
+static void mvsw_pr_port_vlan_flush(struct mvsw_pr_port *port,
+				    bool flush_default)
 {
-	struct net_device *dev = port->dev;
+	struct mvsw_pr_port_vlan *port_vlan, *tmp;
 
-	cancel_delayed_work_sync(&port->cached_hw_stats.caching_dw);
-	prestera_devlink_port_clear(port);
-	unregister_netdev(dev);
-	prestera_port_list_del(port);
-	prestera_devlink_port_unregister(port);
-	free_netdev(dev);
+	list_for_each_entry_safe(port_vlan, tmp, &port->vlans_list, list) {
+		if (!flush_default && port_vlan->vid == MVSW_PR_DEFAULT_VID)
+			continue;
+
+		mvsw_pr_port_vlan_destroy(port_vlan);
+	}
+}
+
+int mvsw_pr_8021d_bridge_create(struct mvsw_pr_switch *sw, u16 *bridge_id)
+{
+	return mvsw_pr_hw_bridge_create(sw, bridge_id);
+}
+
+int mvsw_pr_8021d_bridge_delete(struct mvsw_pr_switch *sw, u16 bridge_id)
+{
+	return mvsw_pr_hw_bridge_delete(sw, bridge_id);
+}
+
+int mvsw_pr_8021d_bridge_port_add(struct mvsw_pr_port *port, u16 bridge_id)
+{
+	return mvsw_pr_hw_bridge_port_add(port, bridge_id);
+}
+
+int mvsw_pr_8021d_bridge_port_delete(struct mvsw_pr_port *port, u16 bridge_id)
+{
+	return mvsw_pr_hw_bridge_port_delete(port, bridge_id);
+}
+
+int mvsw_pr_switch_ageing_set(struct mvsw_pr_switch *sw, u32 ageing_time)
+{
+	return mvsw_pr_hw_switch_ageing_set(sw, ageing_time / 1000);
+}
+
+int mvsw_pr_dev_if_type(const struct net_device *dev)
+{
+	struct macvlan_dev *vlan;
+
+	if (is_vlan_dev(dev) && netif_is_bridge_master(vlan_dev_real_dev(dev)))
+		return MVSW_IF_VID_E;
+	else if (netif_is_bridge_master(dev))
+		return MVSW_IF_VID_E;
+	else if (netif_is_lag_master(dev))
+		return MVSW_IF_LAG_E;
+	else if (netif_is_macvlan(dev)) {
+		vlan = netdev_priv(dev);
+		return mvsw_pr_dev_if_type(vlan->lowerdev);
+	}
+	else
+		return MVSW_IF_PORT_E;
+}
+
+int mvsw_pr_lpm_add(struct mvsw_pr_switch *sw, u16 hw_vr_id,
+		    struct mvsw_pr_ip_addr *addr, u32 prefix_len, u32 grp_id)
+{
+	/* Dont waste time on hw requests,
+	 * if router (and probably vr) aborted
+	 */
+	if (sw->router->aborted)
+		return -ENOENT;
+
+	/* TODO: ipv6 key type check before call designated hw cb */
+	return mvsw_pr_hw_lpm_add(sw, hw_vr_id, addr->u.ipv4,
+				  prefix_len, grp_id);
+}
+
+int mvsw_pr_lpm_del(struct mvsw_pr_switch *sw, u16 hw_vr_id,
+		    struct mvsw_pr_ip_addr *addr, u32 prefix_len)
+{
+	/* Dont waste time on hw requests,
+	 * if router (and probably vr) aborted
+	 */
+	if (sw->router->aborted)
+		return -ENOENT;
+
+	/* TODO: ipv6 key type check before call designated hw cb */
+	return mvsw_pr_hw_lpm_del(sw, hw_vr_id, addr->u.ipv4,
+				  prefix_len);
+}
+
+int mvsw_pr_nh_entries_set(const struct mvsw_pr_switch *sw, int count,
+			   struct mvsw_pr_neigh_info *nhs, u32 grp_id)
+{
+	/* Dont waste time on hw requests,
+	 * if router (and probably vr) aborted
+	 */
+	if (sw->router->aborted)
+		return -ENOENT;
+
+	return mvsw_pr_hw_nh_entries_set(sw, count, nhs, grp_id);
+}
+
+int mvsw_pr_nh_entries_get(const struct mvsw_pr_switch *sw, int count,
+			   struct mvsw_pr_neigh_info *nhs, u32 grp_id)
+{
+	/* Dont waste time on hw requests,
+	 * if router (and probably vr) aborted
+	 */
+	if (sw->router->aborted)
+		return -ENOENT;
+
+	return mvsw_pr_hw_nh_entries_get(sw, count, nhs, grp_id);
+}
+
+int mvsw_pr_nh_group_create(const struct mvsw_pr_switch *sw, u16 nh_count,
+			    u32 *grp_id)
+{
+	/* Dont waste time on hw requests,
+	 * if router (and probably vr) aborted
+	 */
+	if (sw->router->aborted)
+		return -ENOENT;
+
+	return mvsw_pr_hw_nh_group_create(sw, nh_count, grp_id);
+}
+
+int mvsw_pr_nh_group_delete(const struct mvsw_pr_switch *sw, u16 nh_count,
+			    u32 grp_id)
+{
+	/* Dont waste time on hw requests,
+	 * if router (and probably vr) aborted
+	 */
+	if (sw->router->aborted)
+		return -ENOENT;
+
+	return mvsw_pr_hw_nh_group_delete(sw, nh_count, grp_id);
+}
+
+int mvsw_pr_mp4_hash_set(const struct mvsw_pr_switch *sw, u8 hash_policy)
+{
+	return mvsw_pr_hw_mp4_hash_set(sw, hash_policy);
+}
+
+int mvsw_pr_fdb_flush_vlan(struct mvsw_pr_switch *sw, u16 vid,
+			   enum mvsw_pr_fdb_flush_mode mode)
+{
+	return mvsw_pr_hw_fdb_flush_vlan(sw, vid, mode);
+}
+
+int mvsw_pr_fdb_flush_port_vlan(struct mvsw_pr_port *port, u16 vid,
+				enum mvsw_pr_fdb_flush_mode mode)
+{
+	if (mvsw_pr_port_is_lag_member(port))
+		return mvsw_pr_hw_fdb_flush_lag_vlan(port->sw, port->lag_id,
+						     vid, mode);
+	else
+		return mvsw_pr_hw_fdb_flush_port_vlan(port, vid, mode);
+}
+
+int mvsw_pr_fdb_flush_port(struct mvsw_pr_port *port,
+			   enum mvsw_pr_fdb_flush_mode mode)
+{
+	if (mvsw_pr_port_is_lag_member(port))
+		return mvsw_pr_hw_fdb_flush_lag(port->sw, port->lag_id, mode);
+	else
+		return mvsw_pr_hw_fdb_flush_port(port, mode);
+}
+
+int mvsw_pr_macvlan_add(const struct mvsw_pr_switch *sw, u16 vr_id,
+			const u8 *mac, u16 vid)
+{
+	return mvsw_pr_hw_macvlan_add(sw, vr_id, mac,  vid);
+}
+
+int mvsw_pr_macvlan_del(const struct mvsw_pr_switch *sw, u16 vr_id,
+			const u8 *mac, u16 vid)
+{
+	return mvsw_pr_hw_macvlan_del(sw, vr_id, mac,  vid);
+}
+
+static struct prestera_lag *
+prestera_lag_get(struct mvsw_pr_switch *sw, u8 id)
+{
+	return id < sw->lag_max ? &sw->lags[id] : NULL;
 }
 
-static void prestera_destroy_ports(struct prestera_switch *sw)
+static void mvsw_pr_port_lag_create(struct mvsw_pr_switch *sw, u16 lag_id,
+				    struct net_device *lag_dev)
 {
-	struct prestera_port *port, *tmp;
+	INIT_LIST_HEAD(&sw->lags[lag_id].members);
+	sw->lags[lag_id].dev = lag_dev;
+}
+
+static void mvsw_pr_port_lag_destroy(struct mvsw_pr_switch *sw, u16 lag_id)
+{
+	WARN_ON(!list_empty(&sw->lags[lag_id].members));
+	sw->lags[lag_id].dev = NULL;
+	sw->lags[lag_id].member_count = 0;
+}
+
+int prestera_lag_member_add(struct mvsw_pr_port *port,
+			    struct net_device *lag_dev, u16 lag_id)
+{
+	struct mvsw_pr_switch *sw = port->sw;
+	struct prestera_lag_member *member;
+	struct prestera_lag *lag;
+
+	lag = prestera_lag_get(sw, lag_id);
+
+	if (lag->member_count >= sw->lag_member_max)
+		return -ENOSPC;
+	else if (!lag->member_count)
+		mvsw_pr_port_lag_create(sw, lag_id, lag_dev);
 
-	list_for_each_entry_safe(port, tmp, &sw->port_list, list)
-		prestera_port_destroy(port);
+	member = kzalloc(sizeof(*member), GFP_KERNEL);
+	if (!member)
+		return -ENOMEM;
+
+	if (mvsw_pr_hw_lag_member_add(port, lag_id)) {
+		kfree(member);
+		if (!lag->member_count)
+			mvsw_pr_port_lag_destroy(sw, lag_id);
+		return -EBUSY;
+	}
+
+	member->port = port;
+	list_add(&member->list, &lag->members);
+	lag->member_count++;
+	port->lag_id = lag_id;
+	return 0;
 }
 
-static int prestera_create_ports(struct prestera_switch *sw)
+int prestera_lag_member_del(struct mvsw_pr_port *port)
 {
-	struct prestera_port *port, *tmp;
-	u32 port_idx;
+	struct mvsw_pr_switch *sw = port->sw;
+	struct prestera_lag_member *member;
+	struct list_head *pos, *n;
+	u16 lag_id = port->lag_id;
+	struct prestera_lag *lag;
 	int err;
 
-	for (port_idx = 0; port_idx < sw->port_count; port_idx++) {
-		err = prestera_port_create(sw, port_idx);
-		if (err)
-			goto err_port_create;
+	lag = prestera_lag_get(sw, lag_id);
+	if (!lag || !lag->member_count)
+		return -EINVAL;
+
+	err = mvsw_pr_hw_lag_member_del(port, lag_id);
+	if (err)
+		return err;
+
+	lag->member_count--;
+	port->lag_id = sw->lag_max;
+
+	list_for_each_safe(pos, n, &lag->members) {
+		member = list_entry(pos, typeof(*member), list);
+		if (member->port->id == port->id) {
+			list_del(&member->list);
+			kfree(member);
+			break;
+		}
+	}
+
+	if (!lag->member_count) {
+		prestera_lag_router_leave(sw, lag->dev);
+		mvsw_pr_port_lag_destroy(sw, lag_id);
 	}
 
 	return 0;
+}
 
-err_port_create:
-	list_for_each_entry_safe(port, tmp, &sw->port_list, list)
-		prestera_port_destroy(port);
+int prestera_lag_member_enable(struct mvsw_pr_port *port, bool enable)
+{
+	return mvsw_pr_hw_lag_member_enable(port, port->lag_id, enable);
+}
 
-	return err;
+bool mvsw_pr_port_is_lag_member(const struct mvsw_pr_port *port)
+{
+	return port->lag_id < port->sw->lag_max;
+}
+
+int prestera_lag_id_find(struct mvsw_pr_switch *sw, struct net_device *lag_dev,
+			 u16 *lag_id)
+{
+	struct prestera_lag *lag;
+	int free_id = -1;
+	int id;
+
+	for (id = 0; id < sw->lag_max; id++) {
+		lag = prestera_lag_get(sw, id);
+		if (lag->member_count) {
+			if (lag->dev == lag_dev) {
+				*lag_id = id;
+				return 0;
+			}
+		} else if (free_id < 0) {
+			free_id = id;
+		}
+	}
+	if (free_id < 0)
+		return -ENOSPC;
+	*lag_id = free_id;
+	return 0;
+}
+
+void prestera_lag_member_rif_leave(const struct mvsw_pr_port *port,
+				   u16 lag_id, u16 vr_id)
+{
+	mvsw_pr_hw_lag_member_rif_leave(port, lag_id, vr_id);
+}
+
+static int prestera_lag_init(struct mvsw_pr_switch *sw)
+{
+	sw->lags = kcalloc(sw->lag_max, sizeof(*sw->lags), GFP_KERNEL);
+	return sw->lags ? 0 : -ENOMEM;
+}
+
+static void prestera_lag_fini(struct mvsw_pr_switch *sw)
+{
+	u8 idx;
+
+	for (idx = 0; idx < sw->lag_max; idx++)
+		WARN_ON(sw->lags[idx].member_count);
+
+	kfree(sw->lags);
+}
+
+static int mvsw_pr_clear_ports(struct mvsw_pr_switch *sw)
+{
+	struct net_device *net_dev;
+	struct list_head *pos, *n;
+	struct mvsw_pr_port *port;
+
+	list_for_each_safe(pos, n, &sw->port_list) {
+		port = list_entry(pos, typeof(*port), list);
+		net_dev = port->net_dev;
+
+		cancel_delayed_work_sync(&port->cached_hw_stats.caching_dw);
+		prestera_devlink_port_clear(port);
+		unregister_netdev(net_dev);
+#ifdef CONFIG_PHYLINK
+		if (port->phy_link)
+			phylink_destroy(port->phy_link);
+#endif
+		mvsw_pr_port_vlan_flush(port, true);
+		WARN_ON_ONCE(!list_empty(&port->vlans_list));
+		mvsw_pr_port_router_leave(port);
+		prestera_devlink_port_unregister(port);
+		free_netdev(net_dev);
+		list_del(pos);
+	}
+	return (!list_empty(&sw->port_list));
 }
 
-static void prestera_port_handle_event(struct prestera_switch *sw,
-				       struct prestera_event *evt, void *arg)
+static void mvsw_pr_port_handle_event(struct mvsw_pr_switch *sw,
+				      struct mvsw_pr_event *evt, void *arg)
 {
+	struct mvsw_pr_port *port;
 	struct delayed_work *caching_dw;
-	struct prestera_port *port;
 
-	port = prestera_find_port(sw, evt->port_evt.port_id);
-	if (!port || !port->dev)
+	port = __find_pr_port(sw, evt->port_evt.port_id);
+	if (!port)
 		return;
 
 	caching_dw = &port->cached_hw_stats.caching_dw;
 
-	if (evt->id == PRESTERA_PORT_EVENT_STATE_CHANGED) {
-		if (evt->port_evt.data.oper_state) {
-			netif_carrier_on(port->dev);
+	switch (evt->id) {
+	case MVSW_PORT_EVENT_STATE_CHANGED:
+		port->hw_oper_state = evt->port_evt.data.oper_state;
+
+		if (port->hw_oper_state) {
+			port->hw_duplex = evt->port_evt.data.duplex;
+			port->hw_speed = evt->port_evt.data.speed;
+#ifdef CONFIG_PHYLINK
+			if (port->caps.transceiver ==
+			    MVSW_PORT_TRANSCEIVER_SFP)
+				phylink_mac_change(port->phy_link, true);
+			else
+				netif_carrier_on(port->net_dev);
+#else
+			netif_carrier_on(port->net_dev);
+#endif
+
 			if (!delayed_work_pending(caching_dw))
-				queue_delayed_work(prestera_wq, caching_dw, 0);
+				queue_delayed_work(mvsw_pr_wq, caching_dw, 0);
 		} else {
-			netif_carrier_off(port->dev);
+			port->hw_duplex = 0;
+			port->hw_speed = 0;
+#ifdef CONFIG_PHYLINK
+			if (port->caps.transceiver ==
+			    MVSW_PORT_TRANSCEIVER_SFP)
+				phylink_mac_change(port->phy_link, false);
+			else
+				netif_carrier_off(port->net_dev);
+#else
+			netif_carrier_off(port->net_dev);
+#endif
+
 			if (delayed_work_pending(caching_dw))
 				cancel_delayed_work(caching_dw);
 		}
+		break;
 	}
 }
 
-static int prestera_event_handlers_register(struct prestera_switch *sw)
+static bool prestera_lag_exists(const struct mvsw_pr_switch *sw, u16 lag_id)
 {
-	return prestera_hw_event_handler_register(sw, PRESTERA_EVENT_TYPE_PORT,
-						  prestera_port_handle_event,
-						  NULL);
+	return lag_id < sw->lag_max &&
+	       sw->lags[lag_id].member_count != 0;
 }
 
-static void prestera_event_handlers_unregister(struct prestera_switch *sw)
+static void mvsw_pr_fdb_handle_event(struct mvsw_pr_switch *sw,
+				     struct mvsw_pr_event *evt, void *arg)
 {
-	prestera_hw_event_handler_unregister(sw, PRESTERA_EVENT_TYPE_PORT,
-					     prestera_port_handle_event);
-}
+	struct switchdev_notifier_fdb_info info;
+	struct net_device *dev = NULL;
+	struct mvsw_pr_port *port;
+	u16 lag_id;
+
+	switch (evt->fdb_evt.type) {
+	case MVSW_PR_FDB_ENTRY_TYPE_REG_PORT:
+		port = __find_pr_port(sw, evt->fdb_evt.dest.port_id);
+		if (port)
+			dev = port->net_dev;
+		break;
+	case MVSW_PR_FDB_ENTRY_TYPE_LAG:
+		lag_id = evt->fdb_evt.dest.lag_id;
+		if (prestera_lag_exists(sw, lag_id))
+			dev = sw->lags[lag_id].dev;
+		break;
+	default:
+		return;
+	}
 
-static int prestera_switch_set_base_mac_addr(struct prestera_switch *sw)
-{
-	struct device_node *base_mac_np;
-	struct device_node *np;
-	const char *base_mac;
+	if (!dev)
+		return;
 
-	np = of_find_compatible_node(NULL, NULL, "marvell,prestera");
-	base_mac_np = of_parse_phandle(np, "base-mac-provider", 0);
+	info.addr = evt->fdb_evt.data.mac;
+	info.vid = evt->fdb_evt.vid;
+	info.offloaded = true;
+
+	rtnl_lock();
+	switch (evt->id) {
+	case MVSW_FDB_EVENT_LEARNED:
+		call_switchdev_notifiers(SWITCHDEV_FDB_ADD_TO_BRIDGE,
+					 dev, &info.info, NULL);
+		break;
+	case MVSW_FDB_EVENT_AGED:
+		call_switchdev_notifiers(SWITCHDEV_FDB_DEL_TO_BRIDGE,
+					 dev, &info.info, NULL);
+		break;
+	}
+	rtnl_unlock();
+}
 
-	base_mac = of_get_mac_address(base_mac_np);
-	of_node_put(base_mac_np);
-	if (!IS_ERR(base_mac))
-		ether_addr_copy(sw->base_mac, base_mac);
+int mvsw_pr_fdb_add(struct mvsw_pr_port *port, const unsigned char *mac,
+		    u16 vid, bool dynamic)
+{
+	if (mvsw_pr_port_is_lag_member(port))
+		return mvsw_pr_hw_lag_fdb_add(port->sw, port->lag_id,
+					      mac, vid, dynamic);
+	else
+		return mvsw_pr_hw_fdb_add(port, mac, vid, dynamic);
+}
 
-	if (!is_valid_ether_addr(sw->base_mac)) {
-		eth_random_addr(sw->base_mac);
-		dev_info(prestera_dev(sw), "using random base mac address\n");
-	}
+int mvsw_pr_fdb_del(struct mvsw_pr_port *port, const unsigned char *mac,
+		    u16 vid)
+{
+	if (mvsw_pr_port_is_lag_member(port))
+		return mvsw_pr_hw_lag_fdb_del(port->sw, port->lag_id,
+					      mac, vid);
+	else
+		return mvsw_pr_hw_fdb_del(port, mac, vid);
+}
 
-	return prestera_hw_switch_mac_set(sw, sw->base_mac);
+static void mvsw_pr_fdb_event_handler_unregister(struct mvsw_pr_switch *sw)
+{
+	mvsw_pr_hw_event_handler_unregister(sw, MVSW_EVENT_TYPE_FDB);
 }
 
-bool prestera_netdev_check(const struct net_device *dev)
+static void mvsw_pr_port_event_handler_unregister(struct mvsw_pr_switch *sw)
 {
-	return dev->netdev_ops == &prestera_netdev_ops;
+	mvsw_pr_hw_event_handler_unregister(sw, MVSW_EVENT_TYPE_PORT);
 }
 
-static int prestera_lower_dev_walk(struct net_device *dev,
-				   struct netdev_nested_priv *priv)
+static void mvsw_pr_event_handlers_unregister(struct mvsw_pr_switch *sw)
 {
-	struct prestera_port **pport = (struct prestera_port **)priv->data;
+	mvsw_pr_fdb_event_handler_unregister(sw);
+	mvsw_pr_port_event_handler_unregister(sw);
+}
 
-	if (prestera_netdev_check(dev)) {
-		*pport = netdev_priv(dev);
-		return 1;
-	}
+static int mvsw_pr_fdb_event_handler_register(struct mvsw_pr_switch *sw)
+{
+	return mvsw_pr_hw_event_handler_register(sw, MVSW_EVENT_TYPE_FDB,
+						 mvsw_pr_fdb_handle_event,
+						 NULL);
+}
 
-	return 0;
+static int mvsw_pr_port_event_handler_register(struct mvsw_pr_switch *sw)
+{
+	return mvsw_pr_hw_event_handler_register(sw, MVSW_EVENT_TYPE_PORT,
+						 mvsw_pr_port_handle_event,
+						 NULL);
 }
 
-struct prestera_port *prestera_port_dev_lower_find(struct net_device *dev)
+static int mvsw_pr_event_handlers_register(struct mvsw_pr_switch *sw)
 {
-	struct prestera_port *port = NULL;
-	struct netdev_nested_priv priv = {
-		.data = (void *)&port,
-	};
+	int err;
 
-	if (prestera_netdev_check(dev))
-		return netdev_priv(dev);
+	err = mvsw_pr_port_event_handler_register(sw);
+	if (err)
+		return err;
 
-	netdev_walk_all_lower_dev(dev, prestera_lower_dev_walk, &priv);
+	err = mvsw_pr_fdb_event_handler_register(sw);
+	if (err)
+		goto err_fdb_handler_register;
 
-	return port;
+	return 0;
+
+err_fdb_handler_register:
+	mvsw_pr_port_event_handler_unregister(sw);
+	return err;
 }
 
-static int prestera_netdev_port_event(struct net_device *dev,
-				      unsigned long event, void *ptr)
+int mvsw_pr_schedule_dw(struct delayed_work *dwork, unsigned long delay)
 {
-	switch (event) {
-	case NETDEV_PRECHANGEUPPER:
-	case NETDEV_CHANGEUPPER:
-		return prestera_bridge_port_event(dev, event, ptr);
-	default:
-		return 0;
+	return queue_delayed_work(mvsw_pr_wq, dwork, delay);
+}
+
+const struct mvsw_pr_port *mvsw_pr_port_find(u32 dev_hw_id, u32 port_hw_id)
+{
+	struct mvsw_pr_port *port = NULL;
+	struct mvsw_pr_switch *sw;
+
+	list_for_each_entry(sw, &switches_registered, list) {
+		list_for_each_entry(port, &sw->port_list, list) {
+			if (port->hw_id == port_hw_id &&
+			    port->dev_id == dev_hw_id)
+				return port;
+		}
 	}
+	return NULL;
 }
 
-static int prestera_netdev_event_handler(struct notifier_block *nb,
-					 unsigned long event, void *ptr)
+static int mvsw_pr_sw_init_base_mac(struct mvsw_pr_switch *sw)
 {
-	struct net_device *dev = netdev_notifier_info_to_dev(ptr);
-	int err = 0;
+	struct device_node *mac_dev_np;
+	u32 lsb;
+	int err;
 
-	if (prestera_netdev_check(dev))
-		err = prestera_netdev_port_event(dev, event, ptr);
+	if (sw->np) {
+		mac_dev_np = of_parse_phandle(sw->np, "base-mac-provider", 0);
+		if (mac_dev_np) {
+			const char *base_mac;
 
-	return notifier_from_errno(err);
-}
+			base_mac = of_get_mac_address(mac_dev_np);
+			if (!IS_ERR(base_mac))
+				ether_addr_copy(sw->base_mac, base_mac);
+		}
+	}
 
-static int prestera_netdev_event_handler_register(struct prestera_switch *sw)
-{
-	sw->netdev_nb.notifier_call = prestera_netdev_event_handler;
+	if (!is_valid_ether_addr(sw->base_mac))
+		eth_random_addr(sw->base_mac);
 
-	return register_netdevice_notifier(&sw->netdev_nb);
-}
+	lsb = sw->base_mac[ETH_ALEN - 1];
+	if (lsb + sw->port_count + MVSW_PR_MAC_ADDR_OFFSET > 0xFF)
+		sw->base_mac[ETH_ALEN - 1] = 0;
 
-static void prestera_netdev_event_handler_unregister(struct prestera_switch *sw)
-{
-	unregister_netdevice_notifier(&sw->netdev_nb);
+	err = mvsw_pr_hw_switch_mac_set(sw, sw->base_mac);
+	if (err)
+		return err;
+
+	return 0;
 }
 
-static int prestera_switch_init(struct prestera_switch *sw)
+static int mvsw_pr_init(struct mvsw_pr_switch *sw)
 {
+	u32 port;
 	int err;
 
-	err = prestera_hw_switch_init(sw);
+	sw->np = of_find_compatible_node(NULL, NULL, "marvell,prestera");
+
+	err = mvsw_pr_hw_switch_init(sw);
 	if (err) {
-		dev_err(prestera_dev(sw), "Failed to init Switch device\n");
+		dev_err(mvsw_dev(sw), "Failed to init Switch device\n");
 		return err;
 	}
 
-	rwlock_init(&sw->port_list_lock);
-	INIT_LIST_HEAD(&sw->port_list);
+	err = mvsw_pr_hw_switch_trap_policer_set(sw, trap_policer_profile);
+	if (err) {
+		dev_err(mvsw_dev(sw), "Failed to set trap policer profile\n");
+		return err;
+	}
 
-	err = prestera_switch_set_base_mac_addr(sw);
+	err = mvsw_pr_sw_init_base_mac(sw);
 	if (err)
 		return err;
 
-	err = prestera_netdev_event_handler_register(sw);
+	dev_info(mvsw_dev(sw), "Initialized Switch device\n");
+
+	err = prestera_lag_init(sw);
 	if (err)
 		return err;
 
-	err = prestera_switchdev_init(sw);
+	err = prestera_switchdev_register(sw);
 	if (err)
-		goto err_swdev_register;
+		return err;
 
-	err = prestera_rxtx_switch_init(sw);
+	err = prestera_devlink_register(sw);
 	if (err)
-		goto err_rxtx_register;
+		goto err_devl_reg;
+
+	INIT_LIST_HEAD(&sw->port_list);
 
-	err = prestera_event_handlers_register(sw);
+	for (port = 0; port < sw->port_count; port++) {
+		err = mvsw_pr_port_create(sw, port);
+		if (err)
+			goto err_ports_init;
+	}
+
+	err = mvsw_pr_rxtx_switch_init(sw);
 	if (err)
-		goto err_handlers_register;
+		goto err_rxtx_init;
 
-	err = prestera_devlink_register(sw);
+	err = mvsw_pr_event_handlers_register(sw);
 	if (err)
-		goto err_dl_register;
+		goto err_event_handlers;
 
-	err = prestera_create_ports(sw);
+	err = mvsw_pr_debugfs_init(sw);
 	if (err)
-		goto err_ports_create;
+		goto err_debugfs_init;
 
-	return 0;
+	err = prestera_acl_init(sw);
+	if (err)
+		goto err_acl_init;
 
-err_ports_create:
-	prestera_devlink_unregister(sw);
-err_dl_register:
-	prestera_event_handlers_unregister(sw);
-err_handlers_register:
-	prestera_rxtx_switch_fini(sw);
-err_rxtx_register:
-	prestera_switchdev_fini(sw);
-err_swdev_register:
-	prestera_netdev_event_handler_unregister(sw);
-	prestera_hw_switch_fini(sw);
+	return 0;
 
+err_acl_init:
+err_debugfs_init:
+	mvsw_pr_event_handlers_unregister(sw);
+err_event_handlers:
+	mvsw_pr_rxtx_switch_fini(sw);
+err_rxtx_init:
+err_ports_init:
+	mvsw_pr_clear_ports(sw);
+err_devl_reg:
+	prestera_switchdev_unregister(sw);
 	return err;
 }
 
-static void prestera_switch_fini(struct prestera_switch *sw)
+static void mvsw_pr_fini(struct mvsw_pr_switch *sw)
 {
-	prestera_destroy_ports(sw);
+	mvsw_pr_debugfs_fini(sw);
+
+	mvsw_pr_event_handlers_unregister(sw);
+
+	mvsw_pr_clear_ports(sw);
+	mvsw_pr_rxtx_switch_fini(sw);
 	prestera_devlink_unregister(sw);
-	prestera_event_handlers_unregister(sw);
-	prestera_rxtx_switch_fini(sw);
-	prestera_switchdev_fini(sw);
-	prestera_netdev_event_handler_unregister(sw);
-	prestera_hw_switch_fini(sw);
+	prestera_switchdev_unregister(sw);
+	prestera_acl_fini(sw);
+	prestera_lag_fini(sw);
+	of_node_put(sw->np);
 }
 
 int prestera_device_register(struct prestera_device *dev)
 {
-	struct prestera_switch *sw;
+	struct mvsw_pr_switch *sw;
 	int err;
 
 	sw = prestera_devlink_alloc();
@@ -629,41 +2506,63 @@ int prestera_device_register(struct prestera_device *dev)
 	dev->priv = sw;
 	sw->dev = dev;
 
-	err = prestera_switch_init(sw);
+	err = mvsw_pr_init(sw);
 	if (err) {
 		prestera_devlink_free(sw);
 		return err;
 	}
 
+	list_add(&sw->list, &switches_registered);
+
 	return 0;
 }
 EXPORT_SYMBOL(prestera_device_register);
 
 void prestera_device_unregister(struct prestera_device *dev)
 {
-	struct prestera_switch *sw = dev->priv;
+	struct mvsw_pr_switch *sw = dev->priv;
 
-	prestera_switch_fini(sw);
+	list_del(&sw->list);
+	mvsw_pr_fini(sw);
 	prestera_devlink_free(sw);
 }
 EXPORT_SYMBOL(prestera_device_unregister);
 
-static int __init prestera_module_init(void)
+static int __init mvsw_pr_module_init(void)
 {
-	prestera_wq = alloc_workqueue("prestera", 0, 0);
-	if (!prestera_wq)
+	int err;
+
+	INIT_LIST_HEAD(&switches_registered);
+
+	mvsw_pr_wq = alloc_workqueue(mvsw_driver_name, 0, 0);
+	if (!mvsw_pr_wq)
 		return -ENOMEM;
 
+	err = mvsw_pr_rxtx_init();
+	if (err) {
+		pr_err("failed to initialize prestera rxtx\n");
+		destroy_workqueue(mvsw_pr_wq);
+		return err;
+	}
+
+	pr_info("Loading Marvell Prestera Switch Driver\n");
 	return 0;
 }
 
-static void __exit prestera_module_exit(void)
+static void __exit mvsw_pr_module_exit(void)
 {
-	destroy_workqueue(prestera_wq);
+	destroy_workqueue(mvsw_pr_wq);
+	mvsw_pr_rxtx_fini();
+
+	pr_info("Unloading Marvell Prestera Switch Driver\n");
 }
 
-module_init(prestera_module_init);
-module_exit(prestera_module_exit);
+module_init(mvsw_pr_module_init);
+module_exit(mvsw_pr_module_exit);
 
-MODULE_LICENSE("Dual BSD/GPL");
+MODULE_AUTHOR("Marvell Semi.");
+MODULE_LICENSE("GPL");
 MODULE_DESCRIPTION("Marvell Prestera switch driver");
+MODULE_VERSION(PRESTERA_DRV_VER);
+
+module_param(trap_policer_profile, byte, 0444);
diff --git a/drivers/net/ethernet/marvell/prestera/prestera_pci.c b/drivers/net/ethernet/marvell/prestera/prestera_pci.c
index be5677623..28ef761c7 100644
--- a/drivers/net/ethernet/marvell/prestera/prestera_pci.c
+++ b/drivers/net/ethernet/marvell/prestera/prestera_pci.c
@@ -1,52 +1,60 @@
-// SPDX-License-Identifier: BSD-3-Clause OR GPL-2.0
-/* Copyright (c) 2019-2020 Marvell International Ltd. All rights reserved */
+/* SPDX-License-Identifier: BSD-3-Clause OR GPL-2.0
+ *
+ * Copyright (c) 2019-2020 Marvell International Ltd. All rights reserved.
+ *
+ */
 
-#include <linux/circ_buf.h>
-#include <linux/device.h>
-#include <linux/firmware.h>
-#include <linux/iopoll.h>
-#include <linux/kernel.h>
 #include <linux/module.h>
+#include <linux/kernel.h>
+#include <linux/device.h>
 #include <linux/pci.h>
+#include <linux/circ_buf.h>
+#include <linux/firmware.h>
 
 #include "prestera.h"
 
-#define PRESTERA_MSG_MAX_SIZE 1500
+#define MVSW_FW_FILENAME	"marvell/mvsw_prestera_fw.img"
 
-#define PRESTERA_SUPP_FW_MAJ_VER	2
-#define PRESTERA_SUPP_FW_MIN_VER	0
+#define MVSW_SUPP_FW_MAJ_VER 2
+#define MVSW_SUPP_FW_MIN_VER 8
+#define MVSW_SUPP_FW_PATCH_VER 0
 
-#define PRESTERA_FW_PATH_FMT	"mrvl/prestera/mvsw_prestera_fw-v%u.%u.img"
+#define mvsw_wait_timeout(cond, waitms) \
+({ \
+	unsigned long __wait_end = jiffies + msecs_to_jiffies(waitms); \
+	bool __wait_ret = false; \
+	do { \
+		if (cond) { \
+			__wait_ret = true; \
+			break; \
+		} \
+		cond_resched(); \
+	} while (time_before(jiffies, __wait_end)); \
+	__wait_ret; \
+})
 
-#define PRESTERA_FW_HDR_MAGIC		0x351D9D06
-#define PRESTERA_FW_DL_TIMEOUT_MS	50000
-#define PRESTERA_FW_BLK_SZ		1024
+#define MVSW_FW_HDR_MAGIC 0x351D9D06
+#define MVSW_FW_DL_TIMEOUT 50000
+#define MVSW_FW_BLK_SZ 1024
 
-#define PRESTERA_FW_VER_MAJ_MUL 1000000
-#define PRESTERA_FW_VER_MIN_MUL 1000
+#define FW_VER_MAJ_MUL 1000000
+#define FW_VER_MIN_MUL 1000
 
-#define PRESTERA_FW_VER_MAJ(v)	((v) / PRESTERA_FW_VER_MAJ_MUL)
+#define FW_VER_MAJ(v)	((v) / FW_VER_MAJ_MUL)
 
-#define PRESTERA_FW_VER_MIN(v) \
-	(((v) - (PRESTERA_FW_VER_MAJ(v) * PRESTERA_FW_VER_MAJ_MUL)) / \
-			PRESTERA_FW_VER_MIN_MUL)
+#define FW_VER_MIN(v) \
+	(((v) - (FW_VER_MAJ(v) * FW_VER_MAJ_MUL)) / FW_VER_MIN_MUL)
 
-#define PRESTERA_FW_VER_PATCH(v) \
-	((v) - (PRESTERA_FW_VER_MAJ(v) * PRESTERA_FW_VER_MAJ_MUL) - \
-			(PRESTERA_FW_VER_MIN(v) * PRESTERA_FW_VER_MIN_MUL))
+#define FW_VER_PATCH(v) \
+	(v - (FW_VER_MAJ(v) * FW_VER_MAJ_MUL) - (FW_VER_MIN(v) * FW_VER_MIN_MUL))
 
-enum prestera_pci_bar_t {
-	PRESTERA_PCI_BAR_FW = 2,
-	PRESTERA_PCI_BAR_PP = 4,
-};
-
-struct prestera_fw_header {
+struct mvsw_pr_fw_header {
 	__be32 magic_number;
 	__be32 version_value;
 	u8 reserved[8];
-};
+} __packed;
 
-struct prestera_ldr_regs {
+struct mvsw_pr_ldr_regs {
 	u32 ldr_ready;
 	u32 pad1;
 
@@ -61,35 +69,57 @@ struct prestera_ldr_regs {
 	u32 ldr_buf_wr;
 
 	u32 ldr_status;
-};
+} __packed __aligned(4);
 
-#define PRESTERA_LDR_REG_OFFSET(f)	offsetof(struct prestera_ldr_regs, f)
+#define MVSW_LDR_REG_OFFSET(f)	offsetof(struct mvsw_pr_ldr_regs, f)
 
-#define PRESTERA_LDR_READY_MAGIC	0xf00dfeed
+#define MVSW_LDR_READY_MAGIC	0xf00dfeed
 
-#define PRESTERA_LDR_STATUS_IMG_DL	BIT(0)
-#define PRESTERA_LDR_STATUS_START_FW	BIT(1)
-#define PRESTERA_LDR_STATUS_INVALID_IMG	BIT(2)
-#define PRESTERA_LDR_STATUS_NOMEM	BIT(3)
+#define MVSW_LDR_STATUS_IMG_DL		BIT(0)
+#define MVSW_LDR_STATUS_START_FW	BIT(1)
+#define MVSW_LDR_STATUS_INVALID_IMG	BIT(2)
+#define MVSW_LDR_STATUS_NOMEM		BIT(3)
 
-#define PRESTERA_LDR_REG_BASE(fw)	((fw)->ldr_regs)
-#define PRESTERA_LDR_REG_ADDR(fw, reg)	(PRESTERA_LDR_REG_BASE(fw) + (reg))
+#define mvsw_ldr_write(fw, reg, val) \
+	writel(val, (fw)->ldr_regs + (reg))
+#define mvsw_ldr_read(fw, reg)	\
+	readl((fw)->ldr_regs + (reg))
 
 /* fw loader registers */
-#define PRESTERA_LDR_READY_REG		PRESTERA_LDR_REG_OFFSET(ldr_ready)
-#define PRESTERA_LDR_IMG_SIZE_REG	PRESTERA_LDR_REG_OFFSET(ldr_img_size)
-#define PRESTERA_LDR_CTL_REG		PRESTERA_LDR_REG_OFFSET(ldr_ctl_flags)
-#define PRESTERA_LDR_BUF_SIZE_REG	PRESTERA_LDR_REG_OFFSET(ldr_buf_size)
-#define PRESTERA_LDR_BUF_OFFS_REG	PRESTERA_LDR_REG_OFFSET(ldr_buf_offs)
-#define PRESTERA_LDR_BUF_RD_REG		PRESTERA_LDR_REG_OFFSET(ldr_buf_rd)
-#define PRESTERA_LDR_BUF_WR_REG		PRESTERA_LDR_REG_OFFSET(ldr_buf_wr)
-#define PRESTERA_LDR_STATUS_REG		PRESTERA_LDR_REG_OFFSET(ldr_status)
-
-#define PRESTERA_LDR_CTL_DL_START	BIT(0)
-
-#define PRESTERA_EVT_QNUM_MAX	4
-
-struct prestera_fw_evtq_regs {
+#define MVSW_LDR_READY_REG	MVSW_LDR_REG_OFFSET(ldr_ready)
+#define MVSW_LDR_IMG_SIZE_REG	MVSW_LDR_REG_OFFSET(ldr_img_size)
+#define MVSW_LDR_CTL_REG	MVSW_LDR_REG_OFFSET(ldr_ctl_flags)
+#define MVSW_LDR_BUF_SIZE_REG	MVSW_LDR_REG_OFFSET(ldr_buf_size)
+#define MVSW_LDR_BUF_OFFS_REG	MVSW_LDR_REG_OFFSET(ldr_buf_offs)
+#define MVSW_LDR_BUF_RD_REG	MVSW_LDR_REG_OFFSET(ldr_buf_rd)
+#define MVSW_LDR_BUF_WR_REG	MVSW_LDR_REG_OFFSET(ldr_buf_wr)
+#define MVSW_LDR_STATUS_REG	MVSW_LDR_REG_OFFSET(ldr_status)
+
+#define MVSW_LDR_CTL_DL_START	BIT(0)
+
+#define MVSW_LDR_WR_IDX_MOVE(fw, n) \
+do { \
+	typeof(fw) __fw = (fw); \
+	(__fw)->ldr_wr_idx = ((__fw)->ldr_wr_idx + (n)) & \
+				((__fw)->ldr_buf_len - 1); \
+} while (0)
+
+#define MVSW_LDR_WR_IDX_COMMIT(fw) \
+({ \
+	typeof(fw) __fw = (fw); \
+	mvsw_ldr_write((__fw), MVSW_LDR_BUF_WR_REG, \
+		       (__fw)->ldr_wr_idx); \
+})
+
+#define MVSW_LDR_WR_PTR(fw) \
+({ \
+	typeof(fw) __fw = (fw); \
+	((__fw)->ldr_ring_buf + (__fw)->ldr_wr_idx); \
+})
+
+#define MVSW_EVT_QNUM_MAX	4
+
+struct mvsw_pr_fw_evtq_regs {
 	u32 rd_idx;
 	u32 pad1;
 	u32 wr_idx;
@@ -98,7 +128,7 @@ struct prestera_fw_evtq_regs {
 	u32 len;
 };
 
-struct prestera_fw_regs {
+struct mvsw_pr_fw_regs {
 	u32 fw_ready;
 	u32 pad;
 	u32 cmd_offs;
@@ -114,315 +144,379 @@ struct prestera_fw_regs {
 	u32 fw_status;
 	u32 rx_status;
 
-	struct prestera_fw_evtq_regs evtq_list[PRESTERA_EVT_QNUM_MAX];
+	struct mvsw_pr_fw_evtq_regs evtq_list[MVSW_EVT_QNUM_MAX];
 };
 
-#define PRESTERA_FW_REG_OFFSET(f)	offsetof(struct prestera_fw_regs, f)
+#define MVSW_FW_REG_OFFSET(f)	offsetof(struct mvsw_pr_fw_regs, f)
 
-#define PRESTERA_FW_READY_MAGIC		0xcafebabe
+#define MVSW_FW_READY_MAGIC	0xcafebabe
 
 /* fw registers */
-#define PRESTERA_FW_READY_REG		PRESTERA_FW_REG_OFFSET(fw_ready)
+#define MVSW_FW_READY_REG		MVSW_FW_REG_OFFSET(fw_ready)
 
-#define PRESTERA_CMD_BUF_OFFS_REG	PRESTERA_FW_REG_OFFSET(cmd_offs)
-#define PRESTERA_CMD_BUF_LEN_REG	PRESTERA_FW_REG_OFFSET(cmd_len)
-#define PRESTERA_EVT_BUF_OFFS_REG	PRESTERA_FW_REG_OFFSET(evt_offs)
-#define PRESTERA_EVT_QNUM_REG		PRESTERA_FW_REG_OFFSET(evt_qnum)
+#define MVSW_CMD_BUF_OFFS_REG		MVSW_FW_REG_OFFSET(cmd_offs)
+#define MVSW_CMD_BUF_LEN_REG		MVSW_FW_REG_OFFSET(cmd_len)
+#define MVSW_EVT_BUF_OFFS_REG		MVSW_FW_REG_OFFSET(evt_offs)
+#define MVSW_EVT_QNUM_REG		MVSW_FW_REG_OFFSET(evt_qnum)
 
-#define PRESTERA_CMD_REQ_CTL_REG	PRESTERA_FW_REG_OFFSET(cmd_req_ctl)
-#define PRESTERA_CMD_REQ_LEN_REG	PRESTERA_FW_REG_OFFSET(cmd_req_len)
+#define MVSW_CMD_REQ_CTL_REG		MVSW_FW_REG_OFFSET(cmd_req_ctl)
+#define MVSW_CMD_REQ_LEN_REG		MVSW_FW_REG_OFFSET(cmd_req_len)
 
-#define PRESTERA_CMD_RCV_CTL_REG	PRESTERA_FW_REG_OFFSET(cmd_rcv_ctl)
-#define PRESTERA_CMD_RCV_LEN_REG	PRESTERA_FW_REG_OFFSET(cmd_rcv_len)
-#define PRESTERA_FW_STATUS_REG		PRESTERA_FW_REG_OFFSET(fw_status)
-#define PRESTERA_RX_STATUS_REG		PRESTERA_FW_REG_OFFSET(rx_status)
+#define MVSW_CMD_RCV_CTL_REG		MVSW_FW_REG_OFFSET(cmd_rcv_ctl)
+#define MVSW_CMD_RCV_LEN_REG		MVSW_FW_REG_OFFSET(cmd_rcv_len)
+#define MVSW_FW_STATUS_REG		MVSW_FW_REG_OFFSET(fw_status)
+#define MVSW_RX_STATUS_REG		MVSW_FW_REG_OFFSET(rx_status)
 
-/* PRESTERA_CMD_REQ_CTL_REG flags */
-#define PRESTERA_CMD_F_REQ_SENT		BIT(0)
-#define PRESTERA_CMD_F_REPL_RCVD	BIT(1)
+/* MVSW_CMD_REQ_CTL_REG flags */
+#define MVSW_CMD_F_REQ_SENT		BIT(0)
+#define MVSW_CMD_F_REPL_RCVD		BIT(1)
 
-/* PRESTERA_CMD_RCV_CTL_REG flags */
-#define PRESTERA_CMD_F_REPL_SENT	BIT(0)
+/* MVSW_CMD_RCV_CTL_REG flags */
+#define MVSW_CMD_F_REPL_SENT		BIT(0)
 
-#define PRESTERA_EVTQ_REG_OFFSET(q, f)			\
-	(PRESTERA_FW_REG_OFFSET(evtq_list) +		\
-	 (q) * sizeof(struct prestera_fw_evtq_regs) +	\
-	 offsetof(struct prestera_fw_evtq_regs, f))
+/* MVSW_FW_STATUS_REG flags */
+#define MVSW_STATUS_F_EVT_OFF		BIT(0)
 
-#define PRESTERA_EVTQ_RD_IDX_REG(q)	PRESTERA_EVTQ_REG_OFFSET(q, rd_idx)
-#define PRESTERA_EVTQ_WR_IDX_REG(q)	PRESTERA_EVTQ_REG_OFFSET(q, wr_idx)
-#define PRESTERA_EVTQ_OFFS_REG(q)	PRESTERA_EVTQ_REG_OFFSET(q, offs)
-#define PRESTERA_EVTQ_LEN_REG(q)	PRESTERA_EVTQ_REG_OFFSET(q, len)
+#define MVSW_EVTQ_REG_OFFSET(q, f)			\
+	(MVSW_FW_REG_OFFSET(evtq_list) +		\
+	 (q) * sizeof(struct mvsw_pr_fw_evtq_regs) +	\
+	 offsetof(struct mvsw_pr_fw_evtq_regs, f))
 
-#define PRESTERA_FW_REG_BASE(fw)	((fw)->dev.ctl_regs)
-#define PRESTERA_FW_REG_ADDR(fw, reg)	PRESTERA_FW_REG_BASE((fw)) + (reg)
+#define MVSW_EVTQ_RD_IDX_REG(q)		MVSW_EVTQ_REG_OFFSET(q, rd_idx)
+#define MVSW_EVTQ_WR_IDX_REG(q)		MVSW_EVTQ_REG_OFFSET(q, wr_idx)
+#define MVSW_EVTQ_OFFS_REG(q)		MVSW_EVTQ_REG_OFFSET(q, offs)
+#define MVSW_EVTQ_LEN_REG(q)		MVSW_EVTQ_REG_OFFSET(q, len)
 
-#define PRESTERA_FW_CMD_DEFAULT_WAIT_MS	30000
-#define PRESTERA_FW_READY_WAIT_MS	20000
+#define mvsw_fw_write(fw, reg, val)	writel(val, (fw)->hw_regs + (reg))
+#define mvsw_fw_read(fw, reg)		readl((fw)->hw_regs + (reg))
 
-struct prestera_fw_evtq {
+struct mvsw_pr_fw_evtq {
 	u8 __iomem *addr;
 	size_t len;
 };
 
-struct prestera_fw {
+struct mvsw_pr_fw {
 	struct workqueue_struct *wq;
 	struct prestera_device dev;
+	struct pci_dev *pci_dev;
+	u8 __iomem *mem_addr;
+
 	u8 __iomem *ldr_regs;
+	u8 __iomem *hw_regs;
+
 	u8 __iomem *ldr_ring_buf;
 	u32 ldr_buf_len;
 	u32 ldr_wr_idx;
-	struct mutex cmd_mtx; /* serialize access to dev->send_req */
+	bool active;
+
+	/* serialize access to dev->send_req */
+	struct mutex cmd_mtx;
 	size_t cmd_mbox_len;
 	u8 __iomem *cmd_mbox;
-	struct prestera_fw_evtq evt_queue[PRESTERA_EVT_QNUM_MAX];
+	struct mvsw_pr_fw_evtq evt_queue[MVSW_EVT_QNUM_MAX];
 	u8 evt_qnum;
 	struct work_struct evt_work;
 	u8 __iomem *evt_buf;
 	u8 *evt_msg;
 };
 
-static int prestera_fw_load(struct prestera_fw *fw);
-
-static void prestera_fw_write(struct prestera_fw *fw, u32 reg, u32 val)
-{
-	writel(val, PRESTERA_FW_REG_ADDR(fw, reg));
-}
+#define mvsw_fw_dev(fw)	((fw)->dev.dev)
+
+#define PRESTERA_DEVICE(id) PCI_VDEVICE(MARVELL, (id))
+
+static struct mvsw_pr_pci_match {
+	struct pci_driver driver;
+	const struct pci_device_id id;
+	bool registered;
+} mvsw_pci_devices[] = {
+	{
+		.driver = { .name = "AC3x B2B 98DX3255", },
+		.id = { PRESTERA_DEVICE(0xC804), 0 },
+	},
+	{
+		.driver = { .name = "AC3x B2B 98DX3265", },
+		.id = { PRESTERA_DEVICE(0xC80C), 0 },
+	},
+	{
+		.driver = { .name = "Aldrin2", },
+		.id = { PRESTERA_DEVICE(0xCC1E), 0 },
+	},
+	{{ }, }
+};
 
-static u32 prestera_fw_read(struct prestera_fw *fw, u32 reg)
-{
-	return readl(PRESTERA_FW_REG_ADDR(fw, reg));
-}
+static int mvsw_pr_fw_load(struct mvsw_pr_fw *fw);
 
-static u32 prestera_fw_evtq_len(struct prestera_fw *fw, u8 qid)
+static u32 mvsw_pr_fw_evtq_len(struct mvsw_pr_fw *fw, u8 qid)
 {
 	return fw->evt_queue[qid].len;
 }
 
-static u32 prestera_fw_evtq_avail(struct prestera_fw *fw, u8 qid)
+static u32 mvsw_pr_fw_evtq_avail(struct mvsw_pr_fw *fw, u8 qid)
 {
-	u32 wr_idx = prestera_fw_read(fw, PRESTERA_EVTQ_WR_IDX_REG(qid));
-	u32 rd_idx = prestera_fw_read(fw, PRESTERA_EVTQ_RD_IDX_REG(qid));
+	u32 wr_idx = mvsw_fw_read(fw, MVSW_EVTQ_WR_IDX_REG(qid));
+	u32 rd_idx = mvsw_fw_read(fw, MVSW_EVTQ_RD_IDX_REG(qid));
 
-	return CIRC_CNT(wr_idx, rd_idx, prestera_fw_evtq_len(fw, qid));
+	return CIRC_CNT(wr_idx, rd_idx, mvsw_pr_fw_evtq_len(fw, qid));
 }
 
-static void prestera_fw_evtq_rd_set(struct prestera_fw *fw,
-				    u8 qid, u32 idx)
+static void mvsw_pr_fw_evtq_rd_set(struct mvsw_pr_fw *fw,
+				   u8 qid, u32 idx)
 {
-	u32 rd_idx = idx & (prestera_fw_evtq_len(fw, qid) - 1);
+	u32 rd_idx = idx & (mvsw_pr_fw_evtq_len(fw, qid) - 1);
 
-	prestera_fw_write(fw, PRESTERA_EVTQ_RD_IDX_REG(qid), rd_idx);
+	mvsw_fw_write(fw, MVSW_EVTQ_RD_IDX_REG(qid), rd_idx);
 }
 
-static u8 __iomem *prestera_fw_evtq_buf(struct prestera_fw *fw, u8 qid)
+static u8 __iomem *mvsw_pr_fw_evtq_buf(struct mvsw_pr_fw *fw,
+				       u8 qid)
 {
 	return fw->evt_queue[qid].addr;
 }
 
-static u32 prestera_fw_evtq_read32(struct prestera_fw *fw, u8 qid)
+static u32 mvsw_pr_fw_evtq_read32(struct mvsw_pr_fw *fw, u8 qid)
 {
-	u32 rd_idx = prestera_fw_read(fw, PRESTERA_EVTQ_RD_IDX_REG(qid));
+	u32 rd_idx = mvsw_fw_read(fw, MVSW_EVTQ_RD_IDX_REG(qid));
 	u32 val;
 
-	val = readl(prestera_fw_evtq_buf(fw, qid) + rd_idx);
-	prestera_fw_evtq_rd_set(fw, qid, rd_idx + 4);
+	val = readl(mvsw_pr_fw_evtq_buf(fw, qid) + rd_idx);
+	mvsw_pr_fw_evtq_rd_set(fw, qid, rd_idx + 4);
 	return val;
 }
 
-static ssize_t prestera_fw_evtq_read_buf(struct prestera_fw *fw,
-					 u8 qid, void *buf, size_t len)
+static ssize_t mvsw_pr_fw_evtq_read_buf(struct mvsw_pr_fw *fw,
+					u8 qid, u8 *buf, size_t len)
 {
-	u32 idx = prestera_fw_read(fw, PRESTERA_EVTQ_RD_IDX_REG(qid));
-	u8 __iomem *evtq_addr = prestera_fw_evtq_buf(fw, qid);
-	u32 *buf32 = buf;
+	u32 idx = mvsw_fw_read(fw, MVSW_EVTQ_RD_IDX_REG(qid));
+	u8 __iomem *evtq_addr = mvsw_pr_fw_evtq_buf(fw, qid);
+	u32 *buf32 = (u32 *)buf;
 	int i;
 
 	for (i = 0; i < len / 4; buf32++, i++) {
 		*buf32 = readl_relaxed(evtq_addr + idx);
-		idx = (idx + 4) & (prestera_fw_evtq_len(fw, qid) - 1);
+		idx = (idx + 4) & (mvsw_pr_fw_evtq_len(fw, qid) - 1);
 	}
 
-	prestera_fw_evtq_rd_set(fw, qid, idx);
+	mvsw_pr_fw_evtq_rd_set(fw, qid, idx);
 
 	return i;
 }
 
-static u8 prestera_fw_evtq_pick(struct prestera_fw *fw)
+static u8 mvsw_pr_fw_evtq_pick(struct mvsw_pr_fw *fw)
 {
 	int qid;
 
 	for (qid = 0; qid < fw->evt_qnum; qid++) {
-		if (prestera_fw_evtq_avail(fw, qid) >= 4)
+		if (mvsw_pr_fw_evtq_avail(fw, qid) >= 4)
 			return qid;
 	}
 
-	return PRESTERA_EVT_QNUM_MAX;
+	return MVSW_EVT_QNUM_MAX;
 }
 
-static void prestera_fw_evt_work_fn(struct work_struct *work)
+static void mvsw_pr_fw_status_set(struct mvsw_pr_fw *fw, unsigned int val)
 {
-	struct prestera_fw *fw;
-	void *msg;
+	u32 status = mvsw_fw_read(fw, MVSW_FW_STATUS_REG);
+
+	status |= val;
+
+	mvsw_fw_write(fw, MVSW_FW_STATUS_REG, status);
+}
+
+static void mvsw_pr_fw_status_clear(struct mvsw_pr_fw *fw, u32 val)
+{
+	u32 status = mvsw_fw_read(fw, MVSW_FW_STATUS_REG);
+
+	status &= ~val;
+
+	mvsw_fw_write(fw, MVSW_FW_STATUS_REG, status);
+}
+
+static void mvsw_pr_fw_evt_work_fn(struct work_struct *work)
+{
+	struct mvsw_pr_fw *fw;
+	u8 *msg;
 	u8 qid;
 
-	fw = container_of(work, struct prestera_fw, evt_work);
+	fw = container_of(work, struct mvsw_pr_fw, evt_work);
 	msg = fw->evt_msg;
 
-	while ((qid = prestera_fw_evtq_pick(fw)) < PRESTERA_EVT_QNUM_MAX) {
+	mvsw_pr_fw_status_set(fw, MVSW_STATUS_F_EVT_OFF);
+
+	while ((qid = mvsw_pr_fw_evtq_pick(fw)) < MVSW_EVT_QNUM_MAX) {
 		u32 idx;
 		u32 len;
 
-		len = prestera_fw_evtq_read32(fw, qid);
-		idx = prestera_fw_read(fw, PRESTERA_EVTQ_RD_IDX_REG(qid));
+		len = mvsw_pr_fw_evtq_read32(fw, qid);
+		idx = mvsw_fw_read(fw, MVSW_EVTQ_RD_IDX_REG(qid));
 
-		WARN_ON(prestera_fw_evtq_avail(fw, qid) < len);
+		WARN_ON(mvsw_pr_fw_evtq_avail(fw, qid) < len);
 
-		if (WARN_ON(len > PRESTERA_MSG_MAX_SIZE)) {
-			prestera_fw_evtq_rd_set(fw, qid, idx + len);
+		if (WARN_ON(len > MVSW_MSG_MAX_SIZE)) {
+			mvsw_pr_fw_evtq_rd_set(fw, qid, idx + len);
 			continue;
 		}
 
-		prestera_fw_evtq_read_buf(fw, qid, msg, len);
+		mvsw_pr_fw_evtq_read_buf(fw, qid, msg, len);
 
 		if (fw->dev.recv_msg)
 			fw->dev.recv_msg(&fw->dev, msg, len);
 	}
+
+	mvsw_pr_fw_status_clear(fw, MVSW_STATUS_F_EVT_OFF);
 }
 
-static int prestera_fw_wait_reg32(struct prestera_fw *fw, u32 reg, u32 cmp,
-				  unsigned int waitms)
+static int mvsw_pr_fw_wait_reg32(struct mvsw_pr_fw *fw,
+				 u32 reg, u32 val, unsigned int wait)
 {
-	u8 __iomem *addr = PRESTERA_FW_REG_ADDR(fw, reg);
-	u32 val;
+	if (mvsw_wait_timeout(mvsw_fw_read(fw, reg) == val, wait))
+		return 0;
 
-	return readl_poll_timeout(addr, val, cmp == val,
-				  1 * USEC_PER_MSEC, waitms * USEC_PER_MSEC);
+	return -EBUSY;
 }
 
-static int prestera_fw_cmd_send(struct prestera_fw *fw,
-				void *in_msg, size_t in_size,
-				void *out_msg, size_t out_size,
-				unsigned int waitms)
+static void mvsw_pci_copy_to(u8 __iomem *dst, u8 *src, size_t len)
 {
-	u32 ret_size;
-	int err;
+	u32 __iomem *dst32 = (u32 __iomem *)dst;
+	u32 *src32 = (u32 *)src;
+	int i;
+
+	for (i = 0; i < (len / 4); dst32++, src32++, i++)
+		writel_relaxed(*src32, dst32);
+}
 
-	if (!waitms)
-		waitms = PRESTERA_FW_CMD_DEFAULT_WAIT_MS;
+static void mvsw_pci_copy_from(u8 *dst, u8 __iomem *src, size_t len)
+{
+	u32 *dst32 = (u32 *)dst;
+	u32 __iomem *src32 = (u32 __iomem *)src;
+	int i;
+
+	for (i = 0; i < (len / 4); dst32++, src32++, i++)
+		*dst32 = readl_relaxed(src32);
+}
+
+static int mvsw_pr_fw_cmd_send(struct mvsw_pr_fw *fw,
+			       u8 *in_msg, size_t in_size,
+			       u8 *out_msg, size_t out_size,
+			       unsigned int wait)
+{
+	u32 ret_size = 0;
+	int err = 0;
+
+	if (!wait)
+		wait = 30000;
 
 	if (ALIGN(in_size, 4) > fw->cmd_mbox_len)
 		return -EMSGSIZE;
 
 	/* wait for finish previous reply from FW */
-	err = prestera_fw_wait_reg32(fw, PRESTERA_CMD_RCV_CTL_REG, 0, 30);
+	err = mvsw_pr_fw_wait_reg32(fw, MVSW_CMD_RCV_CTL_REG, 0, 1000);
 	if (err) {
-		dev_err(fw->dev.dev, "finish reply from FW is timed out\n");
+		dev_err(mvsw_fw_dev(fw), "finish reply from FW is timed out\n");
 		return err;
 	}
 
-	prestera_fw_write(fw, PRESTERA_CMD_REQ_LEN_REG, in_size);
-	memcpy_toio(fw->cmd_mbox, in_msg, in_size);
+	mvsw_fw_write(fw, MVSW_CMD_REQ_LEN_REG, in_size);
+	mvsw_pci_copy_to(fw->cmd_mbox, in_msg, in_size);
 
-	prestera_fw_write(fw, PRESTERA_CMD_REQ_CTL_REG, PRESTERA_CMD_F_REQ_SENT);
+	mvsw_fw_write(fw, MVSW_CMD_REQ_CTL_REG, MVSW_CMD_F_REQ_SENT);
 
 	/* wait for reply from FW */
-	err = prestera_fw_wait_reg32(fw, PRESTERA_CMD_RCV_CTL_REG,
-				     PRESTERA_CMD_F_REPL_SENT, waitms);
+	err = mvsw_pr_fw_wait_reg32(fw, MVSW_CMD_RCV_CTL_REG, MVSW_CMD_F_REPL_SENT,
+				    wait);
 	if (err) {
-		dev_err(fw->dev.dev, "reply from FW is timed out\n");
+		dev_err(mvsw_fw_dev(fw), "reply from FW is timed out\n");
+		fw->active = false;
 		goto cmd_exit;
 	}
 
-	ret_size = prestera_fw_read(fw, PRESTERA_CMD_RCV_LEN_REG);
+	ret_size = mvsw_fw_read(fw, MVSW_CMD_RCV_LEN_REG);
 	if (ret_size > out_size) {
-		dev_err(fw->dev.dev, "ret_size (%u) > out_len(%zu)\n",
+		dev_err(mvsw_fw_dev(fw), "ret_size (%u) > out_len(%zu)\n",
 			ret_size, out_size);
 		err = -EMSGSIZE;
 		goto cmd_exit;
 	}
 
-	memcpy_fromio(out_msg, fw->cmd_mbox + in_size, ret_size);
+	mvsw_pci_copy_from(out_msg, fw->cmd_mbox + in_size, ret_size);
 
 cmd_exit:
-	prestera_fw_write(fw, PRESTERA_CMD_REQ_CTL_REG, PRESTERA_CMD_F_REPL_RCVD);
+	mvsw_fw_write(fw, MVSW_CMD_REQ_CTL_REG, MVSW_CMD_F_REPL_RCVD);
 	return err;
 }
 
-static int prestera_fw_send_req(struct prestera_device *dev,
-				void *in_msg, size_t in_size, void *out_msg,
-				size_t out_size, unsigned int waitms)
+static int mvsw_pr_fw_send_req(struct prestera_device *dev,
+			       u8 *in_msg, size_t in_size, u8 *out_msg,
+			       size_t out_size, unsigned int wait)
 {
-	struct prestera_fw *fw;
+	struct mvsw_pr_fw *fw;
 	ssize_t ret;
 
-	fw = container_of(dev, struct prestera_fw, dev);
+	fw = container_of(dev, struct mvsw_pr_fw, dev);
+
+	if (!fw->active)
+		return -1;
 
 	mutex_lock(&fw->cmd_mtx);
-	ret = prestera_fw_cmd_send(fw, in_msg, in_size, out_msg, out_size, waitms);
+	ret = mvsw_pr_fw_cmd_send(fw, in_msg, in_size, out_msg, out_size, wait);
 	mutex_unlock(&fw->cmd_mtx);
 
 	return ret;
 }
 
-static int prestera_fw_init(struct prestera_fw *fw)
+static int mvsw_pr_fw_init(struct mvsw_pr_fw *fw)
 {
 	u8 __iomem *base;
 	int err;
 	u8 qid;
 
-	fw->dev.send_req = prestera_fw_send_req;
-	fw->ldr_regs = fw->dev.ctl_regs;
-
-	err = prestera_fw_load(fw);
+	err = mvsw_pr_fw_load(fw);
 	if (err)
 		return err;
 
-	err = prestera_fw_wait_reg32(fw, PRESTERA_FW_READY_REG,
-				     PRESTERA_FW_READY_MAGIC,
-				     PRESTERA_FW_READY_WAIT_MS);
+	err = mvsw_pr_fw_wait_reg32(fw, MVSW_FW_READY_REG,
+				    MVSW_FW_READY_MAGIC, 20000);
 	if (err) {
-		dev_err(fw->dev.dev, "FW failed to start\n");
+		dev_err(mvsw_fw_dev(fw), "FW is failed to start\n");
 		return err;
 	}
 
-	base = fw->dev.ctl_regs;
+	base = fw->mem_addr;
 
-	fw->cmd_mbox = base + prestera_fw_read(fw, PRESTERA_CMD_BUF_OFFS_REG);
-	fw->cmd_mbox_len = prestera_fw_read(fw, PRESTERA_CMD_BUF_LEN_REG);
+	fw->cmd_mbox = base + mvsw_fw_read(fw, MVSW_CMD_BUF_OFFS_REG);
+	fw->cmd_mbox_len = mvsw_fw_read(fw, MVSW_CMD_BUF_LEN_REG);
 	mutex_init(&fw->cmd_mtx);
 
-	fw->evt_buf = base + prestera_fw_read(fw, PRESTERA_EVT_BUF_OFFS_REG);
-	fw->evt_qnum = prestera_fw_read(fw, PRESTERA_EVT_QNUM_REG);
-	fw->evt_msg = kmalloc(PRESTERA_MSG_MAX_SIZE, GFP_KERNEL);
+	fw->evt_buf = base + mvsw_fw_read(fw, MVSW_EVT_BUF_OFFS_REG);
+	fw->evt_qnum = mvsw_fw_read(fw, MVSW_EVT_QNUM_REG);
+	fw->evt_msg = kmalloc(MVSW_MSG_MAX_SIZE, GFP_KERNEL);
 	if (!fw->evt_msg)
 		return -ENOMEM;
 
 	for (qid = 0; qid < fw->evt_qnum; qid++) {
-		u32 offs = prestera_fw_read(fw, PRESTERA_EVTQ_OFFS_REG(qid));
-		struct prestera_fw_evtq *evtq = &fw->evt_queue[qid];
+		u32 offs = mvsw_fw_read(fw, MVSW_EVTQ_OFFS_REG(qid));
+		struct mvsw_pr_fw_evtq *evtq = &fw->evt_queue[qid];
 
-		evtq->len = prestera_fw_read(fw, PRESTERA_EVTQ_LEN_REG(qid));
+		evtq->len = mvsw_fw_read(fw, MVSW_EVTQ_LEN_REG(qid));
 		evtq->addr = fw->evt_buf + offs;
 	}
 
 	return 0;
 }
 
-static void prestera_fw_uninit(struct prestera_fw *fw)
+static void mvsw_pr_fw_uninit(struct mvsw_pr_fw *fw)
 {
 	kfree(fw->evt_msg);
 }
 
-static irqreturn_t prestera_pci_irq_handler(int irq, void *dev_id)
+static irqreturn_t mvsw_pci_irq_handler(int irq, void *dev_id)
 {
-	struct prestera_fw *fw = dev_id;
+	struct mvsw_pr_fw *fw = dev_id;
 
-	if (prestera_fw_read(fw, PRESTERA_RX_STATUS_REG)) {
-		prestera_fw_write(fw, PRESTERA_RX_STATUS_REG, 0);
-
-		if (fw->dev.recv_pkt)
+	if (mvsw_fw_read(fw, MVSW_RX_STATUS_REG)) {
+		if (fw->dev.recv_pkt) {
+			mvsw_fw_write(fw, MVSW_RX_STATUS_REG, 0);
 			fw->dev.recv_pkt(&fw->dev);
+		}
 	}
 
 	queue_work(fw->wq, &fw->evt_work);
@@ -430,285 +524,274 @@ static irqreturn_t prestera_pci_irq_handler(int irq, void *dev_id)
 	return IRQ_HANDLED;
 }
 
-static void prestera_ldr_write(struct prestera_fw *fw, u32 reg, u32 val)
-{
-	writel(val, PRESTERA_LDR_REG_ADDR(fw, reg));
-}
-
-static u32 prestera_ldr_read(struct prestera_fw *fw, u32 reg)
-{
-	return readl(PRESTERA_LDR_REG_ADDR(fw, reg));
-}
-
-static int prestera_ldr_wait_reg32(struct prestera_fw *fw,
-				   u32 reg, u32 cmp, unsigned int waitms)
-{
-	u8 __iomem *addr = PRESTERA_LDR_REG_ADDR(fw, reg);
-	u32 val;
-
-	return readl_poll_timeout(addr, val, cmp == val,
-				  10 * USEC_PER_MSEC, waitms * USEC_PER_MSEC);
-}
-
-static u32 prestera_ldr_wait_buf(struct prestera_fw *fw, size_t len)
-{
-	u8 __iomem *addr = PRESTERA_LDR_REG_ADDR(fw, PRESTERA_LDR_BUF_RD_REG);
-	u32 buf_len = fw->ldr_buf_len;
-	u32 wr_idx = fw->ldr_wr_idx;
-	u32 rd_idx;
-
-	return readl_poll_timeout(addr, rd_idx,
-				 CIRC_SPACE(wr_idx, rd_idx, buf_len) >= len,
-				 1 * USEC_PER_MSEC, 100 * USEC_PER_MSEC);
-}
-
-static int prestera_ldr_wait_dl_finish(struct prestera_fw *fw)
+static int mvsw_pr_ldr_wait_reg32(struct mvsw_pr_fw *fw,
+				  u32 reg, u32 val, unsigned int wait)
 {
-	u8 __iomem *addr = PRESTERA_LDR_REG_ADDR(fw, PRESTERA_LDR_STATUS_REG);
-	unsigned long mask = ~(PRESTERA_LDR_STATUS_IMG_DL);
-	u32 val;
-	int err;
-
-	err = readl_poll_timeout(addr, val, val & mask, 10 * USEC_PER_MSEC,
-				 PRESTERA_FW_DL_TIMEOUT_MS * USEC_PER_MSEC);
-	if (err) {
-		dev_err(fw->dev.dev, "Timeout to load FW img [state=%d]",
-			prestera_ldr_read(fw, PRESTERA_LDR_STATUS_REG));
-		return err;
-	}
-
-	return 0;
-}
+	if (mvsw_wait_timeout(mvsw_ldr_read(fw, reg) == val, wait))
+		return 0;
 
-static void prestera_ldr_wr_idx_move(struct prestera_fw *fw, unsigned int n)
-{
-	fw->ldr_wr_idx = (fw->ldr_wr_idx + (n)) & (fw->ldr_buf_len - 1);
+	return -EBUSY;
 }
 
-static void prestera_ldr_wr_idx_commit(struct prestera_fw *fw)
+static u32 mvsw_pr_ldr_buf_avail(struct mvsw_pr_fw *fw)
 {
-	prestera_ldr_write(fw, PRESTERA_LDR_BUF_WR_REG, fw->ldr_wr_idx);
-}
+	u32 rd_idx = mvsw_ldr_read(fw, MVSW_LDR_BUF_RD_REG);
 
-static u8 __iomem *prestera_ldr_wr_ptr(struct prestera_fw *fw)
-{
-	return fw->ldr_ring_buf + fw->ldr_wr_idx;
+	return CIRC_SPACE(fw->ldr_wr_idx, rd_idx, fw->ldr_buf_len);
 }
 
-static int prestera_ldr_send(struct prestera_fw *fw, const u8 *buf, size_t len)
+static int mvsw_pr_ldr_send_buf(struct mvsw_pr_fw *fw, const u8 *buf,
+				size_t len)
 {
-	int err;
 	int i;
 
-	err = prestera_ldr_wait_buf(fw, len);
-	if (err) {
-		dev_err(fw->dev.dev, "failed wait for sending firmware\n");
-		return err;
+	if (!mvsw_wait_timeout(mvsw_pr_ldr_buf_avail(fw) >= len, 100)) {
+		dev_err(mvsw_fw_dev(fw), "failed wait for sending firmware\n");
+		return -EBUSY;
 	}
 
 	for (i = 0; i < len; i += 4) {
-		writel_relaxed(*(u32 *)(buf + i), prestera_ldr_wr_ptr(fw));
-		prestera_ldr_wr_idx_move(fw, 4);
+		writel_relaxed(*(u32 *)(buf + i), MVSW_LDR_WR_PTR(fw));
+		MVSW_LDR_WR_IDX_MOVE(fw, 4);
 	}
 
-	prestera_ldr_wr_idx_commit(fw);
+	MVSW_LDR_WR_IDX_COMMIT(fw);
 	return 0;
 }
 
-static int prestera_ldr_fw_send(struct prestera_fw *fw,
-				const char *img, u32 fw_size)
+static int mvsw_pr_ldr_send(struct mvsw_pr_fw *fw,
+			    const char *img, u32 fw_size)
 {
+	unsigned long mask;
 	u32 status;
 	u32 pos;
 	int err;
 
-	err = prestera_ldr_wait_reg32(fw, PRESTERA_LDR_STATUS_REG,
-				      PRESTERA_LDR_STATUS_IMG_DL,
-				      5 * MSEC_PER_SEC);
-	if (err) {
-		dev_err(fw->dev.dev, "Loader is not ready to load image\n");
-		return err;
+	if (mvsw_pr_ldr_wait_reg32(fw, MVSW_LDR_STATUS_REG,
+				   MVSW_LDR_STATUS_IMG_DL, 1000)) {
+		dev_err(mvsw_fw_dev(fw), "Loader is not ready to load image\n");
+		return -EBUSY;
 	}
 
-	for (pos = 0; pos < fw_size; pos += PRESTERA_FW_BLK_SZ) {
-		if (pos + PRESTERA_FW_BLK_SZ > fw_size)
+	for (pos = 0; pos < fw_size; pos += MVSW_FW_BLK_SZ) {
+		if (pos + MVSW_FW_BLK_SZ > fw_size)
 			break;
 
-		err = prestera_ldr_send(fw, img + pos, PRESTERA_FW_BLK_SZ);
-		if (err)
+		err = mvsw_pr_ldr_send_buf(fw, img + pos, MVSW_FW_BLK_SZ);
+		if (err) {
+			if (mvsw_fw_read(fw, MVSW_LDR_STATUS_REG) ==
+					 MVSW_LDR_STATUS_NOMEM) {
+				dev_err(mvsw_fw_dev(fw),
+					"Fw image is too big or invalid\n");
+				return -EINVAL;
+			}
 			return err;
+		}
 	}
 
 	if (pos < fw_size) {
-		err = prestera_ldr_send(fw, img + pos, fw_size - pos);
+		err = mvsw_pr_ldr_send_buf(fw, img + pos, fw_size - pos);
 		if (err)
 			return err;
 	}
 
-	err = prestera_ldr_wait_dl_finish(fw);
-	if (err)
-		return err;
+	/* Waiting for status IMG_DOWNLOADING to change to something else */
+	mask = ~(MVSW_LDR_STATUS_IMG_DL);
 
-	status = prestera_ldr_read(fw, PRESTERA_LDR_STATUS_REG);
+	if (!mvsw_wait_timeout(mvsw_ldr_read(fw, MVSW_LDR_STATUS_REG) & mask,
+			       MVSW_FW_DL_TIMEOUT)) {
+		dev_err(mvsw_fw_dev(fw), "Timeout to load FW img [state=%d]",
+			mvsw_ldr_read(fw, MVSW_LDR_STATUS_REG));
+		return -ETIMEDOUT;
+	}
 
-	switch (status) {
-	case PRESTERA_LDR_STATUS_INVALID_IMG:
-		dev_err(fw->dev.dev, "FW img has bad CRC\n");
-		return -EINVAL;
-	case PRESTERA_LDR_STATUS_NOMEM:
-		dev_err(fw->dev.dev, "Loader has no enough mem\n");
-		return -ENOMEM;
+	status = mvsw_ldr_read(fw, MVSW_LDR_STATUS_REG);
+	if (status != MVSW_LDR_STATUS_START_FW) {
+		switch (status) {
+		case MVSW_LDR_STATUS_INVALID_IMG:
+			dev_err(mvsw_fw_dev(fw), "FW img has bad crc\n");
+			return -EINVAL;
+		case MVSW_LDR_STATUS_NOMEM:
+			dev_err(mvsw_fw_dev(fw), "Loader has no enough mem\n");
+			return -ENOMEM;
+		default:
+			break;
+		}
 	}
 
 	return 0;
 }
 
-static void prestera_fw_rev_parse(const struct prestera_fw_header *hdr,
-				  struct prestera_fw_rev *rev)
+static bool mvsw_pr_ldr_is_ready(struct mvsw_pr_fw *fw)
+{
+	return mvsw_ldr_read(fw, MVSW_LDR_READY_REG) == MVSW_LDR_READY_MAGIC;
+}
+
+static void mvsw_pr_fw_rev_parse(const struct mvsw_pr_fw_header *hdr,
+				 struct prestera_fw_rev *rev)
 {
 	u32 version = be32_to_cpu(hdr->version_value);
 
-	rev->maj = PRESTERA_FW_VER_MAJ(version);
-	rev->min = PRESTERA_FW_VER_MIN(version);
-	rev->sub = PRESTERA_FW_VER_PATCH(version);
+	rev->maj = FW_VER_MAJ(version);
+	rev->min = FW_VER_MIN(version);
+	rev->sub = FW_VER_PATCH(version);
 }
 
-static int prestera_fw_rev_check(struct prestera_fw *fw)
+static int mvsw_pr_fw_rev_check(struct mvsw_pr_fw *fw)
 {
 	struct prestera_fw_rev *rev = &fw->dev.fw_rev;
-	u16 maj_supp = PRESTERA_SUPP_FW_MAJ_VER;
-	u16 min_supp = PRESTERA_SUPP_FW_MIN_VER;
 
-	if (rev->maj == maj_supp && rev->min >= min_supp)
+	if (rev->maj == MVSW_SUPP_FW_MAJ_VER &&
+	    rev->min == MVSW_SUPP_FW_MIN_VER) {
 		return 0;
-
-	dev_err(fw->dev.dev, "Driver supports FW version only '%u.%u.x'",
-		PRESTERA_SUPP_FW_MAJ_VER, PRESTERA_SUPP_FW_MIN_VER);
+	}
 
 	return -EINVAL;
 }
 
-static int prestera_fw_hdr_parse(struct prestera_fw *fw,
-				 const struct firmware *img)
+static int mvsw_pr_fw_hdr_parse(struct mvsw_pr_fw *fw,
+				const struct firmware *img)
 {
-	struct prestera_fw_header *hdr = (struct prestera_fw_header *)img->data;
+	struct mvsw_pr_fw_header *hdr = (struct mvsw_pr_fw_header *)img->data;
 	struct prestera_fw_rev *rev = &fw->dev.fw_rev;
 	u32 magic;
 
 	magic = be32_to_cpu(hdr->magic_number);
-	if (magic != PRESTERA_FW_HDR_MAGIC) {
-		dev_err(fw->dev.dev, "FW img hdr magic is invalid");
+	if (magic != MVSW_FW_HDR_MAGIC) {
+		dev_err(mvsw_fw_dev(fw), "FW img type is invalid");
 		return -EINVAL;
 	}
 
-	prestera_fw_rev_parse(hdr, rev);
+	mvsw_pr_fw_rev_parse(hdr, rev);
 
-	dev_info(fw->dev.dev, "FW version '%u.%u.%u'\n",
+	dev_info(mvsw_fw_dev(fw), "FW version '%u.%u.%u'\n",
 		 rev->maj, rev->min, rev->sub);
+	dev_info(mvsw_fw_dev(fw), "Driver version '%u.%u.%u'\n",
+		 MVSW_SUPP_FW_MAJ_VER, MVSW_SUPP_FW_MIN_VER,
+		 MVSW_SUPP_FW_PATCH_VER);
 
-	return prestera_fw_rev_check(fw);
+	if (mvsw_pr_fw_rev_check(fw)) {
+		dev_err(mvsw_fw_dev(fw),
+			"Driver is incomatible with FW: version mismatch");
+		return -EINVAL;
+	}
+
+	return 0;
 }
 
-static int prestera_fw_load(struct prestera_fw *fw)
+static int mvsw_pr_fw_load(struct mvsw_pr_fw *fw)
 {
-	size_t hlen = sizeof(struct prestera_fw_header);
+	size_t hlen = sizeof(struct mvsw_pr_fw_header);
 	const struct firmware *f;
-	char fw_path[128];
+	bool has_ldr;
 	int err;
 
-	err = prestera_ldr_wait_reg32(fw, PRESTERA_LDR_READY_REG,
-				      PRESTERA_LDR_READY_MAGIC,
-				      5 * MSEC_PER_SEC);
-	if (err) {
-		dev_err(fw->dev.dev, "waiting for FW loader is timed out");
-		return err;
+	has_ldr = mvsw_wait_timeout(mvsw_pr_ldr_is_ready(fw), 1000);
+	if (!has_ldr) {
+		dev_err(mvsw_fw_dev(fw), "waiting for FW loader is timed out");
+		return -ETIMEDOUT;
 	}
 
 	fw->ldr_ring_buf = fw->ldr_regs +
-		prestera_ldr_read(fw, PRESTERA_LDR_BUF_OFFS_REG);
+		mvsw_ldr_read(fw, MVSW_LDR_BUF_OFFS_REG);
 
 	fw->ldr_buf_len =
-		prestera_ldr_read(fw, PRESTERA_LDR_BUF_SIZE_REG);
+		mvsw_ldr_read(fw, MVSW_LDR_BUF_SIZE_REG);
 
 	fw->ldr_wr_idx = 0;
 
-	snprintf(fw_path, sizeof(fw_path), PRESTERA_FW_PATH_FMT,
-		 PRESTERA_SUPP_FW_MAJ_VER, PRESTERA_SUPP_FW_MIN_VER);
-
-	err = request_firmware_direct(&f, fw_path, fw->dev.dev);
+	err = request_firmware_direct(&f, MVSW_FW_FILENAME, &fw->pci_dev->dev);
 	if (err) {
-		dev_err(fw->dev.dev, "failed to request firmware file\n");
+		dev_err(mvsw_fw_dev(fw), "failed to request firmware file\n");
 		return err;
 	}
 
-	err = prestera_fw_hdr_parse(fw, f);
+	if (!IS_ALIGNED(f->size, 4)) {
+		dev_err(mvsw_fw_dev(fw), "FW image file is not aligned");
+		release_firmware(f);
+		return -EINVAL;
+	}
+
+	err = mvsw_pr_fw_hdr_parse(fw, f);
 	if (err) {
-		dev_err(fw->dev.dev, "FW image header is invalid\n");
-		goto out_release;
+		dev_err(mvsw_fw_dev(fw), "FW image is invalid\n");
+		release_firmware(f);
+		return err;
 	}
 
-	prestera_ldr_write(fw, PRESTERA_LDR_IMG_SIZE_REG, f->size - hlen);
-	prestera_ldr_write(fw, PRESTERA_LDR_CTL_REG, PRESTERA_LDR_CTL_DL_START);
+	mvsw_ldr_write(fw, MVSW_LDR_IMG_SIZE_REG, f->size - hlen);
+	mvsw_ldr_write(fw, MVSW_LDR_CTL_REG, MVSW_LDR_CTL_DL_START);
 
-	dev_info(fw->dev.dev, "Loading %s ...", fw_path);
+	dev_info(mvsw_fw_dev(fw), "Loading prestera FW image ...");
 
-	err = prestera_ldr_fw_send(fw, f->data + hlen, f->size - hlen);
+	err = mvsw_pr_ldr_send(fw, f->data + hlen, f->size - hlen);
 
-out_release:
 	release_firmware(f);
 	return err;
 }
 
-static int prestera_pci_probe(struct pci_dev *pdev,
-			      const struct pci_device_id *id)
+static int mvsw_pr_pci_probe(struct pci_dev *pdev,
+			     const struct pci_device_id *id)
 {
 	const char *driver_name = pdev->driver->name;
-	struct prestera_fw *fw;
+	u8 __iomem *mem_addr, *pp_addr;
+	struct mvsw_pr_fw *fw;
 	int err;
 
-	err = pcim_enable_device(pdev);
-	if (err)
-		return err;
-
-	err = pcim_iomap_regions(pdev, BIT(PRESTERA_PCI_BAR_FW) |
-				 BIT(PRESTERA_PCI_BAR_PP),
-				 pci_name(pdev));
-	if (err)
-		return err;
+	err = pci_enable_device(pdev);
+	if (err) {
+		dev_err(&pdev->dev, "pci_enable_device failed\n");
+		goto err_pci_enable_device;
+	}
 
-	err = dma_set_mask_and_coherent(&pdev->dev, DMA_BIT_MASK(30));
+	err = pci_request_regions(pdev, driver_name);
 	if (err) {
+		dev_err(&pdev->dev, "pci_request_regions failed\n");
+		goto err_pci_request_regions;
+	}
+
+	if (dma_set_mask_and_coherent(&pdev->dev, DMA_BIT_MASK(30))) {
 		dev_err(&pdev->dev, "fail to set DMA mask\n");
 		goto err_dma_mask;
 	}
 
+	mem_addr = pci_ioremap_bar(pdev, 2);
+	if (!mem_addr) {
+		dev_err(&pdev->dev, "pci mem ioremap failed\n");
+		err = -EIO;
+		goto err_mem_ioremap;
+	}
+
+	pp_addr = ioremap(pci_resource_start(pdev, 4),
+			  pci_resource_len(pdev, 4));
+	if (!pp_addr) {
+		dev_err(&pdev->dev, "pp regs ioremap failed\n");
+		err = -EIO;
+		goto err_pp_ioremap;
+	}
+
 	pci_set_master(pdev);
 
-	fw = devm_kzalloc(&pdev->dev, sizeof(*fw), GFP_KERNEL);
+	fw = kzalloc(sizeof(*fw), GFP_KERNEL);
 	if (!fw) {
 		err = -ENOMEM;
 		goto err_pci_dev_alloc;
 	}
 
-	fw->dev.ctl_regs = pcim_iomap_table(pdev)[PRESTERA_PCI_BAR_FW];
-	fw->dev.pp_regs = pcim_iomap_table(pdev)[PRESTERA_PCI_BAR_PP];
+	fw->pci_dev = pdev;
 	fw->dev.dev = &pdev->dev;
-
-	pci_set_drvdata(pdev, fw);
-
-	err = prestera_fw_init(fw);
-	if (err)
-		goto err_prestera_fw_init;
-
-	dev_info(fw->dev.dev, "Prestera FW is ready\n");
-
-	fw->wq = alloc_workqueue("prestera_fw_wq", WQ_HIGHPRI, 1);
-	if (!fw->wq) {
-		err = -ENOMEM;
+	fw->dev.send_req = mvsw_pr_fw_send_req;
+	fw->dev.pp_regs = pp_addr;
+	fw->mem_addr = mem_addr;
+	fw->ldr_regs = mem_addr;
+	fw->hw_regs = mem_addr;
+	fw->active = true;
+
+	fw->wq = alloc_workqueue("mvsw_fw_wq", WQ_HIGHPRI, 1);
+	if (!fw->wq)
 		goto err_wq_alloc;
-	}
 
-	INIT_WORK(&fw->evt_work, prestera_fw_evt_work_fn);
+	INIT_WORK(&fw->evt_work, mvsw_pr_fw_evt_work_fn);
 
 	err = pci_alloc_irq_vectors(pdev, 1, 1, PCI_IRQ_MSI);
 	if (err < 0) {
@@ -716,57 +799,119 @@ static int prestera_pci_probe(struct pci_dev *pdev,
 		goto err_irq_alloc;
 	}
 
-	err = request_irq(pci_irq_vector(pdev, 0), prestera_pci_irq_handler,
+	err = request_irq(pci_irq_vector(pdev, 0), mvsw_pci_irq_handler,
 			  0, driver_name, fw);
 	if (err) {
 		dev_err(&pdev->dev, "fail to request IRQ\n");
 		goto err_request_irq;
 	}
 
+	pci_set_drvdata(pdev, fw);
+
+	err = mvsw_pr_fw_init(fw);
+	if (err)
+		goto err_mvsw_fw_init;
+
+	dev_info(mvsw_fw_dev(fw), "Prestera Switch FW is ready\n");
+
 	err = prestera_device_register(&fw->dev);
 	if (err)
-		goto err_prestera_dev_register;
+		goto err_mvsw_dev_register;
 
 	return 0;
 
-err_prestera_dev_register:
+err_mvsw_dev_register:
+	mvsw_pr_fw_uninit(fw);
+err_mvsw_fw_init:
 	free_irq(pci_irq_vector(pdev, 0), fw);
 err_request_irq:
 	pci_free_irq_vectors(pdev);
 err_irq_alloc:
 	destroy_workqueue(fw->wq);
 err_wq_alloc:
-	prestera_fw_uninit(fw);
-err_prestera_fw_init:
+	kfree(fw);
 err_pci_dev_alloc:
+	iounmap(pp_addr);
+err_pp_ioremap:
+	iounmap(mem_addr);
+err_mem_ioremap:
 err_dma_mask:
+	pci_release_regions(pdev);
+err_pci_request_regions:
+	pci_disable_device(pdev);
+err_pci_enable_device:
 	return err;
 }
 
-static void prestera_pci_remove(struct pci_dev *pdev)
+static void mvsw_pr_pci_remove(struct pci_dev *pdev)
 {
-	struct prestera_fw *fw = pci_get_drvdata(pdev);
+	struct mvsw_pr_fw *fw = pci_get_drvdata(pdev);
 
-	prestera_device_unregister(&fw->dev);
 	free_irq(pci_irq_vector(pdev, 0), fw);
 	pci_free_irq_vectors(pdev);
+	prestera_device_unregister(&fw->dev);
+	flush_workqueue(fw->wq);
 	destroy_workqueue(fw->wq);
-	prestera_fw_uninit(fw);
+	mvsw_pr_fw_uninit(fw);
+	iounmap(fw->dev.pp_regs);
+	iounmap(fw->mem_addr);
+	pci_release_regions(pdev);
+	pci_disable_device(pdev);
+	kfree(fw);
 }
 
-static const struct pci_device_id prestera_pci_devices[] = {
-	{ PCI_DEVICE(PCI_VENDOR_ID_MARVELL, 0xC804) },
-	{ }
-};
-MODULE_DEVICE_TABLE(pci, prestera_pci_devices);
+static int __init mvsw_pr_pci_init(void)
+{
+	struct mvsw_pr_pci_match *match;
+	int err = 0;
 
-static struct pci_driver prestera_pci_driver = {
-	.name     = "Prestera DX",
-	.id_table = prestera_pci_devices,
-	.probe    = prestera_pci_probe,
-	.remove   = prestera_pci_remove,
-};
-module_pci_driver(prestera_pci_driver);
+	for (match = mvsw_pci_devices; match->driver.name; match++) {
+		match->driver.probe = mvsw_pr_pci_probe;
+		match->driver.remove = mvsw_pr_pci_remove;
+		match->driver.id_table = &match->id;
+
+		err = pci_register_driver(&match->driver);
+		if (err) {
+			pr_err("prestera_pci: failed to register %s\n",
+			       match->driver.name);
+			break;
+		}
+
+		match->registered = true;
+	}
+
+	if (err) {
+		for (match = mvsw_pci_devices; match->driver.name; match++) {
+			if (!match->registered)
+				break;
+
+			pci_unregister_driver(&match->driver);
+		}
+
+		return err;
+	}
+
+	pr_info("prestera_pci: Registered Marvell Prestera PCI driver\n");
+	return 0;
+}
+
+static void __exit mvsw_pr_pci_exit(void)
+{
+	struct mvsw_pr_pci_match *match;
+
+	for (match = mvsw_pci_devices; match->driver.name; match++) {
+		if (!match->registered)
+			break;
+
+		pci_unregister_driver(&match->driver);
+	}
+
+	pr_info("prestera_pci: Unregistered Marvell Prestera PCI driver\n");
+}
+
+module_init(mvsw_pr_pci_init);
+module_exit(mvsw_pr_pci_exit);
 
-MODULE_LICENSE("Dual BSD/GPL");
+MODULE_AUTHOR("Marvell Semi.");
+MODULE_LICENSE("GPL");
 MODULE_DESCRIPTION("Marvell Prestera switch PCI interface");
diff --git a/drivers/net/ethernet/marvell/prestera/prestera_router.c b/drivers/net/ethernet/marvell/prestera/prestera_router.c
new file mode 100644
index 000000000..141ac01b9
--- /dev/null
+++ b/drivers/net/ethernet/marvell/prestera/prestera_router.c
@@ -0,0 +1,3069 @@
+// SPDX-License-Identifier: BSD-3-Clause OR GPL-2.0
+/*
+ * Copyright (c) 2019-2020 Marvell International Ltd. All rights reserved.
+ *
+ */
+
+#include <linux/kernel.h>
+#include <linux/types.h>
+#include <linux/notifier.h>
+#include <linux/sort.h>
+#include <linux/inetdevice.h>
+#include <linux/netdevice.h>
+#include <linux/if_bridge.h>
+#include <linux/rhashtable.h>
+#include <net/netevent.h>
+#include <net/neighbour.h>
+#include <net/addrconf.h>
+#include <net/fib_notifier.h>
+#include <net/switchdev.h>
+#include <net/arp.h>
+#include <net/nexthop.h>
+
+#include "prestera.h"
+#include "prestera_hw.h"
+#include "prestera_log.h"
+
+#define MVSW_PR_IMPLICITY_RESOLVE_DEAD_NEIGH
+#define MVSW_PR_NH_PROBE_INTERVAL 5000 /* ms */
+#define MVSW_PR_NH_ACTIVE_JIFFER_FILTER 3000 /* ms */
+#define MVSW_PR_NHGR_UNUSED (0)
+#define MVSW_PR_NHGR_DROP (0xFFFFFFFF)
+
+static const char mvsw_driver_name[] = "mrvl_switchdev";
+
+struct mvsw_pr_rif {
+	struct mvsw_pr_iface iface;
+	struct net_device *dev;
+	struct list_head router_node;
+	unsigned char addr[ETH_ALEN];
+	unsigned int mtu;
+	bool is_active;
+	u16 rif_id;
+	struct mvsw_pr_vr *vr;
+	struct mvsw_pr_switch *sw;
+	unsigned int ref_cnt;
+};
+
+struct mvsw_pr_rif_params {
+	struct net_device *dev;
+	u16 vid;
+};
+
+struct mvsw_pr_fib_node {
+	struct rhash_head ht_node; /* node of mvsw_pr_vr */
+	struct mvsw_pr_fib_key key;
+	struct mvsw_pr_fib_info info; /* action related info */
+};
+
+struct mvsw_pr_vr {
+	u16 hw_vr_id;			/* virtual router ID */
+	u32 tb_id;			/* key (kernel fib table id) */
+	struct list_head router_node;
+	unsigned int ref_cnt;
+};
+
+struct mvsw_pr_nexthop_group_key {
+	struct mvsw_pr_nh_neigh_key neigh[MVSW_PR_NHGR_SIZE_MAX];
+};
+
+struct mvsw_pr_nexthop_group {
+	struct mvsw_pr_nexthop_group_key key;
+	/* Store intermediate object here.
+	 * This prevent overhead kzalloc call.
+	 */
+	/* nh_neigh is used only to notify nexthop_group */
+	struct mvsw_pr_nh_neigh_head {
+		struct mvsw_pr_nexthop_group *this;
+		struct list_head head;
+		/* ptr to neigh is not necessary.
+		 * It used to prevent lookup of nh_neigh by key (n) on destroy
+		 */
+		struct mvsw_pr_nh_neigh *neigh;
+	} nh_neigh_head[MVSW_PR_NHGR_SIZE_MAX];
+	u32 grp_id; /* hw */
+	unsigned long hw_last_connected; /* jiffies */
+	struct rhash_head ht_node; /* node of mvsw_pr_vr */
+	unsigned int ref_cnt;
+};
+
+enum mvsw_pr_mp_hash_policy {
+	MVSW_MP_L3_HASH_POLICY,
+	MVSW_MP_L4_HASH_POLICY,
+	MVSW_MP_HASH_POLICY_MAX,
+};
+
+struct mvsw_pr_kern_neigh_cache {
+	struct mvsw_pr_nh_neigh_key key;
+	bool offloaded;
+	struct rhash_head ht_node;
+	struct list_head kern_fib_cache_list;
+	bool lpm_added; /* Indicate if neigh is reachable by connected route */
+	bool in_kernel; /* Valid in kernel */
+};
+
+/* Used to track offloaded fib entries */
+struct mvsw_pr_kern_fib_cache {
+	struct mvsw_pr_fib_key key;
+	struct fib_info *fi;
+	struct rhash_head ht_node; /* node of mvsw_pr_router */
+	struct mvsw_pr_kern_neigh_cache_head {
+		struct mvsw_pr_kern_fib_cache *this;
+		struct list_head head;
+		struct mvsw_pr_kern_neigh_cache *n_cache;
+	} kern_neigh_cache_head[MVSW_PR_NHGR_SIZE_MAX];
+};
+
+static const struct rhashtable_params __mvsw_pr_kern_neigh_cache_ht_params = {
+	.key_offset  = offsetof(struct mvsw_pr_kern_neigh_cache, key),
+	.head_offset = offsetof(struct mvsw_pr_kern_neigh_cache, ht_node),
+	.key_len     = sizeof(struct mvsw_pr_nh_neigh_key),
+	.automatic_shrinking = true,
+};
+
+static const struct rhashtable_params __mvsw_pr_kern_fib_cache_ht_params = {
+	.key_offset  = offsetof(struct mvsw_pr_kern_fib_cache, key),
+	.head_offset = offsetof(struct mvsw_pr_kern_fib_cache, ht_node),
+	.key_len     = sizeof(struct mvsw_pr_fib_key),
+	.automatic_shrinking = true,
+};
+
+static const struct rhashtable_params __mvsw_pr_fib_ht_params = {
+	.key_offset  = offsetof(struct mvsw_pr_fib_node, key),
+	.head_offset = offsetof(struct mvsw_pr_fib_node, ht_node),
+	.key_len     = sizeof(struct mvsw_pr_fib_key),
+	.automatic_shrinking = true,
+};
+
+static const struct rhashtable_params __mvsw_pr_nh_neigh_ht_params = {
+	.key_offset  = offsetof(struct mvsw_pr_nh_neigh, key),
+	.key_len     = sizeof(struct mvsw_pr_nh_neigh_key),
+	.head_offset = offsetof(struct mvsw_pr_nh_neigh, ht_node),
+};
+
+static const struct rhashtable_params __mvsw_pr_nexthop_group_ht_params = {
+	.key_offset  = offsetof(struct mvsw_pr_nexthop_group, key),
+	.key_len     = sizeof(struct mvsw_pr_nexthop_group_key),
+	.head_offset = offsetof(struct mvsw_pr_nexthop_group, ht_node),
+};
+
+static struct workqueue_struct *mvsw_r_wq;
+static struct workqueue_struct *mvsw_r_owq;
+
+static DEFINE_MUTEX(mvsw_owq_mutex_wip); /* owq function is in progress */
+static bool mvsw_owq_flushing;
+static void mvsw_owq_lock(void)
+{
+	while (true) {
+		if (!mutex_trylock(&mvsw_owq_mutex_wip))
+			goto wip_again;
+
+		if (!rtnl_trylock() && !READ_ONCE(mvsw_owq_flushing))
+			goto rtnl_again;
+
+		break;
+rtnl_again:
+		mutex_unlock(&mvsw_owq_mutex_wip);
+wip_again:
+		schedule();
+	}
+}
+
+static void mvsw_owq_unlock(void)
+{
+	if (!READ_ONCE(mvsw_owq_flushing))
+		rtnl_unlock();
+
+	mutex_unlock(&mvsw_owq_mutex_wip);
+}
+
+/* Must be called under rtnl_lock */
+static void mvsw_owq_flush(void)
+{
+	/* Sanity check */
+	if (rtnl_trylock())
+		panic("%s: called without rtnl_lock !", __func__);
+
+	mutex_lock(&mvsw_owq_mutex_wip);
+	WRITE_ONCE(mvsw_owq_flushing, true);
+	mutex_unlock(&mvsw_owq_mutex_wip);
+
+	flush_workqueue(mvsw_r_owq);
+
+	mutex_lock(&mvsw_owq_mutex_wip);
+	WRITE_ONCE(mvsw_owq_flushing, false);
+	mutex_unlock(&mvsw_owq_mutex_wip);
+}
+
+static const unsigned char mvsw_pr_mac_mask[ETH_ALEN] = {
+	0xff, 0xff, 0xff, 0xff, 0xfc, 0x00
+};
+
+static struct mvsw_pr_vr *mvsw_pr_vr_get(struct mvsw_pr_switch *sw, u32 tb_id,
+					 struct netlink_ext_ack *extack);
+static u32 mvsw_pr_fix_tb_id(u32 tb_id);
+static void mvsw_pr_vr_put(struct mvsw_pr_switch *sw, struct mvsw_pr_vr *vr);
+static void mvsw_pr_vr_util_hw_abort(struct mvsw_pr_switch *sw);
+static struct mvsw_pr_rif *mvsw_pr_rif_create(struct mvsw_pr_switch *sw,
+					      const struct mvsw_pr_rif_params
+					      *params,
+					      struct netlink_ext_ack *extack);
+static int mvsw_pr_rif_vr_update(struct mvsw_pr_switch *sw,
+				 struct mvsw_pr_rif *rif,
+				 struct netlink_ext_ack *extack);
+static void mvsw_pr_rif_destroy(struct mvsw_pr_rif *rif);
+static void mvsw_pr_rif_put(struct mvsw_pr_rif *rif);
+static int mvsw_pr_rif_update(struct mvsw_pr_rif *rif, char *mac);
+static struct mvsw_pr_rif *mvsw_pr_rif_find(const struct mvsw_pr_switch *sw,
+					    const struct net_device *dev);
+static u16 mvsw_pr_rif_vr_id(struct mvsw_pr_rif *rif);
+static bool
+mvsw_pr_nh_neigh_util_hw_state(struct mvsw_pr_switch *sw,
+			       struct mvsw_pr_nh_neigh *nh_neigh);
+static bool
+mvsw_pr_nexthop_group_util_hw_state(struct mvsw_pr_switch *sw,
+				    struct mvsw_pr_nexthop_group *nh_grp);
+static struct mvsw_pr_nh_neigh *
+mvsw_pr_nh_neigh_find(struct mvsw_pr_switch *sw,
+		      struct mvsw_pr_nh_neigh_key *key);
+static int mvsw_pr_nh_neigh_set(struct mvsw_pr_switch *sw,
+				struct mvsw_pr_nh_neigh *neigh);
+static int mvsw_pr_nexthop_group_set(struct mvsw_pr_switch *sw,
+				     struct mvsw_pr_nexthop_group *nh_grp);
+static struct mvsw_pr_fib_node *
+mvsw_pr_fib_node_find(struct mvsw_pr_switch *sw, struct mvsw_pr_fib_key *key);
+static void mvsw_pr_fib_node_destroy(struct mvsw_pr_switch *sw,
+				     struct mvsw_pr_fib_node *fib_node);
+static struct mvsw_pr_fib_node *
+mvsw_pr_fib_node_uc_nh_create(struct mvsw_pr_switch *sw,
+			      struct mvsw_pr_fib_key *key,
+			      struct mvsw_pr_nexthop_group_key *nh_grp_key);
+static bool mvsw_pr_fi_is_direct(struct fib_info *fi);
+static bool mvsw_pr_fi_is_nh(struct fib_info *fi);
+static bool
+mvsw_pr_fib_node_util_is_neighbour(struct mvsw_pr_fib_node *fib_node);
+static void
+mvsw_pr_kern_fib_cache_offload_set(struct mvsw_pr_switch *sw,
+				   struct mvsw_pr_kern_fib_cache *fib_cache);
+
+static u16 mvsw_pr_nh_dev_to_vid(struct mvsw_pr_switch *sw,
+				 struct net_device *dev)
+{
+	struct macvlan_dev *vlan;
+	u16 vid = 0;
+
+	if (is_vlan_dev(dev) &&
+	    netif_is_bridge_master(vlan_dev_real_dev(dev))) {
+		vid = vlan_dev_vlan_id(dev);
+	} else if (netif_is_bridge_master(dev) && br_vlan_enabled(dev)) {
+		br_vlan_get_pvid(dev, &vid);
+	} else if (netif_is_bridge_master(dev)) {
+		vid = mvsw_pr_vlan_dev_vlan_id(sw->bridge, dev);
+	} else if (netif_is_macvlan(dev)) {
+		vlan = netdev_priv(dev);
+		return mvsw_pr_nh_dev_to_vid(sw, vlan->lowerdev);
+	}
+
+	return vid;
+}
+
+static struct net_device*
+mvsw_pr_nh_dev_egress(struct mvsw_pr_switch *sw, struct net_device *dev,
+		      u8 *ha)
+{
+	struct net_device *bridge_dev, *egress_dev = dev;
+	u16 vid = mvsw_pr_nh_dev_to_vid(sw, dev);
+	struct macvlan_dev *vlan;
+
+	if (is_vlan_dev(dev) &&
+	    netif_is_bridge_master(vlan_dev_real_dev(dev))) {
+		bridge_dev = vlan_dev_priv(dev)->real_dev;
+		egress_dev = br_fdb_find_port(bridge_dev, ha,
+					      vid);
+	} else if (netif_is_bridge_master(dev) && br_vlan_enabled(dev)) {
+		egress_dev = br_fdb_find_port(dev, ha, vid);
+	} else if (netif_is_bridge_master(dev)) {
+		/* vid in .1d bridge is 0 */
+		egress_dev = br_fdb_find_port(dev, ha, 0);
+	} else if (netif_is_macvlan(dev)) {
+		vlan = netdev_priv(dev);
+		return mvsw_pr_nh_dev_egress(sw, vlan->lowerdev, ha);
+	}
+
+	return egress_dev;
+}
+
+static u16 mvsw_pr_rif_vr_id(struct mvsw_pr_rif *rif)
+{
+	return rif->vr->hw_vr_id;
+}
+
+static int
+mvsw_pr_rif_iface_init(struct mvsw_pr_rif *rif)
+{
+	struct net_device *dev = rif->dev;
+	struct mvsw_pr_switch *sw = rif->sw;
+	struct mvsw_pr_port *port;
+	int if_type = mvsw_pr_dev_if_type(dev);
+
+	switch (if_type) {
+	case MVSW_IF_PORT_E:
+		port = netdev_priv(dev);
+		rif->iface.dev_port.hw_dev_num = port->dev_id;
+		rif->iface.dev_port.port_num = port->hw_id;
+		break;
+	case MVSW_IF_LAG_E:
+		prestera_lag_id_find(sw, dev, &rif->iface.lag_id);
+		break;
+	case MVSW_IF_VID_E:
+		break;
+	default:
+		pr_err("Unsupported rif type");
+		return -EINVAL;
+	}
+
+	rif->iface.type = if_type;
+	rif->iface.vlan_id = mvsw_pr_nh_dev_to_vid(sw, dev);
+	rif->iface.vr_id = rif->vr->hw_vr_id;
+
+	return 0;
+}
+
+static int
+__mvsw_pr_neigh_iface_init(struct mvsw_pr_switch *sw,
+			   struct mvsw_pr_iface *iface,
+			   struct neighbour *n,
+			   struct net_device *dev)
+{
+	bool is_nud_perm = n->nud_state & NUD_PERMANENT;
+	struct net_device *egress_dev;
+	struct mvsw_pr_port *port;
+
+	iface->type = mvsw_pr_dev_if_type(dev);
+
+	switch (iface->type) {
+	case MVSW_IF_PORT_E:
+	case MVSW_IF_VID_E:
+		egress_dev = mvsw_pr_nh_dev_egress(sw, dev, n->ha);
+		if (!egress_dev && is_nud_perm) {
+		/* Permanent neighbours on a bridge are not bounded to any
+		 * of the ports which is needed by the hardware, therefore
+		 * use any valid lower
+		 */
+			port = mvsw_pr_port_dev_lower_find(dev);
+			egress_dev = port->net_dev;
+		}
+		if (!egress_dev)
+			return -ENOENT;
+
+		if (!mvsw_pr_netdev_check(egress_dev))
+			return __mvsw_pr_neigh_iface_init(sw, iface, n,
+							  egress_dev);
+
+		port = netdev_priv(egress_dev);
+		iface->dev_port.hw_dev_num = port->dev_id;
+		iface->dev_port.port_num = port->hw_id;
+		break;
+	case MVSW_IF_LAG_E:
+		prestera_lag_id_find(sw, dev, &iface->lag_id);
+		break;
+	default:
+		MVSW_LOG_ERROR("Unsupported nexthop device");
+		return -EINVAL;
+	}
+
+	return 0;
+}
+
+static int
+mvsw_pr_neigh_iface_init(struct mvsw_pr_switch *sw,
+			struct mvsw_pr_iface *iface,
+			struct neighbour *n)
+{
+	/* TODO vr_id is obsolete in iface ? */
+	iface->vlan_id = mvsw_pr_nh_dev_to_vid(sw, n->dev);
+	return __mvsw_pr_neigh_iface_init(sw, iface, n, n->dev);
+}
+
+static void mvsw_pr_util_kern_set_neigh_offload(struct neighbour *n,
+						bool offloaded)
+{
+	if (offloaded)
+		n->flags |= NTF_OFFLOADED;
+	else
+		n->flags &= ~NTF_OFFLOADED;
+}
+
+static void
+__mvsw_pr_util_kern_unset_allneigh_offload_cb(struct neighbour *n,
+					      void *cookie)
+{
+	mvsw_pr_util_kern_set_neigh_offload(n, false);
+}
+
+static void mvsw_pr_util_kern_unset_allneigh_offload(void)
+{
+	/* Walk through every neighbour in kernel */
+	neigh_for_each(&arp_tbl,
+		       __mvsw_pr_util_kern_unset_allneigh_offload_cb,
+		       NULL);
+}
+
+static void
+mvsw_pr_util_kern_set_nh_offload(struct fib_nh *fib_nh, bool offloaded)
+{
+		if (offloaded)
+			fib_nh->fib_nh_flags |= RTNH_F_OFFLOAD;
+		else
+			fib_nh->fib_nh_flags &= ~RTNH_F_OFFLOAD;
+}
+
+/* must be called with rcu_read_lock() */
+static int mvsw_pr_util_kern_get_route(struct fib_result *res,
+				       u32 tb_id,
+				       struct mvsw_pr_ip_addr *addr)
+{
+	struct fib_table *tb;
+	struct flowi4 fl4;
+	int ret;
+
+	/* TODO: walkthrough appropriate tables in kernel
+	 * to know if the same prefix exists in several tables
+	 */
+	tb = fib_new_table(&init_net, tb_id);
+	if (!tb)
+		return -ENOENT;
+
+	memset(&fl4, 0, sizeof(fl4));
+	fl4.daddr = addr->u.ipv4;
+	ret = fib_table_lookup(tb, &fl4, res, FIB_LOOKUP_NOREF);
+	if (ret)
+		return ret;
+
+	return 0;
+}
+
+static int
+mvsw_pr_util_fib_nh2nh_neigh_key(struct mvsw_pr_switch *sw,
+				 struct fib_nh *fib_nh,
+				 struct mvsw_pr_nh_neigh_key *nh_key)
+{
+	memset(nh_key, 0, sizeof(*nh_key));
+	nh_key->addr.u.ipv4 = fib_nh->fib_nh_gw4;
+	nh_key->rif = mvsw_pr_rif_find(sw, fib_nh->fib_nh_dev);
+	if (!nh_key->rif)
+		return -ENOENT;
+
+	return 0;
+}
+
+static struct mvsw_pr_kern_neigh_cache *
+mvsw_pr_kern_neigh_cache_find(struct mvsw_pr_switch *sw,
+			      struct mvsw_pr_nh_neigh_key *key)
+{
+	struct mvsw_pr_kern_neigh_cache *n_cache;
+
+	n_cache =
+	 rhashtable_lookup_fast(&sw->router->kern_neigh_cache_ht, key,
+				__mvsw_pr_kern_neigh_cache_ht_params);
+	return IS_ERR(n_cache) ? NULL : n_cache;
+}
+
+static void
+__mvsw_pr_kern_neigh_cache_destroy(struct mvsw_pr_switch *sw,
+				   struct mvsw_pr_kern_neigh_cache *n_cache)
+{
+	n_cache->key.rif->ref_cnt--;
+	mvsw_pr_rif_put(n_cache->key.rif);
+	rhashtable_remove_fast(&sw->router->kern_neigh_cache_ht,
+			       &n_cache->ht_node,
+			       __mvsw_pr_kern_neigh_cache_ht_params);
+	kfree(n_cache);
+}
+
+static struct mvsw_pr_kern_neigh_cache *
+__mvsw_pr_kern_neigh_cache_create(struct mvsw_pr_switch *sw,
+				  struct mvsw_pr_nh_neigh_key *key)
+{
+	struct mvsw_pr_kern_neigh_cache *n_cache;
+	int err;
+
+	n_cache = kzalloc(sizeof(*n_cache), GFP_KERNEL);
+	if (!n_cache)
+		goto err_kzalloc;
+
+	memcpy(&n_cache->key, key, sizeof(*key));
+	n_cache->key.rif->ref_cnt++;
+
+	INIT_LIST_HEAD(&n_cache->kern_fib_cache_list);
+	err = rhashtable_insert_fast(&sw->router->kern_neigh_cache_ht,
+				     &n_cache->ht_node,
+				     __mvsw_pr_kern_neigh_cache_ht_params);
+	if (err)
+		goto err_ht_insert;
+
+	return n_cache;
+
+err_ht_insert:
+	n_cache->key.rif->ref_cnt--;
+	mvsw_pr_rif_put(n_cache->key.rif);
+	kfree(n_cache);
+err_kzalloc:
+	return NULL;
+}
+
+static struct mvsw_pr_kern_neigh_cache *
+mvsw_pr_kern_neigh_cache_get(struct mvsw_pr_switch *sw,
+			     struct mvsw_pr_nh_neigh_key *key)
+{
+	struct mvsw_pr_kern_neigh_cache *n_cache;
+
+	n_cache = mvsw_pr_kern_neigh_cache_find(sw, key);
+	if (!n_cache)
+		n_cache = __mvsw_pr_kern_neigh_cache_create(sw, key);
+
+	return n_cache;
+}
+
+static void
+mvsw_pr_kern_neigh_cache_put(struct mvsw_pr_switch *sw,
+			     struct mvsw_pr_kern_neigh_cache *n_cache)
+{
+	if (!n_cache->in_kernel && !n_cache->lpm_added &&
+	    list_empty(&n_cache->kern_fib_cache_list))
+		__mvsw_pr_kern_neigh_cache_destroy(sw, n_cache);
+}
+
+static void
+mvsw_pr_kern_neigh_cache_offload_set(struct mvsw_pr_switch *sw,
+				     struct mvsw_pr_kern_neigh_cache *n_cache)
+{
+	struct mvsw_pr_kern_neigh_cache_head *n_head;
+
+	list_for_each_entry(n_head, &n_cache->kern_fib_cache_list, head) {
+		mvsw_pr_kern_fib_cache_offload_set(sw, n_head->this);
+	}
+}
+
+static void
+mvsw_pr_kern_neigh_cache_lpm_set(struct mvsw_pr_switch *sw,
+				 struct mvsw_pr_kern_neigh_cache *n_cache)
+{
+	struct mvsw_pr_fib_key fib_key;
+	struct mvsw_pr_fib_node *fib_node;
+	struct mvsw_pr_nexthop_group_key nh_grp_key;
+
+	memset(&fib_key, 0, sizeof(fib_key));
+	fib_key.addr = n_cache->key.addr;
+	fib_key.prefix_len = 32;
+	fib_key.tb_id = n_cache->key.rif->vr->tb_id;
+	fib_node = mvsw_pr_fib_node_find(sw, &fib_key);
+	if (!n_cache->lpm_added && fib_node) {
+		if (mvsw_pr_fib_node_util_is_neighbour(fib_node))
+			mvsw_pr_fib_node_destroy(sw, fib_node);
+		return;
+	}
+
+	if (n_cache->lpm_added && !fib_node) {
+		memset(&nh_grp_key, 0, sizeof(nh_grp_key));
+		nh_grp_key.neigh[0] = n_cache->key;
+		fib_node = mvsw_pr_fib_node_uc_nh_create(sw, &fib_key,
+							 &nh_grp_key);
+		if (!fib_node)
+			MVSW_LOG_ERROR("%s failed ip=%pI4n",
+				       "mvsw_pr_fib_node_uc_nh_create",
+				       &fib_key.addr.u.ipv4);
+		return;
+	}
+}
+
+static struct mvsw_pr_kern_fib_cache *
+mvsw_pr_kern_fib_cache_find(struct mvsw_pr_switch *sw,
+			    struct mvsw_pr_fib_key *key)
+{
+	struct mvsw_pr_kern_fib_cache *fib_cache;
+
+	fib_cache =
+	 rhashtable_lookup_fast(&sw->router->kern_fib_cache_ht, key,
+				__mvsw_pr_kern_fib_cache_ht_params);
+	return IS_ERR(fib_cache) ? NULL : fib_cache;
+}
+
+static void
+__mvsw_pr_kern_fib_cache_destruct(struct mvsw_pr_switch *sw,
+				  struct mvsw_pr_kern_fib_cache *fib_cache)
+{
+	int i;
+	struct mvsw_pr_kern_neigh_cache *n_cache;
+	struct fib_nh *fib_nh;
+
+	if (mvsw_pr_fi_is_direct(fib_cache->fi)) {
+		fib_nh = fib_info_nh(fib_cache->fi, 0);
+		mvsw_pr_util_kern_set_nh_offload(fib_nh, true);
+		goto out;
+	}
+
+	for (i = 0; i < MVSW_PR_NHGR_SIZE_MAX; i++) {
+		n_cache = fib_cache->kern_neigh_cache_head[i].n_cache;
+		if (n_cache) {
+			list_del(&fib_cache->kern_neigh_cache_head[i].head);
+			mvsw_pr_kern_neigh_cache_put(sw, n_cache);
+			fib_nh = fib_info_nh(fib_cache->fi, i);
+			mvsw_pr_util_kern_set_nh_offload(fib_nh, false);
+		}
+	}
+
+out:
+	fib_info_put(fib_cache->fi);
+}
+
+static void
+mvsw_pr_kern_fib_cache_offload_set(struct mvsw_pr_switch *sw,
+				   struct mvsw_pr_kern_fib_cache *fib_cache)
+{
+	int i;
+	struct mvsw_pr_kern_neigh_cache *n_cache;
+	struct fib_nh *fib_nh;
+	bool offloaded;
+
+	if (mvsw_pr_fi_is_direct(fib_cache->fi)) {
+		fib_nh = fib_info_nh(fib_cache->fi, 0);
+		if (mvsw_pr_rif_find(sw, fib_nh->fib_nh_dev))
+			mvsw_pr_util_kern_set_nh_offload(fib_nh, true);
+		goto out;
+	}
+
+	for (i = 0; i < MVSW_PR_NHGR_SIZE_MAX; i++) {
+		n_cache = fib_cache->kern_neigh_cache_head[i].n_cache;
+		if (!n_cache)
+			continue;
+
+		offloaded = n_cache->offloaded;
+		fib_nh = fib_info_nh(fib_cache->fi, i);
+		mvsw_pr_util_kern_set_nh_offload(fib_nh, offloaded);
+	}
+
+out:
+	return;
+}
+
+static void
+mvsw_pr_kern_fib_cache_destroy(struct mvsw_pr_switch *sw,
+			       struct mvsw_pr_kern_fib_cache *fib_cache)
+{
+	__mvsw_pr_kern_fib_cache_destruct(sw, fib_cache);
+	rhashtable_remove_fast(&sw->router->kern_fib_cache_ht,
+			       &fib_cache->ht_node,
+			       __mvsw_pr_kern_fib_cache_ht_params);
+	kfree(fib_cache);
+}
+
+static void
+mvsw_pr_kern_fib_cache_destroy_ht(struct mvsw_pr_switch *sw)
+{
+	struct mvsw_pr_kern_fib_cache *fib_cache, *tfib_cache;
+	struct rhashtable_iter iter;
+
+	tfib_cache = NULL;
+	rhashtable_walk_enter(&sw->router->kern_fib_cache_ht, &iter);
+	rhashtable_walk_start(&iter);
+	while (1) {
+		fib_cache = rhashtable_walk_next(&iter);
+		if (tfib_cache) {
+			rhashtable_remove_fast(&sw->router->kern_fib_cache_ht,
+					       &tfib_cache->ht_node,
+					    __mvsw_pr_kern_fib_cache_ht_params);
+			kfree(tfib_cache);
+			tfib_cache = NULL;
+		}
+
+		if (!fib_cache)
+			break;
+
+		if (IS_ERR(fib_cache))
+			continue;
+
+		__mvsw_pr_kern_fib_cache_destruct(sw, fib_cache);
+		tfib_cache = fib_cache;
+	}
+	rhashtable_walk_stop(&iter);
+	rhashtable_walk_exit(&iter);
+}
+
+static struct mvsw_pr_kern_fib_cache *
+mvsw_pr_kern_fib_cache_create(struct mvsw_pr_switch *sw,
+			      struct mvsw_pr_fib_key *key,
+			      struct fib_info *fi)
+{
+	struct mvsw_pr_kern_fib_cache *fib_cache;
+	struct mvsw_pr_kern_neigh_cache *n_cache;
+	struct mvsw_pr_nh_neigh_key nh_key;
+	struct fib_nh *fib_nh;
+	int err, i, nhs;
+
+	fib_cache = kzalloc(sizeof(*fib_cache), GFP_KERNEL);
+	if (!fib_cache)
+		goto err_kzalloc;
+
+	memcpy(&fib_cache->key, key, sizeof(*key));
+	fib_info_hold(fi);
+	fib_cache->fi = fi;
+
+	err = rhashtable_insert_fast(&sw->router->kern_fib_cache_ht,
+				     &fib_cache->ht_node,
+				     __mvsw_pr_kern_fib_cache_ht_params);
+	if (err)
+		goto err_ht_insert;
+
+	if (!mvsw_pr_fi_is_nh(fi))
+		goto out;
+
+	nhs = fib_info_num_path(fi);
+	for (i = 0; i < nhs; i++) {
+		fib_nh = fib_info_nh(fi, i);
+		err = mvsw_pr_util_fib_nh2nh_neigh_key(sw, fib_nh, &nh_key);
+		if (err)
+			continue;
+
+		n_cache = mvsw_pr_kern_neigh_cache_get(sw, &nh_key);
+		if (!n_cache)
+			continue;
+
+		fib_cache->kern_neigh_cache_head[i].this = fib_cache;
+		fib_cache->kern_neigh_cache_head[i].n_cache = n_cache;
+		list_add(&fib_cache->kern_neigh_cache_head[i].head,
+			 &n_cache->kern_fib_cache_list);
+	}
+
+out:
+	mvsw_pr_kern_fib_cache_offload_set(sw, fib_cache);
+
+	return fib_cache;
+
+err_ht_insert:
+	fib_info_put(fi);
+	kfree(fib_cache);
+err_kzalloc:
+	return NULL;
+}
+
+static int mvsw_pr_util_neigh2nh_neigh_key(struct mvsw_pr_switch *sw,
+					   struct neighbour *n,
+					   struct mvsw_pr_nh_neigh_key *key)
+{
+	memset(key, 0, sizeof(*key));
+	key->addr.u.ipv4 = *(__be32 *)n->primary_key;
+	key->rif = mvsw_pr_rif_find(sw, n->dev);
+	if (!key->rif)
+		return -ENOENT;
+
+	return 0;
+}
+
+static void __mvsw_pr_neigh2nh_neigh_update(struct mvsw_pr_switch *sw,
+					    struct neighbour *n)
+{
+	struct mvsw_pr_nh_neigh_key nh_neigh_key;
+	struct mvsw_pr_nh_neigh *nh_neigh;
+	struct mvsw_pr_neigh_info new_info;
+	struct mvsw_pr_kern_neigh_cache *n_cache;
+	bool offloaded;
+	int err;
+
+	err = mvsw_pr_util_neigh2nh_neigh_key(sw, n, &nh_neigh_key);
+	if (err)
+		return;
+
+	nh_neigh = mvsw_pr_nh_neigh_find(sw, &nh_neigh_key);
+	if (!nh_neigh)
+		return;
+
+	memset(&new_info, 0, sizeof(new_info));
+	read_lock_bh(&n->lock);
+	if (n->nud_state & NUD_VALID && !n->dead) {
+		memcpy(&new_info.ha[0], &n->ha[0], ETH_ALEN);
+		err = mvsw_pr_neigh_iface_init(sw, &new_info.iface, n);
+		if (err) {
+			MVSW_LOG_ERROR("Cannot initialize iface for %pI4n %pM",
+				       n->primary_key, &n->ha[0]);
+			new_info.connected = false;
+		} else {
+			new_info.connected = true;
+		}
+	} else {
+		new_info.connected = false;
+	}
+	read_unlock_bh(&n->lock);
+
+	offloaded = new_info.connected;
+	/* Do hw update only if something changed to prevent nh flap */
+	if (memcmp(&new_info, &nh_neigh->info, sizeof(new_info))) {
+		memcpy(&nh_neigh->info, &new_info, sizeof(new_info));
+		err = mvsw_pr_nh_neigh_set(sw, nh_neigh);
+		if (err) {
+			offloaded = false;
+			MVSW_LOG_ERROR("%s failed with err=%d ip=%pI4n mac=%pM",
+				       "mvsw_pr_nh_neigh_set", err,
+				       &nh_neigh->key.addr.u.ipv4,
+				       &nh_neigh->info.ha[0]);
+		}
+	}
+
+	mvsw_pr_util_kern_set_neigh_offload(n, offloaded);
+	n_cache = mvsw_pr_kern_neigh_cache_find(sw, &nh_neigh_key);
+	if (!n_cache) {
+		MVSW_LOG_ERROR("Cannot get neigh cache for %pI4n %pM",
+			       n->primary_key, &n->ha[0]);
+	} else {
+		n_cache->offloaded = offloaded;
+		mvsw_pr_kern_neigh_cache_offload_set(sw, n_cache);
+	}
+}
+
+static void __mvsw_pr_neigh2nh_neigh_update_all(struct mvsw_pr_switch *sw)
+{
+	struct mvsw_pr_nh_neigh *nh_neigh;
+	struct rhashtable_iter iter;
+	struct neighbour *n;
+	struct mvsw_pr_nh_neigh_key *nkey;
+
+	rhashtable_walk_enter(&sw->router->nh_neigh_ht, &iter);
+	rhashtable_walk_start(&iter);
+	while ((nh_neigh = rhashtable_walk_next(&iter))) {
+		if (IS_ERR(nh_neigh))
+			continue;
+
+		nkey = &nh_neigh->key;
+		n = neigh_lookup(&arp_tbl, &nkey->addr.u.ipv4, nkey->rif->dev);
+		if (n) {
+			__mvsw_pr_neigh2nh_neigh_update(sw, n);
+			neigh_release(n);
+		}
+	}
+	rhashtable_walk_stop(&iter);
+	rhashtable_walk_exit(&iter);
+}
+
+/* I dont think that optimiztaion of this
+ * function withone neighbour will make sense...
+ * Just select direction nh_neigh -> kernel or vice versa
+ */
+static void
+__mvsw_pr_neigh_hwstate_update_all(struct mvsw_pr_switch *sw)
+{
+	struct mvsw_pr_nh_neigh *nh_neigh;
+	struct rhashtable_iter iter;
+	struct neighbour *n;
+	struct mvsw_pr_nh_neigh_key *nkey;
+	bool n_resolved, hw_active;
+
+	rhashtable_walk_enter(&sw->router->nh_neigh_ht, &iter);
+	rhashtable_walk_start(&iter);
+	while ((nh_neigh = rhashtable_walk_next(&iter))) {
+		if (IS_ERR(nh_neigh))
+			continue;
+
+		hw_active = mvsw_pr_nh_neigh_util_hw_state(sw, nh_neigh);
+		nkey = &nh_neigh->key;
+		n = neigh_lookup(&arp_tbl, &nkey->addr.u.ipv4, nkey->rif->dev);
+		if (n) {
+			read_lock_bh(&n->lock);
+			if (n->dead || !(n->nud_state & NUD_VALID))
+				n_resolved = false;
+			else
+				n_resolved = true;
+			read_unlock_bh(&n->lock);
+		} else {
+			n_resolved = false;
+		}
+
+#ifdef MVSW_PR_IMPLICITY_RESOLVE_DEAD_NEIGH
+		if (!hw_active && n_resolved)
+			goto next_nh_neigh;
+#else /* MVSW_PR_IMPLICITY_RESOLVE_DEAD_NEIGH */
+		if (!hw_active)
+			goto next_nh_neigh;
+#endif /* MVSW_PR_IMPLICITY_RESOLVE_DEAD_NEIGH */
+
+		if (!n) {
+			MVSW_LOG_INFO("Push active neighbour %pI4n to kernel",
+				      &nkey->addr.u.ipv4);
+			n = neigh_create(&arp_tbl, &nkey->addr.u.ipv4,
+					 nkey->rif->dev);
+			if (IS_ERR(n)) {
+				n = NULL;
+				MVSW_LOG_ERROR("Cannot create neighbour %pI4n",
+					       &nkey->addr.u.ipv4);
+
+				goto next_nh_neigh;
+			}
+		}
+
+		neigh_event_send(n, NULL);
+
+next_nh_neigh:
+		if (n)
+			neigh_release(n);
+	}
+	rhashtable_walk_stop(&iter);
+	rhashtable_walk_exit(&iter);
+}
+
+static void
+__mvsw_pr_sync_neigh_cache_kernel(struct mvsw_pr_switch *sw,
+				  struct mvsw_pr_kern_neigh_cache *n_cache)
+{
+	struct neighbour *n;
+	struct fib_result res;
+	struct fib_nh *fib_nh;
+	struct mvsw_pr_nh_neigh_key *key;
+
+	key = &n_cache->key;
+
+	n_cache->in_kernel = false;
+	n_cache->lpm_added = false;
+	n = neigh_lookup(&arp_tbl, &key->addr.u.ipv4, key->rif->dev);
+	if (n) {
+		read_lock_bh(&n->lock);
+		if (!n->dead && (n->nud_state & NUD_VALID))
+			n_cache->in_kernel = true;
+		read_unlock_bh(&n->lock);
+	}
+
+	if (n_cache->in_kernel) {
+		if (!mvsw_pr_util_kern_get_route(&res, key->rif->vr->tb_id,
+						 &key->addr))
+			if (res.type == RTN_UNICAST &&
+			    mvsw_pr_fi_is_direct(res.fi)) {
+				fib_nh = fib_info_nh(res.fi, 0);
+				if (n->dev == fib_nh->fib_nh_dev)
+					n_cache->lpm_added = true;
+			}
+	}
+
+	if (n)
+		neigh_release(n);
+
+	mvsw_pr_kern_neigh_cache_lpm_set(sw, n_cache);
+}
+
+static void __mvsw_pr_sync_neigh_cache_kernel_all(struct mvsw_pr_switch *sw)
+{
+	struct mvsw_pr_kern_neigh_cache *n_cache, *tn_cache;
+	struct rhashtable_iter iter;
+
+	tn_cache = NULL;
+	rhashtable_walk_enter(&sw->router->kern_neigh_cache_ht, &iter);
+	rhashtable_walk_start(&iter);
+	while (1) {
+		n_cache = rhashtable_walk_next(&iter);
+		if (tn_cache) {
+			mvsw_pr_kern_neigh_cache_put(sw, tn_cache);
+			tn_cache = NULL;
+		}
+
+		if (!n_cache)
+			break;
+
+		if (IS_ERR(n_cache))
+			continue;
+
+		__mvsw_pr_sync_neigh_cache_kernel(sw, n_cache);
+		tn_cache = n_cache;
+	}
+	rhashtable_walk_stop(&iter);
+	rhashtable_walk_exit(&iter);
+}
+
+/* Propagate kernel event to hw */
+static void mvsw_pr_neigh_arbiter_n_evt(struct mvsw_pr_switch *sw,
+					struct neighbour *n)
+{
+	struct mvsw_pr_kern_neigh_cache *n_cache;
+	struct mvsw_pr_nh_neigh_key n_key;
+	int err;
+
+	err = mvsw_pr_util_neigh2nh_neigh_key(sw, n, &n_key);
+	if (err)
+		return;
+
+	n_cache = mvsw_pr_kern_neigh_cache_get(sw, &n_key);
+	if (!n_cache)
+		return;
+
+	__mvsw_pr_sync_neigh_cache_kernel(sw, n_cache);
+	mvsw_pr_kern_neigh_cache_put(sw, n_cache);
+
+	__mvsw_pr_neigh2nh_neigh_update(sw, n);
+}
+
+/* Propagate hw state to kernel */
+static void mvsw_pr_neigh_arbiter_hw_evt(struct mvsw_pr_switch *sw)
+{
+	__mvsw_pr_neigh_hwstate_update_all(sw);
+}
+
+/* Propagate fib changes to hw neighs */
+static void
+mvsw_pr_neigh_arbiter_fib_evt(struct mvsw_pr_switch *sw,
+			      bool replace, /* replace or del */
+			      struct mvsw_pr_fib_key *fib_key,
+			      struct fib_info *fi)
+{
+	struct mvsw_pr_kern_fib_cache *fib_cache;
+
+	fib_cache = mvsw_pr_kern_fib_cache_find(sw, fib_key);
+		if (fib_cache)
+			mvsw_pr_kern_fib_cache_destroy(sw, fib_cache);
+
+	/* TODO: add util function IS_NH / IS_DIR */
+	if (replace) {
+		fib_cache = mvsw_pr_kern_fib_cache_create(sw, fib_key, fi);
+		if (!fib_cache)
+			MVSW_LOG_ERROR("%s failed for %pI4n",
+				       "mvsw_pr_kern_fib_cache_create",
+				       &fib_key->addr.u.ipv4);
+	}
+
+	__mvsw_pr_sync_neigh_cache_kernel_all(sw);
+	__mvsw_pr_neigh2nh_neigh_update_all(sw);
+}
+
+struct mvsw_pr_netevent_work {
+	struct work_struct work;
+	struct mvsw_pr_switch *sw;
+	struct neighbour *n;
+};
+
+static void mvsw_pr_router_neigh_event_work(struct work_struct *work)
+{
+	struct mvsw_pr_netevent_work *net_work =
+		container_of(work, struct mvsw_pr_netevent_work, work);
+	struct mvsw_pr_switch *sw = net_work->sw;
+	struct neighbour *n = net_work->n;
+
+	/* neigh - its not hw related object. It stored only in kernel. So... */
+	mvsw_owq_lock();
+
+	if (sw->router->aborted)
+		goto out;
+
+	mvsw_pr_neigh_arbiter_n_evt(sw, n);
+
+out:
+	neigh_release(n);
+	mvsw_owq_unlock();
+	kfree(net_work);
+}
+
+static int mvsw_pr_router_netevent_event(struct notifier_block *nb,
+					 unsigned long event, void *ptr)
+{
+	struct mvsw_pr_netevent_work *net_work;
+	struct mvsw_pr_router *router;
+	struct mvsw_pr_rif *rif;
+	struct neighbour *n = ptr;
+
+	router = container_of(nb, struct mvsw_pr_router, netevent_nb);
+
+	switch (event) {
+	case NETEVENT_NEIGH_UPDATE:
+		if (n->tbl != &arp_tbl)
+			return NOTIFY_DONE;
+
+		rif = mvsw_pr_rif_find(router->sw, n->dev);
+		if (!rif)
+			return NOTIFY_DONE;
+
+		net_work = kzalloc(sizeof(*net_work), GFP_ATOMIC);
+		if (WARN_ON(!net_work))
+			return NOTIFY_BAD;
+
+		neigh_clone(n);
+		net_work->n = n;
+		net_work->sw = router->sw;
+		INIT_WORK(&net_work->work, mvsw_pr_router_neigh_event_work);
+		queue_work(mvsw_r_owq, &net_work->work);
+	}
+
+	return NOTIFY_DONE;
+}
+
+static void
+mvsw_pr_router_neighs_update_interval_init(struct mvsw_pr_router *router)
+{
+	router->neighs_update.interval = MVSW_PR_NH_PROBE_INTERVAL;
+}
+
+static void mvsw_pr_router_update_neighs_work(struct work_struct *work)
+{
+	struct mvsw_pr_router *router;
+
+	router = container_of(work, struct mvsw_pr_router,
+			      neighs_update.dw.work);
+	rtnl_lock();
+
+	if (router->aborted)
+		goto out;
+
+	mvsw_pr_neigh_arbiter_hw_evt(router->sw);
+
+out:
+	rtnl_unlock();
+	mvsw_pr_router_neighs_update_interval_init(router);
+	queue_delayed_work(mvsw_r_wq, &router->neighs_update.dw,
+			   msecs_to_jiffies(router->neighs_update.interval));
+}
+
+static int mvsw_pr_neigh_init(struct mvsw_pr_switch *sw)
+{
+	int err;
+
+	err = rhashtable_init(&sw->router->nh_neigh_ht,
+			      &__mvsw_pr_nh_neigh_ht_params);
+	if (err)
+		return err;
+
+	mvsw_pr_router_neighs_update_interval_init(sw->router);
+
+	INIT_DELAYED_WORK(&sw->router->neighs_update.dw,
+			  mvsw_pr_router_update_neighs_work);
+	queue_delayed_work(mvsw_r_wq, &sw->router->neighs_update.dw, 0);
+	return 0;
+}
+
+static void mvsw_pr_neigh_fini(struct mvsw_pr_switch *sw)
+{
+	cancel_delayed_work_sync(&sw->router->neighs_update.dw);
+	rhashtable_destroy(&sw->router->nh_neigh_ht);
+}
+
+static struct mvsw_pr_rif*
+mvsw_pr_rif_find(const struct mvsw_pr_switch *sw,
+		 const struct net_device *dev)
+{
+	struct mvsw_pr_rif *rif;
+
+	list_for_each_entry(rif, &sw->router->rif_list, router_node) {
+		if (rif->dev == dev)
+			return rif;
+	}
+
+	return NULL;
+}
+
+bool mvsw_pr_rif_exists(const struct mvsw_pr_switch *sw,
+			const struct net_device *dev)
+{
+	return !!mvsw_pr_rif_find(sw, dev);
+}
+
+static int
+mvsw_pr_port_vlan_router_join(struct mvsw_pr_port_vlan *mvsw_pr_port_vlan,
+			      struct net_device *dev,
+			      struct netlink_ext_ack *extack)
+{
+	struct mvsw_pr_port *mvsw_pr_port = mvsw_pr_port_vlan->mvsw_pr_port;
+	struct mvsw_pr_switch *sw = mvsw_pr_port->sw;
+
+	struct mvsw_pr_rif_params params = {
+		.dev = dev,
+	};
+	struct mvsw_pr_rif *rif;
+
+	MVSW_LOG_ERROR("NOT IMPLEMENTED!!!");
+
+	rif = mvsw_pr_rif_find(sw, dev);
+	if (!rif)
+		rif = mvsw_pr_rif_create(sw, &params, extack);
+
+	if (IS_ERR(rif))
+		return PTR_ERR(rif);
+
+	rif->is_active = true;
+
+	/* TODO:
+	 * - vid learning set (false)
+	 * - stp state set (FORWARDING)
+	 */
+
+	return 0;
+}
+
+static void
+mvsw_pr_port_vlan_router_leave(struct mvsw_pr_port_vlan *mvsw_pr_port_vlan,
+			       struct net_device *dev)
+
+{
+	struct mvsw_pr_port *mvsw_pr_port = mvsw_pr_port_vlan->mvsw_pr_port;
+	struct mvsw_pr_switch *sw = mvsw_pr_port->sw;
+	struct mvsw_pr_rif *rif;
+
+	rif = mvsw_pr_rif_find(sw, dev);
+
+	if (rif)
+		rif->is_active = false;
+	/* TODO:
+	 * - stp state set (BLOCKING)
+	 * - vid learning set (true)
+	 */
+}
+
+static int
+mvsw_pr_port_router_join(struct mvsw_pr_port *mvsw_pr_port,
+			 struct net_device *dev,
+			 struct netlink_ext_ack *extack)
+{
+	struct mvsw_pr_switch *sw = mvsw_pr_port->sw;
+
+	struct mvsw_pr_rif_params params = {
+		.dev = dev,
+	};
+	struct mvsw_pr_rif *rif;
+
+	rif = mvsw_pr_rif_find(sw, dev);
+	if (!rif)
+		rif = mvsw_pr_rif_create(sw, &params, extack);
+
+	if (IS_ERR(rif))
+		return PTR_ERR(rif);
+
+	rif->is_active = true;
+
+	/* TODO:
+	 * - vid learning set (false)
+	 * - stp state set (FORWARDING)
+	 */
+
+	return 0;
+}
+
+void mvsw_pr_port_router_leave(struct mvsw_pr_port *mvsw_pr_port)
+{
+	struct mvsw_pr_rif *rif;
+
+	/* TODO:
+	 * - stp state set (BLOCKING)
+	 * - vid learning set (true)
+	 */
+
+	rif = mvsw_pr_rif_find(mvsw_pr_port->sw, mvsw_pr_port->net_dev);
+	if (rif) {
+		rif->is_active = false;
+		mvsw_pr_rif_put(rif);
+	}
+}
+
+static int mvsw_pr_rif_fdb_op(struct mvsw_pr_rif *rif, const char *mac,
+			      bool adding)
+{
+	if (adding)
+		mvsw_pr_macvlan_add(rif->sw, mvsw_pr_rif_vr_id(rif), mac,
+				    rif->iface.vlan_id);
+	else
+		mvsw_pr_macvlan_del(rif->sw, mvsw_pr_rif_vr_id(rif), mac,
+				    rif->iface.vlan_id);
+
+	return 0;
+}
+
+static int mvsw_pr_rif_macvlan_add(struct mvsw_pr_switch *sw,
+				   const struct net_device *macvlan_dev,
+				   struct netlink_ext_ack *extack)
+{
+	struct macvlan_dev *vlan = netdev_priv(macvlan_dev);
+	struct mvsw_pr_rif *rif;
+	int err;
+
+	rif = mvsw_pr_rif_find(sw, vlan->lowerdev);
+	if (!rif) {
+		NL_SET_ERR_MSG_MOD(extack,
+				   "macvlan is only supported on top of RIF");
+		return -EOPNOTSUPP;
+	}
+
+	err = mvsw_pr_rif_fdb_op(rif, macvlan_dev->dev_addr, true);
+	if (err)
+		return err;
+
+	return err;
+}
+
+static void __mvsw_pr_rif_macvlan_del(struct mvsw_pr_switch *sw,
+				      const struct net_device *macvlan_dev)
+{
+	struct macvlan_dev *vlan = netdev_priv(macvlan_dev);
+	struct mvsw_pr_rif *rif;
+
+	rif = mvsw_pr_rif_find(sw, vlan->lowerdev);
+	if (!rif)
+		return;
+
+	mvsw_pr_rif_fdb_op(rif, macvlan_dev->dev_addr,  false);
+}
+
+static void mvsw_pr_rif_macvlan_del(struct mvsw_pr_switch *sw,
+				    const struct net_device *macvlan_dev)
+{
+	__mvsw_pr_rif_macvlan_del(sw, macvlan_dev);
+}
+
+static int mvsw_pr_inetaddr_macvlan_event(struct mvsw_pr_switch *sw,
+					  struct net_device *macvlan_dev,
+					  unsigned long event,
+					  struct netlink_ext_ack *extack)
+{
+	switch (event) {
+	case NETDEV_UP:
+		return mvsw_pr_rif_macvlan_add(sw, macvlan_dev, extack);
+	case NETDEV_DOWN:
+		mvsw_pr_rif_macvlan_del(sw, macvlan_dev);
+		break;
+	}
+
+	return 0;
+}
+
+static int mvsw_pr_router_port_check_rif_addr(struct mvsw_pr_switch *sw,
+					      struct net_device *dev,
+					      const unsigned char *dev_addr,
+					      struct netlink_ext_ack *extack)
+{
+	if (netif_is_macvlan(dev) || netif_is_l3_master(dev))
+		return 0;
+
+	if (!ether_addr_equal_masked(sw->base_mac, dev_addr,
+				     mvsw_pr_mac_mask)) {
+		NL_SET_ERR_MSG_MOD(extack,
+				   "RIF MAC must have the same prefix");
+		return -EINVAL;
+	}
+
+	return 0;
+}
+
+static int mvsw_pr_inetaddr_port_event(struct net_device *port_dev,
+				       unsigned long event,
+				       struct netlink_ext_ack *extack)
+{
+	struct mvsw_pr_port *mvsw_pr_port = netdev_priv(port_dev);
+
+	MVSW_LOG_ERROR("dev=%s", port_dev->name);
+
+	switch (event) {
+	case NETDEV_UP:
+		if (netif_is_bridge_port(port_dev) ||
+		    netif_is_lag_port(port_dev) || netif_is_ovs_port(port_dev))
+			return 0;
+		return mvsw_pr_port_router_join(mvsw_pr_port, port_dev, extack);
+	case NETDEV_DOWN:
+		mvsw_pr_port_router_leave(mvsw_pr_port);
+		break;
+	}
+
+	return 0;
+}
+
+static int mvsw_pr_inetaddr_bridge_event(struct mvsw_pr_switch *sw,
+					 struct net_device *dev,
+					 unsigned long event,
+					 struct netlink_ext_ack *extack)
+{
+	struct mvsw_pr_rif_params params = {
+		.dev = dev,
+	};
+	struct mvsw_pr_rif *rif;
+
+	switch (event) {
+	case NETDEV_UP:
+		rif = mvsw_pr_rif_find(sw, dev);
+		if (!rif)
+			rif = mvsw_pr_rif_create(sw, &params, extack);
+
+		if (IS_ERR(rif))
+			return PTR_ERR(rif);
+		rif->is_active = true;
+		break;
+	case NETDEV_DOWN:
+		rif = mvsw_pr_rif_find(sw, dev);
+		rif->is_active = false;
+		mvsw_pr_rif_put(rif);
+		break;
+	}
+
+	return 0;
+}
+
+static int mvsw_pr_inetaddr_port_vlan_event(struct net_device *l3_dev,
+					    struct net_device *port_dev,
+					    unsigned long event, u16 vid,
+					    struct netlink_ext_ack *extack)
+{
+	struct mvsw_pr_port *mvsw_pr_port = netdev_priv(port_dev);
+	struct mvsw_pr_port_vlan *mvsw_pr_port_vlan;
+
+	mvsw_pr_port_vlan = mvsw_pr_port_vlan_find_by_vid(mvsw_pr_port, vid);
+	if (WARN_ON(!mvsw_pr_port_vlan))
+		return -EINVAL;
+
+	switch (event) {
+	case NETDEV_UP:
+		return mvsw_pr_port_vlan_router_join(mvsw_pr_port_vlan,
+						     l3_dev, extack);
+	case NETDEV_DOWN:
+		mvsw_pr_port_vlan_router_leave(mvsw_pr_port_vlan, l3_dev);
+		break;
+	}
+
+	return 0;
+}
+
+static int mvsw_pr_inetaddr_vlan_event(struct mvsw_pr_switch *sw,
+				       struct net_device *vlan_dev,
+				       unsigned long event,
+				       struct netlink_ext_ack *extack)
+{
+	struct net_device *real_dev = vlan_dev_real_dev(vlan_dev);
+	u16 vid = vlan_dev_vlan_id(vlan_dev);
+
+	MVSW_LOG_ERROR("vlan_dev=%s, real_dev=%s", vlan_dev->name,
+		       real_dev->name);
+	if (netif_is_bridge_port(vlan_dev))
+		return 0;
+
+	if (mvsw_pr_netdev_check(real_dev))
+		return mvsw_pr_inetaddr_port_vlan_event(vlan_dev, real_dev,
+							event, vid, extack);
+	else if (netif_is_bridge_master(real_dev) && br_vlan_enabled(real_dev))
+		return mvsw_pr_inetaddr_bridge_event(sw, vlan_dev, event,
+						     extack);
+
+	return 0;
+}
+
+static int mvsw_pr_inetaddr_lag_event(struct mvsw_pr_switch *sw,
+				      struct net_device *lag_dev,
+				      unsigned long event,
+				      struct netlink_ext_ack *extack)
+{
+	struct mvsw_pr_rif_params params = {
+		.dev = lag_dev,
+	};
+	struct mvsw_pr_rif *rif;
+
+	MVSW_LOG_ERROR("lag_dev=%s", lag_dev->name);
+
+	switch (event) {
+	case NETDEV_UP:
+		rif = mvsw_pr_rif_find(sw, lag_dev);
+		if (!rif)
+			rif = mvsw_pr_rif_create(sw, &params, extack);
+
+		if (IS_ERR(rif))
+			return PTR_ERR(rif);
+		rif->is_active = true;
+		break;
+	case NETDEV_DOWN:
+		rif = mvsw_pr_rif_find(sw, lag_dev);
+		rif->is_active = false;
+		mvsw_pr_rif_put(rif);
+		break;
+	}
+
+	return 0;
+}
+
+static int __mvsw_pr_inetaddr_event(struct mvsw_pr_switch *sw,
+				    struct net_device *dev,
+				    unsigned long event,
+				    struct netlink_ext_ack *extack)
+{
+	if (mvsw_pr_netdev_check(dev))
+		return mvsw_pr_inetaddr_port_event(dev, event, extack);
+	else if (is_vlan_dev(dev))
+		return mvsw_pr_inetaddr_vlan_event(sw, dev, event, extack);
+	else if (netif_is_bridge_master(dev))
+		return mvsw_pr_inetaddr_bridge_event(sw, dev, event, extack);
+	else if (netif_is_lag_master(dev))
+		return mvsw_pr_inetaddr_lag_event(sw, dev, event, extack);
+	else if (netif_is_macvlan(dev))
+		return mvsw_pr_inetaddr_macvlan_event(sw, dev, event, extack);
+	else
+		return 0;
+}
+
+static bool
+mvsw_pr_rif_should_config(struct mvsw_pr_rif *rif, struct net_device *dev,
+			  unsigned long event)
+{
+	bool addr_list_empty = true;
+	struct in_device *idev;
+
+	switch (event) {
+	case NETDEV_UP:
+		return !rif;
+	case NETDEV_DOWN:
+		idev = __in_dev_get_rtnl(dev);
+		if (idev && idev->ifa_list)
+			addr_list_empty = false;
+
+		if (netif_is_macvlan(dev) && addr_list_empty)
+			return true;
+
+		if (rif && addr_list_empty)
+			return true;
+
+		return false;
+	}
+
+	return false;
+}
+
+static int mvsw_pr_inetaddr_event(struct notifier_block *nb,
+				  unsigned long event, void *ptr)
+{
+	struct in_ifaddr *ifa = (struct in_ifaddr *)ptr;
+	struct net_device *dev = ifa->ifa_dev->dev;
+	struct mvsw_pr_router *router;
+	struct mvsw_pr_switch *sw;
+	struct mvsw_pr_rif *rif;
+	int err = 0;
+
+	/* Wait until previously created works finished (e.g. neigh events) */
+	mvsw_owq_flush();
+	/* NETDEV_UP event is handled by mvsw_pr_inetaddr_valid_event */
+	if (event == NETDEV_UP)
+		goto out;
+
+	MVSW_LOG_ERROR("dev=%s", dev->name);
+	router = container_of(nb, struct mvsw_pr_router, inetaddr_nb);
+	sw = router->sw;
+
+	if (netif_is_macvlan(dev))
+		goto mac_vlan;
+
+	rif = mvsw_pr_rif_find(sw, dev);
+	if (!rif)
+		goto out;
+
+	if (!mvsw_pr_rif_should_config(rif, dev, event))
+		goto out;
+mac_vlan:
+	err = __mvsw_pr_inetaddr_event(sw, dev, event, NULL);
+out:
+	return notifier_from_errno(err);
+}
+
+int mvsw_pr_inetaddr_valid_event(struct notifier_block *unused,
+				 unsigned long event, void *ptr)
+{
+	struct in_validator_info *ivi = (struct in_validator_info *)ptr;
+	struct net_device *dev = ivi->ivi_dev->dev;
+	struct mvsw_pr_switch *sw;
+	struct mvsw_pr_rif *rif;
+	int err = 0;
+
+	sw = mvsw_pr_switch_get(dev);
+	if (!sw)
+		goto out;
+
+	/* Wait until previously created works finished (e.g. neigh events) */
+	mvsw_owq_flush();
+
+	if (ipv4_is_multicast(ivi->ivi_addr))
+		return notifier_from_errno(-EINVAL);
+
+	rif = mvsw_pr_rif_find(sw, dev);
+	if (!mvsw_pr_rif_should_config(rif, dev, event))
+		goto out;
+
+	err = mvsw_pr_router_port_check_rif_addr(sw, dev, dev->dev_addr,
+						 ivi->extack);
+	if (err)
+		goto out;
+
+	err = __mvsw_pr_inetaddr_event(sw, dev, event, ivi->extack);
+out:
+	return notifier_from_errno(err);
+}
+
+static bool __mvsw_pr_fi_is_direct(struct fib_info *fi)
+{
+	struct fib_nh *fib_nh;
+
+	if (fib_info_num_path(fi) == 1) {
+		fib_nh = fib_info_nh(fi, 0);
+		if (fib_nh->fib_nh_scope == RT_SCOPE_HOST)
+			return true;
+	}
+
+	return false;
+}
+
+static bool mvsw_pr_fi_is_direct(struct fib_info *fi)
+{
+	if (fi->fib_type != RTN_UNICAST)
+		return false;
+
+	return __mvsw_pr_fi_is_direct(fi);
+}
+
+static bool mvsw_pr_fi_is_nh(struct fib_info *fi)
+{
+	if (fi->fib_type != RTN_UNICAST)
+		return false;
+
+	return !__mvsw_pr_fi_is_direct(fi);
+}
+
+static void __mvsw_pr_nh_neigh_destroy(struct mvsw_pr_switch *sw,
+				       struct mvsw_pr_nh_neigh *neigh)
+{
+	neigh->key.rif->ref_cnt--;
+	mvsw_pr_rif_put(neigh->key.rif);
+	rhashtable_remove_fast(&sw->router->nh_neigh_ht,
+			       &neigh->ht_node,
+			       __mvsw_pr_nh_neigh_ht_params);
+	kfree(neigh);
+}
+
+static struct mvsw_pr_nh_neigh *
+__mvsw_pr_nh_neigh_create(struct mvsw_pr_switch *sw,
+			  struct mvsw_pr_nh_neigh_key *key)
+{
+	struct mvsw_pr_nh_neigh *neigh;
+	int err;
+
+	neigh = kzalloc(sizeof(*neigh), GFP_KERNEL);
+	if (!neigh)
+		goto err_kzalloc;
+
+	memcpy(&neigh->key, key, sizeof(*key));
+	neigh->key.rif->ref_cnt++;
+	neigh->info.connected = false;
+	INIT_LIST_HEAD(&neigh->nexthop_group_list);
+	err = rhashtable_insert_fast(&sw->router->nh_neigh_ht,
+				     &neigh->ht_node,
+				     __mvsw_pr_nh_neigh_ht_params);
+	if (err)
+		goto err_rhashtable_insert;
+
+	return neigh;
+
+err_rhashtable_insert:
+	kfree(neigh);
+err_kzalloc:
+	return NULL;
+}
+
+static struct mvsw_pr_nh_neigh *
+mvsw_pr_nh_neigh_find(struct mvsw_pr_switch *sw,
+		      struct mvsw_pr_nh_neigh_key *key)
+{
+	struct mvsw_pr_nh_neigh *nh_neigh;
+
+	nh_neigh = rhashtable_lookup_fast(&sw->router->nh_neigh_ht,
+					  key, __mvsw_pr_nh_neigh_ht_params);
+	return IS_ERR(nh_neigh) ? NULL : nh_neigh;
+}
+
+static struct mvsw_pr_nh_neigh *
+mvsw_pr_nh_neigh_get(struct mvsw_pr_switch *sw,
+		     struct mvsw_pr_nh_neigh_key *key)
+{
+	struct mvsw_pr_nh_neigh *neigh;
+
+	neigh = mvsw_pr_nh_neigh_find(sw, key);
+	if (!neigh)
+		return __mvsw_pr_nh_neigh_create(sw, key);
+
+	return neigh;
+}
+
+static void mvsw_pr_nh_neigh_put(struct mvsw_pr_switch *sw,
+				 struct mvsw_pr_nh_neigh *neigh)
+{
+	if (list_empty(&neigh->nexthop_group_list))
+		__mvsw_pr_nh_neigh_destroy(sw, neigh);
+}
+
+/* Updates new mvsw_pr_neigh_info */
+static int mvsw_pr_nh_neigh_set(struct mvsw_pr_switch *sw,
+				struct mvsw_pr_nh_neigh *neigh)
+{
+	struct mvsw_pr_nh_neigh_head *nh_head;
+	struct mvsw_pr_nexthop_group *nh_grp;
+	int err;
+
+	list_for_each_entry(nh_head, &neigh->nexthop_group_list, head) {
+		nh_grp = nh_head->this;
+		err = mvsw_pr_nexthop_group_set(sw, nh_grp);
+		if (err)
+			return err;
+	}
+
+	return 0;
+}
+
+static bool __mvsw_pr_nh_neigh_key_is_valid(struct mvsw_pr_nh_neigh_key *key)
+{
+	return memchr_inv(key, 0, sizeof(*key)) ? true : false;
+}
+
+static bool
+mvsw_pr_nh_neigh_util_hw_state(struct mvsw_pr_switch *sw,
+			       struct mvsw_pr_nh_neigh *nh_neigh)
+{
+	bool state;
+	struct mvsw_pr_nh_neigh_head *nh_head, *tmp;
+
+	state = false;
+	list_for_each_entry_safe(nh_head, tmp,
+				 &nh_neigh->nexthop_group_list, head) {
+		state = mvsw_pr_nexthop_group_util_hw_state(sw, nh_head->this);
+		if (state)
+			break;
+	}
+
+	return state;
+}
+
+static size_t
+__mvsw_pr_nexthop_group_key_size(struct mvsw_pr_nexthop_group_key *key)
+{
+	size_t nh_cnt;
+
+	for (nh_cnt = 0; nh_cnt < MVSW_PR_NHGR_SIZE_MAX; nh_cnt++) {
+		if (!__mvsw_pr_nh_neigh_key_is_valid(&key->neigh[nh_cnt]))
+			break;
+	}
+
+	return nh_cnt;
+}
+
+static struct mvsw_pr_nexthop_group *
+__mvsw_pr_nexthop_group_create(struct mvsw_pr_switch *sw,
+			       struct mvsw_pr_nexthop_group_key *key)
+{
+	struct mvsw_pr_nexthop_group *nh_grp;
+	struct mvsw_pr_nh_neigh *nh_neigh;
+	int nh_cnt, err;
+
+	nh_grp = kzalloc(sizeof(*nh_grp), GFP_KERNEL);
+	if (!nh_grp)
+		goto err_kzalloc;
+
+	memcpy(&nh_grp->key, key, sizeof(*key));
+	for (nh_cnt = 0; nh_cnt < MVSW_PR_NHGR_SIZE_MAX; nh_cnt++) {
+		if (!__mvsw_pr_nh_neigh_key_is_valid(&nh_grp->key.neigh[nh_cnt])
+		   )
+			break;
+
+		nh_neigh = mvsw_pr_nh_neigh_get(sw, &nh_grp->key.neigh[nh_cnt]);
+		if (!nh_neigh)
+			goto err_nh_neigh_get;
+
+		nh_grp->nh_neigh_head[nh_cnt].neigh = nh_neigh;
+		nh_grp->nh_neigh_head[nh_cnt].this = nh_grp;
+		list_add(&nh_grp->nh_neigh_head[nh_cnt].head,
+			 &nh_neigh->nexthop_group_list);
+	}
+
+	err = mvsw_pr_nh_group_create(sw, nh_cnt, &nh_grp->grp_id);
+	if (err)
+		goto err_nh_group_create;
+
+	err = mvsw_pr_nexthop_group_set(sw, nh_grp);
+	if (err)
+		goto err_nexthop_group_set;
+
+	err = rhashtable_insert_fast(&sw->router->nexthop_group_ht,
+				     &nh_grp->ht_node,
+				     __mvsw_pr_nexthop_group_ht_params);
+	if (err)
+		goto err_ht_insert;
+
+	return nh_grp;
+
+err_ht_insert:
+err_nexthop_group_set:
+	mvsw_pr_nh_group_delete(sw, nh_cnt, nh_grp->grp_id);
+err_nh_group_create:
+err_nh_neigh_get:
+	for (nh_cnt--; nh_cnt >= 0; nh_cnt--) {
+		list_del(&nh_grp->nh_neigh_head[nh_cnt].head);
+		mvsw_pr_nh_neigh_put(sw, nh_grp->nh_neigh_head[nh_cnt].neigh);
+	}
+
+	kfree(nh_grp);
+err_kzalloc:
+	return NULL;
+}
+
+static void
+__mvsw_pr_nexthop_group_destroy(struct mvsw_pr_switch *sw,
+				struct mvsw_pr_nexthop_group *nh_grp)
+{
+	struct mvsw_pr_nh_neigh *nh_neigh;
+	int nh_cnt;
+
+	rhashtable_remove_fast(&sw->router->nexthop_group_ht,
+			       &nh_grp->ht_node,
+			       __mvsw_pr_nexthop_group_ht_params);
+
+	for (nh_cnt = 0; nh_cnt < MVSW_PR_NHGR_SIZE_MAX; nh_cnt++) {
+		nh_neigh = nh_grp->nh_neigh_head[nh_cnt].neigh;
+		if (!nh_neigh)
+			break;
+
+		list_del(&nh_grp->nh_neigh_head[nh_cnt].head);
+		mvsw_pr_nh_neigh_put(sw, nh_neigh);
+	}
+
+	mvsw_pr_nh_group_delete(sw, nh_cnt, nh_grp->grp_id);
+	kfree(nh_grp);
+}
+
+static struct mvsw_pr_nexthop_group *
+mvsw_pr_nexthop_group_find(struct mvsw_pr_switch *sw,
+			   struct mvsw_pr_nexthop_group_key *key)
+{
+	struct mvsw_pr_nexthop_group *nh_grp;
+
+	nh_grp = rhashtable_lookup_fast(&sw->router->nexthop_group_ht,
+					key, __mvsw_pr_nexthop_group_ht_params);
+	return IS_ERR(nh_grp) ? NULL : nh_grp;
+}
+
+static struct mvsw_pr_nexthop_group *
+mvsw_pr_nexthop_group_get(struct mvsw_pr_switch *sw,
+			  struct mvsw_pr_nexthop_group_key *key)
+{
+	struct mvsw_pr_nexthop_group *nh_grp;
+
+	nh_grp = mvsw_pr_nexthop_group_find(sw, key);
+	if (!nh_grp)
+		return __mvsw_pr_nexthop_group_create(sw, key);
+
+	return nh_grp;
+}
+
+static void mvsw_pr_nexthop_group_put(struct mvsw_pr_switch *sw,
+				      struct mvsw_pr_nexthop_group *nh_grp)
+{
+	if (!nh_grp->ref_cnt)
+		__mvsw_pr_nexthop_group_destroy(sw, nh_grp);
+}
+
+/* Updates with new nh_neigh's info */
+static int mvsw_pr_nexthop_group_set(struct mvsw_pr_switch *sw,
+				     struct mvsw_pr_nexthop_group *nh_grp)
+{
+	struct mvsw_pr_neigh_info info[MVSW_PR_NHGR_SIZE_MAX];
+	struct mvsw_pr_nh_neigh *neigh;
+	int nh_cnt;
+
+	memset(&info[0], 0, sizeof(info));
+	for (nh_cnt = 0; nh_cnt < MVSW_PR_NHGR_SIZE_MAX; nh_cnt++) {
+		neigh = nh_grp->nh_neigh_head[nh_cnt].neigh;
+		if (!neigh)
+			break;
+
+		memcpy(&info[nh_cnt], &neigh->info, sizeof(neigh->info));
+	}
+
+	return mvsw_pr_nh_entries_set(sw, nh_cnt, &info[0], nh_grp->grp_id);
+}
+
+static bool
+mvsw_pr_nexthop_group_util_hw_state(struct mvsw_pr_switch *sw,
+				    struct mvsw_pr_nexthop_group *nh_grp)
+{
+	int err, nh_cnt;
+	struct mvsw_pr_neigh_info info[MVSW_PR_NHGR_SIZE_MAX];
+	size_t grp_size;
+
+	/* Antijitter
+	 * Prevent situation, when we read state of nh_grp twice in short time,
+	 * and state bit is still cleared on second call. So just stuck active
+	 * state for MVSW_PR_NH_ACTIVE_JIFFER_FILTER, after last occurred.
+	 */
+	if (time_before(jiffies, nh_grp->hw_last_connected +
+			msecs_to_jiffies(MVSW_PR_NH_ACTIVE_JIFFER_FILTER)))
+		return true;
+
+	grp_size =  __mvsw_pr_nexthop_group_key_size(&nh_grp->key);
+	err = mvsw_pr_nh_entries_get(sw, grp_size, &info[0], nh_grp->grp_id);
+	if (err) {
+		MVSW_LOG_ERROR("Failed to get hw state of nh_grp %d",
+			       nh_grp->grp_id);
+		return false;
+	}
+
+	for (nh_cnt = 0; nh_cnt < grp_size; nh_cnt++)
+		if (info[nh_cnt].connected) {
+			nh_grp->hw_last_connected = jiffies;
+			return true;
+		}
+
+	return false;
+}
+
+static struct mvsw_pr_fib_node *
+mvsw_pr_fib_node_find(struct mvsw_pr_switch *sw, struct mvsw_pr_fib_key *key)
+{
+	struct mvsw_pr_fib_node *fib_node;
+
+	fib_node = rhashtable_lookup_fast(&sw->router->fib_ht, key,
+					  __mvsw_pr_fib_ht_params);
+	return IS_ERR(fib_node) ? NULL : fib_node;
+}
+
+static void __mvsw_pr_fib_node_destruct(struct mvsw_pr_switch *sw,
+					struct mvsw_pr_fib_node *fib_node)
+{
+	struct mvsw_pr_vr *vr;
+
+	vr = fib_node->info.vr;
+	mvsw_pr_lpm_del(sw, vr->hw_vr_id, &fib_node->key.addr,
+			fib_node->key.prefix_len);
+	switch (fib_node->info.type) {
+	case MVSW_PR_FIB_TYPE_UC_NH:
+		fib_node->info.nh_grp->ref_cnt--;
+		mvsw_pr_nexthop_group_put(sw, fib_node->info.nh_grp);
+		break;
+	case MVSW_PR_FIB_TYPE_TRAP:
+		break;
+	case MVSW_PR_FIB_TYPE_DROP:
+		break;
+	default:
+	      MVSW_LOG_ERROR("Unknown fib_node->info.type = %d",
+			     fib_node->info.type);
+	}
+
+	vr->ref_cnt--;
+	mvsw_pr_vr_put(sw, vr);
+}
+
+static void mvsw_pr_fib_node_destroy(struct mvsw_pr_switch *sw,
+				     struct mvsw_pr_fib_node *fib_node)
+{
+	__mvsw_pr_fib_node_destruct(sw, fib_node);
+	rhashtable_remove_fast(&sw->router->fib_ht, &fib_node->ht_node,
+			       __mvsw_pr_fib_ht_params);
+	kfree(fib_node);
+}
+
+static void mvsw_pr_fib_node_destroy_ht(struct mvsw_pr_switch *sw)
+{
+	struct mvsw_pr_fib_node *node, *tnode;
+	struct rhashtable_iter iter;
+
+	tnode = NULL;
+	rhashtable_walk_enter(&sw->router->fib_ht, &iter);
+	rhashtable_walk_start(&iter);
+	while (1) {
+		node = rhashtable_walk_next(&iter);
+		if (tnode) {
+			rhashtable_remove_fast(&sw->router->fib_ht,
+					       &tnode->ht_node,
+					       __mvsw_pr_fib_ht_params);
+			kfree(tnode);
+			tnode = NULL;
+		}
+
+		if (!node)
+			break;
+
+		if (IS_ERR(node))
+			continue;
+
+		__mvsw_pr_fib_node_destruct(sw, node);
+		tnode = node;
+	}
+	rhashtable_walk_stop(&iter);
+	rhashtable_walk_exit(&iter);
+}
+
+static struct mvsw_pr_fib_node *
+mvsw_pr_fib_node_uc_nh_create(struct mvsw_pr_switch *sw,
+			      struct mvsw_pr_fib_key *key,
+			      struct mvsw_pr_nexthop_group_key *nh_grp_key)
+{
+	struct mvsw_pr_fib_node *fib_node;
+	struct mvsw_pr_nexthop_group *nh_grp;
+	struct mvsw_pr_vr *vr;
+	int err;
+
+	fib_node = kzalloc(sizeof(*fib_node), GFP_KERNEL);
+	if (!fib_node)
+		goto err_kzalloc;
+
+	memcpy(&fib_node->key, key, sizeof(*key));
+	fib_node->info.type = MVSW_PR_FIB_TYPE_UC_NH;
+
+	vr = mvsw_pr_vr_get(sw, key->tb_id, NULL);
+	if (!vr)
+		goto err_vr_get;
+
+	fib_node->info.vr = vr;
+	vr->ref_cnt++;
+
+	nh_grp = mvsw_pr_nexthop_group_get(sw, nh_grp_key);
+	if (!nh_grp)
+		goto err_nh_grp_get;
+
+	fib_node->info.nh_grp = nh_grp;
+	nh_grp->ref_cnt++;
+
+	err = mvsw_pr_lpm_add(sw, vr->hw_vr_id, &key->addr,
+			      key->prefix_len, nh_grp->grp_id);
+	if (err)
+		goto err_lpm_add;
+
+	err = rhashtable_insert_fast(&sw->router->fib_ht, &fib_node->ht_node,
+				     __mvsw_pr_fib_ht_params);
+	if (err)
+		goto err_ht_insert;
+
+	return fib_node;
+
+err_ht_insert:
+	mvsw_pr_lpm_del(sw, vr->hw_vr_id, &key->addr, key->prefix_len);
+err_lpm_add:
+	nh_grp->ref_cnt--;
+	mvsw_pr_nexthop_group_put(sw, nh_grp);
+err_nh_grp_get:
+	vr->ref_cnt--;
+	mvsw_pr_vr_put(sw, vr);
+err_vr_get:
+	kfree(fib_node);
+err_kzalloc:
+	return NULL;
+}
+
+/* Decided, that uc_nh route with key==nh is obviously neighbour route */
+static bool
+mvsw_pr_fib_node_util_is_neighbour(struct mvsw_pr_fib_node *fib_node)
+{
+	if (fib_node->info.type != MVSW_PR_FIB_TYPE_UC_NH)
+		return false;
+
+	if (fib_node->info.nh_grp->nh_neigh_head[1].neigh)
+		return false;
+
+	if (!fib_node->info.nh_grp->nh_neigh_head[0].neigh)
+		return false;
+
+	if (memcmp(&fib_node->info.nh_grp->nh_neigh_head[0].neigh->key.addr,
+		   &fib_node->key.addr, sizeof(struct mvsw_pr_ip_addr)))
+		return false;
+
+	return true;
+}
+
+static struct mvsw_pr_fib_node *
+mvsw_pr_fib_node_trap_create(struct mvsw_pr_switch *sw,
+			     struct mvsw_pr_fib_key *key)
+{
+	struct mvsw_pr_fib_node *fib_node;
+	struct mvsw_pr_vr *vr;
+	int err;
+
+	fib_node = kzalloc(sizeof(*fib_node), GFP_KERNEL);
+	if (!fib_node)
+		goto err_kzalloc;
+
+	vr = mvsw_pr_vr_get(sw, key->tb_id, NULL);
+	if (!vr)
+		goto err_vr_get;
+
+	fib_node->info.vr = vr;
+	vr->ref_cnt++;
+
+	memcpy(&fib_node->key, key, sizeof(*key));
+	fib_node->info.type = MVSW_PR_FIB_TYPE_TRAP;
+	err = mvsw_pr_lpm_add(sw, vr->hw_vr_id, &key->addr,
+			      key->prefix_len, MVSW_PR_NHGR_UNUSED);
+	if (err)
+		goto err_lpm_add;
+
+	err = rhashtable_insert_fast(&sw->router->fib_ht, &fib_node->ht_node,
+				     __mvsw_pr_fib_ht_params);
+	if (err)
+		goto err_ht_insert;
+
+	return fib_node;
+
+err_ht_insert:
+	mvsw_pr_lpm_del(sw, vr->hw_vr_id, &key->addr, key->prefix_len);
+err_lpm_add:
+	vr->ref_cnt--;
+	mvsw_pr_vr_put(sw, vr);
+err_vr_get:
+	kfree(fib_node);
+err_kzalloc:
+	return NULL;
+}
+
+static struct mvsw_pr_fib_node *
+mvsw_pr_fib_node_drop_create(struct mvsw_pr_switch *sw,
+			     struct mvsw_pr_fib_key *key)
+{
+	struct mvsw_pr_fib_node *fib_node;
+	struct mvsw_pr_vr *vr;
+	int err;
+
+	fib_node = kzalloc(sizeof(*fib_node), GFP_KERNEL);
+	if (!fib_node)
+		goto err_kzalloc;
+
+	vr = mvsw_pr_vr_get(sw, key->tb_id, NULL);
+	if (!vr)
+		goto err_vr_get;
+
+	fib_node->info.vr = vr;
+	vr->ref_cnt++;
+
+	memcpy(&fib_node->key, key, sizeof(*key));
+	fib_node->info.type = MVSW_PR_FIB_TYPE_DROP;
+	err = mvsw_pr_lpm_add(sw, vr->hw_vr_id,
+			      &key->addr, key->prefix_len, MVSW_PR_NHGR_DROP);
+	if (err)
+		goto err_lpm_add;
+
+	err = rhashtable_insert_fast(&sw->router->fib_ht, &fib_node->ht_node,
+				     __mvsw_pr_fib_ht_params);
+	if (err)
+		goto err_ht_insert;
+
+	return fib_node;
+
+err_ht_insert:
+	mvsw_pr_lpm_del(sw, vr->hw_vr_id,
+			&key->addr, key->prefix_len);
+err_lpm_add:
+	vr->ref_cnt--;
+	mvsw_pr_vr_put(sw, vr);
+err_vr_get:
+	kfree(fib_node);
+err_kzalloc:
+	return NULL;
+}
+
+static void mvsw_pr_fib_nh_del2nh_neigh_set(struct mvsw_pr_switch *sw,
+					    struct fib_nh *fib_nh)
+{
+	struct mvsw_pr_nh_neigh_key nh_neigh_key;
+	struct mvsw_pr_nh_neigh *nh_neigh;
+
+	memset(&nh_neigh_key, 0, sizeof(nh_neigh_key));
+	nh_neigh_key.addr.u.ipv4 = fib_nh->fib_nh_gw4;
+	nh_neigh_key.rif = mvsw_pr_rif_find(sw, fib_nh->fib_nh_dev);
+	if (!nh_neigh_key.rif)
+		return;
+
+	nh_neigh = mvsw_pr_nh_neigh_find(sw, &nh_neigh_key);
+	if (!nh_neigh)
+		return;
+
+	nh_neigh->info.connected = false;
+	mvsw_pr_nh_neigh_set(sw, nh_neigh);
+}
+
+static void __mvsw_pr_fen_info2fib_key(struct fib_entry_notifier_info *fen_info,
+				       struct mvsw_pr_fib_key *key)
+{
+	memset(key, 0, sizeof(*key));
+	key->addr.u.ipv4 = cpu_to_be32(fen_info->dst);
+	key->prefix_len = fen_info->dst_len;
+	key->tb_id = mvsw_pr_fix_tb_id(fen_info->tb_id);
+}
+
+static int
+mvsw_pr_fi2nh_gr_key(struct mvsw_pr_switch *sw, struct fib_info *fi,
+		     size_t limit, struct mvsw_pr_nexthop_group_key *grp_key)
+{
+	int i, nhs, err;
+	struct fib_nh *fib_nh;
+
+	nhs = fib_info_num_path(fi);
+	if (nhs > limit)
+		return 0;
+
+	memset(grp_key, 0, sizeof(*grp_key));
+	for (i = 0; i < nhs; i++) {
+		fib_nh = fib_info_nh(fi, i);
+		err = mvsw_pr_util_fib_nh2nh_neigh_key(sw,
+						       fib_nh,
+						       &grp_key->neigh[i]);
+		if (err)
+			return 0;
+	}
+
+	return nhs;
+}
+
+static int
+mvsw_pr_router_fib_replace(struct mvsw_pr_switch *sw,
+			   struct fib_entry_notifier_info *fen_info)
+{
+	struct mvsw_pr_fib_node *fib_node;
+	struct mvsw_pr_fib_key fib_key;
+	struct mvsw_pr_nexthop_group_key nh_grp_key;
+	int nh_cnt;
+
+	__mvsw_pr_fen_info2fib_key(fen_info, &fib_key);
+
+	fib_node = mvsw_pr_fib_node_find(sw, &fib_key);
+	if (fib_node) {
+		MVSW_LOG_INFO("fib_node found. destroy.");
+		mvsw_pr_fib_node_destroy(sw, fib_node);
+	}
+
+	/* TODO: fib lookup here to check if another route with the same key
+	 * is occurred in another kernel's table
+	 */
+	switch (fen_info->fi->fib_type) {
+	case RTN_UNICAST:
+		if (mvsw_pr_fi_is_nh(fen_info->fi))
+			nh_cnt = mvsw_pr_fi2nh_gr_key(sw, fen_info->fi,
+						      MVSW_PR_NHGR_SIZE_MAX,
+						      &nh_grp_key);
+		else
+			nh_cnt = 0;
+
+		if (nh_cnt) {
+			fib_node = mvsw_pr_fib_node_uc_nh_create(sw, &fib_key,
+								 &nh_grp_key);
+		} else {
+			fib_node = mvsw_pr_fib_node_trap_create(sw, &fib_key);
+		}
+
+		if (!fib_node)
+			goto err_fib_create;
+
+		break;
+	/* Unsupported. Leave it for kernel: */
+	case RTN_BROADCAST:
+	case RTN_MULTICAST:
+	/* Routes we must trap by design: */
+	case RTN_LOCAL:
+	case RTN_UNREACHABLE:
+	case RTN_PROHIBIT:
+		fib_node = mvsw_pr_fib_node_trap_create(sw, &fib_key);
+		if (!fib_node)
+			goto err_fib_create;
+		break;
+	case RTN_BLACKHOLE:
+		fib_node = mvsw_pr_fib_node_drop_create(sw, &fib_key);
+		if (!fib_node)
+			goto err_fib_create;
+
+		break;
+	default:
+		goto err_type;
+	}
+
+	mvsw_pr_neigh_arbiter_fib_evt(sw, true, &fib_key, fen_info->fi);
+
+	return 0;
+
+err_fib_create:
+	MVSW_LOG_ERROR("fib_create failed");
+	goto err_out;
+err_type:
+	MVSW_LOG_ERROR("Invalid fen_info->fi->fib_type %d",
+		       fen_info->fi->fib_type);
+	goto err_out;
+err_out:
+	MVSW_LOG_ERROR("Error when processing %pI4h/%d", &fen_info->dst,
+		       fen_info->dst_len);
+	return -EINVAL;
+}
+
+static void mvsw_pr_router_fib_del(struct mvsw_pr_switch *sw,
+				   struct fib_entry_notifier_info *fen_info)
+{
+	struct mvsw_pr_fib_node *fib_node;
+	struct mvsw_pr_fib_key fib_key;
+
+	/* TODO: fib lookup here to check if another route with the same key
+	 * is occurred in another kernel's table
+	 */
+	__mvsw_pr_fen_info2fib_key(fen_info, &fib_key);
+
+	fib_node = mvsw_pr_fib_node_find(sw, &fib_key);
+	if (fib_node) {
+		mvsw_pr_fib_node_destroy(sw, fib_node);
+	} else {
+		MVSW_LOG_ERROR("Cant find fib_node");
+		goto err_out;
+	}
+
+	mvsw_pr_neigh_arbiter_fib_evt(sw, false, &fib_key, fen_info->fi);
+
+	return;
+
+err_out:
+	MVSW_LOG_ERROR("Error when processing %pI4h/%d", &fen_info->dst,
+		       fen_info->dst_len);
+}
+
+static void mvsw_pr_router_fib_abort(struct mvsw_pr_switch *sw)
+{
+	mvsw_pr_vr_util_hw_abort(sw);
+	mvsw_pr_fib_node_destroy_ht(sw);
+	mvsw_pr_kern_fib_cache_destroy_ht(sw);
+	mvsw_pr_util_kern_unset_allneigh_offload();
+}
+
+struct mvsw_pr_fib_event_work {
+	struct work_struct work;
+	struct mvsw_pr_switch *sw;
+	union {
+		struct fib_entry_notifier_info fen_info;
+		struct fib_nh_notifier_info fnh_info;
+	};
+	unsigned long event;
+};
+
+static void mvsw_pr_router_fib4_event_work(struct work_struct *work)
+{
+	struct mvsw_pr_fib_event_work *fib_work =
+			container_of(work, struct mvsw_pr_fib_event_work, work);
+	struct mvsw_pr_switch *sw = fib_work->sw;
+	int err;
+
+	mvsw_owq_lock();
+
+	if (sw->router->aborted)
+		goto out;
+
+	switch (fib_work->event) {
+	case FIB_EVENT_ENTRY_REPLACE:
+		err = mvsw_pr_router_fib_replace(sw,
+						 &fib_work->fen_info);
+		if (err)
+			goto abort_out;
+		break;
+	case FIB_EVENT_ENTRY_DEL:
+		mvsw_pr_router_fib_del(sw, &fib_work->fen_info);
+		break;
+	}
+
+	goto out;
+
+abort_out:
+	sw->router->aborted = true;
+	mvsw_pr_router_fib_abort(sw);
+	dev_err(sw->dev->dev, "Abort. HW routing offloading disabled");
+out:
+	fib_info_put(fib_work->fen_info.fi);
+	mvsw_owq_unlock();
+	kfree(fib_work);
+}
+
+static void mvsw_pr_router_nh_update_event_work(struct work_struct *work)
+{
+	struct mvsw_pr_fib_event_work *fib_work =
+			container_of(work, struct mvsw_pr_fib_event_work, work);
+	struct mvsw_pr_switch *sw = fib_work->sw;
+	struct fib_nh *fib_nh = fib_work->fnh_info.fib_nh;
+
+	mvsw_owq_lock();
+
+	if (sw->router->aborted)
+		goto out;
+
+	/* For now provided only deletion */
+	if (fib_work->event == FIB_EVENT_NH_DEL)
+		mvsw_pr_fib_nh_del2nh_neigh_set(sw, fib_nh);
+
+out:
+	fib_info_put(fib_nh->nh_parent);
+	mvsw_owq_unlock();
+	kfree(fib_work);
+	return;
+}
+
+/* Called with rcu_read_lock() */
+static int mvsw_pr_router_fib_event(struct notifier_block *nb,
+				    unsigned long event, void *ptr)
+{
+	struct fib_entry_notifier_info *fen_info;
+	struct fib_nh_notifier_info *fnh_info;
+	struct fib_notifier_info *info = ptr;
+	struct mvsw_pr_fib_event_work *fib_work;
+	struct mvsw_pr_router *router;
+	struct fib_info *fi;
+
+	if (info->family != AF_INET)
+		return NOTIFY_DONE;
+
+	router = container_of(nb, struct mvsw_pr_router, fib_nb);
+
+	switch (event) {
+	case FIB_EVENT_ENTRY_REPLACE:
+	case FIB_EVENT_ENTRY_DEL:
+		fen_info = container_of(info, struct fib_entry_notifier_info,
+					info);
+		if (!fen_info->fi)
+			return NOTIFY_DONE;
+		else
+			fi = fen_info->fi;
+
+		/* Sanity */
+		if (event == FIB_EVENT_ENTRY_REPLACE) {
+			if (fi->nh)
+				return notifier_from_errno(-EINVAL);
+
+			if (fi->fib_nh_is_v6)
+				return notifier_from_errno(-EINVAL);
+
+			if (fib_info_num_path(fi) > MVSW_PR_NHGR_SIZE_MAX) {
+				NL_SET_ERR_MSG_MOD(info->extack,
+						   "Exceeded number of nexthops per route"
+						   );
+				return notifier_from_errno(-EINVAL);
+			}
+		}
+
+		fib_work = kzalloc(sizeof(*fib_work), GFP_ATOMIC);
+		if (WARN_ON(!fib_work))
+			return NOTIFY_BAD;
+
+		fib_info_hold(fi);
+		fib_work->fen_info = *fen_info;
+		fib_work->event = event;
+		fib_work->sw = router->sw;
+		INIT_WORK(&fib_work->work, mvsw_pr_router_fib4_event_work);
+		queue_work(mvsw_r_owq, &fib_work->work);
+		break;
+	case FIB_EVENT_NH_DEL:
+		/* Set down nh as fast as possible */
+		fnh_info = container_of(info, struct fib_nh_notifier_info,
+					info);
+		if (!fnh_info->fib_nh->nh_parent)
+			return NOTIFY_DONE;
+
+		fi = fnh_info->fib_nh->nh_parent;
+
+		fib_work = kzalloc(sizeof(*fib_work), GFP_ATOMIC);
+		if (WARN_ON(!fib_work))
+			return NOTIFY_BAD;
+
+		fib_info_hold(fi);
+		fib_work->fnh_info = *fnh_info;
+		fib_work->event = event;
+		fib_work->sw = router->sw;
+		INIT_WORK(&fib_work->work, mvsw_pr_router_nh_update_event_work);
+		queue_work(mvsw_r_owq, &fib_work->work);
+		break;
+	default:
+		return NOTIFY_DONE;
+	}
+
+	return NOTIFY_DONE;
+}
+
+static void mvsw_pr_router_fib_dump_flush(struct notifier_block *nb)
+{
+	struct mvsw_pr_router *router;
+
+	/* Flush pending FIB notifications and then flush the device's
+	 * table before requesting another dump. The FIB notification
+	 * block is unregistered, so no need to take RTNL.
+	 * No neighbours are expected to be present since FIBs  are not
+	 * registered yet
+	 */
+	router = container_of(nb, struct mvsw_pr_router, fib_nb);
+	flush_workqueue(mvsw_r_owq);
+	flush_workqueue(mvsw_r_wq);
+	mvsw_pr_fib_node_destroy_ht(router->sw);
+}
+
+static int
+mvsw_pr_router_port_change(struct mvsw_pr_rif *rif)
+{
+	struct net_device *dev = rif->dev;
+	int err;
+
+	err = mvsw_pr_rif_update(rif, dev->dev_addr);
+	if (err)
+		return err;
+
+	ether_addr_copy(rif->addr, dev->dev_addr);
+	rif->mtu = dev->mtu;
+
+	netdev_dbg(dev, "Updated RIF=%d\n", rif->rif_id);
+
+	return 0;
+}
+
+static int
+mvsw_pr_router_port_pre_change(struct mvsw_pr_rif *rif,
+			       struct netdev_notifier_pre_changeaddr_info *info)
+{
+	struct netlink_ext_ack *extack;
+
+	extack = netdev_notifier_info_to_extack(&info->info);
+	return mvsw_pr_router_port_check_rif_addr(rif->sw, rif->dev,
+						  info->dev_addr, extack);
+}
+
+int mvsw_pr_netdevice_router_port_event(struct net_device *dev,
+					unsigned long event, void *ptr)
+{
+	struct mvsw_pr_switch *sw;
+	struct mvsw_pr_rif *rif;
+
+	sw = mvsw_pr_switch_get(dev);
+	if (!sw)
+		return 0;
+
+	rif = mvsw_pr_rif_find(sw, dev);
+	if (!rif)
+		return 0;
+
+	switch (event) {
+	case NETDEV_CHANGEADDR:
+		return mvsw_pr_router_port_change(rif);
+	case NETDEV_PRE_CHANGEADDR:
+		return mvsw_pr_router_port_pre_change(rif, ptr);
+	}
+
+	return 0;
+}
+
+static int mvsw_pr_port_vrf_join(struct mvsw_pr_switch *sw,
+				 struct net_device *dev,
+				 struct netlink_ext_ack *extack)
+{
+	struct mvsw_pr_rif *rif;
+
+	/* If netdev is already associated with a RIF, then we need to
+	 * destroy it and create a new one with the new virtual router ID.
+	 */
+	rif = mvsw_pr_rif_find(sw, dev);
+	if (rif)
+		__mvsw_pr_inetaddr_event(sw, dev, NETDEV_DOWN, extack);
+
+	__mvsw_pr_inetaddr_event(sw, dev, NETDEV_UP, extack);
+	rif = mvsw_pr_rif_find(sw, dev);
+	return mvsw_pr_rif_vr_update(sw, rif, extack);
+}
+
+static void mvsw_pr_port_vrf_leave(struct mvsw_pr_switch *sw,
+				   struct net_device *dev,
+				   struct netlink_ext_ack *extack)
+{
+	struct mvsw_pr_rif *rif;
+	struct in_device *idev;
+
+	rif = mvsw_pr_rif_find(sw, dev);
+	if (!rif)
+		return;
+
+	__mvsw_pr_inetaddr_event(sw, dev, NETDEV_DOWN, NULL);
+
+	rif = mvsw_pr_rif_find(sw, dev);
+	if (rif)
+		mvsw_pr_rif_vr_update(sw, rif, extack);
+
+	idev = __in_dev_get_rtnl(dev);
+	/* Restore rif in the default vrf: do so only if IF address's present*/
+	if (idev && idev->ifa_list)
+		__mvsw_pr_inetaddr_event(sw, dev, NETDEV_UP, NULL);
+}
+
+int mvsw_pr_netdevice_vrf_event(struct net_device *dev, unsigned long event,
+				struct netdev_notifier_changeupper_info *info)
+{
+	struct mvsw_pr_switch *sw = mvsw_pr_switch_get(dev);
+	struct netlink_ext_ack *extack = NULL;
+	int err = 0;
+
+	if (!sw || netif_is_macvlan(dev))
+		return 0;
+
+	switch (event) {
+	case NETDEV_PRECHANGEUPPER:
+		return 0;
+	case NETDEV_CHANGEUPPER:
+		extack = netdev_notifier_info_to_extack(&info->info);
+		if (info->linking)
+			err = mvsw_pr_port_vrf_join(sw, dev, extack);
+		else
+			mvsw_pr_port_vrf_leave(sw, dev, extack);
+		break;
+	}
+
+	return err;
+}
+
+static int __mvsw_pr_rif_macvlan_flush(struct net_device *dev,
+				       struct netdev_nested_priv *priv)
+{
+	struct mvsw_pr_rif *rif = priv->data;
+
+	if (!netif_is_macvlan(dev))
+		return 0;
+
+	return mvsw_pr_rif_fdb_op(rif, dev->dev_addr, false);
+}
+
+static int mvsw_pr_rif_macvlan_flush(struct mvsw_pr_rif *rif)
+{
+	struct netdev_nested_priv priv = {
+		.data = (void *)rif,
+	};
+
+	if (!netif_is_macvlan_port(rif->dev))
+		return 0;
+
+	netdev_warn(rif->dev,
+		    "Router interface is deleted. Upper macvlans will not work\n");
+	return netdev_walk_all_upper_dev_rcu(rif->dev,
+					     __mvsw_pr_rif_macvlan_flush, &priv);
+}
+
+#ifdef CONFIG_IP_ROUTE_MULTIPATH
+static int mvsw_pr_mp_hash_init(struct mvsw_pr_switch *sw)
+{
+	u8 hash_policy;
+
+	hash_policy = init_net.ipv4.sysctl_fib_multipath_hash_policy;
+	return  mvsw_pr_mp4_hash_set(sw, hash_policy);
+}
+#else
+static int mvsw_pr_mp_hash_init(struct mvsw_pr_switch *sw)
+{
+	return 0;
+}
+#endif
+
+static struct notifier_block mvsw_pr_inetaddr_valid_nb __read_mostly = {
+	.notifier_call = mvsw_pr_inetaddr_valid_event,
+};
+
+int mvsw_pr_router_init(struct mvsw_pr_switch *sw)
+{
+	struct mvsw_pr_router *router;
+	int err;
+
+	router = kzalloc(sizeof(*sw->router), GFP_KERNEL);
+	if (!router)
+		return -ENOMEM;
+	sw->router = router;
+	router->sw = sw;
+
+	err = mvsw_pr_mp_hash_init(sw);
+	if (err)
+		goto err_mp_hash_init;
+
+	err = rhashtable_init(&router->nexthop_group_ht,
+			      &__mvsw_pr_nexthop_group_ht_params);
+	if (err)
+		goto err_nexthop_grp_ht_init;
+
+	err = rhashtable_init(&router->fib_ht,
+			      &__mvsw_pr_fib_ht_params);
+	if (err)
+		goto err_fib_ht_init;
+
+	err = rhashtable_init(&router->kern_fib_cache_ht,
+			      &__mvsw_pr_kern_fib_cache_ht_params);
+	if (err)
+		goto err_kern_fib_cache_ht_init;
+
+	err = rhashtable_init(&router->kern_neigh_cache_ht,
+			      &__mvsw_pr_kern_neigh_cache_ht_params);
+	if (err)
+		goto err_kern_neigh_cache_ht_init;
+
+	INIT_LIST_HEAD(&sw->router->rif_list);
+	INIT_LIST_HEAD(&sw->router->vr_list);
+
+	mvsw_r_wq = alloc_workqueue(mvsw_driver_name, 0, 0);
+	if (!mvsw_r_wq) {
+		err = -ENOMEM;
+		goto err_alloc_workqueue;
+	}
+
+	mvsw_r_owq = alloc_ordered_workqueue("%s_ordered", 0, "mvsw_prestera");
+	if (!mvsw_r_owq) {
+		err = -ENOMEM;
+		goto err_alloc_oworkqueue;
+	}
+
+	err = register_inetaddr_validator_notifier(&mvsw_pr_inetaddr_valid_nb);
+	if (err)
+		goto err_register_inetaddr_validator_notifier;
+
+	router->inetaddr_nb.notifier_call = mvsw_pr_inetaddr_event;
+	err = register_inetaddr_notifier(&router->inetaddr_nb);
+	if (err)
+		goto err_register_inetaddr_notifier;
+
+	err = mvsw_pr_neigh_init(sw);
+	if (err)
+		goto err_neigh_init;
+
+	sw->router->netevent_nb.notifier_call = mvsw_pr_router_netevent_event;
+	err = register_netevent_notifier(&sw->router->netevent_nb);
+	if (err)
+		goto err_register_netevent_notifier;
+
+	sw->router->fib_nb.notifier_call = mvsw_pr_router_fib_event;
+	err = register_fib_notifier(&init_net, &sw->router->fib_nb,
+				    mvsw_pr_router_fib_dump_flush, NULL);
+	if (err)
+		goto err_register_fib_notifier;
+
+	return 0;
+
+err_register_fib_notifier:
+	unregister_netevent_notifier(&sw->router->netevent_nb);
+err_register_netevent_notifier:
+	mvsw_pr_neigh_fini(sw);
+err_neigh_init:
+	unregister_inetaddr_notifier(&router->inetaddr_nb);
+err_register_inetaddr_notifier:
+	unregister_inetaddr_validator_notifier(&mvsw_pr_inetaddr_valid_nb);
+err_register_inetaddr_validator_notifier:
+	destroy_workqueue(mvsw_r_owq);
+err_alloc_oworkqueue:
+	destroy_workqueue(mvsw_r_wq);
+err_alloc_workqueue:
+	rhashtable_destroy(&router->kern_neigh_cache_ht);
+err_kern_neigh_cache_ht_init:
+	rhashtable_destroy(&router->kern_fib_cache_ht);
+err_kern_fib_cache_ht_init:
+	rhashtable_destroy(&router->fib_ht);
+err_fib_ht_init:
+	rhashtable_destroy(&router->nexthop_group_ht);
+err_nexthop_grp_ht_init:
+err_mp_hash_init:
+	kfree(sw->router);
+	return err;
+}
+
+static void mvsw_pr_rifs_fini(struct mvsw_pr_switch *sw)
+{
+	struct mvsw_pr_rif *rif, *tmp;
+
+	list_for_each_entry_safe(rif, tmp, &sw->router->rif_list, router_node) {
+		rif->is_active = false;
+		mvsw_pr_rif_destroy(rif);
+	}
+}
+
+void mvsw_pr_router_fini(struct mvsw_pr_switch *sw)
+{
+	unregister_fib_notifier(&init_net, &sw->router->fib_nb);
+	unregister_netevent_notifier(&sw->router->netevent_nb);
+	mvsw_pr_neigh_fini(sw);
+	/* TODO: check if vrs necessary ? */
+	mvsw_pr_rifs_fini(sw);
+	unregister_inetaddr_notifier(&sw->router->inetaddr_nb);
+	unregister_inetaddr_validator_notifier(&mvsw_pr_inetaddr_valid_nb);
+
+	rhashtable_destroy(&sw->router->fib_ht);
+	rhashtable_destroy(&sw->router->nexthop_group_ht);
+
+	flush_workqueue(mvsw_r_wq);
+	flush_workqueue(mvsw_r_owq);
+	destroy_workqueue(mvsw_r_wq);
+	destroy_workqueue(mvsw_r_owq);
+
+	WARN_ON(!list_empty(&sw->router->rif_list));
+
+	kfree(sw->router);
+	sw->router = NULL;
+}
+
+static u32 mvsw_pr_fix_tb_id(u32 tb_id)
+{
+	if (tb_id == RT_TABLE_UNSPEC ||
+	    tb_id == RT_TABLE_LOCAL ||
+	    tb_id == RT_TABLE_DEFAULT)
+		return tb_id = RT_TABLE_MAIN;
+
+	return tb_id;
+}
+
+static struct mvsw_pr_vr *__mvsw_pr_vr_find(struct mvsw_pr_switch *sw,
+					    u32 tb_id)
+{
+	struct mvsw_pr_vr *vr;
+
+	list_for_each_entry(vr, &sw->router->vr_list, router_node) {
+		if (vr->tb_id == tb_id)
+			return vr;
+	}
+
+	return NULL;
+}
+
+static struct mvsw_pr_vr *__mvsw_pr_vr_create(struct mvsw_pr_switch *sw,
+					      u32 tb_id,
+					      struct netlink_ext_ack *extack)
+{
+	struct mvsw_pr_vr *vr;
+	u16 hw_vr_id;
+	int err;
+
+	err = mvsw_pr_hw_vr_create(sw, &hw_vr_id);
+	if (err)
+		return ERR_PTR(-ENOMEM);
+
+	vr = kzalloc(sizeof(*vr), GFP_KERNEL);
+	if (!vr) {
+		err = -ENOMEM;
+		goto err_alloc_vr;
+	}
+
+	vr->tb_id = tb_id;
+	vr->hw_vr_id = hw_vr_id;
+
+	list_add(&vr->router_node, &sw->router->vr_list);
+
+	return vr;
+
+err_alloc_vr:
+	mvsw_pr_hw_vr_delete(sw, hw_vr_id);
+	kfree(vr);
+	return ERR_PTR(err);
+}
+
+static void __mvsw_pr_vr_destroy(struct mvsw_pr_switch *sw,
+				 struct mvsw_pr_vr *vr)
+{
+	mvsw_pr_hw_vr_delete(sw, vr->hw_vr_id);
+	list_del(&vr->router_node);
+	kfree(vr);
+}
+
+static struct mvsw_pr_vr *mvsw_pr_vr_get(struct mvsw_pr_switch *sw, u32 tb_id,
+					 struct netlink_ext_ack *extack)
+{
+	struct mvsw_pr_vr *vr;
+
+	vr = __mvsw_pr_vr_find(sw, tb_id);
+	if (!vr)
+		vr = __mvsw_pr_vr_create(sw, tb_id, extack);
+	if (IS_ERR(vr))
+		return ERR_CAST(vr);
+
+	return vr;
+}
+
+static void mvsw_pr_vr_put(struct mvsw_pr_switch *sw, struct mvsw_pr_vr *vr)
+{
+	if (!vr->ref_cnt)
+		__mvsw_pr_vr_destroy(sw, vr);
+}
+
+static void mvsw_pr_vr_util_hw_abort(struct mvsw_pr_switch *sw)
+{
+	struct mvsw_pr_vr *vr, *vr_tmp;
+
+	list_for_each_entry_safe(vr, vr_tmp,
+				 &sw->router->vr_list, router_node)
+		mvsw_pr_hw_vr_abort(sw, vr->hw_vr_id);
+}
+
+static struct mvsw_pr_rif*
+mvsw_pr_rif_alloc(struct mvsw_pr_switch *sw,
+		  struct mvsw_pr_vr *vr,
+		  const struct mvsw_pr_rif_params *params)
+{
+	struct mvsw_pr_rif *rif;
+	int err;
+
+	rif = kzalloc(sizeof(*rif), GFP_KERNEL);
+	if (!rif) {
+		err = -ENOMEM;
+		goto err_rif_alloc;
+	}
+
+	rif->sw = sw;
+	rif->vr = vr;
+	rif->dev = params->dev;
+	err = mvsw_pr_rif_iface_init(rif);
+	if (err)
+		goto err_rif_iface_init;
+
+	ether_addr_copy(rif->addr, params->dev->dev_addr);
+	rif->mtu = params->dev->mtu;
+
+	return rif;
+
+err_rif_iface_init:
+	kfree(rif);
+err_rif_alloc:
+	return ERR_PTR(err);
+}
+
+static int mvsw_pr_rif_offload(struct mvsw_pr_rif *rif)
+{
+	return mvsw_pr_hw_rif_create(rif->sw, &rif->iface, rif->addr,
+				     &rif->rif_id);
+}
+
+static struct mvsw_pr_rif *mvsw_pr_rif_create(struct mvsw_pr_switch *sw,
+					      const struct mvsw_pr_rif_params
+					      *params,
+					      struct netlink_ext_ack *extack)
+{
+	u32 tb_id = mvsw_pr_fix_tb_id(l3mdev_fib_table(params->dev));
+	struct mvsw_pr_rif *rif;
+	struct mvsw_pr_vr *vr;
+	int err;
+
+	vr = mvsw_pr_vr_get(sw, tb_id, extack);
+	if (IS_ERR(vr))
+		return ERR_CAST(vr);
+
+	rif = mvsw_pr_rif_alloc(sw, vr, params);
+	if (IS_ERR(rif)) {
+		mvsw_pr_vr_put(sw, vr);
+		return rif;
+	}
+
+	err = mvsw_pr_rif_offload(rif);
+	if (err)  {
+		NL_SET_ERR_MSG_MOD(extack,
+				   "Exceeded number of supported rifs");
+		goto err_rif_offload;
+	}
+
+	vr->ref_cnt++;
+	dev_hold(rif->dev);
+	list_add(&rif->router_node, &sw->router->rif_list);
+
+	return rif;
+
+err_rif_offload:
+	kfree(rif);
+	return ERR_PTR(err);
+}
+
+static int mvsw_pr_rif_delete(struct mvsw_pr_rif *rif)
+{
+	return mvsw_pr_hw_rif_delete(rif->sw, rif->rif_id, &rif->iface);
+}
+
+static void mvsw_pr_rif_destroy(struct mvsw_pr_rif *rif)
+{
+	mvsw_pr_rif_macvlan_flush(rif);
+	if (!rif->is_active) {
+		mvsw_pr_rif_delete(rif);
+		list_del(&rif->router_node);
+		dev_put(rif->dev);
+		rif->vr->ref_cnt--;
+		mvsw_pr_vr_put(rif->sw, rif->vr);
+		kfree(rif);
+	}
+}
+
+static void mvsw_pr_rif_put(struct mvsw_pr_rif *rif)
+{
+	if (!rif->ref_cnt)
+		mvsw_pr_rif_destroy(rif);
+}
+
+void mvsw_pr_rif_enable(struct mvsw_pr_switch *sw,
+			struct net_device *dev, bool enable)
+{
+	struct mvsw_pr_rif *rif;
+
+	rif = mvsw_pr_rif_find(sw, dev);
+	if (!rif)
+		return;
+
+	if (enable)
+		mvsw_pr_rif_offload(rif);
+	else
+		mvsw_pr_rif_delete(rif);
+}
+
+static int mvsw_pr_rif_update(struct mvsw_pr_rif *rif, char *mac)
+{
+	return mvsw_pr_hw_rif_set(rif->sw, &rif->rif_id, &rif->iface, mac);
+}
+
+static int mvsw_pr_rif_vr_update(struct mvsw_pr_switch *sw,
+				 struct mvsw_pr_rif *rif,
+				 struct netlink_ext_ack *extack)
+{
+	u32 tb_id = mvsw_pr_fix_tb_id(l3mdev_fib_table(rif->dev));
+	struct mvsw_pr_vr *vr;
+
+	rif->vr->ref_cnt--;
+	mvsw_pr_rif_delete(rif);
+	mvsw_pr_vr_put(sw, rif->vr);
+	vr = mvsw_pr_vr_get(sw, tb_id, extack);
+	if (IS_ERR(vr))
+		return PTR_ERR(vr);
+	rif->vr = vr;
+	mvsw_pr_rif_iface_init(rif);
+	mvsw_pr_rif_offload(rif);
+	rif->vr->ref_cnt++;
+
+	return 0;
+}
+
+void mvsw_pr_router_lag_member_leave(const struct mvsw_pr_port *port,
+				     const struct net_device *dev)
+{
+	struct mvsw_pr_rif *rif;
+	u16 vr_id;
+
+	rif = mvsw_pr_rif_find(port->sw, dev);
+	if (!rif)
+		return;
+
+	vr_id = mvsw_pr_rif_vr_id(rif);
+	prestera_lag_member_rif_leave(port, port->lag_id, vr_id);
+}
+
+void prestera_lag_router_leave(struct mvsw_pr_switch *sw,
+			       struct net_device *lag_dev)
+{
+	struct mvsw_pr_rif *rif;
+
+	rif = mvsw_pr_rif_find(sw, lag_dev);
+	if (rif) {
+		rif->is_active = false;
+		mvsw_pr_rif_put(rif);
+	}
+}
+
+static int mvsw_pr_bridge_device_rif_put(struct net_device *bridge_dev,
+					struct netdev_nested_priv *priv)
+{
+	struct mvsw_pr_rif *rif;
+	struct mvsw_pr_switch *sw = priv->data;
+
+	rif = mvsw_pr_rif_find(sw, bridge_dev);
+	if (rif) {
+		rif->is_active = false;
+		mvsw_pr_rif_put(rif);
+	}
+
+	return 0;
+}
+
+void mvsw_pr_bridge_device_rifs_destroy(struct mvsw_pr_switch *sw,
+					struct net_device *bridge_dev)
+{
+	struct netdev_nested_priv priv = {
+		.data = (void *)sw,
+	};
+	mvsw_pr_bridge_device_rif_put(bridge_dev, &priv);
+	netdev_walk_all_upper_dev_rcu(bridge_dev,
+				      mvsw_pr_bridge_device_rif_put,
+				      &priv);
+}
diff --git a/drivers/net/ethernet/marvell/prestera/prestera_rxtx.c b/drivers/net/ethernet/marvell/prestera/prestera_rxtx.c
index 2a13c3180..50b811ebf 100644
--- a/drivers/net/ethernet/marvell/prestera/prestera_rxtx.c
+++ b/drivers/net/ethernet/marvell/prestera/prestera_rxtx.c
@@ -1,820 +1,222 @@
 // SPDX-License-Identifier: BSD-3-Clause OR GPL-2.0
-/* Copyright (c) 2019-2020 Marvell International Ltd. All rights reserved */
-
-#include <linux/bitfield.h>
-#include <linux/dmapool.h>
-#include <linux/etherdevice.h>
-#include <linux/if_vlan.h>
-#include <linux/of_address.h>
-#include <linux/of_device.h>
-#include <linux/of.h>
-#include <linux/platform_device.h>
-
-#include "prestera_dsa.h"
+/*
+ * Copyright (c) 2019-2020 Marvell International Ltd. All rights reserved.
+ *
+ */
 #include "prestera.h"
-#include "prestera_hw.h"
-#include "prestera_rxtx.h"
-
-#define PRESTERA_SDMA_WAIT_MUL		10
-
-struct prestera_sdma_desc {
-	__le32 word1;
-	__le32 word2;
-	__le32 buff;
-	__le32 next;
-} __packed __aligned(16);
-
-#define PRESTERA_SDMA_BUFF_SIZE_MAX	1544
-
-#define PRESTERA_SDMA_RX_DESC_PKT_LEN(desc) \
-	((le32_to_cpu((desc)->word2) >> 16) & GENMASK(13, 0))
-
-#define PRESTERA_SDMA_RX_DESC_OWNER(desc) \
-	((le32_to_cpu((desc)->word1) & BIT(31)) >> 31)
-
-#define PRESTERA_SDMA_RX_DESC_IS_RCVD(desc) \
-	(PRESTERA_SDMA_RX_DESC_OWNER(desc) == PRESTERA_SDMA_RX_DESC_CPU_OWN)
-
-#define PRESTERA_SDMA_RX_DESC_CPU_OWN	0
-#define PRESTERA_SDMA_RX_DESC_DMA_OWN	1
-
-#define PRESTERA_SDMA_RX_QUEUE_NUM	8
-
-#define PRESTERA_SDMA_RX_DESC_PER_Q	1000
-
-#define PRESTERA_SDMA_TX_DESC_PER_Q	1000
-#define PRESTERA_SDMA_TX_MAX_BURST	64
-
-#define PRESTERA_SDMA_TX_DESC_OWNER(desc) \
-	((le32_to_cpu((desc)->word1) & BIT(31)) >> 31)
-
-#define PRESTERA_SDMA_TX_DESC_CPU_OWN	0
-#define PRESTERA_SDMA_TX_DESC_DMA_OWN	1U
-
-#define PRESTERA_SDMA_TX_DESC_IS_SENT(desc) \
-	(PRESTERA_SDMA_TX_DESC_OWNER(desc) == PRESTERA_SDMA_TX_DESC_CPU_OWN)
-
-#define PRESTERA_SDMA_TX_DESC_LAST	BIT(20)
-#define PRESTERA_SDMA_TX_DESC_FIRST	BIT(21)
-#define PRESTERA_SDMA_TX_DESC_CALC_CRC	BIT(12)
-
-#define PRESTERA_SDMA_TX_DESC_SINGLE	\
-	(PRESTERA_SDMA_TX_DESC_FIRST | PRESTERA_SDMA_TX_DESC_LAST)
+#include "prestera_rxtx_priv.h"
+#include "prestera_dsa.h"
 
-#define PRESTERA_SDMA_TX_DESC_INIT	\
-	(PRESTERA_SDMA_TX_DESC_SINGLE | PRESTERA_SDMA_TX_DESC_CALC_CRC)
+#include <linux/if_vlan.h>
+#include <net/ip.h>
 
-#define PRESTERA_SDMA_RX_INTR_MASK_REG		0x2814
-#define PRESTERA_SDMA_RX_QUEUE_STATUS_REG	0x2680
-#define PRESTERA_SDMA_RX_QUEUE_DESC_REG(n)	(0x260C + (n) * 16)
+#define MVSW_DSA_TAG_ARP_BROADCAST 5
+#define MVSW_DSA_TAG_IPV4_BROADCAST 19
+#define MVSW_DSA_TAG_IPV4_IPV6_LINK_LOCAL_MC_1 29
+#define MVSW_DSA_TAG_IPV4_IPV6_LINK_LOCAL_MC_2 30
+#define MVSW_DSA_TAG_UDP_BROADCAST 33
+#define MVSW_DSA_TAG_ARP_BROADCAST_TO_ME 179
 
-#define PRESTERA_SDMA_TX_QUEUE_DESC_REG		0x26C0
-#define PRESTERA_SDMA_TX_QUEUE_START_REG	0x2868
+struct mvsw_pr_rxtx;
 
-struct prestera_sdma_buf {
-	struct prestera_sdma_desc *desc;
-	dma_addr_t desc_dma;
-	struct sk_buff *skb;
-	dma_addr_t buf_dma;
-	bool is_used;
+enum mvsw_pr_rxtx_type {
+	MVSW_PR_RXTX_MVPP,
+	MVSW_PR_RXTX_ETH,
+	MVSW_PR_RXTX_SDMA,
 };
 
-struct prestera_rx_ring {
-	struct prestera_sdma_buf *bufs;
-	int next_rx;
-};
+static struct mvsw_pr_rxtx *rxtx_registered;
 
-struct prestera_tx_ring {
-	struct prestera_sdma_buf *bufs;
-	int next_tx;
-	int max_burst;
-	int burst;
-};
+static u64 *cpu_code_stats;
 
-struct prestera_sdma {
-	struct prestera_rx_ring rx_ring[PRESTERA_SDMA_RX_QUEUE_NUM];
-	struct prestera_tx_ring tx_ring;
-	struct prestera_switch *sw;
-	struct dma_pool *desc_pool;
-	struct work_struct tx_work;
-	struct napi_struct rx_napi;
-	struct net_device napi_dev;
-	u32 map_addr;
-	u64 dma_mask;
-	/* protect SDMA with concurrrent access from multiple CPUs */
-	spinlock_t tx_lock;
-};
-
-struct prestera_rxtx {
-	struct prestera_sdma sdma;
-};
-
-static int prestera_sdma_buf_init(struct prestera_sdma *sdma,
-				  struct prestera_sdma_buf *buf)
+netdev_tx_t mvsw_pr_rxtx_xmit(struct sk_buff *skb,
+			      struct mvsw_pr_rxtx_info *info)
 {
-	struct prestera_sdma_desc *desc;
-	dma_addr_t dma;
-
-	desc = dma_pool_alloc(sdma->desc_pool, GFP_DMA | GFP_KERNEL, &dma);
-	if (!desc)
-		return -ENOMEM;
-
-	buf->buf_dma = DMA_MAPPING_ERROR;
-	buf->desc_dma = dma;
-	buf->desc = desc;
-	buf->skb = NULL;
-
-	return 0;
-}
-
-static u32 prestera_sdma_map(struct prestera_sdma *sdma, dma_addr_t pa)
-{
-	return sdma->map_addr + pa;
-}
-
-static void prestera_sdma_rx_desc_init(struct prestera_sdma *sdma,
-				       struct prestera_sdma_desc *desc,
-				       dma_addr_t buf)
-{
-	u32 word = le32_to_cpu(desc->word2);
-
-	u32p_replace_bits(&word, PRESTERA_SDMA_BUFF_SIZE_MAX, GENMASK(15, 0));
-	desc->word2 = cpu_to_le32(word);
-
-	desc->buff = cpu_to_le32(prestera_sdma_map(sdma, buf));
-
-	/* make sure buffer is set before reset the descriptor */
-	wmb();
-
-	desc->word1 = cpu_to_le32(0xA0000000);
-}
-
-static void prestera_sdma_rx_desc_set_next(struct prestera_sdma *sdma,
-					   struct prestera_sdma_desc *desc,
-					   dma_addr_t next)
-{
-	desc->next = cpu_to_le32(prestera_sdma_map(sdma, next));
-}
-
-static int prestera_sdma_rx_skb_alloc(struct prestera_sdma *sdma,
-				      struct prestera_sdma_buf *buf)
-{
-	struct device *dev = sdma->sw->dev->dev;
-	struct sk_buff *skb;
-	dma_addr_t dma;
-
-	skb = alloc_skb(PRESTERA_SDMA_BUFF_SIZE_MAX, GFP_DMA | GFP_ATOMIC);
-	if (!skb)
-		return -ENOMEM;
-
-	dma = dma_map_single(dev, skb->data, skb->len, DMA_FROM_DEVICE);
-	if (dma_mapping_error(dev, dma))
-		goto err_dma_map;
-
-	if (buf->skb)
-		dma_unmap_single(dev, buf->buf_dma, buf->skb->len,
-				 DMA_FROM_DEVICE);
-
-	buf->buf_dma = dma;
-	buf->skb = skb;
-
-	return 0;
-
-err_dma_map:
-	kfree_skb(skb);
-
-	return -ENOMEM;
-}
+	struct mvsw_pr_dsa dsa;
+	struct mvsw_pr_dsa_from_cpu *from_cpu;
+	struct net_device *dev = skb->dev;
+	struct mvsw_pr_port *port = netdev_priv(dev);
+	size_t dsa_resize_len = MVSW_PR_DSA_HLEN;
 
-static struct sk_buff *prestera_sdma_rx_skb_get(struct prestera_sdma *sdma,
-						struct prestera_sdma_buf *buf)
-{
-	dma_addr_t buf_dma = buf->buf_dma;
-	struct sk_buff *skb = buf->skb;
-	u32 len = skb->len;
-	int err;
+	if (!rxtx_registered)
+		return NET_XMIT_DROP;
 
-	err = prestera_sdma_rx_skb_alloc(sdma, buf);
-	if (err) {
-		buf->buf_dma = buf_dma;
-		buf->skb = skb;
+	/* common DSA tag fill-up */
+	memset(&dsa, 0, sizeof(dsa));
+	dsa.dsa_cmd = MVSW_NET_DSA_CMD_FROM_CPU_E;
+
+	from_cpu = &dsa.dsa_info.from_cpu;
+	from_cpu->egr_filter_en = false;
+	from_cpu->egr_filter_registered = false;
+	from_cpu->dst_eport = port->hw_id;
+
+	from_cpu->dst_iface.dev_port.port_num = port->hw_id;
+	from_cpu->dst_iface.dev_port.hw_dev_num = port->dev_id;
+	from_cpu->dst_iface.type = MVSW_IF_PORT_E;
+
+	/* epmorary removing due to issue with vlan sub interface
+	 * on 1.Q bridge
+	 */
+	/* If (skb->protocol == htons(ETH_P_8021Q)) { */
+		/* 802.1q packet tag size is 4 bytes, so DSA len would
+		 * need only allocation of MVSW_PR_DSA_HLEN - size of
+		 * 802.1q tag
+		 */
+		/*dsa.common_params.vpt = skb_vlan_tag_get_prio(skb);
+		 * dsa.common_params.cfi_bit = skb_vlan_tag_get_cfi(skb);
+		 * dsa.common_params.vid = skb_vlan_tag_get_id(skb);
+		 * dsa_resize_len -= VLAN_HLEN;
+		 */
+	/* } */
+
+
+	if (skb_cow_head(skb, dsa_resize_len) < 0)
+		return NET_XMIT_DROP;
 
-		skb = alloc_skb(skb->len, GFP_ATOMIC);
-		if (skb) {
-			skb_put(skb, len);
-			skb_copy_from_linear_data(buf->skb, skb->data, len);
-		}
-	}
+	/* expects skb->data at mac header */
+	skb_push(skb, dsa_resize_len);
+	memmove(skb->data, skb->data + dsa_resize_len, 2 * ETH_ALEN);
 
-	prestera_sdma_rx_desc_init(sdma, buf->desc, buf->buf_dma);
+	if (mvsw_pr_dsa_build(&dsa, skb->data + 2 * ETH_ALEN) != 0)
+		return NET_XMIT_DROP;
 
-	return skb;
+	return rxtx_registered->ops->rxtx_xmit(rxtx_registered, skb);
 }
 
-static int prestera_rxtx_process_skb(struct prestera_sdma *sdma,
-				     struct sk_buff *skb)
+int mvsw_pr_rxtx_recv_skb(struct mvsw_pr_rxtx *rxtx, struct sk_buff *skb)
 {
-	const struct prestera_port *port;
-	struct prestera_dsa dsa;
-	u32 hw_port, dev_id;
+	const struct mvsw_pr_port *port;
+	struct mvsw_pr_dsa dsa;
+	u32 hw_port, hw_id;
 	int err;
 
 	skb_pull(skb, ETH_HLEN);
 
-	/* ethertype field is part of the dsa header */
-	err = prestera_dsa_parse(&dsa, skb->data - ETH_TLEN);
+	/* parse/process DSA tag
+	 * ethertype field is part of the dsa header
+	 */
+	err = mvsw_pr_dsa_parse(skb->data - ETH_TLEN, &dsa);
 	if (err)
 		return err;
 
-	dev_id = dsa.hw_dev_num;
-	hw_port = dsa.port_num;
-
-	port = prestera_port_find_by_hwid(sdma->sw, dev_id, hw_port);
+	/* get switch port */
+	hw_port = dsa.dsa_info.to_cpu.iface.port_num;
+	hw_id = dsa.dsa_info.to_cpu.hw_dev_num;
+	port = mvsw_pr_port_find(hw_id, hw_port);
 	if (unlikely(!port)) {
-		dev_warn_ratelimited(prestera_dev(sdma->sw), "received pkt for non-existent port(%u, %u)\n",
-				     dev_id, hw_port);
-		return -ENOENT;
+		pr_warn_ratelimited("prestera: received pkt for non-existent port(%u, %u)\n",
+				    hw_id, hw_port);
+		return -EEXIST;
 	}
 
-	if (unlikely(!pskb_may_pull(skb, PRESTERA_DSA_HLEN)))
+	if (unlikely(!pskb_may_pull(skb, MVSW_PR_DSA_HLEN)))
 		return -EINVAL;
 
 	/* remove DSA tag and update checksum */
-	skb_pull_rcsum(skb, PRESTERA_DSA_HLEN);
+	skb_pull_rcsum(skb, MVSW_PR_DSA_HLEN);
 
-	memmove(skb->data - ETH_HLEN, skb->data - ETH_HLEN - PRESTERA_DSA_HLEN,
+	memmove(skb->data - ETH_HLEN, skb->data - ETH_HLEN - MVSW_PR_DSA_HLEN,
 		ETH_ALEN * 2);
 
 	skb_push(skb, ETH_HLEN);
 
-	skb->protocol = eth_type_trans(skb, port->dev);
+	skb->protocol = eth_type_trans(skb, port->net_dev);
 
-	if (dsa.vlan.is_tagged) {
-		u16 tci = dsa.vlan.vid & VLAN_VID_MASK;
+	if (dsa.dsa_info.to_cpu.is_tagged) {
+		u16 tci = dsa.common_params.vid & VLAN_VID_MASK;
 
-		tci |= dsa.vlan.vpt << VLAN_PRIO_SHIFT;
-		if (dsa.vlan.cfi_bit)
+		tci |= dsa.common_params.vpt << VLAN_PRIO_SHIFT;
+		if (dsa.common_params.cfi_bit)
 			tci |= VLAN_CFI_MASK;
 
 		__vlan_hwaccel_put_tag(skb, htons(ETH_P_8021Q), tci);
 	}
 
-	return 0;
-}
-
-static int prestera_sdma_next_rx_buf_idx(int buf_idx)
-{
-	return (buf_idx + 1) % PRESTERA_SDMA_RX_DESC_PER_Q;
-}
-
-static int prestera_sdma_rx_poll(struct napi_struct *napi, int budget)
-{
-	int qnum = PRESTERA_SDMA_RX_QUEUE_NUM;
-	unsigned int rxq_done_map = 0;
-	struct prestera_sdma *sdma;
-	struct list_head rx_list;
-	unsigned int qmask;
-	int pkts_done = 0;
-	int q;
-
-	qnum = PRESTERA_SDMA_RX_QUEUE_NUM;
-	qmask = GENMASK(qnum - 1, 0);
-
-	INIT_LIST_HEAD(&rx_list);
-
-	sdma = container_of(napi, struct prestera_sdma, rx_napi);
-
-	while (pkts_done < budget && rxq_done_map != qmask) {
-		for (q = 0; q < qnum && pkts_done < budget; q++) {
-			struct prestera_rx_ring *ring = &sdma->rx_ring[q];
-			struct prestera_sdma_desc *desc;
-			struct prestera_sdma_buf *buf;
-			int buf_idx = ring->next_rx;
-			struct sk_buff *skb;
-
-			buf = &ring->bufs[buf_idx];
-			desc = buf->desc;
-
-			if (PRESTERA_SDMA_RX_DESC_IS_RCVD(desc)) {
-				rxq_done_map &= ~BIT(q);
-			} else {
-				rxq_done_map |= BIT(q);
-				continue;
-			}
-
-			pkts_done++;
-
-			__skb_trim(buf->skb, PRESTERA_SDMA_RX_DESC_PKT_LEN(desc));
-
-			skb = prestera_sdma_rx_skb_get(sdma, buf);
-			if (!skb)
-				goto rx_next_buf;
-
-			if (unlikely(prestera_rxtx_process_skb(sdma, skb)))
-				goto rx_next_buf;
-
-			list_add_tail(&skb->list, &rx_list);
-rx_next_buf:
-			ring->next_rx = prestera_sdma_next_rx_buf_idx(buf_idx);
-		}
+	switch (dsa.dsa_info.to_cpu.cpu_code) {
+	case MVSW_DSA_TAG_ARP_BROADCAST:
+	case MVSW_DSA_TAG_IPV4_BROADCAST:
+	case MVSW_DSA_TAG_IPV4_IPV6_LINK_LOCAL_MC_1:
+	case MVSW_DSA_TAG_IPV4_IPV6_LINK_LOCAL_MC_2:
+	case MVSW_DSA_TAG_UDP_BROADCAST:
+	case MVSW_DSA_TAG_ARP_BROADCAST_TO_ME:
+		skb->offload_fwd_mark = 1;
 	}
-
-	if (pkts_done < budget && napi_complete_done(napi, pkts_done))
-		prestera_write(sdma->sw, PRESTERA_SDMA_RX_INTR_MASK_REG,
-			       GENMASK(9, 2));
-
-	netif_receive_skb_list(&rx_list);
-
-	return pkts_done;
-}
-
-static void prestera_sdma_rx_fini(struct prestera_sdma *sdma)
-{
-	int qnum = PRESTERA_SDMA_RX_QUEUE_NUM;
-	int q, b;
-
-	/* disable all rx queues */
-	prestera_write(sdma->sw, PRESTERA_SDMA_RX_QUEUE_STATUS_REG,
-		       GENMASK(15, 8));
-
-	for (q = 0; q < qnum; q++) {
-		struct prestera_rx_ring *ring = &sdma->rx_ring[q];
-
-		if (!ring->bufs)
-			break;
-
-		for (b = 0; b < PRESTERA_SDMA_RX_DESC_PER_Q; b++) {
-			struct prestera_sdma_buf *buf = &ring->bufs[b];
-
-			if (buf->desc_dma)
-				dma_pool_free(sdma->desc_pool, buf->desc,
-					      buf->desc_dma);
-
-			if (!buf->skb)
-				continue;
-
-			if (buf->buf_dma != DMA_MAPPING_ERROR)
-				dma_unmap_single(sdma->sw->dev->dev,
-						 buf->buf_dma, buf->skb->len,
-						 DMA_FROM_DEVICE);
-			kfree_skb(buf->skb);
-		}
-	}
-}
-
-static int prestera_sdma_rx_init(struct prestera_sdma *sdma)
-{
-	int bnum = PRESTERA_SDMA_RX_DESC_PER_Q;
-	int qnum = PRESTERA_SDMA_RX_QUEUE_NUM;
-	int err;
-	int q;
-
-	/* disable all rx queues */
-	prestera_write(sdma->sw, PRESTERA_SDMA_RX_QUEUE_STATUS_REG,
-		       GENMASK(15, 8));
-
-	for (q = 0; q < qnum; q++) {
-		struct prestera_sdma_buf *head, *tail, *next, *prev;
-		struct prestera_rx_ring *ring = &sdma->rx_ring[q];
-
-		ring->bufs = kmalloc_array(bnum, sizeof(*head), GFP_KERNEL);
-		if (!ring->bufs)
-			return -ENOMEM;
-
-		ring->next_rx = 0;
-
-		tail = &ring->bufs[bnum - 1];
-		head = &ring->bufs[0];
-		next = head;
-		prev = next;
-
-		do {
-			err = prestera_sdma_buf_init(sdma, next);
-			if (err)
-				return err;
-
-			err = prestera_sdma_rx_skb_alloc(sdma, next);
-			if (err)
-				return err;
-
-			prestera_sdma_rx_desc_init(sdma, next->desc,
-						   next->buf_dma);
-
-			prestera_sdma_rx_desc_set_next(sdma, prev->desc,
-						       next->desc_dma);
-
-			prev = next;
-			next++;
-		} while (prev != tail);
-
-		/* join tail with head to make a circular list */
-		prestera_sdma_rx_desc_set_next(sdma, tail->desc, head->desc_dma);
-
-		prestera_write(sdma->sw, PRESTERA_SDMA_RX_QUEUE_DESC_REG(q),
-			       prestera_sdma_map(sdma, head->desc_dma));
-	}
-
-	/* make sure all rx descs are filled before enabling all rx queues */
-	wmb();
-
-	prestera_write(sdma->sw, PRESTERA_SDMA_RX_QUEUE_STATUS_REG,
-		       GENMASK(7, 0));
+	++cpu_code_stats[dsa.dsa_info.to_cpu.cpu_code];
 
 	return 0;
 }
 
-static void prestera_sdma_tx_desc_init(struct prestera_sdma *sdma,
-				       struct prestera_sdma_desc *desc)
-{
-	desc->word1 = cpu_to_le32(PRESTERA_SDMA_TX_DESC_INIT);
-	desc->word2 = 0;
-}
-
-static void prestera_sdma_tx_desc_set_next(struct prestera_sdma *sdma,
-					   struct prestera_sdma_desc *desc,
-					   dma_addr_t next)
-{
-	desc->next = cpu_to_le32(prestera_sdma_map(sdma, next));
-}
-
-static void prestera_sdma_tx_desc_set_buf(struct prestera_sdma *sdma,
-					  struct prestera_sdma_desc *desc,
-					  dma_addr_t buf, size_t len)
-{
-	u32 word = le32_to_cpu(desc->word2);
-
-	u32p_replace_bits(&word, len + ETH_FCS_LEN, GENMASK(30, 16));
-
-	desc->buff = cpu_to_le32(prestera_sdma_map(sdma, buf));
-	desc->word2 = cpu_to_le32(word);
-}
-
-static void prestera_sdma_tx_desc_xmit(struct prestera_sdma_desc *desc)
-{
-	u32 word = le32_to_cpu(desc->word1);
-
-	word |= PRESTERA_SDMA_TX_DESC_DMA_OWN << 31;
-
-	/* make sure everything is written before enable xmit */
-	wmb();
-
-	desc->word1 = cpu_to_le32(word);
-}
+static struct mvsw_pr_rxtx_ops rxtx_driver_ops[] = {
+	[MVSW_PR_RXTX_SDMA] = {
+		.rxtx_init = mvsw_pr_rxtx_sdma_init,
+		.rxtx_fini = mvsw_pr_rxtx_sdma_fini,
+		.rxtx_switch_init = mvsw_pr_rxtx_sdma_switch_init,
+		.rxtx_switch_fini = mvsw_pr_rxtx_sdma_switch_fini,
+		.rxtx_xmit = mvsw_pr_rxtx_sdma_xmit,
+	},
+};
 
-static int prestera_sdma_tx_buf_map(struct prestera_sdma *sdma,
-				    struct prestera_sdma_buf *buf,
-				    struct sk_buff *skb)
+int mvsw_pr_rxtx_init(void)
 {
-	struct device *dma_dev = sdma->sw->dev->dev;
-	dma_addr_t dma;
-
-	dma = dma_map_single(dma_dev, skb->data, skb->len, DMA_TO_DEVICE);
-	if (dma_mapping_error(dma_dev, dma))
+	cpu_code_stats = kzalloc(sizeof(u64) * MVSW_PR_RXTX_CPU_CODE_MAX_NUM,
+				 GFP_KERNEL);
+	if (!cpu_code_stats)
 		return -ENOMEM;
 
-	buf->buf_dma = dma;
-	buf->skb = skb;
-
-	return 0;
-}
-
-static void prestera_sdma_tx_buf_unmap(struct prestera_sdma *sdma,
-				       struct prestera_sdma_buf *buf)
-{
-	struct device *dma_dev = sdma->sw->dev->dev;
-
-	dma_unmap_single(dma_dev, buf->buf_dma, buf->skb->len, DMA_TO_DEVICE);
-}
-
-static void prestera_sdma_tx_recycle_work_fn(struct work_struct *work)
-{
-	int bnum = PRESTERA_SDMA_TX_DESC_PER_Q;
-	struct prestera_tx_ring *tx_ring;
-	struct prestera_sdma *sdma;
-	int b;
-
-	sdma = container_of(work, struct prestera_sdma, tx_work);
-
-	tx_ring = &sdma->tx_ring;
-
-	for (b = 0; b < bnum; b++) {
-		struct prestera_sdma_buf *buf = &tx_ring->bufs[b];
-
-		if (!buf->is_used)
-			continue;
-
-		if (!PRESTERA_SDMA_TX_DESC_IS_SENT(buf->desc))
-			continue;
-
-		prestera_sdma_tx_buf_unmap(sdma, buf);
-		dev_consume_skb_any(buf->skb);
-		buf->skb = NULL;
-
-		/* make sure everything is cleaned up */
-		wmb();
-
-		buf->is_used = false;
-	}
-}
-
-static int prestera_sdma_tx_init(struct prestera_sdma *sdma)
-{
-	struct prestera_sdma_buf *head, *tail, *next, *prev;
-	struct prestera_tx_ring *tx_ring = &sdma->tx_ring;
-	int bnum = PRESTERA_SDMA_TX_DESC_PER_Q;
-	int err;
-
-	INIT_WORK(&sdma->tx_work, prestera_sdma_tx_recycle_work_fn);
-	spin_lock_init(&sdma->tx_lock);
-
-	tx_ring->bufs = kmalloc_array(bnum, sizeof(*head), GFP_KERNEL);
-	if (!tx_ring->bufs)
+	rxtx_registered = kzalloc(sizeof(*rxtx_registered), GFP_KERNEL);
+	if (!rxtx_registered) {
+		kfree(cpu_code_stats);
 		return -ENOMEM;
-
-	tail = &tx_ring->bufs[bnum - 1];
-	head = &tx_ring->bufs[0];
-	next = head;
-	prev = next;
-
-	tx_ring->max_burst = PRESTERA_SDMA_TX_MAX_BURST;
-	tx_ring->burst = tx_ring->max_burst;
-	tx_ring->next_tx = 0;
-
-	do {
-		err = prestera_sdma_buf_init(sdma, next);
-		if (err)
-			return err;
-
-		next->is_used = false;
-
-		prestera_sdma_tx_desc_init(sdma, next->desc);
-
-		prestera_sdma_tx_desc_set_next(sdma, prev->desc,
-					       next->desc_dma);
-
-		prev = next;
-		next++;
-	} while (prev != tail);
-
-	/* join tail with head to make a circular list */
-	prestera_sdma_tx_desc_set_next(sdma, tail->desc, head->desc_dma);
-
-	/* make sure descriptors are written */
-	wmb();
-
-	prestera_write(sdma->sw, PRESTERA_SDMA_TX_QUEUE_DESC_REG,
-		       prestera_sdma_map(sdma, head->desc_dma));
-
-	return 0;
-}
-
-static void prestera_sdma_tx_fini(struct prestera_sdma *sdma)
-{
-	struct prestera_tx_ring *ring = &sdma->tx_ring;
-	int bnum = PRESTERA_SDMA_TX_DESC_PER_Q;
-	int b;
-
-	cancel_work_sync(&sdma->tx_work);
-
-	if (!ring->bufs)
-		return;
-
-	for (b = 0; b < bnum; b++) {
-		struct prestera_sdma_buf *buf = &ring->bufs[b];
-
-		if (buf->desc)
-			dma_pool_free(sdma->desc_pool, buf->desc,
-				      buf->desc_dma);
-
-		if (!buf->skb)
-			continue;
-
-		dma_unmap_single(sdma->sw->dev->dev, buf->buf_dma,
-				 buf->skb->len, DMA_TO_DEVICE);
-
-		dev_consume_skb_any(buf->skb);
 	}
-}
 
-static void prestera_rxtx_handle_event(struct prestera_switch *sw,
-				       struct prestera_event *evt,
-				       void *arg)
-{
-	struct prestera_sdma *sdma = arg;
-
-	if (evt->id != PRESTERA_RXTX_EVENT_RCV_PKT)
-		return;
+	rxtx_registered->ops = &rxtx_driver_ops[MVSW_PR_RXTX_SDMA];
 
-	prestera_write(sdma->sw, PRESTERA_SDMA_RX_INTR_MASK_REG, 0);
-	napi_schedule(&sdma->rx_napi);
-}
-
-static int prestera_sdma_switch_init(struct prestera_switch *sw)
-{
-	struct prestera_sdma *sdma = &sw->rxtx->sdma;
-	struct device *dev = sw->dev->dev;
-	struct prestera_rxtx_params p;
-	int err;
-
-	p.use_sdma = true;
-
-	err = prestera_hw_rxtx_init(sw, &p);
-	if (err) {
-		dev_err(dev, "failed to init rxtx by hw\n");
-		return err;
-	}
-
-	sdma->dma_mask = dma_get_mask(dev);
-	sdma->map_addr = p.map_addr;
-	sdma->sw = sw;
-
-	sdma->desc_pool = dma_pool_create("desc_pool", dev,
-					  sizeof(struct prestera_sdma_desc),
-					  16, 0);
-	if (!sdma->desc_pool)
-		return -ENOMEM;
-
-	err = prestera_sdma_rx_init(sdma);
-	if (err) {
-		dev_err(dev, "failed to init rx ring\n");
-		goto err_rx_init;
-	}
-
-	err = prestera_sdma_tx_init(sdma);
-	if (err) {
-		dev_err(dev, "failed to init tx ring\n");
-		goto err_tx_init;
-	}
-
-	err = prestera_hw_event_handler_register(sw, PRESTERA_EVENT_TYPE_RXTX,
-						 prestera_rxtx_handle_event,
-						 sdma);
-	if (err)
-		goto err_evt_register;
-
-	init_dummy_netdev(&sdma->napi_dev);
-
-	netif_napi_add(&sdma->napi_dev, &sdma->rx_napi, prestera_sdma_rx_poll, 64);
-	napi_enable(&sdma->rx_napi);
+	if (rxtx_registered->ops->rxtx_init)
+		return rxtx_registered->ops->rxtx_init(rxtx_registered);
 
 	return 0;
-
-err_evt_register:
-err_tx_init:
-	prestera_sdma_tx_fini(sdma);
-err_rx_init:
-	prestera_sdma_rx_fini(sdma);
-
-	dma_pool_destroy(sdma->desc_pool);
-	return err;
-}
-
-static void prestera_sdma_switch_fini(struct prestera_switch *sw)
-{
-	struct prestera_sdma *sdma = &sw->rxtx->sdma;
-
-	napi_disable(&sdma->rx_napi);
-	netif_napi_del(&sdma->rx_napi);
-	prestera_hw_event_handler_unregister(sw, PRESTERA_EVENT_TYPE_RXTX,
-					     prestera_rxtx_handle_event);
-	prestera_sdma_tx_fini(sdma);
-	prestera_sdma_rx_fini(sdma);
-	dma_pool_destroy(sdma->desc_pool);
-}
-
-static bool prestera_sdma_is_ready(struct prestera_sdma *sdma)
-{
-	return !(prestera_read(sdma->sw, PRESTERA_SDMA_TX_QUEUE_START_REG) & 1);
 }
 
-static int prestera_sdma_tx_wait(struct prestera_sdma *sdma,
-				 struct prestera_tx_ring *tx_ring)
+void mvsw_pr_rxtx_fini(void)
 {
-	int tx_wait_num = PRESTERA_SDMA_WAIT_MUL * tx_ring->max_burst;
+	struct mvsw_pr_rxtx *rxtx = rxtx_registered;
 
-	do {
-		if (prestera_sdma_is_ready(sdma))
-			return 0;
+	if (rxtx->ops->rxtx_fini)
+		rxtx->ops->rxtx_fini(rxtx);
 
-		udelay(1);
-	} while (--tx_wait_num);
-
-	return -EBUSY;
-}
-
-static void prestera_sdma_tx_start(struct prestera_sdma *sdma)
-{
-	prestera_write(sdma->sw, PRESTERA_SDMA_TX_QUEUE_START_REG, 1);
-	schedule_work(&sdma->tx_work);
+	kfree(rxtx_registered);
+	rxtx_registered = NULL;
+	kfree(cpu_code_stats);
 }
 
-static netdev_tx_t prestera_sdma_xmit(struct prestera_sdma *sdma,
-				      struct sk_buff *skb)
+int mvsw_pr_rxtx_switch_init(struct mvsw_pr_switch *sw)
 {
-	struct device *dma_dev = sdma->sw->dev->dev;
-	struct net_device *dev = skb->dev;
-	struct prestera_tx_ring *tx_ring;
-	struct prestera_sdma_buf *buf;
 	int err;
 
-	spin_lock(&sdma->tx_lock);
-
-	tx_ring = &sdma->tx_ring;
-
-	buf = &tx_ring->bufs[tx_ring->next_tx];
-	if (buf->is_used) {
-		schedule_work(&sdma->tx_work);
-		goto drop_skb;
-	}
-
-	if (unlikely(eth_skb_pad(skb)))
-		goto drop_skb_nofree;
-
-	err = prestera_sdma_tx_buf_map(sdma, buf, skb);
-	if (err)
-		goto drop_skb;
-
-	prestera_sdma_tx_desc_set_buf(sdma, buf->desc, buf->buf_dma, skb->len);
-
-	dma_sync_single_for_device(dma_dev, buf->buf_dma, skb->len,
-				   DMA_TO_DEVICE);
-
-	if (tx_ring->burst) {
-		tx_ring->burst--;
-	} else {
-		tx_ring->burst = tx_ring->max_burst;
-
-		err = prestera_sdma_tx_wait(sdma, tx_ring);
-		if (err)
-			goto drop_skb_unmap;
+	if (!rxtx_registered) {
+		pr_info("No RxTx driver registered");
+		return 0;
 	}
 
-	tx_ring->next_tx = (tx_ring->next_tx + 1) % PRESTERA_SDMA_TX_DESC_PER_Q;
-	prestera_sdma_tx_desc_xmit(buf->desc);
-	buf->is_used = true;
-
-	prestera_sdma_tx_start(sdma);
-
-	goto tx_done;
-
-drop_skb_unmap:
-	prestera_sdma_tx_buf_unmap(sdma, buf);
-drop_skb:
-	dev_consume_skb_any(skb);
-drop_skb_nofree:
-	dev->stats.tx_dropped++;
-tx_done:
-	spin_unlock(&sdma->tx_lock);
-	return NETDEV_TX_OK;
-}
-
-int prestera_rxtx_switch_init(struct prestera_switch *sw)
-{
-	struct prestera_rxtx *rxtx;
-
-	rxtx = kzalloc(sizeof(*rxtx), GFP_KERNEL);
-	if (!rxtx)
-		return -ENOMEM;
-
-	sw->rxtx = rxtx;
-
-	return prestera_sdma_switch_init(sw);
-}
-
-void prestera_rxtx_switch_fini(struct prestera_switch *sw)
-{
-	prestera_sdma_switch_fini(sw);
-	kfree(sw->rxtx);
-}
-
-int prestera_rxtx_port_init(struct prestera_port *port)
-{
-	int err;
+	if (!rxtx_registered->ops->rxtx_switch_init)
+		return 0;
 
-	err = prestera_hw_rxtx_port_init(port);
+	err = rxtx_registered->ops->rxtx_switch_init(rxtx_registered, sw);
 	if (err)
 		return err;
 
-	port->dev->needed_headroom = PRESTERA_DSA_HLEN;
-
 	return 0;
 }
 
-netdev_tx_t prestera_rxtx_xmit(struct prestera_port *port, struct sk_buff *skb)
+void mvsw_pr_rxtx_switch_fini(struct mvsw_pr_switch *sw)
 {
-	struct prestera_dsa dsa;
-
-	dsa.hw_dev_num = port->dev_id;
-	dsa.port_num = port->hw_id;
-
-	if (skb_cow_head(skb, PRESTERA_DSA_HLEN) < 0)
-		return NET_XMIT_DROP;
-
-	skb_push(skb, PRESTERA_DSA_HLEN);
-	memmove(skb->data, skb->data + PRESTERA_DSA_HLEN, 2 * ETH_ALEN);
+	if (!rxtx_registered || !rxtx_registered->ops->rxtx_switch_init)
+		return;
 
-	if (prestera_dsa_build(&dsa, skb->data + 2 * ETH_ALEN) != 0)
-		return NET_XMIT_DROP;
+	return rxtx_registered->ops->rxtx_switch_fini(rxtx_registered, sw);
+}
 
-	return prestera_sdma_xmit(&port->sw->rxtx->sdma, skb);
+u64 mvsw_pr_rxtx_get_cpu_code_stats(u8 cpu_code)
+{
+	return cpu_code_stats[cpu_code];
 }
diff --git a/drivers/net/ethernet/marvell/prestera/prestera_rxtx.h b/drivers/net/ethernet/marvell/prestera/prestera_rxtx.h
index 882a1225c..a10522538 100644
--- a/drivers/net/ethernet/marvell/prestera/prestera_rxtx.h
+++ b/drivers/net/ethernet/marvell/prestera/prestera_rxtx.h
@@ -1,19 +1,32 @@
-/* SPDX-License-Identifier: BSD-3-Clause OR GPL-2.0 */
-/* Copyright (c) 2019-2020 Marvell International Ltd. All rights reserved. */
+/* SPDX-License-Identifier: BSD-3-Clause OR GPL-2.0
+ *
+ * Copyright (c) 2019-2020 Marvell International Ltd. All rights reserved.
+ *
+ */
 
-#ifndef _PRESTERA_RXTX_H_
-#define _PRESTERA_RXTX_H_
+#ifndef _MVSW_PRESTERA_RXTX_H_
+#define _MVSW_PRESTERA_RXTX_H_
 
 #include <linux/netdevice.h>
 
-struct prestera_switch;
-struct prestera_port;
+#define MVSW_PR_RXTX_CPU_CODE_MAX_NUM	256
 
-int prestera_rxtx_switch_init(struct prestera_switch *sw);
-void prestera_rxtx_switch_fini(struct prestera_switch *sw);
+struct mvsw_pr_switch;
 
-int prestera_rxtx_port_init(struct prestera_port *port);
+struct mvsw_pr_rxtx_info {
+	u32 port_id;
+	u32 dev_id;
+};
 
-netdev_tx_t prestera_rxtx_xmit(struct prestera_port *port, struct sk_buff *skb);
+int mvsw_pr_rxtx_init(void);
+void mvsw_pr_rxtx_fini(void);
 
-#endif /* _PRESTERA_RXTX_H_ */
+int mvsw_pr_rxtx_switch_init(struct mvsw_pr_switch *sw);
+void mvsw_pr_rxtx_switch_fini(struct mvsw_pr_switch *sw);
+
+netdev_tx_t mvsw_pr_rxtx_xmit(struct sk_buff *skb,
+			      struct mvsw_pr_rxtx_info *info);
+
+u64 mvsw_pr_rxtx_get_cpu_code_stats(u8 cpu_code);
+
+#endif /* _MVSW_PRESTERA_RXTX_H_ */
diff --git a/drivers/net/ethernet/marvell/prestera/prestera_rxtx_priv.h b/drivers/net/ethernet/marvell/prestera/prestera_rxtx_priv.h
new file mode 100644
index 000000000..13527b9d3
--- /dev/null
+++ b/drivers/net/ethernet/marvell/prestera/prestera_rxtx_priv.h
@@ -0,0 +1,61 @@
+/* SPDX-License-Identifier: BSD-3-Clause OR GPL-2.0
+ *
+ * Copyright (c) 2019-2020 Marvell International Ltd. All rights reserved.
+ *
+ */
+#include <linux/kernel.h>
+#include <linux/netdevice.h>
+#include <linux/rtnetlink.h>
+#include <linux/platform_device.h>
+#include <linux/of.h>
+#include <linux/of_device.h>
+
+#include "prestera_rxtx.h"
+
+struct mvsw_pr_rxtx;
+
+struct mvsw_pr_rxtx_ops {
+	int (*rxtx_init)(struct mvsw_pr_rxtx *rxtx);
+	int (*rxtx_fini)(struct mvsw_pr_rxtx *rxtx);
+
+	int (*rxtx_switch_init)(struct mvsw_pr_rxtx *rxtx,
+				struct mvsw_pr_switch *sw);
+	void (*rxtx_switch_fini)(struct mvsw_pr_rxtx *rxtx,
+				 struct mvsw_pr_switch *sw);
+
+	netdev_tx_t (*rxtx_xmit)(struct mvsw_pr_rxtx *rxtx,
+				 struct sk_buff *skb);
+};
+
+struct mvsw_pr_rxtx {
+	struct platform_device *pdev;
+	struct device *dev;
+
+	const struct mvsw_pr_rxtx_ops *ops;
+	void *priv;
+};
+
+int mvsw_pr_rxtx_recv_skb(struct mvsw_pr_rxtx *rxtx, struct sk_buff *skb);
+
+int mvsw_pr_rxtx_eth_init(struct mvsw_pr_rxtx *rxtx);
+int mvsw_pr_rxtx_eth_fini(struct mvsw_pr_rxtx *rxtx);
+netdev_tx_t mvsw_pr_rxtx_eth_xmit(struct mvsw_pr_rxtx *rxtx,
+				  struct sk_buff *skb);
+
+int mvsw_pr_rxtx_mvpp_init(struct mvsw_pr_rxtx *rxtx);
+int mvsw_pr_rxtx_mvpp_fini(struct mvsw_pr_rxtx *rxtx);
+int mvsw_pr_rxtx_eth_switch_init(struct mvsw_pr_rxtx *rxtx,
+				 struct mvsw_pr_switch *sw);
+void mvsw_pr_rxtx_eth_switch_fini(struct mvsw_pr_rxtx *rxtx,
+				  struct mvsw_pr_switch *sw);
+netdev_tx_t mvsw_pr_rxtx_mvpp_xmit(struct mvsw_pr_rxtx *rxtx,
+				   struct sk_buff *skb);
+
+int mvsw_pr_rxtx_sdma_init(struct mvsw_pr_rxtx *rxtx);
+int mvsw_pr_rxtx_sdma_fini(struct mvsw_pr_rxtx *rxtx);
+int mvsw_pr_rxtx_sdma_switch_init(struct mvsw_pr_rxtx *rxtx,
+				  struct mvsw_pr_switch *sw);
+void mvsw_pr_rxtx_sdma_switch_fini(struct mvsw_pr_rxtx *rxtx,
+				   struct mvsw_pr_switch *sw);
+netdev_tx_t mvsw_pr_rxtx_sdma_xmit(struct mvsw_pr_rxtx *rxtx,
+				   struct sk_buff *skb);
diff --git a/drivers/net/ethernet/marvell/prestera/prestera_rxtx_sdma.c b/drivers/net/ethernet/marvell/prestera/prestera_rxtx_sdma.c
new file mode 100644
index 000000000..9a65f3cd3
--- /dev/null
+++ b/drivers/net/ethernet/marvell/prestera/prestera_rxtx_sdma.c
@@ -0,0 +1,769 @@
+// SPDX-License-Identifier: BSD-3-Clause OR GPL-2.0
+/*
+ * Copyright (c) 2019-2020 Marvell International Ltd. All rights reserved.
+ *
+ */
+
+#include <linux/platform_device.h>
+#include <linux/of.h>
+#include <linux/of_address.h>
+#include <linux/of_device.h>
+#include <linux/dmapool.h>
+
+#include "prestera.h"
+#include "prestera_hw.h"
+#include "prestera_rxtx_priv.h"
+
+struct mvsw_sdma_desc {
+	__le32 word1;
+	__le32 word2;
+	__le32 buff;
+	__le32 next;
+} __packed __aligned(16);
+
+#define SDMA_BUFF_SIZE_MAX	1544
+
+#define SDMA_RX_DESC_PKT_LEN(desc) \
+	((le32_to_cpu((desc)->word2) >> 16) & 0x3FFF)
+
+#define SDMA_RX_DESC_OWNER(desc) \
+	((le32_to_cpu((desc)->word1) & BIT(31)) >> 31)
+
+#define SDMA_RX_DESC_CPU_OWN	0
+#define SDMA_RX_DESC_DMA_OWN	1
+
+#define SDMA_RX_QUEUE_NUM	8
+
+#define SDMA_RX_DESC_PER_Q	1000
+
+#define SDMA_TX_DESC_PER_Q	1000
+#define SDMA_TX_MAX_BURST	32
+
+#define SDMA_TX_DESC_OWNER(desc) \
+	((le32_to_cpu((desc)->word1) & BIT(31)) >> 31)
+
+#define SDMA_TX_DESC_CPU_OWN	0
+#define SDMA_TX_DESC_DMA_OWN	1
+
+#define SDMA_TX_DESC_IS_SENT(desc) \
+	(SDMA_TX_DESC_OWNER(desc) == SDMA_TX_DESC_CPU_OWN)
+
+#define SDMA_TX_DESC_LAST	BIT(20)
+#define SDMA_TX_DESC_FIRST	BIT(21)
+#define SDMA_TX_DESC_SINGLE	(SDMA_TX_DESC_FIRST | SDMA_TX_DESC_LAST)
+#define SDMA_TX_DESC_CALC_CRC	BIT(12)
+
+#define mvsw_reg_write(sw, reg, val) \
+	writel(val, (sw)->dev->pp_regs + (reg))
+#define mvsw_reg_read(sw, reg) \
+	readl((sw)->dev->pp_regs + (reg))
+
+#define SDMA_RX_INTR_MASK_REG		0x2814
+#define SDMA_RX_QUEUE_STATUS_REG	0x2680
+#define SDMA_RX_QUEUE_DESC_REG(n)	(0x260C + (n) * 16)
+
+#define SDMA_TX_QUEUE_DESC_REG		0x26C0
+#define SDMA_TX_QUEUE_START_REG		0x2868
+
+struct mvsw_sdma_buf {
+	struct mvsw_sdma_desc *desc;
+	dma_addr_t desc_dma;
+	struct sk_buff *skb;
+	dma_addr_t buf_dma;
+	bool is_used;
+};
+
+struct mvsw_sdma_rx_ring {
+	struct mvsw_sdma_buf *bufs;
+	int next_rx;
+	int weight;
+	int recvd;
+};
+
+struct mvsw_sdma_tx_ring {
+	struct mvsw_sdma_buf *bufs;
+	int next_tx;
+	int max_burst;
+	int burst;
+};
+
+struct mvsw_pr_rxtx_sdma {
+	struct mvsw_sdma_rx_ring rx_ring[SDMA_RX_QUEUE_NUM];
+	struct mvsw_sdma_tx_ring tx_ring;
+	const struct mvsw_pr_switch *sw;
+	struct dma_pool *desc_pool;
+	struct mvsw_pr_rxtx *rxtx;
+	struct work_struct tx_work;
+	struct napi_struct rx_napi;
+	int next_rxq;
+	struct net_device napi_dev;
+	/* protect SDMA with concurrrent access from multiple CPUs */
+	spinlock_t tx_lock;
+	u32 map_addr;
+	u64 dma_mask;
+};
+
+static int prestera_rx_weight_map[SDMA_RX_QUEUE_NUM] = {
+	1, 2, 2, 2, 2, 4, 4, 8
+};
+
+static int mvsw_sdma_buf_desc_alloc(struct mvsw_pr_rxtx_sdma *sdma,
+				    struct mvsw_sdma_buf *buf)
+{
+	struct device *dma_dev = sdma->sw->dev->dev;
+	struct mvsw_sdma_desc *desc;
+	dma_addr_t dma;
+
+	desc = dma_pool_alloc(sdma->desc_pool, GFP_DMA | GFP_KERNEL, &dma);
+	if (!desc)
+		return -ENOMEM;
+
+	if (dma + sizeof(struct mvsw_sdma_desc) > sdma->dma_mask) {
+		dev_err(dma_dev, "failed to alloc desc\n");
+		dma_pool_free(sdma->desc_pool, desc, dma);
+		return -ENOMEM;
+	}
+
+	buf->desc_dma = dma;
+	buf->desc = desc;
+
+	return 0;
+}
+
+static u32 mvsw_sdma_addr_phy(struct mvsw_pr_rxtx_sdma *sdma, dma_addr_t pa)
+{
+	return sdma->map_addr + pa;
+}
+
+static void mvsw_sdma_rx_desc_set_len(struct mvsw_sdma_desc *desc, size_t val)
+{
+	u32 word = le32_to_cpu(desc->word2);
+
+	word = (word & ~GENMASK(15, 0)) | val;
+	desc->word2 = cpu_to_le32(word);
+}
+
+static void mvsw_sdma_rx_desc_init(struct mvsw_pr_rxtx_sdma *sdma,
+				   struct mvsw_sdma_desc *desc,
+				   dma_addr_t buf)
+{
+	mvsw_sdma_rx_desc_set_len(desc, SDMA_BUFF_SIZE_MAX);
+	desc->buff = cpu_to_le32(mvsw_sdma_addr_phy(sdma, buf));
+	/* make sure buffer is set before reset the descriptor */
+	wmb();
+	desc->word1 = cpu_to_le32(0xA0000000);
+}
+
+static void mvsw_sdma_rx_desc_set_next(struct mvsw_pr_rxtx_sdma *sdma,
+				       struct mvsw_sdma_desc *desc,
+				       dma_addr_t next)
+{
+	desc->next = cpu_to_le32(mvsw_sdma_addr_phy(sdma, next));
+}
+
+static int mvsw_sdma_rx_dma_alloc(struct mvsw_pr_rxtx_sdma *sdma,
+				  struct mvsw_sdma_buf *buf)
+{
+	struct device *dev = sdma->sw->dev->dev;
+
+	buf->skb = alloc_skb(SDMA_BUFF_SIZE_MAX, GFP_DMA | GFP_ATOMIC);
+	if (!buf->skb)
+		return -ENOMEM;
+
+	buf->buf_dma = dma_map_single(dev, buf->skb->data, buf->skb->len,
+				      DMA_FROM_DEVICE);
+
+	if (dma_mapping_error(dev, buf->buf_dma))
+		goto err_dma_map;
+	if (buf->buf_dma + buf->skb->len > sdma->dma_mask)
+		goto err_dma_range;
+
+	return 0;
+
+err_dma_range:
+	dma_unmap_single(dev, buf->buf_dma, buf->skb->len, DMA_FROM_DEVICE);
+	buf->buf_dma = DMA_MAPPING_ERROR;
+err_dma_map:
+	kfree_skb(buf->skb);
+	buf->skb = NULL;
+
+	return -ENOMEM;
+}
+
+static struct sk_buff *mvsw_sdma_rx_buf_get(struct mvsw_pr_rxtx_sdma *sdma,
+					    struct mvsw_sdma_buf *buf)
+{
+	struct sk_buff *skb_orig = buf->skb;
+	dma_addr_t buf_dma = buf->buf_dma;
+	u32 len = skb_orig->len;
+	int err;
+
+	err = mvsw_sdma_rx_dma_alloc(sdma, buf);
+	if (err) {
+		struct sk_buff *skb;
+
+		buf->buf_dma = buf_dma;
+		buf->skb = skb_orig;
+
+		skb = alloc_skb(SDMA_BUFF_SIZE_MAX, GFP_ATOMIC);
+		if (!skb)
+			return NULL;
+
+		skb_copy_from_linear_data(buf->skb, skb_put(skb, len), len);
+		return skb;
+	}
+
+	return skb_orig;
+}
+
+static void mvsw_sdma_rx_set_next_queue(struct mvsw_pr_rxtx_sdma *sdma, int rxq)
+{
+	sdma->next_rxq = rxq % SDMA_RX_QUEUE_NUM;
+}
+
+static int mvsw_sdma_rx_pick_next_queue(struct mvsw_pr_rxtx_sdma *sdma)
+{
+	struct mvsw_sdma_rx_ring *ring = &sdma->rx_ring[sdma->next_rxq];
+
+	if (ring->recvd >= ring->weight) {
+		mvsw_sdma_rx_set_next_queue(sdma, sdma->next_rxq + 1);
+		ring->recvd = 0;
+	}
+
+	return sdma->next_rxq;
+}
+
+static int mvsw_sdma_rx_poll(struct napi_struct *napi, int budget)
+{
+	unsigned int qmask = GENMASK(SDMA_RX_QUEUE_NUM - 1, 0);
+	struct mvsw_pr_rxtx_sdma *sdma;
+	unsigned int rxq_done_map = 0;
+	struct list_head rx_list;
+	int pkts_done = 0;
+
+	INIT_LIST_HEAD(&rx_list);
+
+	sdma = container_of(napi, struct mvsw_pr_rxtx_sdma, rx_napi);
+
+	while (pkts_done < budget && rxq_done_map != qmask) {
+		struct mvsw_sdma_rx_ring *ring;
+		struct mvsw_sdma_desc *desc;
+		struct mvsw_sdma_buf *buf;
+		struct sk_buff *skb;
+		int buf_idx;
+		int rxq;
+
+		rxq = mvsw_sdma_rx_pick_next_queue(sdma);
+		ring = &sdma->rx_ring[rxq];
+
+		buf_idx = ring->next_rx;
+		buf = &ring->bufs[buf_idx];
+		desc = buf->desc;
+
+		if (SDMA_RX_DESC_OWNER(desc) != SDMA_RX_DESC_CPU_OWN) {
+			mvsw_sdma_rx_set_next_queue(sdma, rxq + 1);
+			rxq_done_map |= BIT(rxq);
+			continue;
+		} else {
+			rxq_done_map &= ~BIT(rxq);
+		}
+
+		ring->recvd++;
+		pkts_done++;
+
+		__skb_trim(buf->skb, SDMA_RX_DESC_PKT_LEN(desc));
+
+		skb = mvsw_sdma_rx_buf_get(sdma, buf);
+		if (!skb)
+			goto rx_reset_buf;
+
+		if (unlikely(mvsw_pr_rxtx_recv_skb(sdma->rxtx, skb)))
+			goto rx_reset_buf;
+
+		list_add_tail(&skb->list, &rx_list);
+rx_reset_buf:
+		mvsw_sdma_rx_desc_init(sdma, buf->desc, buf->buf_dma);
+		ring->next_rx = (buf_idx + 1) % SDMA_RX_DESC_PER_Q;
+	}
+
+	if (pkts_done < budget && napi_complete_done(napi, pkts_done))
+		mvsw_reg_write(sdma->sw, SDMA_RX_INTR_MASK_REG, 0xff << 2);
+
+	netif_receive_skb_list(&rx_list);
+
+	return pkts_done;
+}
+
+static void mvsw_sdma_rx_fini(struct mvsw_pr_rxtx_sdma *sdma)
+{
+	int q, b;
+
+	/* disable all rx queues */
+	mvsw_reg_write(sdma->sw, SDMA_RX_QUEUE_STATUS_REG, 0xff00);
+
+	for (q = 0; q < SDMA_RX_QUEUE_NUM; q++) {
+		struct mvsw_sdma_rx_ring *ring = &sdma->rx_ring[q];
+
+		if (!ring->bufs)
+			break;
+
+		for (b = 0; b < SDMA_RX_DESC_PER_Q; b++) {
+			struct mvsw_sdma_buf *buf = &ring->bufs[b];
+
+			if (buf->desc_dma)
+				dma_pool_free(sdma->desc_pool, buf->desc,
+					      buf->desc_dma);
+
+			if (!buf->skb)
+				continue;
+
+			if (buf->buf_dma != DMA_MAPPING_ERROR)
+				dma_unmap_single(sdma->sw->dev->dev,
+						 buf->buf_dma, buf->skb->len,
+						 DMA_FROM_DEVICE);
+			kfree_skb(buf->skb);
+		}
+	}
+}
+
+static int mvsw_sdma_rx_init(struct mvsw_pr_rxtx_sdma *sdma)
+{
+	int q, b;
+	int err;
+
+	/* disable all rx queues */
+	mvsw_reg_write(sdma->sw, SDMA_RX_QUEUE_STATUS_REG, 0xff00);
+
+	for (q = 0; q < SDMA_RX_QUEUE_NUM; q++) {
+		struct mvsw_sdma_rx_ring *ring = &sdma->rx_ring[q];
+		struct mvsw_sdma_buf *head;
+
+		ring->bufs = kmalloc_array(SDMA_RX_DESC_PER_Q, sizeof(*head),
+					   GFP_KERNEL);
+		if (!ring->bufs)
+			return -ENOMEM;
+
+		ring->weight = prestera_rx_weight_map[q];
+		ring->recvd = 0;
+		ring->next_rx = 0;
+
+		head = &ring->bufs[0];
+
+		for (b = 0; b < SDMA_RX_DESC_PER_Q; b++) {
+			struct mvsw_sdma_buf *buf = &ring->bufs[b];
+
+			err = mvsw_sdma_buf_desc_alloc(sdma, buf);
+			if (err)
+				return err;
+
+			err = mvsw_sdma_rx_dma_alloc(sdma, buf);
+			if (err)
+				return err;
+
+			mvsw_sdma_rx_desc_init(sdma, buf->desc, buf->buf_dma);
+
+			if (b == 0)
+				continue;
+
+			mvsw_sdma_rx_desc_set_next(sdma, ring->bufs[b - 1].desc,
+						   buf->desc_dma);
+
+			if (b == SDMA_RX_DESC_PER_Q - 1)
+				mvsw_sdma_rx_desc_set_next(sdma, buf->desc,
+							   head->desc_dma);
+		}
+
+		mvsw_reg_write(sdma->sw, SDMA_RX_QUEUE_DESC_REG(q),
+			       mvsw_sdma_addr_phy(sdma, head->desc_dma));
+	}
+
+	/* make sure all rx descs are filled before enabling all rx queues */
+	wmb();
+	mvsw_reg_write(sdma->sw, SDMA_RX_QUEUE_STATUS_REG, 0xff);
+
+	return 0;
+}
+
+static void mvsw_sdma_tx_desc_init(struct mvsw_pr_rxtx_sdma *sdma,
+				   struct mvsw_sdma_desc *desc)
+{
+	desc->word1 = cpu_to_le32(SDMA_TX_DESC_SINGLE | SDMA_TX_DESC_CALC_CRC);
+	desc->word2 = 0;
+}
+
+static void mvsw_sdma_tx_desc_set_next(struct mvsw_pr_rxtx_sdma *sdma,
+				       struct mvsw_sdma_desc *desc,
+				       dma_addr_t next)
+{
+	desc->next = cpu_to_le32(mvsw_sdma_addr_phy(sdma, next));
+}
+
+static void mvsw_sdma_tx_desc_set_buf(struct mvsw_pr_rxtx_sdma *sdma,
+				      struct mvsw_sdma_desc *desc,
+				      dma_addr_t buf, size_t len)
+{
+	u32 word = le32_to_cpu(desc->word2);
+
+	word = (word & ~GENMASK(30, 16)) | ((len + 4) << 16);
+
+	desc->buff = cpu_to_le32(mvsw_sdma_addr_phy(sdma, buf));
+	desc->word2 = cpu_to_le32(word);
+}
+
+static void mvsw_sdma_tx_desc_xmit(struct mvsw_sdma_desc *desc)
+{
+	u32 word = le32_to_cpu(desc->word1);
+
+	word |= (SDMA_TX_DESC_DMA_OWN << 31);
+
+	/* make sure everything is written before enable xmit */
+	wmb();
+	desc->word1 = cpu_to_le32(word);
+}
+
+static int mvsw_sdma_tx_buf_map(struct mvsw_pr_rxtx_sdma *sdma,
+				struct mvsw_sdma_buf *buf,
+				struct sk_buff *skb)
+{
+	struct device *dma_dev = sdma->sw->dev->dev;
+	struct sk_buff *new_skb;
+	size_t len = skb->len;
+	dma_addr_t dma;
+
+	dma = dma_map_single(dma_dev, skb->data, len, DMA_TO_DEVICE);
+	if (!dma_mapping_error(dma_dev, dma) && dma + len <= sdma->dma_mask) {
+		buf->buf_dma = dma;
+		buf->skb = skb;
+		return 0;
+	}
+
+	if (!dma_mapping_error(dma_dev, dma))
+		dma_unmap_single(dma_dev, dma, len, DMA_TO_DEVICE);
+
+	new_skb = alloc_skb(len, GFP_ATOMIC | GFP_DMA);
+	if (!new_skb)
+		goto err_alloc_skb;
+
+	dma = dma_map_single(dma_dev, new_skb->data, len, DMA_TO_DEVICE);
+	if (dma_mapping_error(dma_dev, dma))
+		goto err_dma_map;
+	if (dma + len > sdma->dma_mask)
+		goto err_dma_range;
+
+	skb_copy_from_linear_data(skb, skb_put(new_skb, len), len);
+
+	dev_consume_skb_any(skb);
+
+	buf->skb = new_skb;
+	buf->buf_dma = dma;
+
+	return 0;
+
+err_dma_range:
+	dma_unmap_single(dma_dev, dma, len, DMA_TO_DEVICE);
+err_dma_map:
+	dev_kfree_skb(new_skb);
+err_alloc_skb:
+	dev_kfree_skb(skb);
+
+	return -ENOMEM;
+}
+
+static void mvsw_sdma_tx_buf_unmap(struct mvsw_pr_rxtx_sdma *sdma,
+				   struct mvsw_sdma_buf *buf)
+{
+	struct device *dma_dev = sdma->sw->dev->dev;
+
+	dma_unmap_single(dma_dev, buf->buf_dma, buf->skb->len, DMA_TO_DEVICE);
+}
+
+static void mvsw_sdma_tx_recycle_work_fn(struct work_struct *work)
+{
+	struct mvsw_sdma_tx_ring *tx_ring;
+	struct mvsw_pr_rxtx_sdma *sdma;
+	struct device *dma_dev;
+	int b;
+
+	sdma = container_of(work, struct mvsw_pr_rxtx_sdma, tx_work);
+
+	dma_dev = sdma->sw->dev->dev;
+	tx_ring = &sdma->tx_ring;
+
+	for (b = 0; b < SDMA_TX_DESC_PER_Q; b++) {
+		struct mvsw_sdma_buf *buf = &tx_ring->bufs[b];
+
+		if (!buf->is_used)
+			continue;
+
+		if (!SDMA_TX_DESC_IS_SENT(buf->desc))
+			continue;
+
+		mvsw_sdma_tx_buf_unmap(sdma, buf);
+		dev_consume_skb_any(buf->skb);
+		buf->skb = NULL;
+
+		/* make sure everything is cleaned up */
+		wmb();
+
+		buf->is_used = false;
+	}
+}
+
+static int mvsw_sdma_tx_init(struct mvsw_pr_rxtx_sdma *sdma)
+{
+	struct mvsw_sdma_tx_ring *tx_ring = &sdma->tx_ring;
+	struct mvsw_sdma_buf *head;
+	int err;
+	int b;
+
+	spin_lock_init(&sdma->tx_lock);
+
+	INIT_WORK(&sdma->tx_work, mvsw_sdma_tx_recycle_work_fn);
+
+	tx_ring->bufs = kmalloc_array(SDMA_TX_DESC_PER_Q, sizeof(*head),
+				      GFP_KERNEL);
+	if (!tx_ring->bufs)
+		return -ENOMEM;
+
+	head = &tx_ring->bufs[0];
+
+	tx_ring->max_burst = SDMA_TX_MAX_BURST;
+	tx_ring->burst = tx_ring->max_burst;
+	tx_ring->next_tx = 0;
+
+	for (b = 0; b < SDMA_TX_DESC_PER_Q; b++) {
+		struct mvsw_sdma_buf *buf = &tx_ring->bufs[b];
+
+		err = mvsw_sdma_buf_desc_alloc(sdma, buf);
+		if (err)
+			return err;
+
+		mvsw_sdma_tx_desc_init(sdma, buf->desc);
+
+		buf->is_used = false;
+		buf->skb = NULL;
+
+		if (b == 0)
+			continue;
+
+		mvsw_sdma_tx_desc_set_next(sdma, tx_ring->bufs[b - 1].desc,
+					   buf->desc_dma);
+
+		if (b == SDMA_TX_DESC_PER_Q - 1)
+			mvsw_sdma_tx_desc_set_next(sdma, buf->desc,
+						   head->desc_dma);
+	}
+
+	/* make sure descriptors are written */
+	wmb();
+	mvsw_reg_write(sdma->sw, SDMA_TX_QUEUE_DESC_REG,
+		       mvsw_sdma_addr_phy(sdma, head->desc_dma));
+
+	return 0;
+}
+
+static void mvsw_sdma_tx_fini(struct mvsw_pr_rxtx_sdma *sdma)
+{
+	struct mvsw_sdma_tx_ring *ring = &sdma->tx_ring;
+	int b;
+
+	cancel_work_sync(&sdma->tx_work);
+
+	if (!ring->bufs)
+		return;
+
+	for (b = 0; b < SDMA_TX_DESC_PER_Q; b++) {
+		struct mvsw_sdma_buf *buf = &ring->bufs[b];
+
+		if (buf->desc)
+			dma_pool_free(sdma->desc_pool, buf->desc,
+				      buf->desc_dma);
+
+		if (!buf->skb)
+			continue;
+
+		dma_unmap_single(sdma->sw->dev->dev, buf->buf_dma,
+				 buf->skb->len, DMA_TO_DEVICE);
+
+		dev_consume_skb_any(buf->skb);
+	}
+}
+
+int mvsw_pr_rxtx_sdma_init(struct mvsw_pr_rxtx *rxtx)
+{
+	struct mvsw_pr_rxtx_sdma *sdma;
+
+	sdma = kzalloc(sizeof(*sdma), GFP_KERNEL);
+	if (!sdma)
+		return -ENOMEM;
+
+	rxtx->priv = sdma;
+	sdma->rxtx = rxtx;
+
+	return 0;
+}
+
+int mvsw_pr_rxtx_sdma_fini(struct mvsw_pr_rxtx *rxtx)
+{
+	kfree(rxtx->priv);
+	return 0;
+}
+
+static void mvsw_rxtx_handle_event(struct mvsw_pr_switch *sw,
+				   struct mvsw_pr_event *evt, void *arg)
+{
+	struct mvsw_pr_rxtx_sdma *sdma = arg;
+
+	if (evt->id != MVSW_RXTX_EVENT_RCV_PKT)
+		return;
+
+	mvsw_reg_write(sdma->sw, SDMA_RX_INTR_MASK_REG, 0);
+	napi_schedule(&sdma->rx_napi);
+}
+
+int mvsw_pr_rxtx_sdma_switch_init(struct mvsw_pr_rxtx *rxtx,
+				  struct mvsw_pr_switch *sw)
+{
+	struct mvsw_pr_rxtx_sdma *sdma = rxtx->priv;
+	int err;
+
+	err = mvsw_pr_hw_rxtx_init(sw, true, &sdma->map_addr);
+	if (err) {
+		dev_err(sw->dev->dev, "failed to init rxtx by hw\n");
+		return err;
+	}
+
+	sdma->dma_mask = dma_get_mask(sw->dev->dev);
+	sdma->sw = sw;
+
+	sdma->desc_pool = dma_pool_create("desc_pool", sdma->sw->dev->dev,
+					  sizeof(struct mvsw_sdma_desc), 16, 0);
+	if (!sdma->desc_pool)
+		return -ENOMEM;
+
+	err = mvsw_sdma_rx_init(sdma);
+	if (err) {
+		dev_err(sw->dev->dev, "failed to init rx ring\n");
+		goto err_rx_init;
+	}
+
+	err = mvsw_sdma_tx_init(sdma);
+	if (err) {
+		dev_err(sw->dev->dev, "failed to init tx ring\n");
+		goto err_tx_init;
+	}
+
+	err = mvsw_pr_hw_event_handler_register(sw, MVSW_EVENT_TYPE_RXTX,
+						mvsw_rxtx_handle_event, sdma);
+	if (err)
+		goto err_evt_register;
+
+	init_dummy_netdev(&sdma->napi_dev);
+
+	netif_napi_add(&sdma->napi_dev, &sdma->rx_napi, mvsw_sdma_rx_poll, 64);
+	napi_enable(&sdma->rx_napi);
+
+	return 0;
+
+err_evt_register:
+err_tx_init:
+	mvsw_sdma_tx_fini(sdma);
+err_rx_init:
+	mvsw_sdma_rx_fini(sdma);
+
+	dma_pool_destroy(sdma->desc_pool);
+	return err;
+}
+
+void mvsw_pr_rxtx_sdma_switch_fini(struct mvsw_pr_rxtx *rxtx,
+				   struct mvsw_pr_switch *sw)
+{
+	struct mvsw_pr_rxtx_sdma *sdma = rxtx->priv;
+
+	mvsw_pr_hw_event_handler_unregister(sw, MVSW_EVENT_TYPE_RXTX);
+	napi_disable(&sdma->rx_napi);
+	netif_napi_del(&sdma->rx_napi);
+	mvsw_sdma_rx_fini(sdma);
+	mvsw_sdma_tx_fini(sdma);
+	dma_pool_destroy(sdma->desc_pool);
+}
+
+static int mvsw_sdma_wait_tx(struct mvsw_pr_rxtx_sdma *sdma,
+			     struct mvsw_sdma_tx_ring *tx_ring)
+{
+	int tx_retry_num = 10 * tx_ring->max_burst;
+
+	while (--tx_retry_num) {
+		if (!(mvsw_reg_read(sdma->sw, SDMA_TX_QUEUE_START_REG) & 1))
+			return 0;
+
+		udelay(5);
+	}
+
+	return -EBUSY;
+}
+
+static void mvsw_sdma_start_tx(struct mvsw_pr_rxtx_sdma *sdma)
+{
+	mvsw_reg_write(sdma->sw, SDMA_TX_QUEUE_START_REG, 1);
+	schedule_work(&sdma->tx_work);
+}
+
+netdev_tx_t mvsw_pr_rxtx_sdma_xmit(struct mvsw_pr_rxtx *rxtx,
+				   struct sk_buff *skb)
+{
+	struct mvsw_pr_rxtx_sdma *sdma = rxtx->priv;
+	struct device *dma_dev = sdma->sw->dev->dev;
+	struct mvsw_sdma_tx_ring *tx_ring;
+	struct net_device *dev = skb->dev;
+	struct mvsw_sdma_buf *buf;
+	int err;
+
+	spin_lock(&sdma->tx_lock);
+
+	tx_ring = &sdma->tx_ring;
+
+	buf = &tx_ring->bufs[tx_ring->next_tx];
+	if (buf->is_used) {
+		schedule_work(&sdma->tx_work);
+		goto drop_skb;
+	}
+
+	if (unlikely(skb_put_padto(skb, ETH_ZLEN)))
+		goto drop_skb_nofree;
+
+	err = mvsw_sdma_tx_buf_map(sdma, buf, skb);
+	if (err)
+		goto drop_skb;
+
+	mvsw_sdma_tx_desc_set_buf(sdma, buf->desc, buf->buf_dma, skb->len);
+
+	dma_sync_single_for_device(dma_dev, buf->buf_dma, skb->len,
+				   DMA_TO_DEVICE);
+
+	if (!tx_ring->burst--) {
+		tx_ring->burst = tx_ring->max_burst;
+
+		err = mvsw_sdma_wait_tx(sdma, tx_ring);
+		if (err)
+			goto drop_skb_unmap;
+	}
+
+	tx_ring->next_tx = (tx_ring->next_tx + 1) % SDMA_TX_DESC_PER_Q;
+	mvsw_sdma_tx_desc_xmit(buf->desc);
+	buf->is_used = true;
+
+	mvsw_sdma_start_tx(sdma);
+
+	goto tx_done;
+
+drop_skb_unmap:
+	mvsw_sdma_tx_buf_unmap(sdma, buf);
+drop_skb:
+	dev_consume_skb_any(skb);
+drop_skb_nofree:
+	dev->stats.tx_dropped++;
+tx_done:
+	spin_unlock(&sdma->tx_lock);
+	return NETDEV_TX_OK;
+}
diff --git a/drivers/net/ethernet/marvell/prestera/prestera_switchdev.c b/drivers/net/ethernet/marvell/prestera/prestera_switchdev.c
index 7d83e1f91..969daebe7 100644
--- a/drivers/net/ethernet/marvell/prestera/prestera_switchdev.c
+++ b/drivers/net/ethernet/marvell/prestera/prestera_switchdev.c
@@ -1,108 +1,127 @@
-// SPDX-License-Identifier: BSD-3-Clause OR GPL-2.0
-/* Copyright (c) 2019-2020 Marvell International Ltd. All rights reserved */
-
-#include <linux/if_bridge.h>
-#include <linux/if_vlan.h>
+/* SPDX-License-Identifier: BSD-3-Clause OR GPL-2.0
+ *
+ * Copyright (c) 2019-2020 Marvell International Ltd. All rights reserved.
+ *
+ */
 #include <linux/kernel.h>
 #include <linux/module.h>
+#include <linux/if_vlan.h>
+#include <linux/if_bridge.h>
 #include <linux/notifier.h>
-#include <net/netevent.h>
 #include <net/switchdev.h>
+#include <net/netevent.h>
+#include <net/vxlan.h>
 
 #include "prestera.h"
-#include "prestera_hw.h"
-#include "prestera_switchdev.h"
-
-#define PRESTERA_VID_ALL (0xffff)
 
-#define PRESTERA_DEFAULT_AGEING_TIME_MS 300000
-#define PRESTERA_MAX_AGEING_TIME_MS 1000000000
-#define PRESTERA_MIN_AGEING_TIME_MS 32000
+#define MVSW_PR_VID_ALL (0xffff)
 
-struct prestera_fdb_event_work {
-	struct work_struct work;
-	struct switchdev_notifier_fdb_info fdb_info;
-	struct net_device *dev;
-	unsigned long event;
-};
-
-struct prestera_switchdev {
-	struct prestera_switch *sw;
+struct mvsw_pr_bridge {
+	struct mvsw_pr_switch *sw;
+	u32 ageing_time;
 	struct list_head bridge_list;
 	bool bridge_8021q_exists;
-	struct notifier_block swdev_nb_blk;
-	struct notifier_block swdev_nb;
 };
 
-struct prestera_bridge {
-	struct list_head head;
+struct mvsw_pr_bridge_device {
 	struct net_device *dev;
-	struct prestera_switchdev *swdev;
+	struct list_head bridge_node;
 	struct list_head port_list;
-	bool vlan_enabled;
 	u16 bridge_id;
+	u8 vlan_enabled:1, multicast_enabled:1, mrouter:1;
 };
 
-struct prestera_bridge_port {
-	struct list_head head;
+struct mvsw_pr_bridge_port {
 	struct net_device *dev;
-	struct prestera_bridge *bridge;
+	struct mvsw_pr_bridge_device *bridge_device;
+	struct list_head bridge_device_node;
 	struct list_head vlan_list;
-	refcount_t ref_count;
-	unsigned long flags;
+	unsigned int ref_count;
 	u8 stp_state;
+	unsigned long flags;
 };
 
-struct prestera_bridge_vlan {
-	struct list_head head;
+struct mvsw_pr_bridge_vlan {
+	struct list_head bridge_port_node;
 	struct list_head port_vlan_list;
 	u16 vid;
 };
 
-struct prestera_port_vlan {
-	struct list_head br_vlan_head;
-	struct list_head port_head;
-	struct prestera_port *port;
-	struct prestera_bridge_port *br_port;
-	u16 vid;
+struct mvsw_pr_event_work {
+	struct work_struct work;
+	struct switchdev_notifier_fdb_info fdb_info;
+	struct net_device *dev;
+	unsigned long event;
 };
 
-static struct workqueue_struct *swdev_wq;
+static struct workqueue_struct *mvsw_owq;
 
-static void prestera_bridge_port_put(struct prestera_bridge_port *br_port);
+static struct mvsw_pr_bridge_port *
+mvsw_pr_bridge_port_get(struct mvsw_pr_bridge *bridge,
+			struct net_device *brport_dev);
 
-static int prestera_port_vid_stp_set(struct prestera_port *port, u16 vid,
-				     u8 state);
+static void mvsw_pr_bridge_port_put(struct mvsw_pr_bridge *bridge,
+				    struct mvsw_pr_bridge_port *br_port);
 
-static struct prestera_bridge_vlan *
-prestera_bridge_vlan_create(struct prestera_bridge_port *br_port, u16 vid)
+struct mvsw_pr_bridge_device *
+mvsw_pr_bridge_device_find(const struct mvsw_pr_bridge *bridge,
+			   const struct net_device *br_dev)
 {
-	struct prestera_bridge_vlan *br_vlan;
+	struct mvsw_pr_bridge_device *bridge_device;
 
-	br_vlan = kzalloc(sizeof(*br_vlan), GFP_KERNEL);
-	if (!br_vlan)
-		return NULL;
+	list_for_each_entry(bridge_device, &bridge->bridge_list,
+			    bridge_node)
+		if (bridge_device->dev == br_dev)
+			return bridge_device;
 
-	INIT_LIST_HEAD(&br_vlan->port_vlan_list);
-	br_vlan->vid = vid;
-	list_add(&br_vlan->head, &br_port->vlan_list);
+	return NULL;
+}
 
-	return br_vlan;
+static bool
+mvsw_pr_bridge_device_is_offloaded(const struct mvsw_pr_switch *sw,
+				   const struct net_device *br_dev)
+{
+	return !!mvsw_pr_bridge_device_find(sw->bridge, br_dev);
 }
 
-static void prestera_bridge_vlan_destroy(struct prestera_bridge_vlan *br_vlan)
+static struct mvsw_pr_bridge_port *
+__mvsw_pr_bridge_port_find(const struct mvsw_pr_bridge_device *bridge_device,
+			   const struct net_device *brport_dev)
 {
-	list_del(&br_vlan->head);
-	WARN_ON(!list_empty(&br_vlan->port_vlan_list));
-	kfree(br_vlan);
+	struct mvsw_pr_bridge_port *br_port;
+
+	list_for_each_entry(br_port, &bridge_device->port_list,
+			    bridge_device_node) {
+		if (br_port->dev == brport_dev)
+			return br_port;
+	}
+
+	return NULL;
+}
+
+static struct mvsw_pr_bridge_port *
+mvsw_pr_bridge_port_find(struct mvsw_pr_bridge *bridge,
+			 struct net_device *brport_dev)
+{
+	struct net_device *br_dev = netdev_master_upper_dev_get(brport_dev);
+	struct mvsw_pr_bridge_device *bridge_device;
+
+	if (!br_dev)
+		return NULL;
+
+	bridge_device = mvsw_pr_bridge_device_find(bridge, br_dev);
+	if (!bridge_device)
+		return NULL;
+
+	return __mvsw_pr_bridge_port_find(bridge_device, brport_dev);
 }
 
-static struct prestera_bridge_vlan *
-prestera_bridge_vlan_by_vid(struct prestera_bridge_port *br_port, u16 vid)
+static struct mvsw_pr_bridge_vlan *
+mvsw_pr_bridge_vlan_find(const struct mvsw_pr_bridge_port *br_port, u16 vid)
 {
-	struct prestera_bridge_vlan *br_vlan;
+	struct mvsw_pr_bridge_vlan *br_vlan;
 
-	list_for_each_entry(br_vlan, &br_port->vlan_list, head) {
+	list_for_each_entry(br_vlan, &br_port->vlan_list, bridge_port_node) {
 		if (br_vlan->vid == vid)
 			return br_vlan;
 	}
@@ -110,619 +129,523 @@ prestera_bridge_vlan_by_vid(struct prestera_bridge_port *br_port, u16 vid)
 	return NULL;
 }
 
-static int prestera_bridge_vlan_port_count(struct prestera_bridge *bridge,
-					   u16 vid)
+u16 mvsw_pr_vlan_dev_vlan_id(struct mvsw_pr_bridge *bridge,
+			     struct net_device *dev)
 {
-	struct prestera_bridge_port *br_port;
-	struct prestera_bridge_vlan *br_vlan;
-	int count = 0;
+	struct mvsw_pr_bridge_device *bridge_dev;
 
-	list_for_each_entry(br_port, &bridge->port_list, head) {
-		list_for_each_entry(br_vlan, &br_port->vlan_list, head) {
-			if (br_vlan->vid == vid) {
-				count += 1;
-				break;
-			}
-		}
-	}
+	bridge_dev = mvsw_pr_bridge_device_find(bridge, dev);
 
-	return count;
+	return bridge_dev ? bridge_dev->bridge_id : 0;
 }
 
-static void prestera_bridge_vlan_put(struct prestera_bridge_vlan *br_vlan)
+static struct mvsw_pr_bridge_vlan *
+mvsw_pr_bridge_vlan_create(struct mvsw_pr_bridge_port *br_port, u16 vid)
 {
-	if (list_empty(&br_vlan->port_vlan_list))
-		prestera_bridge_vlan_destroy(br_vlan);
+	struct mvsw_pr_bridge_vlan *br_vlan;
+
+	br_vlan = kzalloc(sizeof(*br_vlan), GFP_KERNEL);
+	if (!br_vlan)
+		return NULL;
+
+	INIT_LIST_HEAD(&br_vlan->port_vlan_list);
+	br_vlan->vid = vid;
+	list_add(&br_vlan->bridge_port_node, &br_port->vlan_list);
+
+	return br_vlan;
 }
 
-static struct prestera_port_vlan *
-prestera_port_vlan_by_vid(struct prestera_port *port, u16 vid)
+static void
+mvsw_pr_bridge_vlan_destroy(struct mvsw_pr_bridge_vlan *br_vlan)
 {
-	struct prestera_port_vlan *port_vlan;
+	list_del(&br_vlan->bridge_port_node);
+	WARN_ON(!list_empty(&br_vlan->port_vlan_list));
+	kfree(br_vlan);
+}
 
-	list_for_each_entry(port_vlan, &port->vlans_list, port_head) {
-		if (port_vlan->vid == vid)
-			return port_vlan;
-	}
+static struct mvsw_pr_bridge_vlan *
+mvsw_pr_bridge_vlan_get(struct mvsw_pr_bridge_port *br_port, u16 vid)
+{
+	struct mvsw_pr_bridge_vlan *br_vlan;
 
-	return NULL;
+	br_vlan = mvsw_pr_bridge_vlan_find(br_port, vid);
+	if (br_vlan)
+		return br_vlan;
+
+	return mvsw_pr_bridge_vlan_create(br_port, vid);
+}
+
+static void mvsw_pr_bridge_vlan_put(struct mvsw_pr_bridge_vlan *br_vlan)
+{
+	if (list_empty(&br_vlan->port_vlan_list))
+		mvsw_pr_bridge_vlan_destroy(br_vlan);
 }
 
-static struct prestera_port_vlan *
-prestera_port_vlan_create(struct prestera_port *port, u16 vid, bool untagged)
+static int
+mvsw_pr_port_vlan_bridge_join(struct mvsw_pr_port_vlan *port_vlan,
+			      struct mvsw_pr_bridge_port *br_port,
+			      struct netlink_ext_ack *extack)
 {
-	struct prestera_port_vlan *port_vlan;
+	struct mvsw_pr_port *port = port_vlan->mvsw_pr_port;
+	struct mvsw_pr_bridge_vlan *br_vlan;
+	u16 vid = port_vlan->vid;
 	int err;
 
-	port_vlan = prestera_port_vlan_by_vid(port, vid);
-	if (port_vlan)
-		return ERR_PTR(-EEXIST);
+	if (port_vlan->bridge_port)
+		return 0;
 
-	err = prestera_hw_vlan_port_set(port, vid, true, untagged);
+	err = mvsw_pr_port_uc_flood_set(port, br_port->flags & BR_FLOOD);
 	if (err)
-		return ERR_PTR(err);
+		return err;
 
-	port_vlan = kzalloc(sizeof(*port_vlan), GFP_KERNEL);
-	if (!port_vlan) {
+	err = mvsw_pr_port_mc_flood_set(port, br_port->flags & BR_MCAST_FLOOD);
+	if (err)
+		return err;
+
+	err = mvsw_pr_port_learning_set(port, br_port->flags & BR_LEARNING);
+	if (err)
+		goto err_port_learning_set;
+
+	err = mvsw_pr_port_vid_stp_set(port, vid, br_port->stp_state);
+	if (err)
+		goto err_port_vid_stp_set;
+
+	br_vlan = mvsw_pr_bridge_vlan_get(br_port, vid);
+	if (!br_vlan) {
 		err = -ENOMEM;
-		goto err_port_vlan_alloc;
+		goto err_bridge_vlan_get;
 	}
 
-	port_vlan->port = port;
-	port_vlan->vid = vid;
+	list_add(&port_vlan->bridge_vlan_node, &br_vlan->port_vlan_list);
 
-	list_add(&port_vlan->port_head, &port->vlans_list);
+	mvsw_pr_bridge_port_get(port->sw->bridge, br_port->dev);
+	port_vlan->bridge_port = br_port;
 
-	return port_vlan;
+	return 0;
 
-err_port_vlan_alloc:
-	prestera_hw_vlan_port_set(port, vid, false, false);
-	return ERR_PTR(err);
+err_bridge_vlan_get:
+	mvsw_pr_port_vid_stp_set(port, vid, BR_STATE_FORWARDING);
+err_port_vid_stp_set:
+	mvsw_pr_port_learning_set(port, false);
+err_port_learning_set:
+	return err;
 }
 
-static void
-prestera_port_vlan_bridge_leave(struct prestera_port_vlan *port_vlan)
+static int
+mvsw_pr_bridge_vlan_port_count_get(struct mvsw_pr_bridge_device *bridge_device,
+				   u16 vid)
 {
-	u32 fdb_flush_mode = PRESTERA_FDB_FLUSH_MODE_DYNAMIC;
-	struct prestera_port *port = port_vlan->port;
-	struct prestera_bridge_vlan *br_vlan;
-	struct prestera_bridge_port *br_port;
-	bool last_port, last_vlan;
+	int count = 0;
+	struct mvsw_pr_bridge_port *br_port;
+	struct mvsw_pr_bridge_vlan *br_vlan;
+
+	list_for_each_entry(br_port, &bridge_device->port_list,
+			    bridge_device_node) {
+		list_for_each_entry(br_vlan, &br_port->vlan_list,
+				    bridge_port_node) {
+			if (br_vlan->vid == vid) {
+				count += 1;
+				break;
+			}
+		}
+	}
+
+	return count;
+}
+
+void
+mvsw_pr_port_vlan_bridge_leave(struct mvsw_pr_port_vlan *port_vlan)
+{
+	struct mvsw_pr_port *port = port_vlan->mvsw_pr_port;
+	u32 mode = MVSW_PR_FDB_FLUSH_MODE_DYNAMIC;
+	struct mvsw_pr_bridge_vlan *br_vlan;
+	struct mvsw_pr_bridge_port *br_port;
 	u16 vid = port_vlan->vid;
+	bool last_port, last_vlan;
 	int port_count;
 
-	br_port = port_vlan->br_port;
-	port_count = prestera_bridge_vlan_port_count(br_port->bridge, vid);
-	br_vlan = prestera_bridge_vlan_by_vid(br_port, vid);
-
+	br_port = port_vlan->bridge_port;
 	last_vlan = list_is_singular(&br_port->vlan_list);
+	port_count =
+	    mvsw_pr_bridge_vlan_port_count_get(br_port->bridge_device, vid);
+	br_vlan = mvsw_pr_bridge_vlan_find(br_port, vid);
 	last_port = port_count == 1;
-
 	if (last_vlan)
-		prestera_hw_fdb_flush_port(port, fdb_flush_mode);
+		mvsw_pr_fdb_flush_port(port, mode);
 	else if (last_port)
-		prestera_hw_fdb_flush_vlan(port->sw, vid, fdb_flush_mode);
+		mvsw_pr_fdb_flush_vlan(port->sw, vid, mode);
 	else
-		prestera_hw_fdb_flush_port_vlan(port, vid, fdb_flush_mode);
-
-	list_del(&port_vlan->br_vlan_head);
-	prestera_bridge_vlan_put(br_vlan);
-	prestera_bridge_port_put(br_port);
-	port_vlan->br_port = NULL;
-}
+		mvsw_pr_fdb_flush_port_vlan(port, vid, mode);
 
-static void prestera_port_vlan_destroy(struct prestera_port_vlan *port_vlan)
-{
-	struct prestera_port *port = port_vlan->port;
-	u16 vid = port_vlan->vid;
-
-	if (port_vlan->br_port)
-		prestera_port_vlan_bridge_leave(port_vlan);
-
-	prestera_hw_vlan_port_set(port, vid, false, false);
-	list_del(&port_vlan->port_head);
-	kfree(port_vlan);
+	list_del(&port_vlan->bridge_vlan_node);
+	mvsw_pr_bridge_vlan_put(br_vlan);
+	mvsw_pr_port_vid_stp_set(port, vid, BR_STATE_FORWARDING);
+	mvsw_pr_bridge_port_put(port->sw->bridge, br_port);
+	port_vlan->bridge_port = NULL;
 }
 
-static struct prestera_bridge *
-prestera_bridge_create(struct prestera_switchdev *swdev, struct net_device *dev)
+static int
+mvsw_pr_bridge_port_vlan_add(struct mvsw_pr_port *port,
+			     struct mvsw_pr_bridge_port *br_port,
+			     u16 vid, bool is_untagged, bool is_pvid,
+			     struct netlink_ext_ack *extack)
 {
-	bool vlan_enabled = br_vlan_enabled(dev);
-	struct prestera_bridge *bridge;
-	u16 bridge_id;
+	u16 pvid;
+	struct mvsw_pr_port_vlan *port_vlan;
+	u16 old_pvid = port->pvid;
 	int err;
 
-	if (vlan_enabled && swdev->bridge_8021q_exists) {
-		netdev_err(dev, "Only one VLAN-aware bridge is supported\n");
-		return ERR_PTR(-EINVAL);
-	}
+	if (is_pvid)
+		pvid = vid;
+	else
+		pvid = port->pvid == vid ? 0 : port->pvid;
 
-	bridge = kzalloc(sizeof(*bridge), GFP_KERNEL);
-	if (!bridge)
-		return ERR_PTR(-ENOMEM);
+	port_vlan = mvsw_pr_port_vlan_find_by_vid(port, vid);
+	if (port_vlan && port_vlan->bridge_port != br_port)
+		return -EEXIST;
 
-	if (vlan_enabled) {
-		swdev->bridge_8021q_exists = true;
+	if (!port_vlan) {
+		port_vlan = mvsw_pr_port_vlan_create(port, vid, is_untagged);
+		if (IS_ERR(port_vlan))
+			return PTR_ERR(port_vlan);
 	} else {
-		err = prestera_hw_bridge_create(swdev->sw, &bridge_id);
-		if (err) {
-			kfree(bridge);
-			return ERR_PTR(err);
-		}
-
-		bridge->bridge_id = bridge_id;
+		err = mvsw_pr_port_vlan_set(port, vid, true, is_untagged);
+		if (err)
+			goto err_port_vlan_set;
 	}
 
-	bridge->vlan_enabled = vlan_enabled;
-	bridge->swdev = swdev;
-	bridge->dev = dev;
+	err = mvsw_pr_port_pvid_set(port, pvid);
+	if (err)
+		goto err_port_pvid_set;
+
+	err = mvsw_pr_port_vlan_bridge_join(port_vlan, br_port, extack);
+	if (err)
+		goto err_port_vlan_bridge_join;
 
-	INIT_LIST_HEAD(&bridge->port_list);
+	return 0;
 
-	list_add(&bridge->head, &swdev->bridge_list);
+err_port_vlan_bridge_join:
+	mvsw_pr_port_pvid_set(port, old_pvid);
+err_port_pvid_set:
+	mvsw_pr_port_vlan_set(port, vid, false, false);
+err_port_vlan_set:
+	mvsw_pr_port_vlan_destroy(port_vlan);
 
-	return bridge;
+	return err;
 }
 
-static void prestera_bridge_destroy(struct prestera_bridge *bridge)
+static int mvsw_pr_port_vlans_add(struct mvsw_pr_port *port,
+				  const struct switchdev_obj_port_vlan *vlan,
+				  struct switchdev_trans *trans,
+				  struct netlink_ext_ack *extack)
 {
-	struct prestera_switchdev *swdev = bridge->swdev;
+	bool flag_untagged = vlan->flags & BRIDGE_VLAN_INFO_UNTAGGED;
+	bool flag_pvid = vlan->flags & BRIDGE_VLAN_INFO_PVID;
+	struct net_device *orig_dev = vlan->obj.orig_dev;
+	struct mvsw_pr_bridge_port *br_port;
+	struct mvsw_pr_bridge_device *bridge_device;
+	struct mvsw_pr_switch *sw = port->sw;
+	u16 vid;
 
-	list_del(&bridge->head);
+	if (netif_is_bridge_master(orig_dev))
+		return 0;
 
-	if (bridge->vlan_enabled)
-		swdev->bridge_8021q_exists = false;
-	else
-		prestera_hw_bridge_delete(swdev->sw, bridge->bridge_id);
+	if (switchdev_trans_ph_commit(trans))
+		return 0;
 
-	WARN_ON(!list_empty(&bridge->port_list));
-	kfree(bridge);
-}
+	br_port = mvsw_pr_bridge_port_find(sw->bridge, orig_dev);
+	if (WARN_ON(!br_port))
+		return -EINVAL;
 
-static void prestera_bridge_put(struct prestera_bridge *bridge)
-{
-	if (list_empty(&bridge->port_list))
-		prestera_bridge_destroy(bridge);
-}
+	bridge_device = br_port->bridge_device;
+	if (!bridge_device->vlan_enabled)
+		return 0;
 
-static
-struct prestera_bridge *prestera_bridge_by_dev(struct prestera_switchdev *swdev,
-					       const struct net_device *dev)
-{
-	struct prestera_bridge *bridge;
+	for (vid = vlan->vid_begin; vid <= vlan->vid_end; vid++) {
+		int err;
+
+		err = mvsw_pr_bridge_port_vlan_add(port, br_port,
+						   vid, flag_untagged,
+						   flag_pvid, extack);
+		if (err)
+			return err;
+	}
 
-	list_for_each_entry(bridge, &swdev->bridge_list, head)
-		if (bridge->dev == dev)
-			return bridge;
+	if (list_is_singular(&bridge_device->port_list))
+		mvsw_pr_rif_enable(port->sw, bridge_device->dev, true);
 
-	return NULL;
+	return 0;
 }
 
-static struct prestera_bridge_port *
-__prestera_bridge_port_by_dev(struct prestera_bridge *bridge,
-			      struct net_device *dev)
+static int mvsw_pr_port_obj_add(struct net_device *dev,
+				const struct switchdev_obj *obj,
+				struct switchdev_trans *trans,
+				struct netlink_ext_ack *extack)
 {
-	struct prestera_bridge_port *br_port;
+	int err = 0;
+	struct mvsw_pr_port *port = netdev_priv(dev);
+	const struct switchdev_obj_port_vlan *vlan;
 
-	list_for_each_entry(br_port, &bridge->port_list, head) {
-		if (br_port->dev == dev)
-			return br_port;
+	switch (obj->id) {
+	case SWITCHDEV_OBJ_ID_PORT_VLAN:
+		vlan = SWITCHDEV_OBJ_PORT_VLAN(obj);
+		err = mvsw_pr_port_vlans_add(port, vlan, trans, extack);
+		break;
+	default:
+		err = -EOPNOTSUPP;
 	}
 
-	return NULL;
+	return err;
 }
 
-static struct prestera_bridge_port *
-prestera_bridge_port_by_dev(struct prestera_switchdev *swdev,
-			    struct net_device *dev)
+static void
+mvsw_pr_bridge_port_vlan_del(struct mvsw_pr_port *port,
+			     struct mvsw_pr_bridge_port *br_port, u16 vid)
 {
-	struct net_device *br_dev = netdev_master_upper_dev_get(dev);
-	struct prestera_bridge *bridge;
-
-	if (!br_dev)
-		return NULL;
+	u16 pvid = port->pvid == vid ? 0 : port->pvid;
+	struct mvsw_pr_port_vlan *port_vlan;
 
-	bridge = prestera_bridge_by_dev(swdev, br_dev);
-	if (!bridge)
-		return NULL;
+	port_vlan = mvsw_pr_port_vlan_find_by_vid(port, vid);
+	if (WARN_ON(!port_vlan))
+		return;
 
-	return __prestera_bridge_port_by_dev(bridge, dev);
+	mvsw_pr_port_vlan_bridge_leave(port_vlan);
+	mvsw_pr_port_pvid_set(port, pvid);
+	mvsw_pr_port_vlan_destroy(port_vlan);
 }
 
-static struct prestera_bridge_port *
-prestera_bridge_port_create(struct prestera_bridge *bridge,
-			    struct net_device *dev)
+static int mvsw_pr_port_vlans_del(struct mvsw_pr_port *port,
+				  const struct switchdev_obj_port_vlan *vlan)
 {
-	struct prestera_bridge_port *br_port;
-
-	br_port = kzalloc(sizeof(*br_port), GFP_KERNEL);
-	if (!br_port)
-		return NULL;
+	struct mvsw_pr_switch *sw = port->sw;
+	struct net_device *orig_dev = vlan->obj.orig_dev;
+	struct mvsw_pr_bridge_port *br_port;
+	u16 vid;
 
-	br_port->flags = BR_LEARNING | BR_FLOOD | BR_LEARNING_SYNC |
-				BR_MCAST_FLOOD;
-	br_port->stp_state = BR_STATE_DISABLED;
-	refcount_set(&br_port->ref_count, 1);
-	br_port->bridge = bridge;
-	br_port->dev = dev;
+	if (netif_is_bridge_master(orig_dev))
+		return -EOPNOTSUPP;
 
-	INIT_LIST_HEAD(&br_port->vlan_list);
-	list_add(&br_port->head, &bridge->port_list);
+	br_port = mvsw_pr_bridge_port_find(sw->bridge, orig_dev);
+	if (WARN_ON(!br_port))
+		return -EINVAL;
 
-	return br_port;
-}
+	if (!br_port->bridge_device->vlan_enabled)
+		return 0;
 
-static void
-prestera_bridge_port_destroy(struct prestera_bridge_port *br_port)
-{
-	list_del(&br_port->head);
-	WARN_ON(!list_empty(&br_port->vlan_list));
-	kfree(br_port);
-}
+	for (vid = vlan->vid_begin; vid <= vlan->vid_end; vid++)
+		mvsw_pr_bridge_port_vlan_del(port, br_port, vid);
 
-static void prestera_bridge_port_get(struct prestera_bridge_port *br_port)
-{
-	refcount_inc(&br_port->ref_count);
+	return 0;
 }
 
-static void prestera_bridge_port_put(struct prestera_bridge_port *br_port)
+static int mvsw_pr_port_obj_del(struct net_device *dev,
+				const struct switchdev_obj *obj)
 {
-	struct prestera_bridge *bridge = br_port->bridge;
+	int err = 0;
+	struct mvsw_pr_port *port = netdev_priv(dev);
 
-	if (refcount_dec_and_test(&br_port->ref_count)) {
-		prestera_bridge_port_destroy(br_port);
-		prestera_bridge_put(bridge);
+	switch (obj->id) {
+	case SWITCHDEV_OBJ_ID_PORT_VLAN:
+		err = mvsw_pr_port_vlans_del(port,
+					     SWITCHDEV_OBJ_PORT_VLAN(obj));
+		break;
+	default:
+		err = -EOPNOTSUPP;
+		break;
 	}
+
+	return err;
 }
 
-static struct prestera_bridge_port *
-prestera_bridge_port_add(struct prestera_bridge *bridge, struct net_device *dev)
+static int mvsw_pr_port_attr_br_vlan_set(struct mvsw_pr_port *port,
+					 struct switchdev_trans *trans,
+					 struct net_device *orig_dev,
+					 bool vlan_enabled)
 {
-	struct prestera_bridge_port *br_port;
+	struct mvsw_pr_switch *sw = port->sw;
+	struct mvsw_pr_bridge_device *bridge_device;
 
-	br_port = __prestera_bridge_port_by_dev(bridge, dev);
-	if (br_port) {
-		prestera_bridge_port_get(br_port);
-		return br_port;
-	}
+	if (!switchdev_trans_ph_prepare(trans))
+		return 0;
 
-	br_port = prestera_bridge_port_create(bridge, dev);
-	if (!br_port)
-		return ERR_PTR(-ENOMEM);
+	bridge_device = mvsw_pr_bridge_device_find(sw->bridge, orig_dev);
+	if (WARN_ON(!bridge_device))
+		return -EINVAL;
 
-	return br_port;
+	if (bridge_device->vlan_enabled == vlan_enabled)
+		return 0;
+
+	netdev_err(bridge_device->dev,
+		   "VLAN filtering can't be changed for existing bridge\n");
+	return -EINVAL;
 }
 
-static int
-prestera_bridge_1d_port_join(struct prestera_bridge_port *br_port)
+static int mvsw_pr_port_attr_br_flags_set(struct mvsw_pr_port *port,
+					  struct switchdev_trans *trans,
+					  struct net_device *orig_dev,
+					  unsigned long flags)
 {
-	struct prestera_port *port = netdev_priv(br_port->dev);
-	struct prestera_bridge *bridge = br_port->bridge;
+	struct mvsw_pr_bridge_port *br_port;
 	int err;
 
-	err = prestera_hw_bridge_port_add(port, bridge->bridge_id);
+	if (switchdev_trans_ph_prepare(trans))
+		return 0;
+
+	br_port = mvsw_pr_bridge_port_find(port->sw->bridge, orig_dev);
+	if (!br_port)
+		return 0;
+
+	err = mvsw_pr_port_uc_flood_set(port, flags & BR_FLOOD);
 	if (err)
 		return err;
 
-	err = prestera_hw_port_flood_set(port, br_port->flags & BR_FLOOD);
+	err = mvsw_pr_port_mc_flood_set(port, flags & BR_MCAST_FLOOD);
 	if (err)
-		goto err_port_flood_set;
+		return err;
 
-	err = prestera_hw_port_learning_set(port, br_port->flags & BR_LEARNING);
+	err = mvsw_pr_port_learning_set(port, flags & BR_LEARNING);
 	if (err)
-		goto err_port_learning_set;
+		return err;
 
+	memcpy(&br_port->flags, &flags, sizeof(flags));
 	return 0;
-
-err_port_learning_set:
-	prestera_hw_port_flood_set(port, false);
-err_port_flood_set:
-	prestera_hw_bridge_port_delete(port, bridge->bridge_id);
-
-	return err;
 }
 
-static int prestera_port_bridge_join(struct prestera_port *port,
-				     struct net_device *upper)
+static int mvsw_pr_port_attr_br_ageing_set(struct mvsw_pr_port *port,
+					   struct switchdev_trans *trans,
+					   unsigned long ageing_clock_t)
 {
-	struct prestera_switchdev *swdev = port->sw->swdev;
-	struct prestera_bridge_port *br_port;
-	struct prestera_bridge *bridge;
 	int err;
+	struct mvsw_pr_switch *sw = port->sw;
+	unsigned long ageing_jiffies = clock_t_to_jiffies(ageing_clock_t);
+	u32 ageing_time = jiffies_to_msecs(ageing_jiffies);
 
-	bridge = prestera_bridge_by_dev(swdev, upper);
-	if (!bridge) {
-		bridge = prestera_bridge_create(swdev, upper);
-		if (IS_ERR(bridge))
-			return PTR_ERR(bridge);
-	}
-
-	br_port = prestera_bridge_port_add(bridge, port->dev);
-	if (IS_ERR(br_port)) {
-		err = PTR_ERR(br_port);
-		goto err_brport_create;
+	if (switchdev_trans_ph_prepare(trans)) {
+		if (ageing_time < MVSW_PR_MIN_AGEING_TIME ||
+		    ageing_time > MVSW_PR_MAX_AGEING_TIME)
+			return -ERANGE;
+		else
+			return 0;
 	}
 
-	if (bridge->vlan_enabled)
-		return 0;
-
-	err = prestera_bridge_1d_port_join(br_port);
-	if (err)
-		goto err_port_join;
-
-	return 0;
+	err = mvsw_pr_switch_ageing_set(sw, ageing_time);
+	if (!err)
+		sw->bridge->ageing_time = ageing_time;
 
-err_port_join:
-	prestera_bridge_port_put(br_port);
-err_brport_create:
-	prestera_bridge_put(bridge);
 	return err;
 }
 
-static void prestera_bridge_1q_port_leave(struct prestera_bridge_port *br_port)
-{
-	struct prestera_port *port = netdev_priv(br_port->dev);
-
-	prestera_hw_fdb_flush_port(port, PRESTERA_FDB_FLUSH_MODE_ALL);
-	prestera_port_pvid_set(port, PRESTERA_DEFAULT_VID);
-}
-
-static void prestera_bridge_1d_port_leave(struct prestera_bridge_port *br_port)
-{
-	struct prestera_port *port = netdev_priv(br_port->dev);
-
-	prestera_hw_fdb_flush_port(port, PRESTERA_FDB_FLUSH_MODE_ALL);
-	prestera_hw_bridge_port_delete(port, br_port->bridge->bridge_id);
-}
-
-static int prestera_port_vid_stp_set(struct prestera_port *port, u16 vid,
-				     u8 state)
-{
-	u8 hw_state = state;
-
-	switch (state) {
-	case BR_STATE_DISABLED:
-		hw_state = PRESTERA_STP_DISABLED;
-		break;
-
-	case BR_STATE_BLOCKING:
-	case BR_STATE_LISTENING:
-		hw_state = PRESTERA_STP_BLOCK_LISTEN;
-		break;
-
-	case BR_STATE_LEARNING:
-		hw_state = PRESTERA_STP_LEARN;
-		break;
-
-	case BR_STATE_FORWARDING:
-		hw_state = PRESTERA_STP_FORWARD;
-		break;
-
-	default:
-		return -EINVAL;
-	}
-
-	return prestera_hw_vlan_port_stp_set(port, vid, hw_state);
-}
-
-static void prestera_port_bridge_leave(struct prestera_port *port,
-				       struct net_device *upper)
-{
-	struct prestera_switchdev *swdev = port->sw->swdev;
-	struct prestera_bridge_port *br_port;
-	struct prestera_bridge *bridge;
-
-	bridge = prestera_bridge_by_dev(swdev, upper);
-	if (!bridge)
-		return;
-
-	br_port = __prestera_bridge_port_by_dev(bridge, port->dev);
-	if (!br_port)
-		return;
-
-	bridge = br_port->bridge;
-
-	if (bridge->vlan_enabled)
-		prestera_bridge_1q_port_leave(br_port);
-	else
-		prestera_bridge_1d_port_leave(br_port);
-
-	prestera_hw_port_learning_set(port, false);
-	prestera_hw_port_flood_set(port, false);
-	prestera_port_vid_stp_set(port, PRESTERA_VID_ALL, BR_STATE_FORWARDING);
-	prestera_bridge_port_put(br_port);
-}
-
-int prestera_bridge_port_event(struct net_device *dev, unsigned long event,
-			       void *ptr)
+static int
+mvsw_pr_port_bridge_vlan_stp_set(struct mvsw_pr_port *port,
+				 struct mvsw_pr_bridge_vlan *br_vlan,
+				 u8 state)
 {
-	struct netdev_notifier_changeupper_info *info = ptr;
-	struct netlink_ext_ack *extack;
-	struct prestera_port *port;
-	struct net_device *upper;
-	int err;
-
-	extack = netdev_notifier_info_to_extack(&info->info);
-	port = netdev_priv(dev);
-	upper = info->upper_dev;
-
-	switch (event) {
-	case NETDEV_PRECHANGEUPPER:
-		if (!netif_is_bridge_master(upper)) {
-			NL_SET_ERR_MSG_MOD(extack, "Unknown upper device type");
-			return -EINVAL;
-		}
-
-		if (!info->linking)
-			break;
-
-		if (netdev_has_any_upper_dev(upper)) {
-			NL_SET_ERR_MSG_MOD(extack, "Upper device is already enslaved");
-			return -EINVAL;
-		}
-		break;
-
-	case NETDEV_CHANGEUPPER:
-		if (!netif_is_bridge_master(upper))
-			break;
+	struct mvsw_pr_port_vlan *port_vlan;
 
-		if (info->linking) {
-			err = prestera_port_bridge_join(port, upper);
-			if (err)
-				return err;
-		} else {
-			prestera_port_bridge_leave(port, upper);
-		}
-		break;
+	list_for_each_entry(port_vlan, &br_vlan->port_vlan_list,
+			    bridge_vlan_node) {
+		if (port_vlan->mvsw_pr_port != port)
+			continue;
+		return mvsw_pr_port_vid_stp_set(port, br_vlan->vid, state);
 	}
 
 	return 0;
 }
 
-static int prestera_port_attr_br_flags_set(struct prestera_port *port,
+static int mvsw_pr_port_attr_stp_state_set(struct mvsw_pr_port *port,
 					   struct switchdev_trans *trans,
-					   struct net_device *dev,
-					   unsigned long flags)
+					   struct net_device *orig_dev,
+					   u8 state)
 {
-	struct prestera_bridge_port *br_port;
+	struct mvsw_pr_bridge_port *br_port;
+	struct mvsw_pr_bridge_vlan *br_vlan;
 	int err;
+	u16 vid;
 
 	if (switchdev_trans_ph_prepare(trans))
 		return 0;
 
-	br_port = prestera_bridge_port_by_dev(port->sw->swdev, dev);
+	br_port = mvsw_pr_bridge_port_find(port->sw->bridge, orig_dev);
 	if (!br_port)
 		return 0;
 
-	err = prestera_hw_port_flood_set(port, flags & BR_FLOOD);
-	if (err)
-		return err;
-
-	err = prestera_hw_port_learning_set(port, flags & BR_LEARNING);
-	if (err)
-		return err;
+	if (!br_port->bridge_device->vlan_enabled) {
+		vid = br_port->bridge_device->bridge_id;
+		err = mvsw_pr_port_vid_stp_set(port, vid, state);
+		if (err)
+			goto err_port_bridge_stp_set;
+	} else {
+		list_for_each_entry(br_vlan, &br_port->vlan_list,
+				    bridge_port_node) {
+			err = mvsw_pr_port_bridge_vlan_stp_set(port, br_vlan,
+							       state);
+			if (err)
+				goto err_port_bridge_vlan_stp_set;
+		}
+	}
 
-	memcpy(&br_port->flags, &flags, sizeof(flags));
+	br_port->stp_state = state;
 
 	return 0;
-}
 
-static int prestera_port_attr_br_ageing_set(struct prestera_port *port,
-					    struct switchdev_trans *trans,
-					    unsigned long ageing_clock_t)
-{
-	unsigned long ageing_jiffies = clock_t_to_jiffies(ageing_clock_t);
-	u32 ageing_time_ms = jiffies_to_msecs(ageing_jiffies);
-	struct prestera_switch *sw = port->sw;
+err_port_bridge_vlan_stp_set:
+	list_for_each_entry_continue_reverse(br_vlan, &br_port->vlan_list,
+					     bridge_port_node)
+		mvsw_pr_port_bridge_vlan_stp_set(port, br_vlan,
+						 br_port->stp_state);
+	return err;
 
-	if (switchdev_trans_ph_prepare(trans)) {
-		if (ageing_time_ms < PRESTERA_MIN_AGEING_TIME_MS ||
-		    ageing_time_ms > PRESTERA_MAX_AGEING_TIME_MS)
-			return -ERANGE;
-		else
-			return 0;
-	}
+err_port_bridge_stp_set:
+	mvsw_pr_port_vid_stp_set(port, vid, br_port->stp_state);
 
-	return prestera_hw_switch_ageing_set(sw, ageing_time_ms);
+	return err;
 }
 
-static int prestera_port_attr_br_vlan_set(struct prestera_port *port,
-					  struct switchdev_trans *trans,
-					  struct net_device *dev,
-					  bool vlan_enabled)
+static int mvsw_pr_port_attr_br_mc_disabled_set(struct mvsw_pr_port *port,
+						struct switchdev_trans *trans,
+						struct net_device *orig_dev,
+						bool mc_disabled)
 {
-	struct prestera_switch *sw = port->sw;
-	struct prestera_bridge *bridge;
+	struct mvsw_pr_switch *sw = port->sw;
+	struct mvsw_pr_bridge_device *br_dev;
+	struct mvsw_pr_bridge_port *br_port;
+	bool enabled = !mc_disabled;
+	int err;
 
 	if (!switchdev_trans_ph_prepare(trans))
 		return 0;
 
-	bridge = prestera_bridge_by_dev(sw->swdev, dev);
-	if (WARN_ON(!bridge))
-		return -EINVAL;
-
-	if (bridge->vlan_enabled == vlan_enabled)
-		return 0;
-
-	netdev_err(bridge->dev, "VLAN filtering can't be changed for existing bridge\n");
-
-	return -EINVAL;
-}
-
-static int prestera_port_bridge_vlan_stp_set(struct prestera_port *port,
-					     struct prestera_bridge_vlan *br_vlan,
-					     u8 state)
-{
-	struct prestera_port_vlan *port_vlan;
-
-	list_for_each_entry(port_vlan, &br_vlan->port_vlan_list, br_vlan_head) {
-		if (port_vlan->port != port)
-			continue;
-
-		return prestera_port_vid_stp_set(port, br_vlan->vid, state);
-	}
-
-	return 0;
-}
-
-static int presterar_port_attr_stp_state_set(struct prestera_port *port,
-					     struct switchdev_trans *trans,
-					     struct net_device *dev,
-					     u8 state)
-{
-	struct prestera_bridge_port *br_port;
-	struct prestera_bridge_vlan *br_vlan;
-	int err;
-	u16 vid;
-
-	if (switchdev_trans_ph_prepare(trans))
+	br_dev = mvsw_pr_bridge_device_find(sw->bridge, orig_dev);
+	if (!br_dev)
 		return 0;
 
-	br_port = prestera_bridge_port_by_dev(port->sw->swdev, dev);
-	if (!br_port)
+	if (br_dev->multicast_enabled == enabled)
 		return 0;
 
-	if (!br_port->bridge->vlan_enabled) {
-		vid = br_port->bridge->bridge_id;
-		err = prestera_port_vid_stp_set(port, vid, state);
+	list_for_each_entry(br_port, &br_dev->port_list, bridge_device_node) {
+		err = mvsw_pr_port_mc_flood_set(netdev_priv(br_port->dev),
+						enabled);
 		if (err)
-			goto err_port_stp_set;
-	} else {
-		list_for_each_entry(br_vlan, &br_port->vlan_list, head) {
-			err = prestera_port_bridge_vlan_stp_set(port, br_vlan,
-								state);
-			if (err)
-				goto err_port_vlan_stp_set;
-		}
+			return err;
 	}
 
-	br_port->stp_state = state;
+	br_dev->multicast_enabled = enabled;
 
 	return 0;
-
-err_port_vlan_stp_set:
-	list_for_each_entry_continue_reverse(br_vlan, &br_port->vlan_list, head)
-		prestera_port_bridge_vlan_stp_set(port, br_vlan, br_port->stp_state);
-	return err;
-
-err_port_stp_set:
-	prestera_port_vid_stp_set(port, vid, br_port->stp_state);
-
-	return err;
 }
 
-static int prestera_port_obj_attr_set(struct net_device *dev,
-				      const struct switchdev_attr *attr,
-				      struct switchdev_trans *trans)
+static int mvsw_pr_port_obj_attr_set(struct net_device *dev,
+				     const struct switchdev_attr *attr,
+				     struct switchdev_trans *trans)
 {
-	struct prestera_port *port = netdev_priv(dev);
 	int err = 0;
+	struct mvsw_pr_port *port = netdev_priv(dev);
 
 	switch (attr->id) {
 	case SWITCHDEV_ATTR_ID_PORT_STP_STATE:
-		err = presterar_port_attr_stp_state_set(port, trans,
-							attr->orig_dev,
-							attr->u.stp_state);
+		err = mvsw_pr_port_attr_stp_state_set(port, trans,
+						      attr->orig_dev,
+						      attr->u.stp_state);
 		break;
 	case SWITCHDEV_ATTR_ID_PORT_PRE_BRIDGE_FLAGS:
 		if (attr->u.brport_flags &
@@ -730,18 +653,23 @@ static int prestera_port_obj_attr_set(struct net_device *dev,
 			err = -EINVAL;
 		break;
 	case SWITCHDEV_ATTR_ID_PORT_BRIDGE_FLAGS:
-		err = prestera_port_attr_br_flags_set(port, trans,
-						      attr->orig_dev,
-						      attr->u.brport_flags);
+		err = mvsw_pr_port_attr_br_flags_set(port, trans,
+						     attr->orig_dev,
+						     attr->u.brport_flags);
 		break;
 	case SWITCHDEV_ATTR_ID_BRIDGE_AGEING_TIME:
-		err = prestera_port_attr_br_ageing_set(port, trans,
-						       attr->u.ageing_time);
+		err = mvsw_pr_port_attr_br_ageing_set(port, trans,
+						      attr->u.ageing_time);
 		break;
 	case SWITCHDEV_ATTR_ID_BRIDGE_VLAN_FILTERING:
-		err = prestera_port_attr_br_vlan_set(port, trans,
-						     attr->orig_dev,
-						     attr->u.vlan_filtering);
+		err = mvsw_pr_port_attr_br_vlan_set(port, trans,
+						    attr->orig_dev,
+						    attr->u.vlan_filtering);
+		break;
+	case SWITCHDEV_ATTR_ID_BRIDGE_MC_DISABLED:
+		err = mvsw_pr_port_attr_br_mc_disabled_set(port, trans,
+							   attr->orig_dev,
+							   attr->u.mc_disabled);
 		break;
 	default:
 		err = -EOPNOTSUPP;
@@ -750,126 +678,121 @@ static int prestera_port_obj_attr_set(struct net_device *dev,
 	return err;
 }
 
-static void
-prestera_fdb_offload_notify(struct prestera_port *port,
-			    struct switchdev_notifier_fdb_info *info)
+static void mvsw_fdb_offload_notify(struct mvsw_pr_port *port,
+				    struct switchdev_notifier_fdb_info *info)
 {
 	struct switchdev_notifier_fdb_info send_info;
 
 	send_info.addr = info->addr;
 	send_info.vid = info->vid;
 	send_info.offloaded = true;
-
-	call_switchdev_notifiers(SWITCHDEV_FDB_OFFLOADED, port->dev,
-				 &send_info.info, NULL);
+	call_switchdev_notifiers(SWITCHDEV_FDB_OFFLOADED,
+				 port->net_dev, &send_info.info, NULL);
 }
 
-static int prestera_port_fdb_set(struct prestera_port *port,
-				 struct switchdev_notifier_fdb_info *fdb_info,
-				 bool adding)
+static int
+mvsw_pr_port_fdb_set(struct mvsw_pr_port *port,
+		     struct switchdev_notifier_fdb_info *fdb_info, bool adding)
 {
-	struct prestera_switch *sw = port->sw;
-	struct prestera_bridge_port *br_port;
-	struct prestera_bridge *bridge;
+	struct mvsw_pr_switch *sw = port->sw;
+	struct mvsw_pr_bridge_port *br_port;
+	struct mvsw_pr_bridge_device *bridge_device;
+	struct net_device *orig_dev = fdb_info->info.dev;
 	int err;
 	u16 vid;
 
-	br_port = prestera_bridge_port_by_dev(sw->swdev, port->dev);
+	br_port = mvsw_pr_bridge_port_find(sw->bridge, orig_dev);
 	if (!br_port)
 		return -EINVAL;
 
-	bridge = br_port->bridge;
+	bridge_device = br_port->bridge_device;
 
-	if (bridge->vlan_enabled)
+	if (bridge_device->vlan_enabled)
 		vid = fdb_info->vid;
 	else
-		vid = bridge->bridge_id;
+		vid = bridge_device->bridge_id;
 
 	if (adding)
-		err = prestera_hw_fdb_add(port, fdb_info->addr, vid, false);
+		err = mvsw_pr_fdb_add(port, fdb_info->addr, vid, false);
 	else
-		err = prestera_hw_fdb_del(port, fdb_info->addr, vid);
+		err = mvsw_pr_fdb_del(port, fdb_info->addr, vid);
 
 	return err;
 }
 
-static void prestera_fdb_event_work(struct work_struct *work)
+static void mvsw_pr_bridge_fdb_event_work(struct work_struct *work)
 {
+	int err = 0;
+	struct mvsw_pr_event_work *switchdev_work =
+	    container_of(work, struct mvsw_pr_event_work, work);
+	struct net_device *dev = switchdev_work->dev;
 	struct switchdev_notifier_fdb_info *fdb_info;
-	struct prestera_fdb_event_work *swdev_work;
-	struct prestera_port *port;
-	struct net_device *dev;
-	int err;
-
-	swdev_work = container_of(work, struct prestera_fdb_event_work, work);
-	dev = swdev_work->dev;
+	struct mvsw_pr_port *port;
 
 	rtnl_lock();
+	if (netif_is_vxlan(dev))
+		goto out;
 
-	port = prestera_port_dev_lower_find(dev);
+	port = mvsw_pr_port_dev_lower_find(dev);
 	if (!port)
-		goto out_unlock;
+		goto out;
 
-	switch (swdev_work->event) {
+	switch (switchdev_work->event) {
 	case SWITCHDEV_FDB_ADD_TO_DEVICE:
-		fdb_info = &swdev_work->fdb_info;
+		fdb_info = &switchdev_work->fdb_info;
 		if (!fdb_info->added_by_user)
 			break;
-
-		err = prestera_port_fdb_set(port, fdb_info, true);
+		err = mvsw_pr_port_fdb_set(port, fdb_info, true);
 		if (err)
 			break;
-
-		prestera_fdb_offload_notify(port, fdb_info);
+		mvsw_fdb_offload_notify(port, fdb_info);
 		break;
-
 	case SWITCHDEV_FDB_DEL_TO_DEVICE:
-		fdb_info = &swdev_work->fdb_info;
-		prestera_port_fdb_set(port, fdb_info, false);
+		fdb_info = &switchdev_work->fdb_info;
+		mvsw_pr_port_fdb_set(port, fdb_info, false);
+		break;
+	case SWITCHDEV_FDB_ADD_TO_BRIDGE:
+	case SWITCHDEV_FDB_DEL_TO_BRIDGE:
 		break;
 	}
 
-out_unlock:
+out:
 	rtnl_unlock();
-
-	kfree(swdev_work->fdb_info.addr);
-	kfree(swdev_work);
+	kfree(switchdev_work->fdb_info.addr);
+	kfree(switchdev_work);
 	dev_put(dev);
 }
 
 static int prestera_switchdev_event(struct notifier_block *unused,
 				    unsigned long event, void *ptr)
 {
-	struct net_device *dev = switchdev_notifier_info_to_dev(ptr);
+	int err = 0;
+	struct net_device *net_dev = switchdev_notifier_info_to_dev(ptr);
+	struct mvsw_pr_event_work *switchdev_work;
 	struct switchdev_notifier_fdb_info *fdb_info;
 	struct switchdev_notifier_info *info = ptr;
-	struct prestera_fdb_event_work *swdev_work;
-	struct net_device *upper;
-	int err;
+	struct net_device *upper_br;
 
 	if (event == SWITCHDEV_PORT_ATTR_SET) {
-		err = switchdev_handle_port_attr_set(dev, ptr,
-						     prestera_netdev_check,
-						     prestera_port_obj_attr_set);
+		err = switchdev_handle_port_attr_set(net_dev, ptr,
+						     mvsw_pr_netdev_check,
+						     mvsw_pr_port_obj_attr_set);
 		return notifier_from_errno(err);
 	}
 
-	if (!prestera_netdev_check(dev))
-		return NOTIFY_DONE;
-
-	upper = netdev_master_upper_dev_get_rcu(dev);
-	if (!upper)
+	upper_br = netdev_master_upper_dev_get_rcu(net_dev);
+	if (!upper_br)
 		return NOTIFY_DONE;
 
-	if (!netif_is_bridge_master(upper))
+	if (!netif_is_bridge_master(upper_br))
 		return NOTIFY_DONE;
 
-	swdev_work = kzalloc(sizeof(*swdev_work), GFP_ATOMIC);
-	if (!swdev_work)
+	switchdev_work = kzalloc(sizeof(*switchdev_work), GFP_ATOMIC);
+	if (!switchdev_work)
 		return NOTIFY_BAD;
 
-	swdev_work->event = event;
-	swdev_work->dev = dev;
+	switchdev_work->dev = net_dev;
+	switchdev_work->event = event;
 
 	switch (event) {
 	case SWITCHDEV_FDB_ADD_TO_DEVICE:
@@ -878,400 +801,847 @@ static int prestera_switchdev_event(struct notifier_block *unused,
 					struct switchdev_notifier_fdb_info,
 					info);
 
-		INIT_WORK(&swdev_work->work, prestera_fdb_event_work);
-		memcpy(&swdev_work->fdb_info, ptr,
-		       sizeof(swdev_work->fdb_info));
-
-		swdev_work->fdb_info.addr = kzalloc(ETH_ALEN, GFP_ATOMIC);
-		if (!swdev_work->fdb_info.addr)
-			goto out_bad;
-
-		ether_addr_copy((u8 *)swdev_work->fdb_info.addr,
+		INIT_WORK(&switchdev_work->work, mvsw_pr_bridge_fdb_event_work);
+		memcpy(&switchdev_work->fdb_info, ptr,
+		       sizeof(switchdev_work->fdb_info));
+		switchdev_work->fdb_info.addr = kzalloc(ETH_ALEN, GFP_ATOMIC);
+		if (!switchdev_work->fdb_info.addr)
+			goto out;
+		ether_addr_copy((u8 *)switchdev_work->fdb_info.addr,
 				fdb_info->addr);
-		dev_hold(dev);
-		break;
+		dev_hold(net_dev);
 
+		break;
+	case SWITCHDEV_FDB_ADD_TO_BRIDGE:
+	case SWITCHDEV_FDB_DEL_TO_BRIDGE:
+	case SWITCHDEV_VXLAN_FDB_ADD_TO_DEVICE:
+	case SWITCHDEV_VXLAN_FDB_DEL_TO_DEVICE:
 	default:
-		kfree(swdev_work);
+		kfree(switchdev_work);
 		return NOTIFY_DONE;
 	}
 
-	queue_work(swdev_wq, &swdev_work->work);
+	queue_work(mvsw_owq, &switchdev_work->work);
 	return NOTIFY_DONE;
-
-out_bad:
-	kfree(swdev_work);
+out:
+	kfree(switchdev_work);
 	return NOTIFY_BAD;
 }
 
-static int
-prestera_port_vlan_bridge_join(struct prestera_port_vlan *port_vlan,
-			       struct prestera_bridge_port *br_port)
+static int prestera_switchdev_blocking_event(struct notifier_block *unused,
+					     unsigned long event, void *ptr)
 {
-	struct prestera_port *port = port_vlan->port;
-	struct prestera_bridge_vlan *br_vlan;
-	u16 vid = port_vlan->vid;
-	int err;
-
-	if (port_vlan->br_port)
-		return 0;
-
-	err = prestera_hw_port_flood_set(port, br_port->flags & BR_FLOOD);
-	if (err)
-		return err;
-
-	err = prestera_hw_port_learning_set(port, br_port->flags & BR_LEARNING);
-	if (err)
-		goto err_port_learning_set;
-
-	err = prestera_port_vid_stp_set(port, vid, br_port->stp_state);
-	if (err)
-		goto err_port_vid_stp_set;
+	int err = 0;
+	struct net_device *net_dev = switchdev_notifier_info_to_dev(ptr);
 
-	br_vlan = prestera_bridge_vlan_by_vid(br_port, vid);
-	if (!br_vlan) {
-		br_vlan = prestera_bridge_vlan_create(br_port, vid);
-		if (!br_vlan) {
-			err = -ENOMEM;
-			goto err_bridge_vlan_get;
+	switch (event) {
+	case SWITCHDEV_PORT_OBJ_ADD:
+		if (netif_is_vxlan(net_dev)) {
+			err = -EOPNOTSUPP;
+		} else {
+			err = switchdev_handle_port_obj_add
+			    (net_dev, ptr, mvsw_pr_netdev_check,
+			     mvsw_pr_port_obj_add);
+		}
+		break;
+	case SWITCHDEV_PORT_OBJ_DEL:
+		if (netif_is_vxlan(net_dev)) {
+			err = -EOPNOTSUPP;
+		} else {
+			err = switchdev_handle_port_obj_del
+			    (net_dev, ptr, mvsw_pr_netdev_check,
+			     mvsw_pr_port_obj_del);
 		}
+		break;
+	case SWITCHDEV_PORT_ATTR_SET:
+		err = switchdev_handle_port_attr_set
+		    (net_dev, ptr, mvsw_pr_netdev_check,
+		    mvsw_pr_port_obj_attr_set);
+		break;
+	default:
+		err = -EOPNOTSUPP;
 	}
 
-	list_add(&port_vlan->br_vlan_head, &br_vlan->port_vlan_list);
-
-	prestera_bridge_port_get(br_port);
-	port_vlan->br_port = br_port;
-
-	return 0;
-
-err_bridge_vlan_get:
-	prestera_port_vid_stp_set(port, vid, BR_STATE_FORWARDING);
-err_port_vid_stp_set:
-	prestera_hw_port_learning_set(port, false);
-err_port_learning_set:
-	return err;
+	return notifier_from_errno(err);
 }
 
-static int
-prestera_bridge_port_vlan_add(struct prestera_port *port,
-			      struct prestera_bridge_port *br_port,
-			      u16 vid, bool is_untagged, bool is_pvid,
-			      struct netlink_ext_ack *extack)
+static struct mvsw_pr_bridge_device *
+mvsw_pr_bridge_device_create(struct mvsw_pr_bridge *bridge,
+			     struct net_device *br_dev)
 {
-	struct prestera_port_vlan *port_vlan;
-	u16 old_pvid = port->pvid;
-	u16 pvid;
+	struct mvsw_pr_bridge_device *bridge_device;
+	bool vlan_enabled = br_vlan_enabled(br_dev);
+	u16 bridge_id;
 	int err;
 
-	if (is_pvid)
-		pvid = vid;
-	else
-		pvid = port->pvid == vid ? 0 : port->pvid;
+	if (vlan_enabled && bridge->bridge_8021q_exists) {
+		netdev_err(br_dev, "Only one VLAN-aware bridge is supported\n");
+		return ERR_PTR(-EINVAL);
+	}
 
-	port_vlan = prestera_port_vlan_by_vid(port, vid);
-	if (port_vlan && port_vlan->br_port != br_port)
-		return -EEXIST;
+	bridge_device = kzalloc(sizeof(*bridge_device), GFP_KERNEL);
+	if (!bridge_device)
+		return ERR_PTR(-ENOMEM);
 
-	if (!port_vlan) {
-		port_vlan = prestera_port_vlan_create(port, vid, is_untagged);
-		if (IS_ERR(port_vlan))
-			return PTR_ERR(port_vlan);
+	if (vlan_enabled) {
+		bridge->bridge_8021q_exists = true;
 	} else {
-		err = prestera_hw_vlan_port_set(port, vid, true, is_untagged);
-		if (err)
-			goto err_port_vlan_set;
+		err = mvsw_pr_8021d_bridge_create(bridge->sw, &bridge_id);
+		if (err) {
+			kfree(bridge_device);
+			return ERR_PTR(err);
+		}
+
+		bridge_device->bridge_id = bridge_id;
 	}
 
-	err = prestera_port_pvid_set(port, pvid);
-	if (err)
-		goto err_port_pvid_set;
+	bridge_device->dev = br_dev;
+	bridge_device->vlan_enabled = vlan_enabled;
+	bridge_device->multicast_enabled = br_multicast_enabled(br_dev);
+	bridge_device->mrouter = br_multicast_router(br_dev);
+	INIT_LIST_HEAD(&bridge_device->port_list);
 
-	err = prestera_port_vlan_bridge_join(port_vlan, br_port);
-	if (err)
-		goto err_port_vlan_bridge_join;
+	list_add(&bridge_device->bridge_node, &bridge->bridge_list);
 
-	return 0;
+	return bridge_device;
+}
 
-err_port_vlan_bridge_join:
-	prestera_port_pvid_set(port, old_pvid);
-err_port_pvid_set:
-	prestera_hw_vlan_port_set(port, vid, false, false);
-err_port_vlan_set:
-	prestera_port_vlan_destroy(port_vlan);
+static void
+mvsw_pr_bridge_device_destroy(struct mvsw_pr_bridge *bridge,
+			      struct mvsw_pr_bridge_device *bridge_device)
+{
+	list_del(&bridge_device->bridge_node);
+	if (bridge_device->vlan_enabled)
+		bridge->bridge_8021q_exists = false;
+	else
+		mvsw_pr_8021d_bridge_delete(bridge->sw,
+					    bridge_device->bridge_id);
 
-	return err;
+	WARN_ON(!list_empty(&bridge_device->port_list));
+	kfree(bridge_device);
 }
 
-static void
-prestera_bridge_port_vlan_del(struct prestera_port *port,
-			      struct prestera_bridge_port *br_port, u16 vid)
+static struct mvsw_pr_bridge_device *
+mvsw_pr_bridge_device_get(struct mvsw_pr_bridge *bridge,
+			  struct net_device *br_dev)
 {
-	u16 pvid = port->pvid == vid ? 0 : port->pvid;
-	struct prestera_port_vlan *port_vlan;
+	struct mvsw_pr_bridge_device *bridge_device;
 
-	port_vlan = prestera_port_vlan_by_vid(port, vid);
-	if (WARN_ON(!port_vlan))
-		return;
+	bridge_device = mvsw_pr_bridge_device_find(bridge, br_dev);
+	if (bridge_device)
+		return bridge_device;
 
-	prestera_port_vlan_bridge_leave(port_vlan);
-	prestera_port_pvid_set(port, pvid);
-	prestera_port_vlan_destroy(port_vlan);
+	return mvsw_pr_bridge_device_create(bridge, br_dev);
 }
 
-static int prestera_port_vlans_add(struct prestera_port *port,
-				   const struct switchdev_obj_port_vlan *vlan,
-				   struct switchdev_trans *trans,
-				   struct netlink_ext_ack *extack)
+static void
+mvsw_pr_bridge_device_put(struct mvsw_pr_bridge *bridge,
+			  struct mvsw_pr_bridge_device *bridge_device)
 {
-	bool flag_untagged = vlan->flags & BRIDGE_VLAN_INFO_UNTAGGED;
-	bool flag_pvid = vlan->flags & BRIDGE_VLAN_INFO_PVID;
-	struct net_device *dev = vlan->obj.orig_dev;
-	struct prestera_bridge_port *br_port;
-	struct prestera_switch *sw = port->sw;
-	struct prestera_bridge *bridge;
-	u16 vid;
+	if (list_empty(&bridge_device->port_list))
+		mvsw_pr_bridge_device_destroy(bridge, bridge_device);
+}
 
-	if (netif_is_bridge_master(dev))
-		return 0;
+static struct mvsw_pr_bridge_port *
+mvsw_pr_bridge_port_create(struct mvsw_pr_bridge_device *bridge_device,
+			   struct net_device *brport_dev)
+{
+	struct mvsw_pr_bridge_port *br_port;
+	struct mvsw_pr_port *port;
 
-	if (switchdev_trans_ph_commit(trans))
-		return 0;
+	br_port = kzalloc(sizeof(*br_port), GFP_KERNEL);
+	if (!br_port)
+		return NULL;
 
-	br_port = prestera_bridge_port_by_dev(sw->swdev, dev);
-	if (WARN_ON(!br_port))
-		return -EINVAL;
+	port = mvsw_pr_port_dev_lower_find(brport_dev);
 
-	bridge = br_port->bridge;
-	if (!bridge->vlan_enabled)
-		return 0;
+	br_port->dev = brport_dev;
+	br_port->bridge_device = bridge_device;
+	br_port->stp_state = BR_STATE_DISABLED;
+	br_port->flags = BR_LEARNING | BR_FLOOD | BR_LEARNING_SYNC |
+				BR_MCAST_FLOOD;
+	INIT_LIST_HEAD(&br_port->vlan_list);
+	list_add(&br_port->bridge_device_node, &bridge_device->port_list);
+	br_port->ref_count = 1;
 
-	for (vid = vlan->vid_begin; vid <= vlan->vid_end; vid++) {
-		int err;
+	return br_port;
+}
 
-		err = prestera_bridge_port_vlan_add(port, br_port,
-						    vid, flag_untagged,
-						    flag_pvid, extack);
-		if (err)
-			return err;
+static void
+mvsw_pr_bridge_port_destroy(struct mvsw_pr_bridge_port *br_port)
+{
+	list_del(&br_port->bridge_device_node);
+	WARN_ON(!list_empty(&br_port->vlan_list));
+	kfree(br_port);
+}
+
+static struct mvsw_pr_bridge_port *
+mvsw_pr_bridge_port_get(struct mvsw_pr_bridge *bridge,
+			struct net_device *brport_dev)
+{
+	struct net_device *br_dev = netdev_master_upper_dev_get(brport_dev);
+	struct mvsw_pr_bridge_device *bridge_device;
+	struct mvsw_pr_bridge_port *br_port;
+	int err;
+
+	br_port = mvsw_pr_bridge_port_find(bridge, brport_dev);
+	if (br_port) {
+		br_port->ref_count++;
+		return br_port;
+	}
+
+	bridge_device = mvsw_pr_bridge_device_get(bridge, br_dev);
+	if (IS_ERR(bridge_device))
+		return ERR_CAST(bridge_device);
+
+	br_port = mvsw_pr_bridge_port_create(bridge_device, brport_dev);
+	if (!br_port) {
+		err = -ENOMEM;
+		goto err_brport_create;
+	}
+
+	return br_port;
+
+err_brport_create:
+	mvsw_pr_bridge_device_put(bridge, bridge_device);
+	return ERR_PTR(err);
+}
+
+static void mvsw_pr_bridge_port_put(struct mvsw_pr_bridge *bridge,
+				    struct mvsw_pr_bridge_port *br_port)
+{
+	struct mvsw_pr_bridge_device *bridge_device;
+
+	if (--br_port->ref_count != 0)
+		return;
+	bridge_device = br_port->bridge_device;
+	mvsw_pr_bridge_port_destroy(br_port);
+	if (list_empty(&bridge_device->port_list)) {
+		mvsw_pr_rif_enable(bridge->sw, bridge_device->dev, false);
+		mvsw_pr_bridge_device_rifs_destroy(bridge->sw,
+						   bridge_device->dev);
+	}
+	mvsw_pr_bridge_device_put(bridge, bridge_device);
+}
+
+static int
+mvsw_pr_bridge_8021q_port_join(struct mvsw_pr_bridge_device *bridge_device,
+			       struct mvsw_pr_bridge_port *br_port,
+			       struct mvsw_pr_port *port,
+			       struct netlink_ext_ack *extack)
+{
+	if (is_vlan_dev(br_port->dev)) {
+		NL_SET_ERR_MSG_MOD(extack,
+				   "Can not enslave a VLAN device to a VLAN-aware bridge");
+		return -EINVAL;
 	}
 
 	return 0;
 }
 
-static int prestera_port_obj_add(struct net_device *dev,
-				 const struct switchdev_obj *obj,
-				 struct switchdev_trans *trans,
-				 struct netlink_ext_ack *extack)
+static int
+mvsw_pr_bridge_8021d_port_join(struct mvsw_pr_bridge_device *bridge_device,
+			       struct mvsw_pr_bridge_port *br_port,
+			       struct mvsw_pr_port *port,
+			       struct netlink_ext_ack *extack)
 {
-	struct prestera_port *port = netdev_priv(dev);
-	const struct switchdev_obj_port_vlan *vlan;
+	int err;
 
-	switch (obj->id) {
-	case SWITCHDEV_OBJ_ID_PORT_VLAN:
-		vlan = SWITCHDEV_OBJ_PORT_VLAN(obj);
-		return prestera_port_vlans_add(port, vlan, trans, extack);
-	default:
-		return -EOPNOTSUPP;
+	if (is_vlan_dev(br_port->dev)) {
+		NL_SET_ERR_MSG_MOD(extack,
+				   "Enslaving of a VLAN device is not supported");
+		return -ENOTSUPP;
 	}
+	err = mvsw_pr_8021d_bridge_port_add(port, bridge_device->bridge_id);
+	if (err)
+		return err;
+
+	err = mvsw_pr_port_uc_flood_set(port, br_port->flags & BR_FLOOD);
+	if (err)
+		goto err_port_uc_flood_set;
+
+	err = mvsw_pr_port_mc_flood_set(port, br_port->flags & BR_MCAST_FLOOD);
+	if (err)
+		goto err_port_mc_flood_set;
+
+	err = mvsw_pr_port_learning_set(port, br_port->flags & BR_LEARNING);
+	if (err)
+		goto err_port_learning_set;
+
+	if (list_is_singular(&bridge_device->port_list))
+		mvsw_pr_rif_enable(port->sw, bridge_device->dev, true);
+
+	return err;
+
+err_port_learning_set:
+	mvsw_pr_port_mc_flood_set(port, false);
+err_port_mc_flood_set:
+	mvsw_pr_port_uc_flood_set(port, false);
+err_port_uc_flood_set:
+	mvsw_pr_8021d_bridge_port_delete(port, bridge_device->bridge_id);
+	return err;
 }
 
-static int prestera_port_vlans_del(struct prestera_port *port,
-				   const struct switchdev_obj_port_vlan *vlan)
+static int mvsw_pr_port_bridge_join(struct mvsw_pr_port *port,
+				    struct net_device *brport_dev,
+				    struct net_device *br_dev,
+				    struct netlink_ext_ack *extack)
 {
-	struct net_device *dev = vlan->obj.orig_dev;
-	struct prestera_bridge_port *br_port;
-	struct prestera_switch *sw = port->sw;
-	u16 vid;
+	struct mvsw_pr_bridge_device *bridge_device;
+	struct mvsw_pr_switch *sw = port->sw;
+	struct mvsw_pr_bridge_port *br_port;
+	int err;
 
-	if (netif_is_bridge_master(dev))
-		return -EOPNOTSUPP;
+	br_port = mvsw_pr_bridge_port_get(sw->bridge, brport_dev);
+	if (IS_ERR(br_port))
+		return PTR_ERR(br_port);
 
-	br_port = prestera_bridge_port_by_dev(sw->swdev, dev);
-	if (WARN_ON(!br_port))
-		return -EINVAL;
+	bridge_device = br_port->bridge_device;
 
-	if (!br_port->bridge->vlan_enabled)
-		return 0;
+	/* Enslaved port is not usable as a router interface */
+	if (mvsw_pr_rif_exists(sw, port->net_dev))
+		mvsw_pr_rif_enable(sw, port->net_dev, false);
 
-	for (vid = vlan->vid_begin; vid <= vlan->vid_end; vid++)
-		prestera_bridge_port_vlan_del(port, br_port, vid);
+	if (bridge_device->vlan_enabled) {
+		err = mvsw_pr_bridge_8021q_port_join(bridge_device, br_port,
+						     port, extack);
+	} else {
+		err = mvsw_pr_bridge_8021d_port_join(bridge_device, br_port,
+						     port, extack);
+	}
+
+	if (err)
+		goto err_port_join;
 
 	return 0;
+
+err_port_join:
+	mvsw_pr_bridge_port_put(sw->bridge, br_port);
+	return err;
 }
 
-static int prestera_port_obj_del(struct net_device *dev,
-				 const struct switchdev_obj *obj)
+static void
+mvsw_pr_bridge_8021d_port_leave(struct mvsw_pr_bridge_device *bridge_device,
+				struct mvsw_pr_bridge_port *br_port,
+				struct mvsw_pr_port *port)
 {
-	struct prestera_port *port = netdev_priv(dev);
+	mvsw_pr_fdb_flush_port(port, MVSW_PR_FDB_FLUSH_MODE_ALL);
+	mvsw_pr_8021d_bridge_port_delete(port, bridge_device->bridge_id);
+}
 
-	switch (obj->id) {
-	case SWITCHDEV_OBJ_ID_PORT_VLAN:
-		return prestera_port_vlans_del(port, SWITCHDEV_OBJ_PORT_VLAN(obj));
-	default:
-		return -EOPNOTSUPP;
+static void
+mvsw_pr_bridge_8021q_port_leave(struct mvsw_pr_bridge_device *bridge_device,
+				struct mvsw_pr_bridge_port *br_port,
+				struct mvsw_pr_port *port)
+{
+	mvsw_pr_fdb_flush_port(port, MVSW_PR_FDB_FLUSH_MODE_ALL);
+	mvsw_pr_port_pvid_set(port, MVSW_PR_DEFAULT_VID);
+}
+
+static void mvsw_pr_port_bridge_leave(struct mvsw_pr_port *port,
+				      struct net_device *brport_dev,
+				      struct net_device *br_dev)
+{
+	struct mvsw_pr_switch *sw = port->sw;
+	struct mvsw_pr_bridge_device *bridge_device;
+	struct mvsw_pr_bridge_port *br_port;
+
+	bridge_device = mvsw_pr_bridge_device_find(sw->bridge, br_dev);
+	if (!bridge_device)
+		return;
+	br_port = __mvsw_pr_bridge_port_find(bridge_device, brport_dev);
+	if (!br_port)
+		return;
+
+	if (bridge_device->vlan_enabled)
+		mvsw_pr_bridge_8021q_port_leave(bridge_device, br_port, port);
+	else
+		mvsw_pr_bridge_8021d_port_leave(bridge_device, br_port, port);
+
+	mvsw_pr_port_learning_set(port, false);
+	mvsw_pr_port_uc_flood_set(port, false);
+	mvsw_pr_port_mc_flood_set(port, false);
+	mvsw_pr_port_vid_stp_set(port, MVSW_PR_VID_ALL, BR_STATE_FORWARDING);
+	mvsw_pr_bridge_port_put(sw->bridge, br_port);
+
+	/* Offload rif that was previosly disabled */
+	if (mvsw_pr_rif_exists(sw, port->net_dev))
+		mvsw_pr_rif_enable(sw, port->net_dev, true);
+
+}
+
+static bool
+prestera_lag_master_check(struct mvsw_pr_switch *sw, struct net_device *lag_dev,
+			  struct netdev_lag_upper_info *upper_info,
+			  struct netlink_ext_ack *ext_ack)
+{
+	u16 lag_id;
+
+	if (prestera_lag_id_find(sw, lag_dev, &lag_id)) {
+		NL_SET_ERR_MSG_MOD(ext_ack,
+				   "Exceeded max supported LAG devices");
+		return false;
 	}
+	if (upper_info->tx_type != NETDEV_LAG_TX_TYPE_HASH) {
+		NL_SET_ERR_MSG_MOD(ext_ack, "Unsupported LAG Tx type");
+		return false;
+	}
+	return true;
 }
 
-static int prestera_switchdev_blk_event(struct notifier_block *unused,
-					unsigned long event, void *ptr)
+static void mvsw_pr_port_lag_clean(struct mvsw_pr_port *port,
+				   struct net_device *lag_dev)
 {
-	struct net_device *dev = switchdev_notifier_info_to_dev(ptr);
+	struct net_device *br_dev = netdev_master_upper_dev_get(lag_dev);
+	struct mvsw_pr_port_vlan *port_vlan, *tmp;
+	struct net_device *upper_dev;
+	struct list_head *iter;
+
+	list_for_each_entry_safe(port_vlan, tmp, &port->vlans_list, list) {
+		mvsw_pr_port_vlan_bridge_leave(port_vlan);
+		mvsw_pr_port_vlan_destroy(port_vlan);
+	}
+
+	if (netif_is_bridge_port(lag_dev))
+		mvsw_pr_port_bridge_leave(port, lag_dev, br_dev);
+
+	netdev_for_each_upper_dev_rcu(lag_dev, upper_dev, iter) {
+		if (!netif_is_bridge_port(upper_dev))
+			continue;
+		br_dev = netdev_master_upper_dev_get(upper_dev);
+		mvsw_pr_port_bridge_leave(port, upper_dev, br_dev);
+	}
+
+	mvsw_pr_port_pvid_set(port, MVSW_PR_DEFAULT_VID);
+}
+
+static int mvsw_pr_port_lag_join(struct mvsw_pr_port *port,
+				 struct net_device *lag_dev)
+{
+	u16 lag_id;
 	int err;
 
+	err = prestera_lag_id_find(port->sw, lag_dev, &lag_id);
+	if (err)
+		return err;
+
+	err = prestera_lag_member_add(port, lag_dev, lag_id);
+		return err;
+
+	/* TODO: Port should no be longer usable as a router interface */
+
+	return 0;
+}
+
+static void mvsw_pr_port_lag_leave(struct mvsw_pr_port *port,
+				   struct net_device *lag_dev)
+{
+	mvsw_pr_router_lag_member_leave(port, lag_dev);
+
+	if (prestera_lag_member_del(port))
+		return;
+
+	mvsw_pr_port_lag_clean(port, lag_dev);
+}
+
+static int mvsw_pr_netdevice_port_upper_event(struct net_device *lower_dev,
+					      struct net_device *dev,
+					      unsigned long event, void *ptr)
+{
+	struct netdev_notifier_changeupper_info *info;
+	struct mvsw_pr_port *port;
+	struct netlink_ext_ack *extack;
+	struct net_device *upper_dev;
+	struct mvsw_pr_switch *sw;
+	int err = 0;
+
+	port = netdev_priv(dev);
+	sw = port->sw;
+	info = ptr;
+	extack = netdev_notifier_info_to_extack(&info->info);
+
 	switch (event) {
-	case SWITCHDEV_PORT_OBJ_ADD:
-		err = switchdev_handle_port_obj_add(dev, ptr,
-						    prestera_netdev_check,
-						    prestera_port_obj_add);
-		break;
-	case SWITCHDEV_PORT_OBJ_DEL:
-		err = switchdev_handle_port_obj_del(dev, ptr,
-						    prestera_netdev_check,
-						    prestera_port_obj_del);
+	case NETDEV_PRECHANGEUPPER:
+		upper_dev = info->upper_dev;
+		if (!netif_is_bridge_master(upper_dev) &&
+		    !netif_is_lag_master(upper_dev) &&
+		    !netif_is_macvlan(upper_dev)) {
+			NL_SET_ERR_MSG_MOD(extack, "Unknown upper device type");
+			return -EINVAL;
+		}
+		if (!info->linking)
+			break;
+		if (netdev_has_any_upper_dev(upper_dev) &&
+		    (!netif_is_bridge_master(upper_dev) ||
+		     !mvsw_pr_bridge_device_is_offloaded(sw, upper_dev))) {
+			NL_SET_ERR_MSG_MOD(extack,
+					   "Enslaving a port to a device that already has an upper device is not supported");
+			return -EINVAL;
+		}
+		if (netif_is_lag_master(upper_dev) &&
+		    !prestera_lag_master_check(sw, upper_dev,
+					      info->upper_info, extack))
+			return -EINVAL;
+		if (netif_is_lag_master(upper_dev) && vlan_uses_dev(dev)) {
+			NL_SET_ERR_MSG_MOD(extack,
+					   "Master device is a LAG master and port has a VLAN");
+			return -EINVAL;
+		}
+		if (netif_is_lag_port(dev) && is_vlan_dev(upper_dev) &&
+		    !netif_is_lag_master(vlan_dev_real_dev(upper_dev))) {
+			NL_SET_ERR_MSG_MOD(extack,
+					   "Can not put a VLAN on a LAG port");
+			return -EINVAL;
+		}
+		if (netif_is_macvlan(upper_dev) &&
+		    !mvsw_pr_rif_exists(sw, lower_dev)) {
+			NL_SET_ERR_MSG_MOD(extack,
+					   "macvlan is only supported on top of router interfaces");
+			return -EOPNOTSUPP;
+		}
 		break;
-	case SWITCHDEV_PORT_ATTR_SET:
-		err = switchdev_handle_port_attr_set(dev, ptr,
-						     prestera_netdev_check,
-						     prestera_port_obj_attr_set);
+	case NETDEV_CHANGEUPPER:
+		upper_dev = info->upper_dev;
+		if (netif_is_bridge_master(upper_dev)) {
+			if (info->linking)
+				err = mvsw_pr_port_bridge_join(port,
+							       lower_dev,
+							       upper_dev,
+							       extack);
+			else
+				mvsw_pr_port_bridge_leave(port,
+							  lower_dev,
+							  upper_dev);
+		} else if (netif_is_lag_master(upper_dev)) {
+			if (info->linking)
+				err = mvsw_pr_port_lag_join(port,
+							    upper_dev);
+			else
+				mvsw_pr_port_lag_leave(port,
+						       upper_dev);
+		}
 		break;
-	default:
-		err = -EOPNOTSUPP;
 	}
 
-	return notifier_from_errno(err);
+	return err;
 }
 
-static void prestera_fdb_event(struct prestera_switch *sw,
-			       struct prestera_event *evt, void *arg)
+static int mvsw_pr_netdevice_port_lower_event(struct net_device *dev,
+					      unsigned long event, void *ptr)
 {
-	struct switchdev_notifier_fdb_info info;
-	struct prestera_port *port;
+	struct netdev_notifier_changelowerstate_info *info = ptr;
+	struct netdev_lag_lower_state_info *lower_state_info;
+	struct mvsw_pr_port *port = netdev_priv(dev);
+	bool enabled;
 
-	port = prestera_find_port(sw, evt->fdb_evt.port_id);
-	if (!port)
-		return;
+	if (event != NETDEV_CHANGELOWERSTATE)
+		return 0;
+	if (!netif_is_lag_port(dev))
+		return 0;
+	if (!mvsw_pr_port_is_lag_member(port))
+		return 0;
 
-	info.addr = evt->fdb_evt.data.mac;
-	info.vid = evt->fdb_evt.vid;
-	info.offloaded = true;
+	lower_state_info = info->lower_state_info;
+	enabled = lower_state_info->tx_enabled;
+	return prestera_lag_member_enable(port, enabled);
+}
 
-	rtnl_lock();
+static int mvsw_pr_netdevice_port_event(struct net_device *lower_dev,
+					struct net_device *port_dev,
+					unsigned long event, void *ptr)
+{
+	switch (event) {
+	case NETDEV_PRECHANGEUPPER:
+	case NETDEV_CHANGEUPPER:
+		return mvsw_pr_netdevice_port_upper_event(lower_dev, port_dev,
+							  event, ptr);
+	case NETDEV_CHANGELOWERSTATE:
+		return mvsw_pr_netdevice_port_lower_event(port_dev,
+							  event, ptr);
+	}
 
-	switch (evt->id) {
-	case PRESTERA_FDB_EVENT_LEARNED:
-		call_switchdev_notifiers(SWITCHDEV_FDB_ADD_TO_BRIDGE,
-					 port->dev, &info.info, NULL);
+	return 0;
+}
+
+static int mvsw_pr_netdevice_bridge_event(struct net_device *br_dev,
+					  unsigned long event, void *ptr)
+{
+	struct mvsw_pr_switch *sw = mvsw_pr_switch_get(br_dev);
+	struct netdev_notifier_changeupper_info *info = ptr;
+	struct netlink_ext_ack *extack;
+	struct net_device *upper_dev;
+
+	if (!sw)
+		return 0;
+
+	extack = netdev_notifier_info_to_extack(&info->info);
+
+	switch (event) {
+	case NETDEV_PRECHANGEUPPER:
+		upper_dev = info->upper_dev;
+		if (!is_vlan_dev(upper_dev) && !netif_is_macvlan(upper_dev)) {
+			NL_SET_ERR_MSG_MOD(extack, "Unknown upper device type");
+			return -EOPNOTSUPP;
+		}
+		if (!info->linking)
+			break;
+		if (netif_is_macvlan(upper_dev) &&
+		    !mvsw_pr_rif_exists(sw, br_dev)) {
+			NL_SET_ERR_MSG_MOD(extack,
+					   "macvlan is only supported on top of router interfaces");
+			return -EOPNOTSUPP;
+		}
 		break;
-	case PRESTERA_FDB_EVENT_AGED:
-		call_switchdev_notifiers(SWITCHDEV_FDB_DEL_TO_BRIDGE,
-					 port->dev, &info.info, NULL);
+	case NETDEV_CHANGEUPPER:
+		/* TODO:  */
 		break;
 	}
 
-	rtnl_unlock();
+	return 0;
 }
 
-static int prestera_fdb_init(struct prestera_switch *sw)
+static int mvsw_pr_netdevice_macvlan_event(struct net_device *macvlan_dev,
+					   unsigned long event, void *ptr)
 {
-	int err;
+	struct mvsw_pr_switch *sw = mvsw_pr_switch_get(macvlan_dev);
+	struct netdev_notifier_changeupper_info *info = ptr;
+	struct netlink_ext_ack *extack;
 
-	err = prestera_hw_event_handler_register(sw, PRESTERA_EVENT_TYPE_FDB,
-						 prestera_fdb_event, NULL);
-	if (err)
-		return err;
+	if (!sw || event != NETDEV_PRECHANGEUPPER)
+		return 0;
 
-	err = prestera_hw_switch_ageing_set(sw, PRESTERA_DEFAULT_AGEING_TIME_MS);
-	if (err)
-		goto err_ageing_set;
+	extack = netdev_notifier_info_to_extack(&info->info);
 
-	return 0;
+	NL_SET_ERR_MSG_MOD(extack, "Unknown upper device type");
 
-err_ageing_set:
-	prestera_hw_event_handler_unregister(sw, PRESTERA_EVENT_TYPE_FDB,
-					     prestera_fdb_event);
-	return err;
+	return -EOPNOTSUPP;
 }
 
-static void prestera_fdb_fini(struct prestera_switch *sw)
+static bool mvsw_pr_is_vrf_event(unsigned long event, void *ptr)
 {
-	prestera_hw_event_handler_unregister(sw, PRESTERA_EVENT_TYPE_FDB,
-					     prestera_fdb_event);
+	struct netdev_notifier_changeupper_info *info = ptr;
+
+	if (event != NETDEV_PRECHANGEUPPER && event != NETDEV_CHANGEUPPER)
+		return false;
+
+	return netif_is_l3_master(info->upper_dev);
 }
 
-static int prestera_switchdev_handler_init(struct prestera_switchdev *swdev)
+static int mvsw_pr_netdevice_lag_event(struct net_device *lag_dev,
+				       unsigned long event, void *ptr)
 {
+	struct net_device *dev;
+	struct list_head *iter;
 	int err;
 
-	swdev->swdev_nb.notifier_call = prestera_switchdev_event;
-	err = register_switchdev_notifier(&swdev->swdev_nb);
-	if (err)
-		goto err_register_swdev_notifier;
+	netdev_for_each_lower_dev(lag_dev, dev, iter) {
+		if (mvsw_pr_netdev_check(dev)) {
+			err = mvsw_pr_netdevice_port_event(lag_dev, dev, event,
+							   ptr);
+			if (err)
+				return err;
+		}
+	}
 
-	swdev->swdev_nb_blk.notifier_call = prestera_switchdev_blk_event;
-	err = register_switchdev_blocking_notifier(&swdev->swdev_nb_blk);
-	if (err)
-		goto err_register_blk_swdev_notifier;
+	return 0;
+}
+
+static int mvsw_pr_netdevice_vlan_event(struct net_device *vlan_dev,
+					unsigned long event, void *ptr)
+{
+	struct net_device *real_dev = vlan_dev_real_dev(vlan_dev);
+	struct mvsw_pr_switch *sw = mvsw_pr_switch_get(real_dev);
+	struct netdev_notifier_changeupper_info *info = ptr;
+	struct netlink_ext_ack *extack;
+	struct net_device *upper_dev;
+
+	if (!sw)
+		return 0;
+
+	extack = netdev_notifier_info_to_extack(&info->info);
+
+	switch (event) {
+	case NETDEV_PRECHANGEUPPER:
+		upper_dev = info->upper_dev;
+
+		if (!info->linking)
+			break;
+
+		if (mvsw_pr_bridge_device_is_offloaded(sw, real_dev) &&
+		    netif_is_bridge_master(upper_dev)) {
+			NL_SET_ERR_MSG_MOD(extack,
+					   "Enslaving offloaded bridge to a bridge is not supported");
+			return -EOPNOTSUPP;
+		}
+		break;
+	case NETDEV_CHANGEUPPER:
+		/* empty */
+		break;
+	}
 
 	return 0;
+}
 
-err_register_blk_swdev_notifier:
-	unregister_switchdev_notifier(&swdev->swdev_nb);
-err_register_swdev_notifier:
-	destroy_workqueue(swdev_wq);
-	return err;
+static int mvsw_pr_netdevice_event(struct notifier_block *nb,
+				   unsigned long event, void *ptr)
+{
+	struct net_device *dev = netdev_notifier_info_to_dev(ptr);
+	struct mvsw_pr_switch *sw;
+	int err = 0;
+
+	sw = container_of(nb, struct mvsw_pr_switch, netdevice_nb);
+
+	if (event == NETDEV_PRE_CHANGEADDR ||
+	    event == NETDEV_CHANGEADDR)
+		err = mvsw_pr_netdevice_router_port_event(dev, event, ptr);
+	else if (mvsw_pr_is_vrf_event(event, ptr))
+		err = mvsw_pr_netdevice_vrf_event(dev, event, ptr);
+	else if (mvsw_pr_netdev_check(dev))
+		err = mvsw_pr_netdevice_port_event(dev, dev, event, ptr);
+	else if (netif_is_bridge_master(dev))
+		err = mvsw_pr_netdevice_bridge_event(dev, event, ptr);
+	else if (netif_is_lag_master(dev))
+		err = mvsw_pr_netdevice_lag_event(dev, event, ptr);
+	else if (is_vlan_dev(dev))
+		err = mvsw_pr_netdevice_vlan_event(dev, event, ptr);
+	else if (netif_is_macvlan(dev))
+		err = mvsw_pr_netdevice_macvlan_event(dev, event, ptr);
+
+	return notifier_from_errno(err);
 }
 
-static void prestera_switchdev_handler_fini(struct prestera_switchdev *swdev)
+static int mvsw_pr_fdb_init(struct mvsw_pr_switch *sw)
 {
-	unregister_switchdev_blocking_notifier(&swdev->swdev_nb_blk);
-	unregister_switchdev_notifier(&swdev->swdev_nb);
+	int err;
+
+	err = mvsw_pr_switch_ageing_set(sw, MVSW_PR_DEFAULT_AGEING_TIME);
+	if (err)
+		return err;
+
+	return 0;
 }
 
-int prestera_switchdev_init(struct prestera_switch *sw)
+static int prestera_switchdev_init(struct mvsw_pr_switch *sw)
 {
+	int err = 0;
 	struct prestera_switchdev *swdev;
-	int err;
+	struct mvsw_pr_bridge *bridge;
 
-	swdev = kzalloc(sizeof(*swdev), GFP_KERNEL);
-	if (!swdev)
+	if (sw->switchdev)
+		return -EPERM;
+
+	bridge = kzalloc(sizeof(*sw->bridge), GFP_KERNEL);
+	if (!bridge)
 		return -ENOMEM;
 
-	sw->swdev = swdev;
+	swdev = kzalloc(sizeof(*sw->switchdev), GFP_KERNEL);
+	if (!swdev) {
+		kfree(bridge);
+		return -ENOMEM;
+	}
+
+	sw->bridge = bridge;
+	bridge->sw = sw;
+	sw->switchdev = swdev;
 	swdev->sw = sw;
 
-	INIT_LIST_HEAD(&swdev->bridge_list);
+	INIT_LIST_HEAD(&sw->bridge->bridge_list);
 
-	swdev_wq = alloc_ordered_workqueue("%s_ordered", 0, "prestera_br");
-	if (!swdev_wq) {
+	mvsw_owq = alloc_ordered_workqueue("%s_ordered", 0, "prestera_sw");
+	if (!mvsw_owq) {
 		err = -ENOMEM;
-		goto err_alloc_wq;
+		goto err_alloc_workqueue;
 	}
 
-	err = prestera_switchdev_handler_init(swdev);
+	swdev->swdev_n.notifier_call = prestera_switchdev_event;
+	err = register_switchdev_notifier(&swdev->swdev_n);
 	if (err)
-		goto err_swdev_init;
+		goto err_register_switchdev_notifier;
 
-	err = prestera_fdb_init(sw);
+	swdev->swdev_blocking_n.notifier_call =
+			prestera_switchdev_blocking_event;
+	err = register_switchdev_blocking_notifier(&swdev->swdev_blocking_n);
 	if (err)
-		goto err_fdb_init;
+		goto err_register_block_switchdev_notifier;
+
+	mvsw_pr_fdb_init(sw);
 
 	return 0;
 
-err_fdb_init:
-err_swdev_init:
-	destroy_workqueue(swdev_wq);
-err_alloc_wq:
+err_register_block_switchdev_notifier:
+	unregister_switchdev_notifier(&swdev->swdev_n);
+err_register_switchdev_notifier:
+	destroy_workqueue(mvsw_owq);
+err_alloc_workqueue:
 	kfree(swdev);
+	kfree(bridge);
+	return err;
+}
 
+static void prestera_switchdev_fini(struct mvsw_pr_switch *sw)
+{
+	if (!sw->switchdev)
+		return;
+
+	unregister_switchdev_notifier(&sw->switchdev->swdev_n);
+	unregister_switchdev_blocking_notifier
+	    (&sw->switchdev->swdev_blocking_n);
+	flush_workqueue(mvsw_owq);
+	destroy_workqueue(mvsw_owq);
+	kfree(sw->switchdev);
+	sw->switchdev = NULL;
+	kfree(sw->bridge);
+}
+
+static int mvsw_pr_netdev_init(struct mvsw_pr_switch *sw)
+{
+	int err = 0;
+
+	if (sw->netdevice_nb.notifier_call)
+		return -EPERM;
+
+	sw->netdevice_nb.notifier_call = mvsw_pr_netdevice_event;
+	err = register_netdevice_notifier(&sw->netdevice_nb);
 	return err;
 }
 
-void prestera_switchdev_fini(struct prestera_switch *sw)
+static void mvsw_pr_netdev_fini(struct mvsw_pr_switch *sw)
 {
-	struct prestera_switchdev *swdev = sw->swdev;
+	if (sw->netdevice_nb.notifier_call)
+		unregister_netdevice_notifier(&sw->netdevice_nb);
+}
 
-	prestera_fdb_fini(sw);
-	prestera_switchdev_handler_fini(swdev);
-	destroy_workqueue(swdev_wq);
-	kfree(swdev);
+int prestera_switchdev_register(struct mvsw_pr_switch *sw)
+{
+	int err;
+
+	err = prestera_switchdev_init(sw);
+	if (err)
+		return err;
+
+	err = mvsw_pr_router_init(sw);
+
+	if (err) {
+		pr_err("Failed to initialize fib notifier\n");
+		goto err_fib_notifier;
+	}
+
+	err = mvsw_pr_netdev_init(sw);
+	if (err)
+		goto err_netdevice_notifier;
+
+	return 0;
+
+err_netdevice_notifier:
+	mvsw_pr_router_fini(sw);
+err_fib_notifier:
+	prestera_switchdev_fini(sw);
+	return err;
+}
+
+void prestera_switchdev_unregister(struct mvsw_pr_switch *sw)
+{
+	mvsw_pr_netdev_fini(sw);
+	mvsw_pr_router_fini(sw);
+	prestera_switchdev_fini(sw);
 }
diff --git a/drivers/net/ethernet/marvell/prestera/prestera_switchdev.h b/drivers/net/ethernet/marvell/prestera/prestera_switchdev.h
deleted file mode 100644
index 606e21d23..000000000
--- a/drivers/net/ethernet/marvell/prestera/prestera_switchdev.h
+++ /dev/null
@@ -1,13 +0,0 @@
-/* SPDX-License-Identifier: BSD-3-Clause OR GPL-2.0 */
-/* Copyright (c) 2019-2020 Marvell International Ltd. All rights reserved. */
-
-#ifndef _PRESTERA_SWITCHDEV_H_
-#define _PRESTERA_SWITCHDEV_H_
-
-int prestera_switchdev_init(struct prestera_switch *sw);
-void prestera_switchdev_fini(struct prestera_switch *sw);
-
-int prestera_bridge_port_event(struct net_device *dev, unsigned long event,
-			       void *ptr);
-
-#endif /* _PRESTERA_SWITCHDEV_H_ */
-- 
2.17.1

