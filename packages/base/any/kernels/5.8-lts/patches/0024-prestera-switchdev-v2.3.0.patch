diff --git a/arch/arm64/boot/dts/marvell/delta-tn4810m.dts b/arch/arm64/boot/dts/marvell/delta-tn4810m.dts
index 13af884..57ce36d 100644
--- a/arch/arm64/boot/dts/marvell/delta-tn4810m.dts
+++ b/arch/arm64/boot/dts/marvell/delta-tn4810m.dts
@@ -68,6 +68,20 @@
                 compatible = "marvell,prestera-switch-rxtx-sdma";
 		status = "okay";
 	};
+
+	onie_eeprom: onie-eeprom {
+		compatible = "onie-nvmem-cells";
+		status = "okay";
+
+		nvmem = <&eeprom_at24>;
+	};
+
+	prestera {
+		compatible = "marvell,prestera";
+		status = "okay";
+
+		base-mac-provider = <&onie_eeprom>;
+	};
 };
 
 &i2c0 {
@@ -100,6 +114,11 @@
 &cp0_i2c0 {
 	status = "okay";
 	clock-frequency = <100000>;
+
+	eeprom_at24: at24@56 {
+		compatible = "atmel,24c32";
+		reg = <0x56>;
+	};
 };
 
 &cp0_i2c1 {
diff --git a/arch/arm64/boot/dts/marvell/delta-tn48m.dts b/arch/arm64/boot/dts/marvell/delta-tn48m.dts
index 6b4a9ea..dc7fd52 100644
--- a/arch/arm64/boot/dts/marvell/delta-tn48m.dts
+++ b/arch/arm64/boot/dts/marvell/delta-tn48m.dts
@@ -124,6 +124,20 @@
                         };
                 };
         };
+
+	onie_eeprom: onie-eeprom {
+		compatible = "onie-nvmem-cells";
+		status = "okay";
+
+		nvmem = <&eeprom_at24>;
+	};
+
+	prestera {
+		compatible = "marvell,prestera";
+		status = "okay";
+
+		base-mac-provider = <&onie_eeprom>;
+	};
 };
 
 &i2c0 {
@@ -149,6 +163,11 @@
 &cp0_i2c0 {
         status = "okay";
         clock-frequency = <100000>;
+
+	eeprom_at24: at24@56 {
+		compatible = "atmel,24c32";
+		reg = <0x56>;
+	};
 };
 
 &cp0_i2c1 {
diff --git a/drivers/net/ethernet/marvell/prestera_sw/Makefile b/drivers/net/ethernet/marvell/prestera_sw/Makefile
index e605b0b..b709130 100644
--- a/drivers/net/ethernet/marvell/prestera_sw/Makefile
+++ b/drivers/net/ethernet/marvell/prestera_sw/Makefile
@@ -10,7 +10,7 @@ prestera_sw-objs := prestera.o \
 	prestera_hw.o prestera_switchdev.o prestera_fw_log.o \
 	prestera_rxtx.o prestera_rxtx_eth.o prestera_rxtx_mvpp.o \
 	prestera_rxtx_sdma.o prestera_dsa.o prestera_router.o \
-	prestera_acl.o prestera_flower.o
+	prestera_acl.o prestera_flower.o prestera_debugfs.o
 
 prestera_sw-$(CONFIG_MRVL_PRESTERA_DEBUG) += prestera_log.o
 ccflags-$(CONFIG_MRVL_PRESTERA_DEBUG) += -DCONFIG_MRVL_PRESTERA_DEBUG
diff --git a/drivers/net/ethernet/marvell/prestera_sw/prestera.c b/drivers/net/ethernet/marvell/prestera_sw/prestera.c
index 7be4d96..a25dd4b 100644
--- a/drivers/net/ethernet/marvell/prestera_sw/prestera.c
+++ b/drivers/net/ethernet/marvell/prestera_sw/prestera.c
@@ -14,17 +14,20 @@
 #include <linux/jiffies.h>
 #include <linux/if_bridge.h>
 #include <net/switchdev.h>
+#include <linux/of.h>
+#include <linux/of_net.h>
 
 #include "prestera.h"
 #include "prestera_hw.h"
-#include "prestera_fw_log.h"
+#include "prestera_debugfs.h"
 #include "prestera_dsa.h"
 #include "prestera_rxtx.h"
 #include "prestera_drv_ver.h"
 
-static char base_mac_addr[ETH_ALEN];
+static u8 trap_policer_profile = 1;
 
 #define MVSW_PR_MTU_DEFAULT 1536
+#define MVSW_PR_MAC_ADDR_OFFSET 4
 
 #define PORT_STATS_CACHE_TIMEOUT_MS	(msecs_to_jiffies(1000))
 #define PORT_STATS_CNT	(sizeof(struct mvsw_pr_port_stats) / sizeof(u64))
@@ -386,6 +389,10 @@ static int mvsw_pr_port_open(struct net_device *dev)
 
 static int mvsw_pr_port_close(struct net_device *dev)
 {
+	struct mvsw_pr_port *port = netdev_priv(dev);
+
+	mvsw_pr_fdb_flush_port(port, MVSW_PR_FDB_FLUSH_MODE_DYNAMIC);
+
 	return mvsw_pr_port_state_set(dev, false);
 }
 
@@ -659,7 +666,9 @@ static void update_stats_cache(struct work_struct *work)
 		container_of(work, struct mvsw_pr_port,
 			     cached_hw_stats.caching_dw.work);
 
+	rtnl_lock();
 	mvsw_pr_port_get_hw_stats(port);
+	rtnl_unlock();
 
 	queue_delayed_work(mvsw_pr_wq, &port->cached_hw_stats.caching_dw,
 			   PORT_STATS_CACHE_TIMEOUT_MS);
@@ -794,6 +803,9 @@ struct mvsw_pr_port *mvsw_pr_port_dev_lower_find(struct net_device *dev)
 {
 	struct mvsw_pr_port *port;
 
+	if (!dev)
+		return NULL;
+
 	if (mvsw_pr_netdev_check(dev))
 		return netdev_priv(dev);
 
@@ -1487,8 +1499,8 @@ static int mvsw_pr_port_create(struct mvsw_pr_switch *sw, u32 id)
 		goto err_register_netdev;
 
 	mac = net_dev->dev_addr;
-	memcpy(mac, sw->base_mac, net_dev->addr_len - 1);
-	mac[net_dev->addr_len - 1] = (char)port->fp_id;
+	memcpy(mac, sw->base_mac, net_dev->addr_len);
+	mac[net_dev->addr_len - 1] += port->fp_id + MVSW_PR_MAC_ADDR_OFFSET;
 
 	err = mvsw_pr_hw_port_mac_set(port, mac);
 	if (err) {
@@ -1565,7 +1577,7 @@ int mvsw_pr_8021d_bridge_port_delete(struct mvsw_pr_port *port, u16 bridge_id)
 
 int mvsw_pr_switch_ageing_set(struct mvsw_pr_switch *sw, u32 ageing_time)
 {
-	return mvsw_pr_hw_switch_ageing_set(sw, ageing_time);
+	return mvsw_pr_hw_switch_ageing_set(sw, ageing_time / 1000);
 }
 
 int mvsw_pr_dev_if_type(const struct net_device *dev)
@@ -1587,35 +1599,75 @@ int mvsw_pr_dev_if_type(const struct net_device *dev)
 }
 
 int mvsw_pr_lpm_update(struct mvsw_pr_switch *sw, u16 vr_id, u32 dst,
-		       u32 dst_len, u16 vid)
+		       u32 dst_len, u16 vid, u32 grp_id)
 {
-	return mvsw_pr_hw_lpm_update(sw, vr_id, htonl(dst), dst_len, vid);
+	return mvsw_pr_hw_lpm_update(sw, vr_id, htonl(dst), dst_len, vid,
+				     grp_id);
 }
 
 int mvsw_pr_lpm_del(struct mvsw_pr_switch *sw, u16 vr_id, u32 dst,
-		    u32 dst_len)
+		    u32 dst_len, u32 grp_id)
+{
+	return mvsw_pr_hw_lpm_del(sw, vr_id, htonl(dst), dst_len, grp_id);
+}
+
+int mvsw_pr_nh_entries_add(const struct mvsw_pr_switch *sw, int count,
+			   struct mvsw_pr_nexthop *nh)
+{
+	return mvsw_pr_hw_nh_entries_add(sw, count, nh);
+}
+
+int mvsw_pr_nh_entries_del(const struct mvsw_pr_switch *sw, int count,
+			   struct mvsw_pr_nexthop *nh)
+{
+	return mvsw_pr_hw_nh_entries_del(sw, count, nh);
+}
+
+int mvsw_pr_nh_entries_set(const struct mvsw_pr_switch *sw, int count,
+			   struct mvsw_pr_nexthop *nh, u32 grp_id)
+{
+	return mvsw_pr_hw_nh_entries_set(sw, count, nh, grp_id);
+}
+
+int mvsw_pr_neigh_entry_add(const struct mvsw_pr_switch *sw,
+			    struct mvsw_pr_neigh_entry *neigh)
+{
+	return mvsw_pr_hw_neigh_entry_add(sw, neigh);
+}
+
+int mvsw_pr_neigh_entry_del(const struct mvsw_pr_switch *sw,
+			    struct mvsw_pr_neigh_entry *neigh)
+{
+	return mvsw_pr_hw_neigh_entry_del(sw, neigh);
+}
+
+int mvsw_pr_neigh_entry_set(const struct mvsw_pr_switch *sw,
+			    struct mvsw_pr_neigh_entry *neigh)
+{
+	return mvsw_pr_hw_neigh_entry_set(sw, neigh);
+}
+
+int mvsw_pr_neigh_entry_get(const struct mvsw_pr_switch *sw,
+			    struct mvsw_pr_neigh_entry *neigh, bool *is_active)
 {
-	return mvsw_pr_hw_lpm_del(sw, vr_id, htonl(dst), dst_len);
+	return mvsw_pr_hw_neigh_entry_get(sw, neigh, is_active);
 }
 
-int mvsw_pr_nh_entry_add(struct mvsw_pr_switch *sw, u16 vr_id, u16 vid,
-			 __be32 dst, u8 *mac, u32 *hw_id)
+int mvsw_pr_nh_group_create(const struct mvsw_pr_switch *sw, u16 nh_count,
+			    u32 *grp_id)
 {
-	return mvsw_pr_hw_nh_entry_add(sw, vr_id, vid, dst, mac, hw_id);
+	return mvsw_pr_hw_nh_group_create(sw, nh_count, grp_id);
 }
 
-int mvsw_pr_nh_entry_delete(struct mvsw_pr_switch *sw, u16 vr_id, __be32 dst,
-			    u32 dst_len, u8 *mac)
+int mvsw_pr_nh_group_delete(const struct mvsw_pr_switch *sw, u16 nh_count,
+			    u32 grp_id)
 {
-	return mvsw_pr_hw_nh_entry_del(sw, vr_id, dst, dst_len, mac);
+	return mvsw_pr_hw_nh_group_delete(sw, nh_count, grp_id);
 }
 
-int mvsw_pr_nh_entry_set(struct mvsw_pr_switch *sw,
-			 struct mvsw_pr_iface *iface, u16 vr_id,
-			 __be32 dst, u32 dst_len, u8 *mac, u32 *hw_id)
+int mvsw_pr_mp4_hash_set(const struct mvsw_pr_switch *sw, u8 hash_policy)
 {
-	return mvsw_pr_hw_nh_entry_set(sw, iface, vr_id, dst, dst_len,
-				       mac, hw_id);
+	return mvsw_pr_hw_mp4_hash_set(sw, hash_policy);
 }
 
 int mvsw_pr_fdb_flush_vlan(struct mvsw_pr_switch *sw, u16 vid,
@@ -1735,8 +1787,10 @@ int mvsw_pr_lag_member_del(struct mvsw_pr_port *port)
 		}
 	}
 
-	if (!lag->member_count)
+	if (!lag->member_count) {
+		mvsw_pr_lag_router_leave(sw, lag->dev);
 		mvsw_pr_port_lag_destroy(sw, lag_id);
+	}
 
 	return 0;
 }
@@ -1776,6 +1830,12 @@ int mvsw_pr_lag_id_find(struct mvsw_pr_switch *sw,
 	return 0;
 }
 
+void mvsw_pr_lag_member_rif_leave(const struct mvsw_pr_port *port,
+				  u16 lag_id, u16 vr_id)
+{
+	mvsw_pr_hw_lag_member_rif_leave(port, lag_id, vr_id);
+}
+
 static int mvsw_pr_lag_init(struct mvsw_pr_switch *sw)
 {
 	sw->lags = kcalloc(sw->lag_max, sizeof(*sw->lags), GFP_KERNEL);
@@ -1979,20 +2039,57 @@ const struct mvsw_pr_port *mvsw_pr_port_find(u32 dev_hw_id, u32 port_hw_id)
 	return NULL;
 }
 
+static int mvsw_pr_sw_init_base_mac(struct mvsw_pr_switch *sw)
+{
+	struct device_node *mac_dev_np;
+	u32 lsb;
+	int err;
+
+	if (sw->np) {
+		mac_dev_np = of_parse_phandle(sw->np, "base-mac-provider", 0);
+		if (mac_dev_np) {
+			const char *base_mac;
+
+			base_mac = of_get_mac_address(mac_dev_np);
+			if (!IS_ERR(base_mac))
+				ether_addr_copy(sw->base_mac, base_mac);
+		}
+	}
+
+	if (!is_valid_ether_addr(sw->base_mac))
+		eth_random_addr(sw->base_mac);
+
+	lsb = sw->base_mac[ETH_ALEN - 1];
+	if (lsb + sw->port_count + MVSW_PR_MAC_ADDR_OFFSET > 0xFF)
+		sw->base_mac[ETH_ALEN - 1] = 0;
+
+	err = mvsw_pr_hw_switch_mac_set(sw, sw->base_mac);
+	if (err)
+		return err;
+
+	return 0;
+}
+
 static int mvsw_pr_init(struct mvsw_pr_switch *sw)
 {
 	u32 port;
 	int err;
 
+	sw->np = of_find_compatible_node(NULL, NULL, "marvell,prestera");
+
 	err = mvsw_pr_hw_switch_init(sw);
 	if (err) {
 		dev_err(mvsw_dev(sw), "Failed to init Switch device\n");
 		return err;
 	}
 
-	memcpy(sw->base_mac, base_mac_addr, sizeof(sw->base_mac));
+	err = mvsw_pr_hw_switch_trap_policer_set(sw, trap_policer_profile);
+	if (err) {
+		dev_err(mvsw_dev(sw), "Failed to set trap policer profile\n");
+		return err;
+	}
 
-	err = mvsw_pr_hw_switch_mac_set(sw, sw->base_mac);
+	err = mvsw_pr_sw_init_base_mac(sw);
 	if (err)
 		return err;
 
@@ -2022,9 +2119,9 @@ static int mvsw_pr_init(struct mvsw_pr_switch *sw)
 	if (err)
 		goto err_event_handlers;
 
-	err = mvsw_pr_fw_log_init(sw);
+	err = mvsw_pr_debugfs_init(sw);
 	if (err)
-		goto err_fw_log_init;
+		goto err_debugfs_init;
 
 	err = mvsw_pr_acl_init(sw);
 	if (err)
@@ -2033,7 +2130,7 @@ static int mvsw_pr_init(struct mvsw_pr_switch *sw)
 	return 0;
 
 err_acl_init:
-err_fw_log_init:
+err_debugfs_init:
 	mvsw_pr_event_handlers_unregister(sw);
 err_event_handlers:
 	mvsw_pr_rxtx_switch_fini(sw);
@@ -2045,15 +2142,17 @@ err_ports_init:
 
 static void mvsw_pr_fini(struct mvsw_pr_switch *sw)
 {
-	mvsw_pr_event_handlers_unregister(sw);
+	mvsw_pr_debugfs_fini(sw);
 
-	mvsw_pr_fw_log_fini(sw);
+	mvsw_pr_event_handlers_unregister(sw);
 
 	mvsw_pr_clear_ports(sw);
 	mvsw_pr_rxtx_switch_fini(sw);
 	mvsw_pr_switchdev_unregister(sw);
 	mvsw_pr_acl_fini(sw);
 	mvsw_pr_lag_fini(sw);
+	if (sw->np)
+		of_node_put(sw->np);
 }
 
 int mvsw_pr_device_register(struct mvsw_pr_device *dev)
@@ -2126,3 +2225,5 @@ MODULE_AUTHOR("Marvell Semi.");
 MODULE_LICENSE("GPL");
 MODULE_DESCRIPTION("Marvell Prestera switch driver");
 MODULE_VERSION(PRESTERA_DRV_VER);
+
+module_param(trap_policer_profile, byte, 0444);
diff --git a/drivers/net/ethernet/marvell/prestera_sw/prestera.h b/drivers/net/ethernet/marvell/prestera_sw/prestera.h
index d9894f5..60878a0 100644
--- a/drivers/net/ethernet/marvell/prestera_sw/prestera.h
+++ b/drivers/net/ethernet/marvell/prestera_sw/prestera.h
@@ -18,9 +18,11 @@
 
 #define MVSW_PR_DEFAULT_VID 1
 
-#define MVSW_PR_MIN_AGEING_TIME 10
-#define MVSW_PR_MAX_AGEING_TIME 1000000
-#define MVSW_PR_DEFAULT_AGEING_TIME 300
+#define MVSW_PR_MIN_AGEING_TIME 10000
+#define MVSW_PR_MAX_AGEING_TIME 1000000000
+#define MVSW_PR_DEFAULT_AGEING_TIME 300000
+
+#define MVSW_PR_NHGR_SIZE_MAX 4
 
 struct mvsw_fw_rev {
 	u16 maj;
@@ -235,6 +237,7 @@ struct mvsw_pr_iface {
 		u32 hw_dev_num;
 		u32 port_num;
 	} dev_port;
+	u16 vr_id;
 	u16 lag_id;
 	u16 vlan_id;
 	u32 hw_dev_num;
@@ -262,6 +265,8 @@ struct mvsw_pr_switch {
 	struct mvsw_pr_router *router;
 	struct mvsw_pr_lag *lags;
 	struct notifier_block netdevice_nb;
+
+	struct device_node *np;
 };
 
 enum mvsw_pr_fdb_flush_mode {
@@ -316,6 +321,43 @@ struct mvsw_pr_acl_rule_match_entry {
 	} keymask;
 };
 
+struct mvsw_pr_nexthop_info {
+	unsigned char ha[ETH_ALEN];
+	bool connected;
+	__be32 nh_addr;
+	u32 prefix_len;
+};
+
+struct mvsw_pr_neigh_entry {
+	struct neighbour *n;
+	struct mvsw_pr_nexthop_info dest;
+	struct mvsw_pr_iface iface;
+	struct mvsw_pr_rif *rif;
+	struct list_head rif_head;
+	struct list_head nexthop_list; /* nexthops using this neigh entry */
+	struct list_head nexthop_neighs_list_node;
+	struct rhash_head ht_node;
+	u32 hw_id;
+};
+
+struct mvsw_pr_nexthop_key {
+	struct fib_nh *fib_nh;
+};
+
+struct mvsw_pr_nexthop_group;
+struct mvsw_pr_nexthop {
+	struct list_head neigh_head; /* member of neigh entry list */
+	struct list_head rif_head;
+	struct list_head router_head;
+	struct mvsw_pr_nexthop_key key;
+	struct mvsw_pr_nexthop_group *nh_grp; /* parent group */
+	struct mvsw_pr_neigh_entry *neigh_entry;
+	struct rhash_head ht_node;
+	struct mvsw_pr_rif *rif;
+	struct mvsw_pr_nexthop_info nh_info;
+#define nh_iface neigh_entry->iface
+};
+
 int mvsw_pr_switch_ageing_set(struct mvsw_pr_switch *sw, u32 ageing_time);
 
 int mvsw_pr_port_learning_set(struct mvsw_pr_port *mvsw_pr_port,
@@ -365,6 +407,8 @@ bool mvsw_pr_port_is_lag_member(const struct mvsw_pr_port *port);
 int mvsw_pr_lag_id_find(struct mvsw_pr_switch *sw,
 			struct net_device *lag_dev,
 			u16 *lag_id);
+void mvsw_pr_lag_member_rif_leave(const struct mvsw_pr_port *port,
+				  u16 lag_id, u16 vr_id);
 
 int mvsw_pr_dev_if_type(const struct net_device *dev);
 
@@ -456,20 +500,36 @@ int mvsw_pr_netdevice_vrf_event(struct net_device *dev, unsigned long event,
 				struct netdev_notifier_changeupper_info *info);
 void mvsw_pr_port_router_leave(struct mvsw_pr_port *mvsw_pr_port);
 int mvsw_pr_lpm_update(struct mvsw_pr_switch *sw, u16 vr_id, u32 dst,
-		       u32 dst_len, u16 vid);
+		       u32 dst_len, u16 vid, u32 grp_id);
 int mvsw_pr_lpm_del(struct mvsw_pr_switch *sw, u16 vr_id, u32 dst,
-		    u32 dst_len);
-int mvsw_pr_nh_entry_add(struct mvsw_pr_switch *sw, u16 vr_id, u16 vid,
-			 __be32 dst, u8 *mac, u32 *hw_id);
-int mvsw_pr_nh_entry_set(struct mvsw_pr_switch *sw,
-			 struct mvsw_pr_iface *iface, u16 vr_id,
-			 __be32 dst, u32 dst_len, u8 *mac, u32 *hw_id);
-int mvsw_pr_nh_entry_delete(struct mvsw_pr_switch *sw, u16 vr_id, __be32 dst,
-			    u32 dst_len, u8 *mac);
+		    u32 dst_len, u32 grp_id);
+int mvsw_pr_nh_entries_add(const struct mvsw_pr_switch *sw, int count,
+			   struct mvsw_pr_nexthop *nh);
+int mvsw_pr_nh_entries_del(const struct mvsw_pr_switch *sw, int count,
+			   struct mvsw_pr_nexthop *nh);
+int mvsw_pr_nh_entries_set(const struct mvsw_pr_switch *sw, int count,
+			   struct mvsw_pr_nexthop *nh, u32 grp_id);
+int mvsw_pr_nh_group_create(const struct mvsw_pr_switch *sw, u16 nh_count,
+			    u32 *grp_id);
+int mvsw_pr_nh_group_delete(const struct mvsw_pr_switch *sw, u16 nh_count,
+			    u32 grp_id);
+int mvsw_pr_neigh_entry_add(const struct mvsw_pr_switch *sw,
+			    struct mvsw_pr_neigh_entry *neigh);
+int mvsw_pr_neigh_entry_del(const struct mvsw_pr_switch *sw,
+			    struct mvsw_pr_neigh_entry *neigh);
+int mvsw_pr_neigh_entry_set(const struct mvsw_pr_switch *sw,
+			    struct mvsw_pr_neigh_entry *neigh);
+int mvsw_pr_neigh_entry_get(const struct mvsw_pr_switch *sw,
+			    struct mvsw_pr_neigh_entry *neigh, bool *is_active);
+int mvsw_pr_mp4_hash_set(const struct mvsw_pr_switch *sw, u8 hash_policy);
 
 void mvsw_pr_rif_enable(struct mvsw_pr_switch *sw, struct net_device *dev,
 			bool enable);
 bool mvsw_pr_rif_exists(const struct mvsw_pr_switch *sw,
 			const struct net_device *dev);
+void mvsw_pr_router_lag_member_leave(const struct mvsw_pr_port *port,
+				     const struct net_device *dev);
+void mvsw_pr_lag_router_leave(struct mvsw_pr_switch *sw,
+			      struct net_device *lag_dev);
 
 #endif /* _MVSW_PRESTERA_H_ */
diff --git a/drivers/net/ethernet/marvell/prestera_sw/prestera_debugfs.c b/drivers/net/ethernet/marvell/prestera_sw/prestera_debugfs.c
new file mode 100644
index 0000000..1b4b855
--- /dev/null
+++ b/drivers/net/ethernet/marvell/prestera_sw/prestera_debugfs.c
@@ -0,0 +1,160 @@
+// SPDX-License-Identifier: BSD-3-Clause OR GPL-2.0
+/*
+ * Copyright (c) 2020 Marvell International Ltd. All rights reserved.
+ *
+ */
+#include <linux/kernel.h>
+#include <linux/fs.h>
+#include <linux/device.h>
+#include <linux/debugfs.h>
+
+#include "prestera_debugfs.h"
+#include "prestera.h"
+#include "prestera_log.h"
+#include "prestera_fw_log.h"
+#include "prestera_rxtx.h"
+
+#define DEBUGFS_ROOTDIR	"prestera"
+
+#define CPU_CODE_SUBDIR_NAME	"traps"
+#define CPU_CODE_MAX_BUF_SIZE	(MVSW_PR_RXTX_CPU_CODE_MAX_NUM * 32)
+
+static ssize_t cpu_code_stats_read(struct file *file,
+				   char __user *ubuf,
+				   size_t count, loff_t *ppos);
+
+struct mvsw_pr_debugfs {
+	struct dentry *root_dir;
+	struct dentry *cpu_code_subdir;
+	const struct file_operations cpu_code_stats_fops;
+	char *cpu_code_stats_buf;
+	/* serialize access to cpu_code_stats_buf */
+	struct mutex cpu_code_stats_mtx;
+};
+
+static struct mvsw_pr_debugfs prestera_debugfs = {
+	.cpu_code_stats_fops = {
+		.read = cpu_code_stats_read,
+		.open = simple_open,
+		.llseek = default_llseek,
+	},
+};
+
+int mvsw_pr_debugfs_init(struct mvsw_pr_switch *sw)
+{
+	const struct file_operations *fops =
+		&prestera_debugfs.cpu_code_stats_fops;
+	char file_name[] = "cpu_code_XXX_stats";
+	int err;
+	int i;
+
+	mutex_init(&prestera_debugfs.cpu_code_stats_mtx);
+
+	prestera_debugfs.cpu_code_stats_buf =
+		kzalloc(CPU_CODE_MAX_BUF_SIZE, GFP_KERNEL);
+
+	if (!prestera_debugfs.cpu_code_stats_buf)
+		return -ENOMEM;
+
+	err = mvsw_pr_fw_log_init(sw);
+	if (err)
+		return err;
+
+	prestera_debugfs.root_dir = debugfs_create_dir(DEBUGFS_ROOTDIR, NULL);
+	if (!prestera_debugfs.root_dir) {
+		err = -ENOMEM;
+		goto root_dir_alloc_failed;
+	}
+
+	prestera_debugfs.cpu_code_subdir =
+		debugfs_create_dir(CPU_CODE_SUBDIR_NAME,
+				   prestera_debugfs.root_dir);
+	if (!prestera_debugfs.cpu_code_subdir) {
+		err = -ENOMEM;
+		goto cpu_code_subdir_alloc_failed;
+	}
+
+	for (i = 0; i < MVSW_PR_RXTX_CPU_CODE_MAX_NUM; ++i) {
+		snprintf(file_name, sizeof(file_name), "cpu_code_%d_stats", i);
+		if (!debugfs_create_file(file_name, 0644,
+					 prestera_debugfs.cpu_code_subdir,
+					 (void *)(long)i, fops)) {
+			err = -ENOMEM;
+			goto cpu_code_single_file_creation_failed;
+		}
+	}
+
+	strncpy(file_name, "cpu_code_stats", sizeof(file_name));
+
+	if (!debugfs_create_file(file_name, 0644,
+				 prestera_debugfs.cpu_code_subdir,
+				 (void *)(long)MVSW_PR_RXTX_CPU_CODE_MAX_NUM,
+				 fops)) {
+		err = -ENOMEM;
+		goto cpu_code_single_file_creation_failed;
+	}
+
+	return 0;
+
+cpu_code_single_file_creation_failed:
+	debugfs_remove(prestera_debugfs.cpu_code_subdir);
+cpu_code_subdir_alloc_failed:
+	debugfs_remove(prestera_debugfs.root_dir);
+root_dir_alloc_failed:
+	mvsw_pr_fw_log_fini(sw);
+
+	return err;
+}
+
+void mvsw_pr_debugfs_fini(struct mvsw_pr_switch *sw)
+{
+	mvsw_pr_fw_log_fini(sw);
+
+	debugfs_remove(prestera_debugfs.cpu_code_subdir);
+	debugfs_remove(prestera_debugfs.root_dir);
+
+	mutex_destroy(&prestera_debugfs.cpu_code_stats_mtx);
+
+	kfree(prestera_debugfs.cpu_code_stats_buf);
+}
+
+static ssize_t cpu_code_stats_read(struct file *file,
+				   char __user *ubuf,
+				   size_t count, loff_t *ppos)
+{
+	char *buf = prestera_debugfs.cpu_code_stats_buf;
+	u16 cpu_code = (u16)(long)file->private_data;
+	u64 cpu_code_stats;
+	/* as the snprintf doesn't count for \0, start with 1 */
+	int buf_len = 1;
+	int ret;
+
+	mutex_lock(&prestera_debugfs.cpu_code_stats_mtx);
+
+	if (cpu_code == MVSW_PR_RXTX_CPU_CODE_MAX_NUM) {
+		int i;
+
+		memset(buf, 0, CPU_CODE_MAX_BUF_SIZE);
+
+		for (i = 0; i < MVSW_PR_RXTX_CPU_CODE_MAX_NUM; ++i) {
+			cpu_code_stats = mvsw_pr_rxtx_get_cpu_code_stats(i);
+
+			if (!cpu_code_stats)
+				continue;
+
+			buf_len += snprintf(buf + buf_len,
+					    CPU_CODE_MAX_BUF_SIZE - buf_len,
+					    "%u:%llu\n", i, cpu_code_stats);
+		}
+
+	} else {
+		cpu_code_stats = mvsw_pr_rxtx_get_cpu_code_stats((u8)cpu_code);
+
+		buf_len += sprintf(buf, "%llu\n", cpu_code_stats);
+	}
+
+	ret = simple_read_from_buffer(ubuf, count, ppos, buf, buf_len);
+	mutex_unlock(&prestera_debugfs.cpu_code_stats_mtx);
+
+	return ret;
+}
diff --git a/drivers/net/ethernet/marvell/prestera_sw/prestera_debugfs.h b/drivers/net/ethernet/marvell/prestera_sw/prestera_debugfs.h
new file mode 100644
index 0000000..b3ce1a4
--- /dev/null
+++ b/drivers/net/ethernet/marvell/prestera_sw/prestera_debugfs.h
@@ -0,0 +1,14 @@
+/* SPDX-License-Identifier: BSD-3-Clause OR GPL-2.0
+ *
+ * Copyright (c) 2020 Marvell International Ltd. All rights reserved.
+ *
+ */
+#ifndef _MVSW_PRESTERA_DEBUGFS_H_
+#define _MVSW_PRESTERA_DEBUGFS_H_
+
+struct mvsw_pr_switch;
+
+int mvsw_pr_debugfs_init(struct mvsw_pr_switch *sw);
+void mvsw_pr_debugfs_fini(struct mvsw_pr_switch *sw);
+
+#endif /* _MVSW_PRESTERA_DEBUGFS_H_ */
diff --git a/drivers/net/ethernet/marvell/prestera_sw/prestera_drv_ver.h b/drivers/net/ethernet/marvell/prestera_sw/prestera_drv_ver.h
index dd9b222..7ad1cac 100644
--- a/drivers/net/ethernet/marvell/prestera_sw/prestera_drv_ver.h
+++ b/drivers/net/ethernet/marvell/prestera_sw/prestera_drv_ver.h
@@ -12,7 +12,7 @@
 #define PRESTERA_DRV_VER_MAJOR	2
 #define PRESTERA_DRV_VER_MINOR	0
 #define PRESTERA_DRV_VER_PATCH	0
-#define PRESTERA_DRV_VER_EXTRA	-v2.1.0
+#define PRESTERA_DRV_VER_EXTRA	-v2.3.0
 
 #define PRESTERA_DRV_VER \
 		__stringify(PRESTERA_DRV_VER_MAJOR)  "." \
diff --git a/drivers/net/ethernet/marvell/prestera_sw/prestera_dsa.c b/drivers/net/ethernet/marvell/prestera_sw/prestera_dsa.c
index ad82cee..3a1b819 100644
--- a/drivers/net/ethernet/marvell/prestera_sw/prestera_dsa.c
+++ b/drivers/net/ethernet/marvell/prestera_sw/prestera_dsa.c
@@ -88,6 +88,9 @@
  */
 #define W3_MASK_HW_DEV_NUM	GENMASK(6, 0)
 
+/* bits 0-7 -- CpuCode */
+#define W1_MASK_CPU_CODE	GENMASK(7, 0)
+
 /* VID becomes 16b eVLAN. eVLAN[15:0] = {Word3[30:27], Word0[11:0]} */
 #define W3_MASK_VID		GENMASK(30, 27)
 
@@ -115,6 +118,9 @@ static int net_if_dsa_to_cpu_parse(const u32 *words_ptr,
 	to_cpu_ptr->hw_dev_num &= W3_MASK_HW_DEV_NUM;
 	to_cpu_ptr->hw_dev_num |= FIELD_PREP(DEV_NUM_MASK, get_value);
 
+	get_value = FIELD_GET(W1_MASK_CPU_CODE, words_ptr[1]);
+	to_cpu_ptr->cpu_code = (u8)get_value;
+
 	if (to_cpu_ptr->src_is_trunk) {
 		to_cpu_ptr->iface.src_trunk_id =
 		    (u16)FIELD_GET(W2_MASK_SRC_TRANK_ID, words_ptr[2]);
diff --git a/drivers/net/ethernet/marvell/prestera_sw/prestera_dsa.h b/drivers/net/ethernet/marvell/prestera_sw/prestera_dsa.h
index fcf5632..a118a48 100644
--- a/drivers/net/ethernet/marvell/prestera_sw/prestera_dsa.h
+++ b/drivers/net/ethernet/marvell/prestera_sw/prestera_dsa.h
@@ -33,6 +33,7 @@ struct mvsw_pr_dsa_to_cpu {
 	bool is_tagged;
 	u32 hw_dev_num;
 	bool src_is_trunk;
+	u8 cpu_code;
 	struct {
 		u16 src_trunk_id;
 		u32 port_num;
diff --git a/drivers/net/ethernet/marvell/prestera_sw/prestera_hw.c b/drivers/net/ethernet/marvell/prestera_sw/prestera_hw.c
index 81bf0ba..f538684 100644
--- a/drivers/net/ethernet/marvell/prestera_sw/prestera_hw.c
+++ b/drivers/net/ethernet/marvell/prestera_sw/prestera_hw.c
@@ -62,9 +62,16 @@ enum mvsw_msg_type {
 	MVSW_MSG_TYPE_ROUTER_NH_ADD = 0x620,
 	MVSW_MSG_TYPE_ROUTER_NH_DELETE = 0x621,
 	MVSW_MSG_TYPE_ROUTER_NH_SET = 0x622,
-	MVSW_MSG_TYPE_ROUTER_NH_GET = 0x623,
+	MVSW_MSG_TYPE_ROUTER_NH_GRP_ADD = 0x623,
+	MVSW_MSG_TYPE_ROUTER_NH_GRP_DELETE = 0x624,
 	MVSW_MSG_TYPE_ROUTER_VR_CREATE = 0x630,
 	MVSW_MSG_TYPE_ROUTER_VR_DELETE = 0x631,
+	MVSW_MSG_TYPE_ROUTER_VR_ABORT = 0x632,
+	MVSW_MSG_TYPE_ROUTER_NEIGH_ADD = 0x641,
+	MVSW_MSG_TYPE_ROUTER_NEIGH_DELETE = 0x642,
+	MVSW_MSG_TYPE_ROUTER_NEIGH_SET = 0x643,
+	MVSW_MSG_TYPE_ROUTER_NEIGH_GET = 0x644,
+	MVSW_MSG_TYPE_ROUTER_MP_HASH_SET = 0x650,
 
 	MVSW_MSG_TYPE_RXTX_INIT = 0x800,
 
@@ -72,6 +79,7 @@ enum mvsw_msg_type {
 	MVSW_MSG_TYPE_LAG_DELETE = 0x901,
 	MVSW_MSG_TYPE_LAG_ENABLE = 0x902,
 	MVSW_MSG_TYPE_LAG_DISABLE = 0x903,
+	MVSW_MSG_TYPE_LAG_ROUTER_LEAVE = 0x904,
 
 	MVSW_MSG_TYPE_STP_PORT_SET = 0x1000,
 
@@ -105,6 +113,7 @@ enum mvsw_msg_port_attr {
 enum mvsw_msg_switch_attr {
 	MVSW_MSG_SWITCH_ATTR_MAC = 1,
 	MVSW_MSG_SWITCH_ATTR_AGEING = 2,
+	MVSW_MSG_SWITCH_ATTR_TRAP_POLICER = 3,
 };
 
 enum {
@@ -194,6 +203,7 @@ struct mvsw_msg_common_response {
 union mvsw_msg_switch_param {
 	u32 ageing_timeout;
 	u8  mac[ETH_ALEN];
+	u32 trap_policer_profile;
 };
 
 struct mvsw_msg_switch_attr_cmd {
@@ -409,6 +419,7 @@ struct mvsw_msg_stp_cmd {
 struct mvsw_msg_iface {
 	u8 type;
 	u16 vid;
+	u16 vr_id;
 	union {
 		struct {
 			u32 dev;
@@ -422,7 +433,6 @@ struct mvsw_msg_rif_cmd {
 	struct mvsw_msg_cmd cmd;
 	struct mvsw_msg_iface iif;
 	u16 rif_id;
-	u16 vr_id;
 	u8 mac[ETH_ALEN];
 	u32 mtu;
 } __packed __aligned(4);
@@ -434,28 +444,57 @@ struct mvsw_msg_rif_ret {
 
 struct mvsw_msg_lpm_cmd {
 	struct mvsw_msg_cmd cmd;
+	u32 grp_id;
 	__be32 dst;
 	u32 dst_len;
 	u16 vid;
 	u16 vr_id;
 } __packed __aligned(4);
 
+struct mvsw_msg_nh {
+	__be32 nh_addr;
+	u32 prefix_len;
+	u8 is_active;
+	u8 mac[ETH_ALEN];
+} __packed __aligned(4);
+
 struct mvsw_msg_nh_cmd {
 	struct mvsw_msg_cmd cmd;
+	u32 size;
+	u32 grp_id;
+	struct mvsw_msg_iface oif[MVSW_PR_NHGR_SIZE_MAX];
+	struct mvsw_msg_nh nh[MVSW_PR_NHGR_SIZE_MAX];
+} __packed __aligned(4);
+
+struct mvsw_msg_nh_grp_cmd {
+	struct mvsw_msg_cmd cmd;
+	u32 grp_id;
+	u32 size;
+} __packed __aligned(4);
+
+struct mvsw_msg_nh_grp_ret {
+	struct mvsw_msg_ret ret;
+	u32 grp_id;
+} __packed __aligned(4);
+
+struct mvsw_msg_neigh_cmd {
+	struct mvsw_msg_cmd cmd;
 	struct mvsw_msg_iface oif;
+	struct mvsw_msg_nh dest;
 	u32 hw_id;
-	__be32 dst;
-	u32 dst_len;
-	u16 vr_id;
-	u8 mac[ETH_ALEN];
 } __packed __aligned(4);
 
-struct mvsw_msg_nh_ret {
+struct mvsw_msg_neigh_ret {
 	struct mvsw_msg_ret ret;
 	u32 hw_id;
 	u8 is_active;
 } __packed __aligned(4);
 
+struct mvsw_msg_mp_cmd {
+	struct mvsw_msg_cmd cmd;
+	u8 hash_policy;
+} __packed __aligned(4);
+
 struct mvsw_msg_rxtx_cmd {
 	struct mvsw_msg_cmd cmd;
 	u8 use_sdma;
@@ -481,6 +520,7 @@ struct mvsw_msg_lag_cmd {
 	u32 port;
 	u32 dev;
 	u16 lag_id;
+	u16 vr_id;
 } __packed __aligned(4);
 
 #define fw_check_resp(_response)	\
@@ -839,6 +879,17 @@ int mvsw_pr_hw_switch_mac_set(const struct mvsw_pr_switch *sw, const u8 *mac)
 	return fw_send_req(sw, MVSW_MSG_TYPE_SWITCH_ATTR_SET, &req);
 }
 
+int mvsw_pr_hw_switch_trap_policer_set(const struct mvsw_pr_switch *sw,
+				       u8 profile)
+{
+	struct mvsw_msg_switch_attr_cmd req = {
+		.param = {.trap_policer_profile = profile},
+		.attr = MVSW_MSG_SWITCH_ATTR_TRAP_POLICER,
+	};
+
+	return fw_send_req(sw, MVSW_MSG_TYPE_SWITCH_ATTR_SET, &req);
+}
+
 int mvsw_pr_hw_port_state_set(const struct mvsw_pr_port *port,
 			      bool admin_state)
 {
@@ -1615,18 +1666,52 @@ static int mvsw_pr_iface_to_msg(struct mvsw_pr_iface *iface,
 		return -ENOTSUPP;
 	}
 
+	msg_if->vr_id = iface->vr_id;
 	msg_if->vid = iface->vlan_id;
 	msg_if->type = iface->type;
 	return 0;
 }
 
+static void mvsw_pr_nh_to_msg(struct mvsw_pr_nexthop_info *dest,
+			      struct mvsw_msg_nh *msg_nh)
+{
+	msg_nh->nh_addr = dest->nh_addr;
+	msg_nh->prefix_len = dest->prefix_len;
+	msg_nh->is_active = dest->connected;
+	memcpy(msg_nh->mac, dest->ha, ETH_ALEN);
+}
+
+static int mvsw_pr_nh_grp_to_msg(struct mvsw_pr_nexthop *nhs, int count,
+				 struct mvsw_msg_nh_cmd *req)
+{
+	struct mvsw_pr_nexthop_info *nh_info;
+	struct mvsw_pr_nexthop *nh;
+	struct mvsw_pr_iface *oif;
+	struct mvsw_msg_iface *msg_iface;
+	struct mvsw_msg_nh *msg_nh;
+	int i = 0, err;
+
+	for (; i < count; i++) {
+		nh = &nhs[i];
+		nh_info = &nh->nh_info;
+		msg_nh = &req->nh[i];
+		oif = &nh->nh_iface;
+		msg_iface = &req->oif[i];
+
+		mvsw_pr_nh_to_msg(nh_info, msg_nh);
+		err = mvsw_pr_iface_to_msg(oif, msg_iface);
+		if (err)
+			return err;
+	}
+
+	req->size = count;
+	return err;
+}
+
 int mvsw_pr_hw_rif_create(const struct mvsw_pr_switch *sw,
-			  struct mvsw_pr_iface *iif, u16 vr_id, u8 *mac,
-			  u16 *rif_id)
+			  struct mvsw_pr_iface *iif, u8 *mac, u16 *rif_id)
 {
-	struct mvsw_msg_rif_cmd req = {
-		.vr_id = vr_id,
-	};
+	struct mvsw_msg_rif_cmd req;
 	struct mvsw_msg_rif_ret resp;
 	int err;
 
@@ -1646,11 +1731,10 @@ int mvsw_pr_hw_rif_create(const struct mvsw_pr_switch *sw,
 }
 
 int mvsw_pr_hw_rif_delete(const struct mvsw_pr_switch *sw, u16 rif_id,
-			  struct mvsw_pr_iface *iif, u16 vr_id)
+			  struct mvsw_pr_iface *iif)
 {
 	struct mvsw_msg_rif_cmd req = {
 		.rif_id = rif_id,
-		.vr_id = vr_id,
 	};
 	int err;
 
@@ -1707,103 +1791,172 @@ int mvsw_pr_hw_vr_delete(const struct mvsw_pr_switch *sw, u16 vr_id)
 	return fw_send_req(sw, MVSW_MSG_TYPE_ROUTER_VR_DELETE, &req);
 }
 
+int mvsw_pr_hw_vr_abort(const struct mvsw_pr_switch *sw, u16 vr_id)
+{
+	struct mvsw_msg_vr_cmd req = {
+		.vr_id = vr_id,
+	};
+
+	return fw_send_req(sw, MVSW_MSG_TYPE_ROUTER_VR_ABORT, &req);
+}
+
 int mvsw_pr_hw_lpm_update(const struct mvsw_pr_switch *sw, u16 vr_id,
-			  __be32 dst, u32 dst_len, u16 vid)
+			  __be32 dst, u32 dst_len, u16 vid, u32 grp_id)
 {
 	struct mvsw_msg_lpm_cmd req = {
 		.dst = dst,
 		.dst_len = dst_len,
 		.vid = vid,
-		.vr_id = vr_id
+		.vr_id = vr_id,
+		.grp_id = grp_id
 	};
 
 	return fw_send_req(sw, MVSW_MSG_TYPE_ROUTER_LPM_UPDATE, &req);
 }
 
 int mvsw_pr_hw_lpm_del(const struct mvsw_pr_switch *sw, u16 vr_id, __be32 dst,
-		       u32 dst_len)
+		       u32 dst_len, u32 grp_id)
 {
 	struct mvsw_msg_lpm_cmd req = {
 		.dst = dst,
 		.dst_len = dst_len,
-		.vr_id = vr_id
+		.vr_id = vr_id,
+		.grp_id = grp_id,
 	};
 
 	return fw_send_req(sw, MVSW_MSG_TYPE_ROUTER_LPM_DELETE, &req);
 }
 
-int mvsw_pr_hw_nh_entry_add(const struct mvsw_pr_switch *sw, u16 vr_id, u16 vid,
-			    __be32 dst, u8 *mac, u32 *hw_id)
+int mvsw_pr_hw_nh_entries_add(const struct mvsw_pr_switch *sw, int count,
+			      struct mvsw_pr_nexthop *nhs)
 {
-	struct mvsw_msg_nh_ret resp;
-	struct mvsw_msg_nh_cmd req = {
-		.oif = { .vid = vid },
-		.dst = dst,
-		.vr_id = vr_id,
-	};
+	struct mvsw_msg_nh_cmd req;
 	int err;
 
-	memcpy(req.mac, mac, ETH_ALEN);
+	err = mvsw_pr_nh_grp_to_msg(nhs, count, &req);
+	if (err)
+		return err;
 
-	err = fw_send_req_resp(sw, MVSW_MSG_TYPE_ROUTER_NH_ADD, &req, &resp);
+	return fw_send_req(sw, MVSW_MSG_TYPE_ROUTER_NH_ADD, &req);
+}
 
+int mvsw_pr_hw_nh_entries_del(const struct mvsw_pr_switch *sw, int count,
+			      struct mvsw_pr_nexthop *nhs)
+{
+	struct mvsw_msg_nh_cmd req;
+	int err;
+
+	err = mvsw_pr_nh_grp_to_msg(nhs, count, &req);
 	if (err)
 		return err;
 
-	*hw_id = resp.hw_id;
+	return fw_send_req(sw, MVSW_MSG_TYPE_ROUTER_NH_DELETE, &req);
+}
+
+int mvsw_pr_hw_nh_entries_set(const struct mvsw_pr_switch *sw, int count,
+			      struct mvsw_pr_nexthop *nhs, u32 grp_id)
+{
+	struct mvsw_msg_nh_cmd req = { .grp_id = grp_id };
+	int err;
+
+	err = mvsw_pr_nh_grp_to_msg(nhs, count, &req);
+	if (err)
+		return err;
+
+	return fw_send_req(sw, MVSW_MSG_TYPE_ROUTER_NH_SET, &req);
+}
+
+int mvsw_pr_hw_nh_group_create(const struct mvsw_pr_switch *sw, u16 nh_count,
+			       u32 *grp_id)
+{
+	struct mvsw_msg_nh_grp_cmd req = { .size = nh_count };
+	struct mvsw_msg_nh_grp_ret resp;
+	int err;
+
+	err = fw_send_req_resp(sw, MVSW_MSG_TYPE_ROUTER_NH_GRP_ADD, &req,
+			       &resp);
+	if (err)
+		return err;
+
+	*grp_id = resp.grp_id;
 	return err;
 }
 
-int mvsw_pr_hw_nh_entry_del(const struct mvsw_pr_switch *sw, u16 vr_id,
-			    __be32 dst, u32 dst_len, u8 *mac)
+int mvsw_pr_hw_nh_group_delete(const struct mvsw_pr_switch *sw, u16 nh_count,
+			       u32 grp_id)
 {
-	struct mvsw_msg_nh_cmd req = {
-		.dst = dst,
-		.dst_len = dst_len,
-		.vr_id = vr_id
+	struct mvsw_msg_nh_grp_cmd req = {
+	    .grp_id = grp_id,
+	    .size = nh_count
 	};
 
-	memcpy(req.mac, mac, ETH_ALEN);
+	return fw_send_req(sw, MVSW_MSG_TYPE_ROUTER_NH_GRP_DELETE, &req);
+}
 
-	return fw_send_req(sw, MVSW_MSG_TYPE_ROUTER_NH_DELETE, &req);
+int mvsw_pr_hw_neigh_entry_add(const struct mvsw_pr_switch *sw,
+			       struct mvsw_pr_neigh_entry *neigh)
+{
+	struct mvsw_pr_nexthop_info *dest = &neigh->dest;
+	struct mvsw_msg_neigh_cmd req;
+	int err;
+
+	mvsw_pr_nh_to_msg(dest, &req.dest);
+	err = mvsw_pr_iface_to_msg(&neigh->iface, &req.oif);
+	if (err)
+		return err;
+
+	return fw_send_req(sw, MVSW_MSG_TYPE_ROUTER_NEIGH_ADD, &req);
 }
 
-int mvsw_pr_hw_nh_entry_set(struct mvsw_pr_switch *sw,
-			    struct mvsw_pr_iface *oif, u16 vr_id,
-			    __be32 dst, u32 dst_len, u8 *mac, u32 *hw_id)
+int mvsw_pr_hw_neigh_entry_del(const struct mvsw_pr_switch *sw,
+			       struct mvsw_pr_neigh_entry *neigh)
 {
-	struct mvsw_msg_nh_ret resp;
-	struct mvsw_msg_nh_cmd req = {
-		.dst = dst,
-		.dst_len = dst_len,
-		.vr_id = vr_id,
-	};
+	struct mvsw_pr_nexthop_info *dest = &neigh->dest;
+	struct mvsw_msg_neigh_cmd req;
 	int err;
 
-	memcpy(req.mac, mac, ETH_ALEN);
+	mvsw_pr_nh_to_msg(dest, &req.dest);
+	err = mvsw_pr_iface_to_msg(&neigh->iface, &req.oif);
+	if (err)
+		return err;
 
-	err = mvsw_pr_iface_to_msg(oif, &req.oif);
+	return fw_send_req(sw, MVSW_MSG_TYPE_ROUTER_NEIGH_DELETE, &req);
+}
+
+int mvsw_pr_hw_neigh_entry_set(const struct mvsw_pr_switch *sw,
+			       struct mvsw_pr_neigh_entry *neigh)
+{
+	struct mvsw_msg_neigh_cmd req;
+	struct mvsw_msg_neigh_ret resp;
+	int err;
+
+	mvsw_pr_nh_to_msg(&neigh->dest, &req.dest);
+	err = mvsw_pr_iface_to_msg(&neigh->iface, &req.oif);
 	if (err)
 		return err;
 
-	err = fw_send_req_resp(sw, MVSW_MSG_TYPE_ROUTER_NH_SET,
-			       &req, &resp);
+	err = fw_send_req_resp(sw, MVSW_MSG_TYPE_ROUTER_NEIGH_SET, &req, &resp);
 	if (err)
 		return err;
 
-	*hw_id = resp.hw_id;
+	neigh->hw_id = resp.hw_id;
 	return err;
 }
 
-int mvsw_pr_hw_nh_get(const struct mvsw_pr_switch *sw, u32 hw_id, u8 *is_active)
+int mvsw_pr_hw_neigh_entry_get(const struct mvsw_pr_switch *sw,
+			       struct mvsw_pr_neigh_entry *neigh,
+			       bool *is_active)
 {
-	struct mvsw_msg_nh_cmd req = {
-		.hw_id = hw_id,
-	};
-	struct mvsw_msg_nh_ret resp;
+	struct mvsw_msg_neigh_ret resp;
+	struct mvsw_msg_neigh_cmd req = { .hw_id = neigh->hw_id };
 	int err;
 
-	err = fw_send_req_resp(sw, MVSW_MSG_TYPE_ROUTER_NH_GET, &req, &resp);
+	mvsw_pr_nh_to_msg(&neigh->dest, &req.dest);
+	err = mvsw_pr_iface_to_msg(&neigh->iface, &req.oif);
+	if (err)
+		return err;
+
+	err = fw_send_req_resp(sw, MVSW_MSG_TYPE_ROUTER_NEIGH_GET, &req, &resp);
 	if (err)
 		return err;
 
@@ -1811,6 +1964,13 @@ int mvsw_pr_hw_nh_get(const struct mvsw_pr_switch *sw, u32 hw_id, u8 *is_active)
 	return err;
 }
 
+int mvsw_pr_hw_mp4_hash_set(const struct mvsw_pr_switch *sw, u8 hash_policy)
+{
+	struct mvsw_msg_mp_cmd req = { .hash_policy = hash_policy};
+
+	return fw_send_req(sw, MVSW_MSG_TYPE_ROUTER_MP_HASH_SET, &req);
+}
+
 int mvsw_pr_hw_rxtx_init(const struct mvsw_pr_switch *sw, bool use_sdma,
 			 u32 *map_addr)
 {
@@ -2095,3 +2255,16 @@ int mvsw_pr_hw_lag_member_enable(struct mvsw_pr_port *port, u16 lag_id,
 	cmd = enable ? MVSW_MSG_TYPE_LAG_ENABLE : MVSW_MSG_TYPE_LAG_DISABLE;
 	return fw_send_req(port->sw, cmd, &req);
 }
+
+int mvsw_pr_hw_lag_member_rif_leave(const struct mvsw_pr_port *port,
+				    u16 lag_id, u16 vr_id)
+{
+	struct mvsw_msg_lag_cmd req = {
+		.port = port->hw_id,
+		.dev = port->dev_id,
+		.lag_id = lag_id,
+		.vr_id = vr_id
+	};
+
+	return fw_send_req(port->sw, MVSW_MSG_TYPE_LAG_ROUTER_LEAVE, &req);
+}
diff --git a/drivers/net/ethernet/marvell/prestera_sw/prestera_hw.h b/drivers/net/ethernet/marvell/prestera_sw/prestera_hw.h
index e667570..b75d369 100644
--- a/drivers/net/ethernet/marvell/prestera_sw/prestera_hw.h
+++ b/drivers/net/ethernet/marvell/prestera_sw/prestera_hw.h
@@ -87,6 +87,8 @@ struct mvsw_pr_port_stats;
 struct mvsw_pr_port_caps;
 struct mvsw_pr_acl_rule;
 struct mvsw_pr_iface;
+struct mvsw_pr_nexthop;
+struct mvsw_pr_neigh_entry;
 
 enum mvsw_pr_event_type;
 struct mvsw_pr_event;
@@ -96,6 +98,8 @@ int mvsw_pr_hw_switch_init(struct mvsw_pr_switch *sw);
 int mvsw_pr_hw_switch_ageing_set(const struct mvsw_pr_switch *sw,
 				 u32 ageing_time);
 int mvsw_pr_hw_switch_mac_set(const struct mvsw_pr_switch *sw, const u8 *mac);
+int mvsw_pr_hw_switch_trap_policer_set(const struct mvsw_pr_switch *sw,
+				       u8 profile);
 
 /* Port API */
 int mvsw_pr_hw_port_info_get(const struct mvsw_pr_port *port,
@@ -184,33 +188,48 @@ int mvsw_pr_hw_acl_port_unbind(const struct mvsw_pr_port *port, u16 ruleset_id);
 
 /* Router API */
 int mvsw_pr_hw_rif_create(const struct mvsw_pr_switch *sw,
-			  struct mvsw_pr_iface *iif, u16 vr_id, u8 *mac,
-			  u16 *rif_id);
+			  struct mvsw_pr_iface *iif, u8 *mac, u16 *rif_id);
 int mvsw_pr_hw_rif_delete(const struct mvsw_pr_switch *sw, u16 rif_id,
-			  struct mvsw_pr_iface *iif, u16 vr_id);
+			  struct mvsw_pr_iface *iif);
 int mvsw_pr_hw_rif_set(const struct mvsw_pr_switch *sw, u16 *rif_id,
 		       struct mvsw_pr_iface *iif, u8 *mac);
 
 /* Virtual Router API */
 int mvsw_pr_hw_vr_create(const struct mvsw_pr_switch *sw, u16 *vr_id);
 int mvsw_pr_hw_vr_delete(const struct mvsw_pr_switch *sw, u16 vr_id);
+int mvsw_pr_hw_vr_abort(const struct mvsw_pr_switch *sw, u16 vr_id);
 
 /* LPM API */
 int mvsw_pr_hw_lpm_update(const struct mvsw_pr_switch *sw, u16 vr_id,
-			  __be32 dst, u32 dst_len, u16 vid);
+			  __be32 dst, u32 dst_len, u16 vid, u32 grp_id);
 int mvsw_pr_hw_lpm_del(const struct mvsw_pr_switch *sw, u16 vr_id, __be32 dst,
-		       u32 dst_len);
+		       u32 dst_len, u32 grp_id);
 
 /* NH API */
-int mvsw_pr_hw_nh_entry_add(const struct mvsw_pr_switch *sw, u16 vr_id, u16 vid,
-			    __be32 dst, u8 *mac, u32 *hw_id);
-int mvsw_pr_hw_nh_entry_del(const struct mvsw_pr_switch *sw, u16 vr_id,
-			    __be32 dst, u32 dst_len, u8 *mac);
-int mvsw_pr_hw_nh_entry_set(struct mvsw_pr_switch *sw,
-			    struct mvsw_pr_iface *iif, u16 vr_id,
-			    __be32 dst, u32 dst_len, u8 *mac, u32 *hw_id);
-int mvsw_pr_hw_nh_get(const struct mvsw_pr_switch *sw, u32 hw_id,
-		      u8 *is_valid);
+int mvsw_pr_hw_nh_entries_add(const struct mvsw_pr_switch *sw, int count,
+			      struct mvsw_pr_nexthop *nh);
+int mvsw_pr_hw_nh_entries_del(const struct mvsw_pr_switch *sw, int count,
+			      struct mvsw_pr_nexthop *nh);
+int mvsw_pr_hw_nh_entries_set(const struct mvsw_pr_switch *sw, int count,
+			      struct mvsw_pr_nexthop *nh, u32 grp_id);
+int mvsw_pr_hw_nh_group_create(const struct mvsw_pr_switch *sw, u16 nh_count,
+			       u32 *grp_id);
+int mvsw_pr_hw_nh_group_delete(const struct mvsw_pr_switch *sw, u16 nh_count,
+			       u32 grp_id);
+
+/* NEIGH API */
+int mvsw_pr_hw_neigh_entry_add(const struct mvsw_pr_switch *sw,
+			       struct mvsw_pr_neigh_entry *neigh);
+int mvsw_pr_hw_neigh_entry_del(const struct mvsw_pr_switch *sw,
+			       struct mvsw_pr_neigh_entry *neigh);
+int mvsw_pr_hw_neigh_entry_set(const struct mvsw_pr_switch *sw,
+			       struct mvsw_pr_neigh_entry *neigh);
+int mvsw_pr_hw_neigh_entry_get(const struct mvsw_pr_switch *sw,
+			       struct mvsw_pr_neigh_entry *neigh,
+			       bool *is_active);
+
+/* MP API */
+int mvsw_pr_hw_mp4_hash_set(const struct mvsw_pr_switch *sw, u8 hash_policy);
 
 /* LAG API */
 int mvsw_pr_hw_lag_member_add(struct mvsw_pr_port *port, u16 lag_id);
@@ -225,6 +244,8 @@ int mvsw_pr_hw_fdb_flush_lag(const struct mvsw_pr_switch *sw, u16 lag_id,
 			     u32 mode);
 int mvsw_pr_hw_fdb_flush_lag_vlan(const struct mvsw_pr_switch *sw,
 				  u16 lag_id, u16 vid, u32 mode);
+int mvsw_pr_hw_lag_member_rif_leave(const struct mvsw_pr_port *port,
+				    u16 lag_id, u16 vr_id);
 
 /* Event handlers */
 int mvsw_pr_hw_event_handler_register(struct mvsw_pr_switch *sw,
diff --git a/drivers/net/ethernet/marvell/prestera_sw/prestera_pci.c b/drivers/net/ethernet/marvell/prestera_sw/prestera_pci.c
index d4d0e3f..1cec495 100644
--- a/drivers/net/ethernet/marvell/prestera_sw/prestera_pci.c
+++ b/drivers/net/ethernet/marvell/prestera_sw/prestera_pci.c
@@ -16,7 +16,7 @@
 #define MVSW_FW_FILENAME	"marvell/mvsw_prestera_fw.img"
 
 #define MVSW_SUPP_FW_MAJ_VER 2
-#define MVSW_SUPP_FW_MIN_VER 0
+#define MVSW_SUPP_FW_MIN_VER 3
 #define MVSW_SUPP_FW_PATCH_VER 0
 
 #define mvsw_wait_timeout(cond, waitms) \
@@ -174,6 +174,9 @@ struct mvsw_pr_fw_regs {
 /* MVSW_CMD_RCV_CTL_REG flags */
 #define MVSW_CMD_F_REPL_SENT		BIT(0)
 
+/* MVSW_FW_STATUS_REG flags */
+#define MVSW_STATUS_F_EVT_OFF		BIT(0)
+
 #define MVSW_EVTQ_REG_OFFSET(q, f)			\
 	(MVSW_FW_REG_OFFSET(evtq_list) +		\
 	 (q) * sizeof(struct mvsw_pr_fw_evtq_regs) +	\
@@ -309,6 +312,24 @@ static u8 mvsw_pr_fw_evtq_pick(struct mvsw_pr_fw *fw)
 	return MVSW_EVT_QNUM_MAX;
 }
 
+static void mvsw_pr_fw_status_set(struct mvsw_pr_fw *fw, unsigned int val)
+{
+	u32 status = mvsw_fw_read(fw, MVSW_FW_STATUS_REG);
+
+	status |= val;
+
+	mvsw_fw_write(fw, MVSW_FW_STATUS_REG, status);
+}
+
+static void mvsw_pr_fw_status_clear(struct mvsw_pr_fw *fw, u32 val)
+{
+	u32 status = mvsw_fw_read(fw, MVSW_FW_STATUS_REG);
+
+	status &= ~val;
+
+	mvsw_fw_write(fw, MVSW_FW_STATUS_REG, status);
+}
+
 static void mvsw_pr_fw_evt_work_fn(struct work_struct *work)
 {
 	struct mvsw_pr_fw *fw;
@@ -318,6 +339,8 @@ static void mvsw_pr_fw_evt_work_fn(struct work_struct *work)
 	fw = container_of(work, struct mvsw_pr_fw, evt_work);
 	msg = fw->evt_msg;
 
+	mvsw_pr_fw_status_set(fw, MVSW_STATUS_F_EVT_OFF);
+
 	while ((qid = mvsw_pr_fw_evtq_pick(fw)) < MVSW_EVT_QNUM_MAX) {
 		u32 idx;
 		u32 len;
@@ -337,6 +360,8 @@ static void mvsw_pr_fw_evt_work_fn(struct work_struct *work)
 		if (fw->dev.recv_msg)
 			fw->dev.recv_msg(&fw->dev, msg, len);
 	}
+
+	mvsw_pr_fw_status_clear(fw, MVSW_STATUS_F_EVT_OFF);
 }
 
 static int mvsw_pr_fw_wait_reg32(struct mvsw_pr_fw *fw,
@@ -383,7 +408,7 @@ static int mvsw_pr_fw_cmd_send(struct mvsw_pr_fw *fw,
 		return -EMSGSIZE;
 
 	/* wait for finish previous reply from FW */
-	err = mvsw_pr_fw_wait_reg32(fw, MVSW_CMD_RCV_CTL_REG, 0, 30);
+	err = mvsw_pr_fw_wait_reg32(fw, MVSW_CMD_RCV_CTL_REG, 0, 1000);
 	if (err) {
 		dev_err(mvsw_fw_dev(fw), "finish reply from FW is timed out\n");
 		return err;
@@ -548,8 +573,15 @@ static int mvsw_pr_ldr_send(struct mvsw_pr_fw *fw,
 			break;
 
 		err = mvsw_pr_ldr_send_buf(fw, img + pos, MVSW_FW_BLK_SZ);
-		if (err)
+		if (err) {
+			if (mvsw_fw_read(fw, MVSW_LDR_STATUS_REG) ==
+					 MVSW_LDR_STATUS_NOMEM) {
+				dev_err(mvsw_fw_dev(fw),
+					"Fw image is too big or invalid\n");
+				return -EINVAL;
+			}
 			return err;
+		}
 	}
 
 	if (pos < fw_size) {
@@ -605,15 +637,10 @@ static int mvsw_pr_fw_rev_check(struct mvsw_pr_fw *fw)
 	struct mvsw_fw_rev *rev = &fw->dev.fw_rev;
 
 	if (rev->maj == MVSW_SUPP_FW_MAJ_VER &&
-	    rev->min >= MVSW_SUPP_FW_MIN_VER) {
+	    rev->min == MVSW_SUPP_FW_MIN_VER) {
 		return 0;
 	}
 
-	dev_err(mvsw_fw_dev(fw), "Driver supports FW version only '%u.%u.%u'",
-		MVSW_SUPP_FW_MAJ_VER,
-		MVSW_SUPP_FW_MIN_VER,
-		MVSW_SUPP_FW_PATCH_VER);
-
 	return -EINVAL;
 }
 
@@ -634,8 +661,17 @@ static int mvsw_pr_fw_hdr_parse(struct mvsw_pr_fw *fw,
 
 	dev_info(mvsw_fw_dev(fw), "FW version '%u.%u.%u'\n",
 		 rev->maj, rev->min, rev->sub);
+	dev_info(mvsw_fw_dev(fw), "Driver version '%u.%u.%u'\n",
+		 MVSW_SUPP_FW_MAJ_VER, MVSW_SUPP_FW_MIN_VER,
+		 MVSW_SUPP_FW_PATCH_VER);
 
-	return mvsw_pr_fw_rev_check(fw);
+	if (mvsw_pr_fw_rev_check(fw)) {
+		dev_err(mvsw_fw_dev(fw),
+			"Driver is incomatible with FW: version mismatch");
+		return -EINVAL;
+	}
+
+	return 0;
 }
 
 static int mvsw_pr_fw_load(struct mvsw_pr_fw *fw)
@@ -673,7 +709,7 @@ static int mvsw_pr_fw_load(struct mvsw_pr_fw *fw)
 
 	err = mvsw_pr_fw_hdr_parse(fw, f);
 	if (err) {
-		dev_err(mvsw_fw_dev(fw), "FW image header is invalid\n");
+		dev_err(mvsw_fw_dev(fw), "FW image is invalid\n");
 		release_firmware(f);
 		return err;
 	}
diff --git a/drivers/net/ethernet/marvell/prestera_sw/prestera_router.c b/drivers/net/ethernet/marvell/prestera_sw/prestera_router.c
index 902cfb9..838b836 100644
--- a/drivers/net/ethernet/marvell/prestera_sw/prestera_router.c
+++ b/drivers/net/ethernet/marvell/prestera_sw/prestera_router.c
@@ -22,20 +22,28 @@
 #include "prestera_hw.h"
 #include "prestera_log.h"
 
-#define MVSW_PR_UNRESOLVED_NH_PROBE_INTERVAL 2000 /* ms */
+#define MVSW_PR_NH_PROBE_INTERVAL 5000 /* ms */
+#define MVSW_PR_NHGR_UNUSED 0
 
 static const char mvsw_driver_name[] = "mrvl_switchdev";
 
+struct mvsw_pr_nexthop_group;
+struct mvsw_pr_nexthop_key;
+struct mvsw_pr_nexthop;
+
 struct mvsw_pr_router {
 	struct mvsw_pr_switch *sw;
 	struct list_head rif_list;	/* list of mvsw_pr_rif */
 	struct list_head vr_list;	/* list of mvsw_pr_vr */
+	struct rhashtable neigh_ht;
+	struct rhashtable nexthop_group_ht;
 	struct list_head nexthop_list;
 	struct list_head nexthop_neighs_list;
 	struct {
 		struct delayed_work dw;
 		unsigned int interval;	/* ms */
 	} neighs_update;
+	struct delayed_work nexthops_update_dw;
 	struct notifier_block netevent_nb;
 	struct notifier_block inetaddr_nb;
 	struct notifier_block fib_nb;
@@ -43,13 +51,17 @@ struct mvsw_pr_router {
 };
 
 struct mvsw_pr_rif {
-	struct net_device *dev;
 	struct mvsw_pr_iface iface;
+	struct net_device *dev;
 	struct list_head router_node;
+	struct list_head nexthop_list;
+	struct list_head neigh_list;
+	struct list_head fib_list;
 	unsigned char addr[ETH_ALEN];
 	unsigned int mtu;
+	bool is_active;
 	u16 rif_id;
-	u16 vr_id;
+	struct mvsw_pr_vr *vr;
 	struct mvsw_pr_switch *sw;
 
 };
@@ -59,19 +71,27 @@ struct mvsw_pr_rif_params {
 	u16 vid;
 };
 
+struct mvsw_pr_fib_key {
+	u32 addr;
+	u32 prefix_len;
+};
+
 struct mvsw_pr_fib_node {
 	struct mvsw_pr_fib4_entry *fib4_entry;
-	struct list_head fib_node;
+	struct rhash_head ht_node;	/* node of mvsw_pr_vr */
 	struct mvsw_pr_vr *vr;
-	u32 dst;
-	int dst_len;
+	struct mvsw_pr_fib_key key;
 };
 
 struct mvsw_pr_fib4_entry {
 	struct mvsw_pr_fib_node *fib_node;
-	struct net_device *nh_dev;
+	struct mvsw_pr_nexthop_group *nh_grp;
+	struct list_head nexthop_group_node;
+	struct list_head rif_head;
 	struct fib_info *fi;
+	bool should_offload;
 	u32 tb_id;
+	u16 vid;
 	u8 type;
 };
 
@@ -79,20 +99,45 @@ struct mvsw_pr_vr {
 	u16 id;				/* virtual router ID */
 	u32 tb_id;			/* kernel fib table id */
 	struct list_head router_node;
-	struct list_head fib_list;	/* list of mvsw_pr_fib_node */
+	struct rhashtable ht;		/* table of mvsw_pr_fib_node */
 	unsigned int ref_cnt;
 };
 
-struct mvsw_pr_nh {
-	struct list_head nh_node;
-	struct neighbour *n;
-	struct mvsw_pr_iface iface;
-	bool connected, has_gw;
-	u8 ha[ETH_ALEN];
-	__be32 gw_ip, n_ip;
-	u32 dst_len, hw_id;
-	u16 vr_id;
-	int ref_cnt;
+struct mvsw_pr_nexthop_group {
+	struct rhash_head ht_node;
+	struct list_head fib_list;	/* list of fib entries that use this */
+					/* group */
+	struct fib_info *fi;
+	u32 grp_id;
+	bool gateway;
+	bool is_ecmp;
+	u16 count;
+	struct mvsw_pr_nexthop nexthops[0];
+};
+
+enum mvsw_pr_mp_hash_policy {
+	MVSW_MP_L3_HASH_POLICY,
+	MVSW_MP_L4_HASH_POLICY,
+	MVSW_MP_HASH_POLICY_MAX,
+};
+
+static const struct rhashtable_params mvsw_pr_fib_ht_params = {
+	.key_offset  = offsetof(struct mvsw_pr_fib_node, key),
+	.head_offset = offsetof(struct mvsw_pr_fib_node, ht_node),
+	.key_len     = sizeof(struct mvsw_pr_fib_key),
+	.automatic_shrinking = true,
+};
+
+static const struct rhashtable_params mvsw_pr_neigh_ht_params = {
+	.key_offset  = offsetof(struct mvsw_pr_neigh_entry, n),
+	.key_len     = sizeof(struct neighbour *),
+	.head_offset = offsetof(struct mvsw_pr_neigh_entry, ht_node),
+};
+
+static const struct rhashtable_params mvsw_pr_nexthop_group_ht_params = {
+	.key_offset  = offsetof(struct mvsw_pr_nexthop_group, fi),
+	.key_len     = sizeof(struct fib_info *),
+	.head_offset = offsetof(struct mvsw_pr_nexthop_group, ht_node),
 };
 
 static struct workqueue_struct *mvsw_r_wq;
@@ -104,53 +149,56 @@ static const unsigned char mvsw_pr_mac_mask[ETH_ALEN] = {
 
 static struct mvsw_pr_vr *mvsw_pr_vr_get(struct mvsw_pr_switch *sw, u32 tb_id,
 					 struct netlink_ext_ack *extack);
-static u16 mvsw_pr_nh_dev_to_vr_id(struct mvsw_pr_switch *sw,
-				   struct net_device *dev);
 static void mvsw_pr_vr_destroy(struct mvsw_pr_switch *sw,
 			       struct mvsw_pr_vr *vr);
 static u32 mvsw_pr_fix_tb_id(u32 tb_id);
 static void mvsw_pr_vr_put(struct mvsw_pr_switch *sw, struct mvsw_pr_vr *vr);
 static struct mvsw_pr_vr *mvsw_pr_vr_find(struct mvsw_pr_switch *sw, u32 tb_id);
-static struct mvsw_pr_vr *mvsw_pr_vr_find_by_id(struct mvsw_pr_switch *sw,
-						u16 vr_id);
 static void mvsw_pr_router_fib_abort(struct mvsw_pr_switch *sw);
 static struct mvsw_pr_rif *mvsw_pr_rif_create(struct mvsw_pr_switch *sw,
 					      const struct mvsw_pr_rif_params
 					      *params,
 					      struct netlink_ext_ack *extack);
+static int mvsw_pr_rif_vr_update(struct mvsw_pr_switch *sw,
+				 struct mvsw_pr_rif *rif,
+				 struct netlink_ext_ack *extack);
 static void mvsw_pr_rif_destroy(struct mvsw_pr_rif *rif);
+static void mvsw_pr_rif_put(struct mvsw_pr_rif *rif);
 static int mvsw_pr_rif_update(struct mvsw_pr_rif *rif, char *mac);
+static struct mvsw_pr_rif *mvsw_pr_rif_find(const struct mvsw_pr_switch *sw,
+					    const struct net_device *dev);
+static u16 mvsw_pr_rif_vr_id(struct mvsw_pr_rif *rif);
+static int mvsw_pr_nexthop4_group_release(struct mvsw_pr_switch *sw,
+					  struct mvsw_pr_nexthop_group *grp);
+static void mvsw_pr_fib_entry_flush(struct mvsw_pr_switch *sw,
+				    struct mvsw_pr_fib4_entry *fib4_entry);
 
-static struct mvsw_pr_nh*
-mvsw_pr_nh_neigh_find(const struct mvsw_pr_switch *sw,
-		      __be32 addr)
+static int mvsw_pr_fib_node_insert(struct mvsw_pr_vr *vr,
+				   struct mvsw_pr_fib_node *fib_node)
 {
-	struct mvsw_pr_nh *neigh;
-
-	list_for_each_entry(neigh, &sw->router->nexthop_list, nh_node) {
-		if (neigh->has_gw && neigh->gw_ip == addr)
-			return neigh;
-	}
-
-	return NULL;
+	return rhashtable_insert_fast(&vr->ht, &fib_node->ht_node,
+				      mvsw_pr_fib_ht_params);
 }
 
-static struct mvsw_pr_nh *mvsw_pr_nh_find(const struct mvsw_pr_switch *sw,
-					  const struct neighbour *n)
+static void mvsw_pr_fib_node_remove(struct mvsw_pr_vr *vr,
+				    struct mvsw_pr_fib_node *fib_node)
 {
-	struct mvsw_pr_nh *neigh;
+	rhashtable_remove_fast(&vr->ht, &fib_node->ht_node,
+			       mvsw_pr_fib_ht_params);
+}
 
-	list_for_each_entry(neigh, &sw->router->nexthop_list, nh_node) {
-		if (neigh->n == n && !neigh->has_gw)
-			return neigh;
-	}
+static struct mvsw_pr_fib_node *
+mvsw_pr_fib_node_lookup(struct mvsw_pr_vr *vr, u32 addr, size_t prefix_len)
+{
+	struct mvsw_pr_fib_key key;
 
-	return NULL;
+	key.addr = addr;
+	key.prefix_len = prefix_len;
+	return rhashtable_lookup_fast(&vr->ht, &key, mvsw_pr_fib_ht_params);
 }
 
 static struct mvsw_pr_fib_node *
-mvsw_pr_fib_node_create(struct mvsw_pr_vr *vr, u32 dst,
-			size_t dst_len)
+mvsw_pr_fib_node_create(struct mvsw_pr_vr *vr, u32 addr, size_t prefix_len)
 {
 	struct mvsw_pr_fib_node *fib_node;
 
@@ -159,18 +207,15 @@ mvsw_pr_fib_node_create(struct mvsw_pr_vr *vr, u32 dst,
 		return NULL;
 
 	fib_node->vr = vr;
-	fib_node->dst = dst;
-	fib_node->dst_len = dst_len;
-
+	fib_node->key.addr = addr;
+	fib_node->key.prefix_len = prefix_len;
 	vr->ref_cnt++;
-	list_add(&fib_node->fib_node, &vr->fib_list);
 
 	return fib_node;
 }
 
 static void mvsw_pr_fib_node_destroy(struct mvsw_pr_fib_node *fib_node)
 {
-	list_del(&fib_node->fib_node);
 	kfree(fib_node);
 }
 
@@ -182,25 +227,12 @@ static void mvsw_pr_fib_node_put(struct mvsw_pr_switch *sw,
 	if (fib_node->fib4_entry)
 		return;
 
+	mvsw_pr_fib_node_remove(vr, fib_node);
 	mvsw_pr_fib_node_destroy(fib_node);
 	mvsw_pr_vr_put(sw, vr);
 }
 
 static struct mvsw_pr_fib_node *
-mvsw_pr_fib_node_lookup(struct mvsw_pr_vr *vr, u32 dst, size_t dst_len)
-{
-	struct mvsw_pr_fib_node *fib_node;
-
-	list_for_each_entry(fib_node, &vr->fib_list, fib_node) {
-		if (fib_node->dst_len == dst_len &&
-		    fib_node->dst == dst)
-			return fib_node;
-	}
-
-	return NULL;
-}
-
-static struct mvsw_pr_fib_node *
 mvsw_pr_fib_node_get(struct mvsw_pr_switch *sw, u32 tb_id, u32 dst,
 		     size_t dst_len)
 {
@@ -222,148 +254,33 @@ mvsw_pr_fib_node_get(struct mvsw_pr_switch *sw, u32 tb_id, u32 dst,
 		goto err_fib_node_create;
 	}
 
+	err = mvsw_pr_fib_node_insert(vr, fib_node);
+	if (err)
+		goto err_fib_node_insert;
+
 	return fib_node;
 
+err_fib_node_insert:
+	kfree(fib_node);
 err_fib_node_create:
 	mvsw_pr_vr_put(sw, vr);
 	return ERR_PTR(err);
 }
 
-static struct mvsw_pr_nh
-*mvsw_pr_nh_neigh_alloc(struct mvsw_pr_switch *sw,
-				 struct neighbour *n,
-				 __be32 dst,
-				 u32 dst_len)
-{
-	struct mvsw_pr_nh *nh;
-
-	nh = kzalloc(sizeof(*nh), GFP_KERNEL);
-	if (!nh)
-		return NULL;
-
-	nh->has_gw = true;
-	nh->n = n;
-	nh->gw_ip = dst;
-	nh->dst_len = dst_len;
-	nh->vr_id = mvsw_pr_nh_dev_to_vr_id(sw, n->dev);
-	nh->ref_cnt = 1;
-
-	list_add_tail(&nh->nh_node, &sw->router->nexthop_list);
-	return nh;
-}
-
-static struct mvsw_pr_nh *mvsw_pr_nh_alloc(struct mvsw_pr_switch *sw,
-					   struct neighbour *n)
-{
-	struct mvsw_pr_nh *nh;
-
-	nh = kzalloc(sizeof(*nh), GFP_KERNEL);
-	if (!nh)
-		return NULL;
-
-	nh->n = n;
-	nh->dst_len = 32;
-
-	list_add(&nh->nh_node, &sw->router->nexthop_list);
-	return nh;
-}
-
-static struct mvsw_pr_nh *mvsw_pr_nh_get(struct mvsw_pr_switch *sw,
-					 struct neighbour *n)
-{
-	struct mvsw_pr_nh *nh;
-
-	nh = mvsw_pr_nh_find(sw, n);
-	if (nh)
-		return nh;
-
-	return mvsw_pr_nh_alloc(sw, n);
-}
-
-static void mvsw_pr_nh_release(struct mvsw_pr_switch *sw,
-			       struct mvsw_pr_nh *nh)
-{
-	__be32 addr = nh->has_gw ? nh->gw_ip : nh->n_ip;
-
-	mvsw_pr_nh_entry_delete(sw,
-				nh->vr_id, addr,
-				nh->dst_len, nh->ha);
-}
-
-static void mvsw_pr_nh_put(struct mvsw_pr_nh *nh)
-{
-	if (!nh->ref_cnt) {
-		list_del(&nh->nh_node);
-		kfree(nh);
-	}
-}
-
-static void mvsw_pr_nh_destroy(struct mvsw_pr_nh *nh)
-{
-	nh->ref_cnt--;
-	mvsw_pr_nh_put(nh);
-}
-
-struct mvsw_net_event_work {
-	struct work_struct work;
-	struct mvsw_pr_router *router;
-	struct neighbour *n;
-	u16 vr_id;
-};
-
-static void mvsw_pr_router_update_nh(struct mvsw_pr_router *router)
-{
-	struct mvsw_pr_nh *nh;
-	u8 is_active;
-	int err;
-
-	rtnl_lock();
-	list_for_each_entry(nh, &router->nexthop_list, nh_node) {
-		/* Keep linux aware of an active neighbours */
-		if (nh->n_ip) {
-			err = mvsw_pr_hw_nh_get(router->sw, nh->hw_id,
-						&is_active);
-			if (err) {
-				pr_err("Failed to get neighbour entry state");
-				continue;
-			}
-
-			if (is_active)
-				neigh_event_send(nh->n, NULL);
-		}
-	}
-	rtnl_unlock();
-}
-
-static u16 mvsw_pr_nh_dev_to_vr_id(struct mvsw_pr_switch *sw,
-				   struct net_device *dev)
-{
-	struct mvsw_pr_vr *vr;
-	u16 tb_id, vr_id = 0;
-
-	tb_id = l3mdev_fib_table(dev);
-	tb_id = mvsw_pr_fix_tb_id(tb_id ? : RT_TABLE_MAIN);
-
-	vr = mvsw_pr_vr_find(sw, tb_id);
-	if (vr)
-		vr_id = vr->id;
-
-	return vr_id;
-}
-
 static u16 mvsw_pr_nh_dev_to_vid(struct mvsw_pr_switch *sw,
 				 struct net_device *dev)
 {
 	struct macvlan_dev *vlan;
 	u16 vid = 0;
 
-	if (is_vlan_dev(dev) && netif_is_bridge_master(vlan_dev_real_dev(dev)))
+	if (is_vlan_dev(dev) &&
+	    netif_is_bridge_master(vlan_dev_real_dev(dev))) {
 		vid = vlan_dev_vlan_id(dev);
-	else if (netif_is_bridge_master(dev) && br_vlan_enabled(dev))
+	} else if (netif_is_bridge_master(dev) && br_vlan_enabled(dev)) {
 		br_vlan_get_pvid(dev, &vid);
-	else if (netif_is_bridge_master(dev))
+	} else if (netif_is_bridge_master(dev)) {
 		vid = mvsw_pr_vlan_dev_vlan_id(sw->bridge, dev);
-	else if (netif_is_macvlan(dev)) {
+	} else if (netif_is_macvlan(dev)) {
 		vlan = netdev_priv(dev);
 		return mvsw_pr_nh_dev_to_vid(sw, vlan->lowerdev);
 	}
@@ -371,336 +288,495 @@ static u16 mvsw_pr_nh_dev_to_vid(struct mvsw_pr_switch *sw,
 	return vid;
 }
 
-static struct mvsw_pr_port *
-mvsw_pr_nh_dev_to_port(struct mvsw_pr_switch *sw, struct net_device *dev,
-		       u8 *ha)
+static struct net_device*
+mvsw_pr_nh_dev_egress(struct mvsw_pr_switch *sw, struct net_device *dev,
+		      u8 *ha)
 {
-	struct net_device *bridge_dev, *port_dev = dev;
+	struct net_device *bridge_dev, *egress_dev = dev;
 	u16 vid = mvsw_pr_nh_dev_to_vid(sw, dev);
 	struct macvlan_dev *vlan;
 
 	if (is_vlan_dev(dev) &&
 	    netif_is_bridge_master(vlan_dev_real_dev(dev))) {
 		bridge_dev = vlan_dev_priv(dev)->real_dev;
-		port_dev = br_fdb_find_port(bridge_dev, ha,
-					    vid);
+		egress_dev = br_fdb_find_port(bridge_dev, ha,
+					      vid);
 	} else if (netif_is_bridge_master(dev) && br_vlan_enabled(dev)) {
-		port_dev = br_fdb_find_port(dev, ha, vid);
+		egress_dev = br_fdb_find_port(dev, ha, vid);
 	} else if (netif_is_bridge_master(dev)) {
 		/* vid in .1d bridge is 0 */
-		port_dev = br_fdb_find_port(dev, ha, 0);
+		egress_dev = br_fdb_find_port(dev, ha, 0);
 	} else if (netif_is_macvlan(dev)) {
 		vlan = netdev_priv(dev);
-		return mvsw_pr_nh_dev_to_port(sw, vlan->lowerdev, ha);
+		return mvsw_pr_nh_dev_egress(sw, vlan->lowerdev, ha);
 	}
 
-	if (!port_dev)
-		return NULL;
+	return egress_dev;
+}
 
-	return netdev_priv(port_dev);
+static u16 mvsw_pr_rif_vr_id(struct mvsw_pr_rif *rif)
+{
+	return rif->vr->id;
 }
 
-static int mvsw_pr_router_gw_nh_refresh(struct mvsw_pr_switch *sw,
-					struct mvsw_pr_nh *nh)
+static int
+mvsw_pr_rif_iface_init(struct mvsw_pr_rif *rif)
 {
+	struct net_device *dev = rif->dev;
+	struct mvsw_pr_switch *sw = rif->sw;
 	struct mvsw_pr_port *port;
-	struct neighbour *n = nh->n;
-	int hw_id, err = 0;
-	u8 mac[ETH_ALEN];
-
-	read_lock_bh(&n->lock);
-	memcpy(mac, n->ha, ETH_ALEN);
-	read_unlock_bh(&n->lock);
+	int if_type = mvsw_pr_dev_if_type(dev);
 
-	port = mvsw_pr_nh_dev_to_port(sw, nh->n->dev, nh->ha);
+	switch (if_type) {
+	case MVSW_IF_PORT_E:
+		port = netdev_priv(dev);
+		rif->iface.dev_port.hw_dev_num = port->dev_id;
+		rif->iface.dev_port.port_num = port->hw_id;
+		break;
+	case MVSW_IF_LAG_E:
+		mvsw_pr_lag_id_find(sw, dev, &rif->iface.lag_id);
+		break;
+	case MVSW_IF_VID_E:
+		break;
+	default:
+		pr_err("Unsupported rif type");
+		return -EINVAL;
+	}
 
-	err = mvsw_pr_nh_entry_set(sw, &nh->iface, nh->vr_id,
-				   nh->gw_ip, nh->dst_len,
-				   mac, &hw_id);
-	if (err)
-		return err;
+	rif->iface.type = if_type;
+	rif->iface.vlan_id = mvsw_pr_nh_dev_to_vid(sw, dev);
+	rif->iface.vr_id = rif->vr->id;
 
-	nh->hw_id = hw_id;
-	return err;
+	return 0;
 }
 
-static int mvsw_pr_nhs_offload(struct mvsw_pr_switch *sw, struct neighbour *n)
+static void
+__mvsw_pr_neigh_iface_init(struct mvsw_pr_switch *sw,
+			   struct mvsw_pr_neigh_entry *neigh_entry,
+			   struct net_device *dev)
 {
-	struct mvsw_pr_nh *nh;
-	__be32 addr;
-	int hw_id, err = 0;
-
-	list_for_each_entry(nh, &sw->router->nexthop_list, nh_node) {
-		if (nh->n == n) {
-			addr = nh->has_gw ? nh->gw_ip : nh->n_ip;
-			err = mvsw_pr_nh_entry_set(sw, &nh->iface,
-						   nh->vr_id, addr, nh->dst_len,
-						   nh->ha, &hw_id);
-			if (err)
-				return err;
+	bool is_nud_perm = neigh_entry->n->nud_state & NUD_PERMANENT;
+	struct mvsw_pr_iface *iface = &neigh_entry->iface;
+	struct net_device *egress_dev;
+	struct mvsw_pr_port *port;
 
-			nh->hw_id = hw_id;
-		}
-	}
+	iface->type = mvsw_pr_dev_if_type(dev);
 
-	n->flags |= NTF_OFFLOADED;
-	return err;
+	switch (iface->type) {
+	case MVSW_IF_PORT_E:
+	case MVSW_IF_VID_E:
+		egress_dev = mvsw_pr_nh_dev_egress(sw, dev,
+						   neigh_entry->dest.ha);
+		if (!egress_dev && is_nud_perm)
+		/* Permanent neighbours on a bridge are not bounded to any
+		 * of the ports which is needed by the hardware, therefore
+		 * use any valid lower
+		 */
+			port = mvsw_pr_port_dev_lower_find(dev);
+		if (!egress_dev)
+			return;
+
+		if (!mvsw_pr_netdev_check(egress_dev))
+			return __mvsw_pr_neigh_iface_init(sw, neigh_entry,
+							  egress_dev);
+
+		port = netdev_priv(egress_dev);
+		iface->dev_port.hw_dev_num = port->dev_id;
+		iface->dev_port.port_num = port->hw_id;
+		break;
+	case MVSW_IF_LAG_E:
+		mvsw_pr_lag_id_find(sw, dev, &iface->lag_id);
+		break;
+	default:
+		MVSW_LOG_ERROR("Unsupported nexthop device");
+		return;
+	}
 }
 
 static void
-mvsw_pr_router_nhs_release(struct mvsw_pr_switch *sw,
-			   struct neighbour *n)
+mvsw_pr_neigh_iface_init(struct mvsw_pr_switch *sw,
+			 struct mvsw_pr_neigh_entry *neigh_entry)
 {
-	struct mvsw_pr_nh *nh, *tmp;
+	struct mvsw_pr_iface *iface = &neigh_entry->iface;
+	struct net_device *dev = neigh_entry->n->dev;
 
-	list_for_each_entry_safe(nh, tmp, &sw->router->nexthop_list, nh_node) {
-		if (nh->n == n) {
-			mvsw_pr_nh_release(sw, nh);
-			if (!nh->has_gw)
-				mvsw_pr_nh_destroy(nh);
-		}
-	}
-	n->flags &= ~NTF_OFFLOADED;
+	iface->vlan_id = mvsw_pr_nh_dev_to_vid(sw, dev);
+	iface->vr_id = mvsw_pr_rif_vr_id(neigh_entry->rif);
+	__mvsw_pr_neigh_iface_init(sw, neigh_entry, dev);
 }
 
-static bool
-mvsw_pr_router_is_defalt_gw_prefix(__be32 dst, u32 dst_len)
+static struct mvsw_pr_neigh_entry *
+mvsw_pr_neigh_entry_alloc(struct mvsw_pr_switch *sw, struct neighbour *n)
 {
-	/* default route has zero dst/dst_len to match any route */
-	return !dst && !dst_len;
+	struct mvsw_pr_neigh_entry *neigh_entry;
+
+	neigh_entry = kzalloc(sizeof(*neigh_entry), GFP_KERNEL);
+	if (!neigh_entry)
+		return NULL;
+
+	neigh_entry->n = n;
+	INIT_LIST_HEAD(&neigh_entry->nexthop_list);
+
+	return neigh_entry;
 }
 
-static bool
-mvsw_pr_router_is_nh_default_gw(struct mvsw_pr_nh *nh)
+static void mvsw_pr_neigh_entry_free(struct mvsw_pr_neigh_entry *neigh_entry)
 {
-	return mvsw_pr_router_is_defalt_gw_prefix(nh->gw_ip, nh->dst_len);
+	kfree(neigh_entry);
 }
 
-static bool
-mvsw_pr_router_nh_match_default_gw(struct mvsw_pr_nh *nh, u16 vr_id)
+static int
+mvsw_pr_neigh_entry_insert(struct mvsw_pr_switch *sw,
+			   struct mvsw_pr_neigh_entry *neigh_entry)
 {
-	return nh->has_gw &&
-		mvsw_pr_router_is_nh_default_gw(nh) &&
-		(nh->vr_id == vr_id);
+	return rhashtable_insert_fast(&sw->router->neigh_ht,
+				      &neigh_entry->ht_node,
+				      mvsw_pr_neigh_ht_params);
 }
 
 static void
-mvsw_pr_router_release_default_gw(struct mvsw_pr_switch *sw, u16 vr_id)
+mvsw_pr_neigh_entry_remove(struct mvsw_pr_switch *sw,
+			   struct mvsw_pr_neigh_entry *neigh_entry)
 {
-	struct mvsw_pr_nh *nh, *tmp;
+	rhashtable_remove_fast(&sw->router->neigh_ht,
+			       &neigh_entry->ht_node,
+			       mvsw_pr_neigh_ht_params);
+}
 
-	list_for_each_entry_safe(nh, tmp, &sw->router->nexthop_list, nh_node) {
-		if (mvsw_pr_router_nh_match_default_gw(nh, vr_id)) {
-			mvsw_pr_nh_entry_delete(sw, nh->vr_id,
-						nh->gw_ip, nh->dst_len, nh->ha);
-			mvsw_pr_nh_destroy(nh);
-			break;
-		}
-	}
+static inline __be32 mvsw_pr_neigh_addr(struct neighbour *n)
+{
+	return *((__be32 *)n->primary_key);
 }
 
-static int
-mvsw_pr_nh_iface_update(struct mvsw_pr_switch *sw, struct mvsw_pr_nh *nh)
+static inline __be32
+mvsw_pr_neigh4_entry_addr(struct mvsw_pr_neigh_entry *neigh_entry)
 {
-	struct net_device *dev = nh->n->dev;
-	struct mvsw_pr_iface *iface = &nh->iface;
-	struct mvsw_pr_port *port;
+	return mvsw_pr_neigh_addr(neigh_entry->n);
+}
 
-	iface->vlan_id = mvsw_pr_nh_dev_to_vid(sw, dev);
-	iface->type = mvsw_pr_dev_if_type(dev);
-	switch (iface->type) {
-	case MVSW_IF_LAG_E:
-		mvsw_pr_lag_id_find(sw, dev, &iface->lag_id);
-		break;
-	case MVSW_IF_PORT_E:
-	case MVSW_IF_VID_E:
-		port = mvsw_pr_nh_dev_to_port(sw, dev, nh->ha);
-		if (!port)
-			return -1;
+static struct mvsw_pr_neigh_entry *
+mvsw_pr_neigh_entry_create(struct mvsw_pr_switch *sw, struct neighbour *n)
+{
+	struct mvsw_pr_neigh_entry *neigh_entry;
+	struct mvsw_pr_rif *rif;
+	int err;
 
-		iface->dev_port.hw_dev_num = port->dev_id;
-		iface->dev_port.port_num = port->hw_id;
-		break;
-	default:
-		MVSW_LOG_ERROR("Unsupported nexthop device");
-		return -1;
-	}
+	rif = mvsw_pr_rif_find(sw, n->dev);
+	if (!rif)
+		return ERR_PTR(-EINVAL);
 
-	return 0;
+	neigh_entry = mvsw_pr_neigh_entry_alloc(sw, n);
+	if (!neigh_entry)
+		return ERR_PTR(-ENOMEM);
+
+	neigh_entry->rif = rif;
+	neigh_entry->dest.nh_addr = mvsw_pr_neigh4_entry_addr(neigh_entry);
+	neigh_entry->dest.prefix_len = 32;
+
+	mvsw_pr_neigh_iface_init(sw, neigh_entry);
+	err = mvsw_pr_neigh_entry_add(sw, neigh_entry);
+	if (err)
+		goto err_neigh_entry_add;
+
+	err = mvsw_pr_neigh_entry_insert(sw, neigh_entry);
+	if (err)
+		goto err_neigh_entry_insert;
+
+	list_add(&neigh_entry->rif_head, &rif->neigh_list);
+	return neigh_entry;
+
+err_neigh_entry_insert:
+	mvsw_pr_neigh_entry_del(sw, neigh_entry);
+err_neigh_entry_add:
+	mvsw_pr_neigh_entry_free(neigh_entry);
+	return ERR_PTR(err);
 }
 
-static int
-mvsw_pr_rif_iface_init(struct mvsw_pr_rif *rif)
+static void
+mvsw_pr_neigh_entry_destroy(struct mvsw_pr_switch *sw,
+			    struct mvsw_pr_neigh_entry *neigh_entry)
 {
-	struct net_device *dev = rif->dev;
-	struct mvsw_pr_switch *sw = rif->sw;
-	struct mvsw_pr_port *port;
-	int if_type;
+	list_del(&neigh_entry->rif_head);
+	mvsw_pr_rif_put(neigh_entry->rif);
+	mvsw_pr_neigh_entry_del(sw, neigh_entry);
+	mvsw_pr_neigh_entry_remove(sw, neigh_entry);
+	mvsw_pr_neigh_entry_free(neigh_entry);
+}
 
-	if_type = mvsw_pr_dev_if_type(dev);
-	switch (if_type) {
-	case MVSW_IF_PORT_E:
-		port = netdev_priv(dev);
-		rif->iface.dev_port.hw_dev_num = port->dev_id;
-		rif->iface.dev_port.port_num = port->hw_id;
-		break;
-	case MVSW_IF_LAG_E:
-		mvsw_pr_lag_id_find(sw, dev, &rif->iface.lag_id);
-		break;
-	case MVSW_IF_VID_E:
-		break;
-	default:
-		pr_err("Unsupported rif type");
-		return -EINVAL;
-	}
+static struct mvsw_pr_neigh_entry *
+mvsw_pr_neigh_entry_lookup(struct mvsw_pr_switch *sw, struct neighbour *n)
+{
+	return rhashtable_lookup_fast(&sw->router->neigh_ht,
+				      &n, mvsw_pr_neigh_ht_params);
+}
 
-	rif->iface.type = if_type;
-	rif->iface.vlan_id = mvsw_pr_nh_dev_to_vid(sw, dev);
+static void
+mvsw_pr_nexthop_group_offload_refresh(struct mvsw_pr_switch *sw,
+				      struct mvsw_pr_nexthop_group *nh_grp)
+{
+	int i;
 
-	return 0;
+	for (i = 0; i < nh_grp->count; i++) {
+		struct mvsw_pr_nexthop *nh = &nh_grp->nexthops[i];
+
+		if (nh->nh_info.connected)
+			nh->key.fib_nh->fib_nh_flags |= RTNH_F_OFFLOAD;
+		else
+			nh->key.fib_nh->fib_nh_flags &= ~RTNH_F_OFFLOAD;
+	}
 }
 
 static int
-mvsw_pr_nhs_update(struct mvsw_pr_switch *sw, struct neighbour *n)
+mvsw_pr_nexthop_adjust_fib(struct mvsw_pr_switch *sw,
+			   struct mvsw_pr_fib4_entry *fib4_entry)
 {
-	struct mvsw_pr_nh *nh;
-	char mac[ETH_ALEN];
-	u16 vr_id;
-	int err;
+	struct mvsw_pr_nexthop_group *nh_grp = fib4_entry->nh_grp;
+	struct mvsw_pr_fib_key *key = &fib4_entry->fib_node->key;
+	struct mvsw_pr_nexthop fib_nh;
 
-	read_lock_bh(&n->lock);
-	memcpy(mac, n->ha, ETH_ALEN);
-	read_unlock_bh(&n->lock);
+	if (!nh_grp->gateway || nh_grp->is_ecmp)
+		return 0;
+
+	memcpy(&fib_nh, &nh_grp->nexthops[0],
+	       sizeof(struct mvsw_pr_nexthop));
+	fib_nh.nh_info.prefix_len = key->prefix_len;
+	fib_nh.nh_info.nh_addr = htonl(key->addr);
+
+	return mvsw_pr_nh_entries_set(sw, 1, &fib_nh,
+				      MVSW_PR_NHGR_UNUSED);
+}
+
+static int
+mvsw_pr_router_adjust_fibs(struct mvsw_pr_switch *sw,
+			   struct mvsw_pr_nexthop_group *nh_grp)
+{
+	struct mvsw_pr_fib4_entry *fib_entry;
+	struct mvsw_pr_fib_key *key;
+	struct mvsw_pr_nexthop fib_nh[MVSW_PR_NHGR_SIZE_MAX];
+	int pos = 0, err;
 
-	vr_id = mvsw_pr_nh_dev_to_vr_id(sw, n->dev);
+	if (nh_grp->is_ecmp)
+		return 0;
 
-	list_for_each_entry(nh, &sw->router->nexthop_list, nh_node) {
-		if (nh->n == n) {
-			nh->vr_id = vr_id;
-			memcpy(nh->ha, mac, ETH_ALEN);
-			err = mvsw_pr_nh_iface_update(sw, nh);
+	/* Ecmp nexhtops are inderectly connected to lpm entry on hw and
+	 * performing nh_set is enough to point lpm entry egress. However,
+	 * non ecmp lpm entries are directly connected and are initially
+	 * pointing to the default one. It is required to mannually repoint
+	 * lpm entrie from the defaut to resolved one and otherwise
+	 */
+	fib_entry = list_first_entry(&nh_grp->fib_list, typeof(*fib_entry),
+				     nexthop_group_node);
+
+	do {
+		key = &fib_entry->fib_node->key;
+		memcpy(&fib_nh[pos], &nh_grp->nexthops[0],
+		       sizeof(struct mvsw_pr_nexthop));
+		fib_nh[pos].nh_info.prefix_len = key->prefix_len;
+		fib_nh[pos].nh_info.nh_addr = htonl(key->addr);
+
+		fib_entry = list_next_entry(fib_entry, nexthop_group_node);
+		if (&fib_entry->nexthop_group_node == &nh_grp->fib_list ||
+		    pos == MVSW_PR_NHGR_SIZE_MAX - 1) {
+			err = mvsw_pr_nh_entries_set(sw, pos + 1, fib_nh,
+						     MVSW_PR_NHGR_UNUSED);
 			if (err)
 				return err;
+			pos = 0;
 		}
-	}
 
-	return 0;
+		pos++;
+	} while (&fib_entry->nexthop_group_node != &nh_grp->fib_list);
+
+	return err;
 }
 
 static void
-mvsw_pr_router_update_resolved_nh(struct mvsw_pr_switch *sw,
-				  struct mvsw_pr_nh *nh, bool is_connected)
+mvsw_pr_nexthop_group_refresh(struct mvsw_pr_switch *sw,
+			      struct mvsw_pr_nexthop_group *nh_grp)
 {
-	int err;
+	struct mvsw_pr_nexthop *nh;
+	int i = 0, err;
 
-	if (nh && !is_connected) {
-		mvsw_pr_router_nhs_release(sw, nh->n);
-	} else if (nh) {
-		err = mvsw_pr_nhs_update(sw, nh->n);
-		if (err) {
-			pr_err("Failed to update neighbours");
-			return;
-		}
+	if (!nh_grp->gateway)
+		return;
 
-		err = mvsw_pr_nhs_offload(sw, nh->n);
-		if (err)
-			pr_err("Failed to offload neighbours");
+	err = mvsw_pr_nh_entries_set(sw, nh_grp->count, nh_grp->nexthops,
+				     nh_grp->grp_id);
+	if (err) {
+		MVSW_LOG_ERROR("Failed to update nh group");
+		goto err_nh_set;
+	}
+
+	err = mvsw_pr_router_adjust_fibs(sw, nh_grp);
+	if (err) {
+		MVSW_LOG_ERROR("Failed to ajust fibs");
+		goto err_nh_set;
+	}
+
+	goto out;
 
-		nh->connected = is_connected;
+err_nh_set:
+	for (; i < nh_grp->count; i++) {
+		nh = &nh_grp->nexthops[i];
+		nh->nh_info.connected = false;
 	}
+	mvsw_pr_nh_entries_set(sw, nh_grp->count, nh_grp->nexthops,
+			       nh_grp->grp_id);
+
+out:
+	mvsw_pr_nexthop_group_offload_refresh(sw, nh_grp);
 }
 
-static struct mvsw_pr_nh*
-mvsw_pr_nh_offload(struct mvsw_pr_switch *sw, struct neighbour *n)
+static void
+mvsw_pr_nexthop_nh_info_refresh(struct mvsw_pr_nexthop *nh,
+				struct mvsw_pr_neigh_entry *neigh)
 {
-	struct mvsw_pr_nh *nh;
-	bool nh_connected;
-	u8 ha[ETH_ALEN];
-	int hw_id, err;
-	__be32 ip_addr;
-	u16 vr_id;
+	memcpy(&nh->nh_info.ha, &neigh->dest.ha, ETH_ALEN);
+	nh->nh_info.connected = neigh->dest.connected;
+}
+
+static int
+mvsw_pr_nexthop_dead_neigh_replace(struct mvsw_pr_switch *sw,
+				   struct mvsw_pr_neigh_entry *neigh_entry)
+{
+	struct neighbour *n, *old_n = neigh_entry->n;
+	struct mvsw_pr_nexthop *nh;
+	bool entry_connected;
+	u8 nud_state, dead;
+	int err;
+
+	nh = list_first_entry(&neigh_entry->nexthop_list,
+			      struct mvsw_pr_nexthop, neigh_head);
+
+	n = neigh_lookup(&arp_tbl, &nh->nh_info.nh_addr, nh->rif->dev);
+	if (!n) {
+		n = neigh_create(&arp_tbl, &nh->nh_info.nh_addr,
+				 nh->rif->dev);
+		if (IS_ERR(n))
+			return PTR_ERR(n);
+
+		neigh_event_send(n, NULL);
+	}
+
+	mvsw_pr_neigh_entry_remove(sw, neigh_entry);
+	neigh_entry->n = n;
+	err = mvsw_pr_neigh_entry_insert(sw, neigh_entry);
+	if (err)
+		goto err_neigh_entry_insert;
 
 	read_lock_bh(&n->lock);
-	memcpy(ha, n->ha, ETH_ALEN);
-	nh_connected = (n->nud_state & NUD_VALID && !n->dead);
-	ip_addr = *(__be32 *)n->primary_key;
+	nud_state = n->nud_state;
+	dead = n->dead;
 	read_unlock_bh(&n->lock);
+	entry_connected = nud_state & NUD_VALID && !dead;
 
-	nh = mvsw_pr_nh_get(sw, n);
-	if (!nh) {
-		MVSW_LOG_ERROR("Failed to allocate new_nh");
-		err = -ENOMEM;
-		goto err_nh_alloc;
+	list_for_each_entry(nh, &neigh_entry->nexthop_list,
+			    neigh_head) {
+		neigh_release(old_n);
+		neigh_clone(n);
+		mvsw_pr_nexthop_group_refresh(sw, nh->nh_grp);
 	}
 
-	vr_id = mvsw_pr_nh_dev_to_vr_id(sw, n->dev);
-	err = mvsw_pr_hw_nh_entry_add(sw, vr_id,
-				      mvsw_pr_nh_dev_to_vid(sw, n->dev),
-				      ip_addr, ha, &hw_id);
-	if (err) {
-		MVSW_LOG_INFO
-			("failed to offload neigh entry (err %d)\n", err);
-		goto err_nh_offload;
-	}
+	neigh_release(n);
 
-	memcpy(nh->ha, ha, ETH_ALEN);
-	nh->connected = nh_connected;
-	nh->n_ip = ip_addr;
-	nh->hw_id = hw_id;
-	nh->vr_id = vr_id;
-	nh->ref_cnt++;
-
-	return nh;
-err_nh_offload:
-	list_del(&nh->nh_node);
-	kfree(nh);
-err_nh_alloc:
-	return ERR_PTR(err);
+	return 0;
+
+err_neigh_entry_insert:
+	neigh_entry->n = old_n;
+	mvsw_pr_neigh_entry_insert(sw, neigh_entry);
+	neigh_release(n);
+	return err;
 }
 
-static struct mvsw_pr_nh*
-mvsw_pr_enque_nh_resolving(struct mvsw_pr_switch *sw, struct neighbour *n)
+static void
+mvsw_pr_nexthop_neigh_update(struct mvsw_pr_switch *sw,
+			     struct mvsw_pr_neigh_entry *neigh_entry,
+			     bool dead)
 {
-	struct mvsw_pr_nh *nh;
+	struct mvsw_pr_nexthop *nh;
+	int err;
+
+	if (list_empty(&neigh_entry->nexthop_list))
+		return;
+
+	if (dead) {
+		err = mvsw_pr_nexthop_dead_neigh_replace(sw, neigh_entry);
+		if (err)
+			pr_err("Failed to replace dead neigh\n");
+
+		return;
+	}
 
-	nh = mvsw_pr_nh_find(sw, n);
-	if (nh)
-		return nh;
+	list_for_each_entry(nh, &neigh_entry->nexthop_list, neigh_head) {
+		mvsw_pr_nexthop_nh_info_refresh(nh, neigh_entry);
+		mvsw_pr_nexthop_group_refresh(sw, nh->nh_grp);
+	}
+}
+
+static void
+mvsw_pr_neigh_entry_update(struct mvsw_pr_switch *sw,
+			   struct mvsw_pr_neigh_entry *neigh_entry,
+			   bool connected)
+{
+	neigh_entry->dest.connected = connected;
+	if (connected) {
+		mvsw_pr_neigh_iface_init(sw, neigh_entry);
+		neigh_entry->n->flags |= NTF_OFFLOADED;
+	} else {
+		neigh_entry->n->flags &= ~NTF_OFFLOADED;
+	}
 
-	return mvsw_pr_nh_offload(sw, n);
+	mvsw_pr_neigh_entry_set(sw, neigh_entry);
 }
 
+struct mvsw_pr_netevent_work {
+	struct work_struct work;
+	struct mvsw_pr_switch *sw;
+	struct neighbour *n;
+};
+
 static void mvsw_pr_router_neigh_event_work(struct work_struct *work)
 {
-	struct mvsw_net_event_work *net_work =
-		container_of(work, struct mvsw_net_event_work, work);
-	struct mvsw_pr_switch *sw = net_work->router->sw;
+	struct mvsw_pr_netevent_work *net_work =
+		container_of(work, struct mvsw_pr_netevent_work, work);
+	struct mvsw_pr_switch *sw = net_work->sw;
+	struct mvsw_pr_neigh_entry *neigh_entry;
 	struct neighbour *n = net_work->n;
-	struct mvsw_pr_nh *nh;
-	bool nh_connected;
+	unsigned char ha[ETH_ALEN];
+	bool entry_connected;
+	u8 nud_state, dead;
+
+	if (sw->router->aborted) {
+		neigh_release(n);
+		kfree(net_work);
+		return;
+	}
 
 	rtnl_lock();
 	read_lock_bh(&n->lock);
-	nh_connected = (n->nud_state & NUD_VALID && !n->dead);
+	memcpy(ha, n->ha, ETH_ALEN);
+	nud_state = n->nud_state;
+	dead = n->dead;
 	read_unlock_bh(&n->lock);
 
-	nh = mvsw_pr_nh_find(sw, n);
-	if (!nh  && !nh_connected)
-		goto out;
+	entry_connected = nud_state & NUD_VALID && !dead;
+	neigh_entry = mvsw_pr_neigh_entry_lookup(sw, n);
 
-	if (!nh) {
-		if (!mvsw_pr_port_dev_lower_find(n->dev))
+	if (!neigh_entry) {
+		if (!entry_connected)
 			goto out;
 
-		nh = mvsw_pr_nh_offload(sw, n);
-		if (IS_ERR(nh)) {
-			pr_err("Failed to queue nh for offloading (%lu)",
-			       PTR_ERR(nh));
+		neigh_entry = mvsw_pr_neigh_entry_create(sw, n);
+		if (IS_ERR(neigh_entry))
 			goto out;
-		}
 	}
 
-	mvsw_pr_router_update_resolved_nh(sw, nh, nh_connected);
+	memcpy(neigh_entry->dest.ha, ha, ETH_ALEN);
+	mvsw_pr_neigh_entry_update(sw, neigh_entry, entry_connected);
+	mvsw_pr_nexthop_neigh_update(sw, neigh_entry, dead);
+
+	if (!entry_connected && list_empty(&neigh_entry->nexthop_list))
+		mvsw_pr_neigh_entry_destroy(sw, neigh_entry);
 
 out:
 	neigh_release(n);
@@ -711,8 +787,9 @@ out:
 static int mvsw_pr_router_netevent_event(struct notifier_block *nb,
 					 unsigned long event, void *ptr)
 {
-	struct mvsw_net_event_work *net_work;
+	struct mvsw_pr_netevent_work *net_work;
 	struct mvsw_pr_router *router;
+	struct mvsw_pr_rif *rif;
 	struct neighbour *n = ptr;
 
 	router = container_of(nb, struct mvsw_pr_router, netevent_nb);
@@ -727,22 +804,19 @@ static int mvsw_pr_router_netevent_event(struct notifier_block *nb,
 		if (n->tbl != &arp_tbl)
 			return NOTIFY_DONE;
 
+		rif = mvsw_pr_rif_find(router->sw, n->dev);
+		if (!rif)
+			return NOTIFY_DONE;
+
 		net_work = kzalloc(sizeof(*net_work), GFP_ATOMIC);
 		if (WARN_ON(!net_work))
 			return NOTIFY_BAD;
 
 		neigh_clone(n);
 		net_work->n = n;
-		net_work->router = router;
+		net_work->sw = router->sw;
 		INIT_WORK(&net_work->work, mvsw_pr_router_neigh_event_work);
 		queue_work(mvsw_r_owq, &net_work->work);
-
-		break;
-	case NETEVENT_DELAY_PROBE_TIME_UPDATE:
-	case NETEVENT_IPV4_MPATH_HASH_UPDATE:
-	case NETEVENT_IPV6_MPATH_HASH_UPDATE:
-	case NETEVENT_IPV4_FWD_UPDATE_PRIORITY_UPDATE:
-		break;
 	}
 
 	return NOTIFY_DONE;
@@ -757,32 +831,117 @@ mvsw_pr_router_neighs_update_interval_init(struct mvsw_pr_router *router)
 	router->neighs_update.interval = jiffies_to_msecs(interval);
 }
 
-static void mvsw_pr_router_update_nh_work(struct work_struct *work)
+static void
+mvsw_pr_neigh_refresh(struct mvsw_pr_neigh_entry *neigh_entry, bool is_active)
+{
+	struct neighbour *n = neigh_entry->n;
+	__be32 addr;
+
+	if (!is_active)
+		return;
+
+	addr = mvsw_pr_neigh_addr(n);
+	netdev_dbg(n->dev, "Updating neighbour with IP=%pI4h\n", &addr);
+	neigh_event_send(n, NULL);
+}
+
+static void mvsw_pr_router_nexthops_refresh(struct mvsw_pr_router *router)
+{
+	struct mvsw_pr_neigh_entry *neigh_entry;
+
+	/* Iterate over nexthop neighbours, find those who are unresolved and
+	 * send arp on them. This solves the chicken-egg problem when
+	 * the nexthop wouldn't get offloaded until the neighbor is resolved
+	 * but it wouldn't get resolved ever in case traffic is flowing in HW
+	 * using different nexthop.
+	 *
+	 * Take RTNL mutex here to prevent lists from changes.
+	 */
+	rtnl_lock();
+	list_for_each_entry(neigh_entry, &router->nexthop_neighs_list,
+			    nexthop_neighs_list_node)
+		mvsw_pr_neigh_refresh(neigh_entry, true);
+	rtnl_unlock();
+}
+
+static void mvsw_pr_router_neighs_refresh(struct mvsw_pr_router *router)
+{
+	struct mvsw_pr_neigh_entry *neigh_entry;
+	struct rhashtable_iter iter;
+	bool is_active;
+	int err;
+
+	rtnl_lock();
+	rhashtable_walk_enter(&router->neigh_ht, &iter);
+	rhashtable_walk_start(&iter);
+	while ((neigh_entry = rhashtable_walk_next(&iter))) {
+		if (IS_ERR(neigh_entry))
+			continue;
+
+		/* Keep linux aware of an active neighbours */
+		err = mvsw_pr_neigh_entry_get(router->sw, neigh_entry,
+					      &is_active);
+		if (err) {
+			pr_err("Failed to get neighbour neigh_entry state");
+			continue;
+		}
+
+		mvsw_pr_neigh_refresh(neigh_entry, is_active);
+	}
+	rhashtable_walk_stop(&iter);
+	rhashtable_walk_exit(&iter);
+	rtnl_unlock();
+}
+
+static void mvsw_pr_router_update_neighs_work(struct work_struct *work)
 {
 	struct mvsw_pr_router *router;
 
 	router = container_of(work, struct mvsw_pr_router,
 			      neighs_update.dw.work);
-	mvsw_pr_router_update_nh(router);
+	mvsw_pr_router_neighs_refresh(router);
 
 	mvsw_pr_router_neighs_update_interval_init(router);
 	queue_delayed_work(mvsw_r_wq, &router->neighs_update.dw,
 			   msecs_to_jiffies(router->neighs_update.interval));
 }
 
+static void mvsw_pr_router_update_nexthops_work(struct work_struct *work)
+{
+	struct mvsw_pr_router *router;
+
+	router = container_of(work, struct mvsw_pr_router,
+			      nexthops_update_dw.work);
+	mvsw_pr_router_nexthops_refresh(router);
+
+	queue_delayed_work(mvsw_r_wq, &router->nexthops_update_dw,
+			   msecs_to_jiffies(MVSW_PR_NH_PROBE_INTERVAL));
+}
+
 static int mvsw_pr_neigh_init(struct mvsw_pr_switch *sw)
 {
+	int err;
+
+	err = rhashtable_init(&sw->router->neigh_ht, &mvsw_pr_neigh_ht_params);
+	if (err)
+		return err;
+
 	mvsw_pr_router_neighs_update_interval_init(sw->router);
 
 	INIT_DELAYED_WORK(&sw->router->neighs_update.dw,
-			  mvsw_pr_router_update_nh_work);
+			  mvsw_pr_router_update_neighs_work);
+	INIT_DELAYED_WORK(&sw->router->nexthops_update_dw,
+			  mvsw_pr_router_update_nexthops_work);
 	queue_delayed_work(mvsw_r_wq, &sw->router->neighs_update.dw, 0);
+	queue_delayed_work(mvsw_r_wq, &sw->router->nexthops_update_dw, 0);
 	return 0;
 }
 
 static void mvsw_pr_neigh_fini(struct mvsw_pr_switch *sw)
 {
 	cancel_delayed_work_sync(&sw->router->neighs_update.dw);
+	cancel_delayed_work_sync(&sw->router->nexthops_update_dw);
+	rhashtable_destroy(&sw->router->neigh_ht);
 }
 
 static struct mvsw_pr_rif*
@@ -827,6 +986,8 @@ mvsw_pr_port_vlan_router_join(struct mvsw_pr_port_vlan *mvsw_pr_port_vlan,
 	if (IS_ERR(rif))
 		return PTR_ERR(rif);
 
+	rif->is_active = true;
+
 	/* TODO:
 	 * - vid learning set (false)
 	 * - stp state set (FORWARDING)
@@ -836,10 +997,18 @@ mvsw_pr_port_vlan_router_join(struct mvsw_pr_port_vlan *mvsw_pr_port_vlan,
 }
 
 static void
-mvsw_pr_port_vlan_router_leave(struct mvsw_pr_port_vlan *mvsw_pr_port_vlan)
+mvsw_pr_port_vlan_router_leave(struct mvsw_pr_port_vlan *mvsw_pr_port_vlan,
+			       struct net_device *dev)
+
 {
-	MVSW_LOG_ERROR("NOT IMPLEMENTED!!!");
+	struct mvsw_pr_port *mvsw_pr_port = mvsw_pr_port_vlan->mvsw_pr_port;
+	struct mvsw_pr_switch *sw = mvsw_pr_port->sw;
+	struct mvsw_pr_rif *rif;
+
+	rif = mvsw_pr_rif_find(sw, dev);
 
+	if (rif)
+		rif->is_active = false;
 	/* TODO:
 	 * - stp state set (BLOCKING)
 	 * - vid learning set (true)
@@ -865,6 +1034,8 @@ mvsw_pr_port_router_join(struct mvsw_pr_port *mvsw_pr_port,
 	if (IS_ERR(rif))
 		return PTR_ERR(rif);
 
+	rif->is_active = true;
+
 	/* TODO:
 	 * - vid learning set (false)
 	 * - stp state set (FORWARDING)
@@ -883,18 +1054,20 @@ void mvsw_pr_port_router_leave(struct mvsw_pr_port *mvsw_pr_port)
 	 */
 
 	rif = mvsw_pr_rif_find(mvsw_pr_port->sw, mvsw_pr_port->net_dev);
-	if (rif)
-		mvsw_pr_rif_destroy(rif);
+	if (rif) {
+		rif->is_active = false;
+		mvsw_pr_rif_put(rif);
+	}
 }
 
 static int mvsw_pr_rif_fdb_op(struct mvsw_pr_rif *rif, const char *mac,
 			      bool adding)
 {
 	if (adding)
-		mvsw_pr_macvlan_add(rif->sw, rif->vr_id, mac,
+		mvsw_pr_macvlan_add(rif->sw, mvsw_pr_rif_vr_id(rif), mac,
 				    rif->iface.vlan_id);
 	else
-		mvsw_pr_macvlan_del(rif->sw, rif->vr_id, mac,
+		mvsw_pr_macvlan_del(rif->sw, mvsw_pr_rif_vr_id(rif), mac,
 				    rif->iface.vlan_id);
 
 	return 0;
@@ -998,6 +1171,16 @@ static int mvsw_pr_inetaddr_port_event(struct net_device *port_dev,
 	return 0;
 }
 
+static void mvsw_pr_rif_flush(struct mvsw_pr_rif *rif)
+{
+	struct mvsw_pr_fib4_entry *fib4_entry;
+
+	list_for_each_entry(fib4_entry, &rif->fib_list, rif_head)
+		mvsw_pr_fib_entry_flush(rif->sw, fib4_entry);
+
+	mvsw_pr_rif_put(rif);
+}
+
 static int mvsw_pr_inetaddr_bridge_event(struct mvsw_pr_switch *sw,
 					 struct net_device *dev,
 					 unsigned long event,
@@ -1010,13 +1193,18 @@ static int mvsw_pr_inetaddr_bridge_event(struct mvsw_pr_switch *sw,
 
 	switch (event) {
 	case NETDEV_UP:
-		rif = mvsw_pr_rif_create(sw, &params, extack);
+		rif = mvsw_pr_rif_find(sw, dev);
+		if (!rif)
+			rif = mvsw_pr_rif_create(sw, &params, extack);
+
 		if (IS_ERR(rif))
 			return PTR_ERR(rif);
+		rif->is_active = true;
 		break;
 	case NETDEV_DOWN:
 		rif = mvsw_pr_rif_find(sw, dev);
-		mvsw_pr_rif_destroy(rif);
+		rif->is_active = false;
+		mvsw_pr_rif_flush(rif);
 		break;
 	}
 
@@ -1040,7 +1228,7 @@ static int mvsw_pr_inetaddr_port_vlan_event(struct net_device *l3_dev,
 		return mvsw_pr_port_vlan_router_join(mvsw_pr_port_vlan,
 						     l3_dev, extack);
 	case NETDEV_DOWN:
-		mvsw_pr_port_vlan_router_leave(mvsw_pr_port_vlan);
+		mvsw_pr_port_vlan_router_leave(mvsw_pr_port_vlan, l3_dev);
 		break;
 	}
 
@@ -1084,13 +1272,18 @@ static int mvsw_pr_inetaddr_lag_event(struct mvsw_pr_switch *sw,
 
 	switch (event) {
 	case NETDEV_UP:
-		rif = mvsw_pr_rif_create(sw, &params, extack);
+		rif = mvsw_pr_rif_find(sw, lag_dev);
+		if (!rif)
+			rif = mvsw_pr_rif_create(sw, &params, extack);
+
 		if (IS_ERR(rif))
 			return PTR_ERR(rif);
+		rif->is_active = true;
 		break;
 	case NETDEV_DOWN:
 		rif = mvsw_pr_rif_find(sw, lag_dev);
-		mvsw_pr_rif_destroy(rif);
+		rif->is_active = false;
+		mvsw_pr_rif_flush(rif);
 		break;
 	}
 
@@ -1143,33 +1336,40 @@ mvsw_pr_rif_should_config(struct mvsw_pr_rif *rif, struct net_device *dev,
 	return false;
 }
 
-struct mvsw_pr_rif_cleanup_work {
-	struct work_struct work;
-	struct mvsw_pr_router *router;
-	struct in_ifaddr ifa;
-};
-
 static bool
-mvsw_pr_nh_match_ifa(struct mvsw_pr_nh *nh, struct in_ifaddr *ifa)
+mvsw_pr_neigh_match_ifa(struct mvsw_pr_neigh_entry *neigh,
+			struct in_ifaddr *ifa)
 {
-	return inet_ifa_match(nh->n_ip, ifa) || inet_ifa_match(nh->gw_ip, ifa);
+	return inet_ifa_match(mvsw_pr_neigh4_entry_addr(neigh), ifa);
 }
 
 static void
-mvsw_pr_rif_neigh_clean(struct mvsw_pr_router *router, struct in_ifaddr *ifa)
+mvsw_pr_neigh_entry_flush(struct mvsw_pr_switch *sw,
+			  struct mvsw_pr_neigh_entry *neigh_entry)
 {
+	neigh_entry->dest.connected = false;
+	mvsw_pr_neigh_entry_update(sw, neigh_entry, false);
+	mvsw_pr_nexthop_neigh_update(sw, neigh_entry, true);
+	if (list_empty(&neigh_entry->nexthop_list))
+		mvsw_pr_neigh_entry_destroy(sw, neigh_entry);
+}
+
+static void
+mvsw_pr_rif_neigh_clean(struct mvsw_pr_switch *sw, struct mvsw_pr_rif *rif,
+			struct in_ifaddr *ifa)
+{
+	struct mvsw_pr_neigh_entry *neigh_entry, *tmp;
 	struct net_device *dev = ifa->ifa_dev->dev;
-	struct mvsw_pr_nh *nh, *tmp;
 
 	/* Linux doesn`t explicitly remove neighbours found on the secondary */
-	/* ifa even though they are not rechable, therefore we need to remove */
+	/* ifa even though they are not reachable, therefore we need to remove*/
 	/* them from hardware mannually to prevent the traffic forwarding */
-	list_for_each_entry_safe(nh, tmp, &router->nexthop_list, nh_node) {
-		if (nh->n->dev == dev && mvsw_pr_nh_match_ifa(nh, ifa)) {
-			mvsw_pr_nh_release(router->sw, nh);
-			mvsw_pr_nh_destroy(nh);
-		}
-	}
+
+	list_for_each_entry_safe(neigh_entry, tmp,
+				 &rif->neigh_list, rif_head)
+		if (neigh_entry->n->dev == dev &&
+		    mvsw_pr_neigh_match_ifa(neigh_entry, ifa))
+			mvsw_pr_neigh_entry_flush(sw, neigh_entry);
 }
 
 static int mvsw_pr_inetaddr_event(struct notifier_block *nb,
@@ -1179,6 +1379,7 @@ static int mvsw_pr_inetaddr_event(struct notifier_block *nb,
 	struct net_device *dev = ifa->ifa_dev->dev;
 	struct in_device *idev = __in_dev_get_rtnl(dev);
 	struct mvsw_pr_router *router;
+	struct mvsw_pr_switch *sw;
 	struct mvsw_pr_rif *rif;
 	int err = 0;
 
@@ -1188,204 +1389,554 @@ static int mvsw_pr_inetaddr_event(struct notifier_block *nb,
 
 	MVSW_LOG_ERROR("dev=%s", dev->name);
 	router = container_of(nb, struct mvsw_pr_router, inetaddr_nb);
+	sw = router->sw;
+
+	if (netif_is_macvlan(dev))
+		goto mac_vlan;
+
+	rif = mvsw_pr_rif_find(sw, dev);
+	if (!rif)
+		goto out;
 
 	if (idev && idev->ifa_list)
-		mvsw_pr_rif_neigh_clean(router, ifa);
+		mvsw_pr_rif_neigh_clean(sw, rif, ifa);
+
+	if (!mvsw_pr_rif_should_config(rif, dev, event))
+		goto out;
+mac_vlan:
+	err = __mvsw_pr_inetaddr_event(sw, dev, event, NULL);
+out:
+	return notifier_from_errno(err);
+}
+
+int mvsw_pr_inetaddr_valid_event(struct notifier_block *unused,
+				 unsigned long event, void *ptr)
+{
+	struct in_validator_info *ivi = (struct in_validator_info *)ptr;
+	struct net_device *dev = ivi->ivi_dev->dev;
+	struct mvsw_pr_switch *sw;
+	struct mvsw_pr_rif *rif;
+	int err = 0;
+
+	sw = mvsw_pr_switch_get(dev);
+	if (!sw)
+		goto out;
 
-	rif = mvsw_pr_rif_find(router->sw, dev);
+	rif = mvsw_pr_rif_find(sw, dev);
 	if (!mvsw_pr_rif_should_config(rif, dev, event))
 		goto out;
 
-	err = __mvsw_pr_inetaddr_event(router->sw, dev, event, NULL);
+	err = mvsw_pr_router_port_check_rif_addr(sw, dev, dev->dev_addr,
+						 ivi->extack);
+	if (err)
+		goto out;
+
+	err = __mvsw_pr_inetaddr_event(sw, dev, event, ivi->extack);
 out:
 	return notifier_from_errno(err);
 }
 
-int mvsw_pr_inetaddr_valid_event(struct notifier_block *unused,
-				 unsigned long event, void *ptr)
+struct mvsw_pr_fib_event_work {
+	struct work_struct work;
+	struct mvsw_pr_switch *sw;
+	union {
+		struct fib_entry_notifier_info fen_info;
+	};
+	unsigned long event;
+};
+
+static bool mvsw_pr_fib4_allow_replace(struct mvsw_pr_fib4_entry *fib4_entry)
+{
+	struct mvsw_pr_fib_node *fib_node = fib4_entry->fib_node;
+	struct mvsw_pr_fib4_entry *fib4_replaced = fib_node->fib4_entry;
+
+	if (!fib4_replaced)
+		return true;
+
+	if (fib4_entry->tb_id == RT_TABLE_MAIN &&
+	    fib4_replaced->tb_id == RT_TABLE_LOCAL)
+		return false;
+
+	return true;
+}
+
+static int mvsw_pr_nexthop_group_insert(struct mvsw_pr_switch *sw,
+					struct mvsw_pr_nexthop_group *nh_grp)
+{
+	if (!nh_grp->gateway)
+		return 0;
+
+	return rhashtable_insert_fast(&sw->router->nexthop_group_ht,
+				      &nh_grp->ht_node,
+				      mvsw_pr_nexthop_group_ht_params);
+}
+
+static void mvsw_pr_nexthop_group_remove(struct mvsw_pr_switch *sw,
+					 struct mvsw_pr_nexthop_group *nh_grp)
+{
+	if (!nh_grp->gateway)
+		return;
+
+	rhashtable_remove_fast(&sw->router->nexthop_group_ht,
+			       &nh_grp->ht_node,
+			       mvsw_pr_nexthop_group_ht_params);
+}
+
+static struct mvsw_pr_nexthop_group *
+mvsw_pr_nexthop4_group_lookup(struct mvsw_pr_switch *sw,
+			      struct fib_info *fi)
+{
+	return rhashtable_lookup_fast(&sw->router->nexthop_group_ht,
+				      &fi, mvsw_pr_nexthop_group_ht_params);
+}
+
+static bool mvsw_pr_fi_is_gateway(const struct mvsw_pr_switch *sw,
+				  struct fib_info *fi)
+{
+	const struct fib_nh *nh = fib_info_nh(fi, 0);
+
+	return nh->fib_nh_scope == RT_SCOPE_LINK;
+}
+
+static int mvsw_pr_nexthop_neigh_init(struct mvsw_pr_switch *sw,
+				      struct mvsw_pr_nexthop *nh)
+{
+	struct mvsw_pr_neigh_entry *neigh_entry;
+	struct neighbour *n;
+	u8 nud_state, dead;
+	int err;
+
+	if (!nh->nh_grp->gateway || nh->neigh_entry)
+		return 0;
+
+	n = neigh_lookup(&arp_tbl, &nh->nh_info.nh_addr, nh->rif->dev);
+	if (!n) {
+		n = neigh_create(&arp_tbl, &nh->nh_info.nh_addr,
+				 nh->rif->dev);
+		if (IS_ERR(n))
+			return PTR_ERR(n);
+	}
+
+	neigh_entry = mvsw_pr_neigh_entry_lookup(sw, n);
+	if (!neigh_entry) {
+		neigh_entry = mvsw_pr_neigh_entry_create(sw, n);
+		if (IS_ERR(neigh_entry)) {
+			err = -EINVAL;
+			goto err_neigh_entry_create;
+		}
+	}
+
+	/* If that is the first nexthop connected to that neigh, add to */
+	/* nexthop_neighs_list */
+	if (list_empty(&neigh_entry->nexthop_list))
+		list_add_tail(&neigh_entry->nexthop_neighs_list_node,
+			      &sw->router->nexthop_neighs_list);
+
+	read_lock_bh(&n->lock);
+	nud_state = n->nud_state;
+	dead = n->dead;
+	memcpy(neigh_entry->dest.ha, n->ha, ETH_ALEN);
+	read_unlock_bh(&n->lock);
+
+	nh->neigh_entry = neigh_entry;
+	neigh_entry->dest.connected = nud_state & NUD_VALID && !dead;
+	mvsw_pr_nexthop_nh_info_refresh(nh, neigh_entry);
+
+	list_add_tail(&nh->neigh_head, &neigh_entry->nexthop_list);
+
+	return 0;
+
+err_neigh_entry_create:
+	neigh_release(n);
+	return err;
+}
+
+static void mvsw_pr_nexthop_neigh_fini(struct mvsw_pr_switch *sw,
+				       struct mvsw_pr_nexthop *nh)
+{
+	struct mvsw_pr_neigh_entry *neigh_entry = nh->neigh_entry;
+	struct neighbour *n;
+
+	if (!neigh_entry)
+		return;
+
+	n = neigh_entry->n;
+	list_del(&nh->neigh_head);
+	nh->neigh_entry = NULL;
+
+	/* If that is the last nexthop connected to that neigh, remove from
+	 * nexthop_neighs_list
+	 */
+	if (list_empty(&neigh_entry->nexthop_list)) {
+		list_del(&neigh_entry->nexthop_neighs_list_node);
+		mvsw_pr_neigh_entry_destroy(sw, neigh_entry);
+	}
+
+	neigh_release(n);
+}
+
+static void mvsw_pr_nexthop_rif_init(struct mvsw_pr_nexthop *nh,
+				     struct mvsw_pr_rif *rif)
+{
+	if (nh->rif)
+		return;
+
+	nh->rif = rif;
+	list_add(&nh->rif_head, &rif->nexthop_list);
+}
+
+static void mvsw_pr_nexthop_rif_fini(struct mvsw_pr_nexthop *nh)
+{
+	if (!nh->rif)
+		return;
+
+	list_del(&nh->rif_head);
+	mvsw_pr_rif_put(nh->rif);
+	nh->rif = NULL;
+}
+
+static void mvsw_pr_nexthop4_fini(struct mvsw_pr_switch *sw,
+				  struct mvsw_pr_nexthop *nh)
+{
+	mvsw_pr_nexthop_neigh_fini(sw, nh);
+	mvsw_pr_nexthop_rif_fini(nh);
+}
+
+static void mvsw_pr_nexthops_fini(struct mvsw_pr_switch *sw, int count,
+				  struct mvsw_pr_nexthop_group *nh_grp)
+{
+	struct mvsw_pr_nexthop *nh;
+	int i;
+
+	for (i = 0; i < count; i++) {
+		nh = &nh_grp->nexthops[i];
+		mvsw_pr_nexthop4_fini(sw, nh);
+	}
+}
+
+static int mvsw_pr_nexthop4_update(struct mvsw_pr_switch *sw,
+				   struct mvsw_pr_nexthop *nh,
+				   struct fib_nh *fib_nh)
+{
+	struct net_device *dev = fib_nh->fib_nh_dev;
+	struct macvlan_dev *vlan = netdev_priv(dev);
+	struct mvsw_pr_rif *rif;
+	int err;
+
+	if (netif_is_macvlan(dev))
+		dev = vlan->lowerdev;
+
+	rif = mvsw_pr_rif_find(sw, dev);
+	if (!rif)
+		return -ENODATA;
+
+	mvsw_pr_nexthop_rif_init(nh, rif);
+
+	err = mvsw_pr_nexthop_neigh_init(sw, nh);
+	if (err)
+		goto err_neigh_init;
+
+	return 0;
+
+err_neigh_init:
+	mvsw_pr_nexthop_rif_fini(nh);
+	return err;
+}
+
+static int mvsw_pr_nexthop4_init(struct mvsw_pr_switch *sw,
+				 struct mvsw_pr_nexthop_group *nh_grp,
+				 struct mvsw_pr_nexthop *nh,
+				 struct fib_nh *fib_nh)
+{
+	int err;
+
+	nh->nh_grp = nh_grp;
+	nh->key.fib_nh = fib_nh;
+	memcpy(&nh->nh_info.nh_addr, &fib_nh->fib_nh_gw4,
+	       sizeof(fib_nh->fib_nh_gw4));
+	nh->nh_info.prefix_len = 32;
+
+	err = mvsw_pr_nexthop4_update(sw, nh, fib_nh);
+	if (err)
+		return err;
+
+	return 0;
+}
+
+static void
+mvsw_pr_nexthop4_group_destroy(struct mvsw_pr_switch *sw,
+			       struct mvsw_pr_nexthop_group *nh_grp)
+{
+	mvsw_pr_nexthop4_group_release(sw, nh_grp);
+	mvsw_pr_nexthop_group_remove(sw, nh_grp);
+	mvsw_pr_nexthops_fini(sw, nh_grp->count, nh_grp);
+	fib_info_put(nh_grp->fi);
+	kfree(nh_grp);
+}
+
+static void mvsw_pr_nexthop4_group_put(struct mvsw_pr_switch *sw,
+				       struct mvsw_pr_fib4_entry *fib4_entry)
+{
+	struct mvsw_pr_nexthop_group *nh_grp = fib4_entry->nh_grp;
+
+	if (!fib4_entry->should_offload)
+		return;
+
+	list_del(&fib4_entry->nexthop_group_node);
+	if (!list_empty(&nh_grp->fib_list))
+		return;
+
+	mvsw_pr_nexthop4_group_destroy(sw, nh_grp);
+}
+
+static int mvsw_pr_nexthop4_group_reserve(struct mvsw_pr_switch *sw,
+					  struct mvsw_pr_nexthop_group *grp)
 {
-	struct in_validator_info *ivi = (struct in_validator_info *)ptr;
-	struct net_device *dev = ivi->ivi_dev->dev;
-	struct mvsw_pr_switch *sw;
-	struct mvsw_pr_rif *rif;
-	int err = 0;
-
-	sw = mvsw_pr_switch_get(dev);
-	if (!sw)
-		goto out;
+	int err;
 
-	rif = mvsw_pr_rif_find(sw, dev);
-	if (!mvsw_pr_rif_should_config(rif, dev, event))
-		goto out;
+	if (!grp->gateway)
+		return 0;
 
-	err = mvsw_pr_router_port_check_rif_addr(sw, dev, dev->dev_addr,
-						 ivi->extack);
-	if (err)
-		goto out;
+	/* ecmp group requires resource reservation on hw side */
+	if (grp->is_ecmp)
+		err = mvsw_pr_nh_group_create(sw, grp->count, &grp->grp_id);
+	else
+		err = mvsw_pr_nh_entries_add(sw, grp->count, grp->nexthops);
 
-	err = __mvsw_pr_inetaddr_event(sw, dev, event, ivi->extack);
-out:
-	return notifier_from_errno(err);
+	return err;
 }
 
-struct mvsw_pr_fib_event_work {
-	struct work_struct work;
-	struct mvsw_pr_switch *sw;
-	union {
-		struct fib_entry_notifier_info fen_info;
-		struct fib_rule_notifier_info fr_info;
-	};
-	unsigned long event;
-};
-
-static struct mvsw_pr_nh *
-mvsw_pr_enque_nh_neigh_resolving(struct mvsw_pr_switch *sw,
-				 struct neighbour *n,
-				 struct fib_entry_notifier_info *fen_info)
+static int mvsw_pr_nexthop4_group_release(struct mvsw_pr_switch *sw,
+					  struct mvsw_pr_nexthop_group *grp)
 {
-	struct mvsw_pr_nh *nh_neigh;
-	struct mvsw_pr_vr *vr;
+	int err;
 
-	vr = mvsw_pr_vr_find(sw, fen_info->tb_id);
-	if (!vr)
-		return NULL;
+	if (!grp->gateway)
+		return 0;
 
-	/* reset old default gw before adding new */
-	if (mvsw_pr_router_is_defalt_gw_prefix
-		(htonl(fen_info->dst), fen_info->dst_len))
-		mvsw_pr_router_release_default_gw(sw, vr->id);
+	if (grp->is_ecmp)
+		err = mvsw_pr_nh_group_delete(sw, grp->count, grp->grp_id);
+	else
+		err = mvsw_pr_nh_entries_del(sw, grp->count, grp->nexthops);
 
-	nh_neigh = mvsw_pr_nh_neigh_alloc(sw, n, htonl(fen_info->dst),
-					  fen_info->dst_len);
-	return nh_neigh;
+	return err;
 }
 
-static int mvsw_pr_router_resolve_nh(struct mvsw_pr_switch *sw,
-				     struct fib_entry_notifier_info *fen_info)
+static struct mvsw_pr_nexthop_group*
+mvsw_pr_nexthop4_group_create(struct mvsw_pr_switch *sw, struct fib_info *fi)
 {
-	struct mvsw_pr_nh *nh, *nh_neigh;
-	struct net_device *nh_dev;
-	struct neighbour *n;
+	unsigned int nhs = fib_info_num_path(fi);
+	struct mvsw_pr_nexthop_group *nh_grp;
+	struct mvsw_pr_nexthop *nh;
 	struct fib_nh *fib_nh;
-	__be32 nh_ip;
-	int err, is_connected;
+	int i, err;
 
-	fib_nh = fib_info_nh(fen_info->fi, 0);
-	nh_dev = fib_nh->fib_nh_dev;
-	nh_ip = fib_nh->fib_nh_gw4;
+	nh_grp = kzalloc(struct_size(nh_grp, nexthops, nhs), GFP_KERNEL);
+	if (!nh_grp)
+		return ERR_PTR(-ENOMEM);
 
-	n = neigh_lookup(&arp_tbl, &nh_ip, nh_dev);
-	if (!n) {
-		n = neigh_create(&arp_tbl, &nh_ip, nh_dev);
-		if (IS_ERR(n))
-			return PTR_ERR(n);
-	}
+	nh_grp->fi = fi;
+	INIT_LIST_HEAD(&nh_grp->fib_list);
 
-	nh = mvsw_pr_enque_nh_resolving(sw, n);
-	if (IS_ERR(nh)) {
-		pr_err("Failed to queue nh for offloading (%lu)",
-		       PTR_ERR(nh));
-		goto out;
-	}
+	nh_grp->gateway = mvsw_pr_fi_is_gateway(sw, fi);
+	nh_grp->is_ecmp = (nhs > 1);
+	nh_grp->count = nhs;
+	fib_info_hold(fi);
 
-	is_connected = nh->connected;
-	nh_neigh = mvsw_pr_enque_nh_neigh_resolving(sw, n, fen_info);
-	if (!nh_neigh) {
-		MVSW_LOG_ERROR("Failed to queue nh_neigh for offloading");
-		err = -ENOMEM;
-		goto out;
+	for (i = 0; i < nh_grp->count; i++) {
+		nh = &nh_grp->nexthops[i];
+		fib_nh = fib_info_nh(fi, i);
+		err = mvsw_pr_nexthop4_init(sw, nh_grp, nh, fib_nh);
+		if (err)
+			goto err_nexthop4_init;
 	}
 
-	/* Update default route if next-hop is already resolved */
-	if (is_connected)
-		mvsw_pr_router_gw_nh_refresh(sw, nh);
+	err = mvsw_pr_nexthop4_group_reserve(sw, nh_grp);
+	if (err)
+		goto err_nexthop_group_add;
 
-	n->flags |= NTF_OFFLOADED;
-out:
-	neigh_release(n);
-	return err;
+	err = mvsw_pr_nexthop_group_insert(sw, nh_grp);
+	if (err)
+		goto err_nexthop_group_insert;
+
+	return nh_grp;
+
+err_nexthop_group_insert:
+	mvsw_pr_nexthop4_group_release(sw, nh_grp);
+err_nexthop_group_add:
+err_nexthop4_init:
+	mvsw_pr_nexthops_fini(sw, i, nh_grp);
+	fib_info_put(fi);
+	kfree(nh_grp);
+	return ERR_PTR(err);
 }
 
-static bool mvsw_pr_fib4_allow_replace(struct mvsw_pr_fib4_entry *fib4_entry)
+static bool mvsw_pr_fi_should_offload(struct fib_info *fi)
 {
-	struct mvsw_pr_fib_node *fib_node = fib4_entry->fib_node;
-	struct mvsw_pr_fib4_entry *fib4_replaced = fib_node->fib4_entry;
+	int i, nhs = fib_info_num_path(fi);
+	struct net_device *dev;
+	struct fib_nh *nh;
 
-	if (!fib4_replaced)
-		return true;
+	for (i = 0; i < nhs; i++) {
+		nh = fib_info_nh(fi, i);
+		dev = nh->fib_nh_dev;
 
-	if (fib4_entry->tb_id == RT_TABLE_MAIN &&
-	    fib4_replaced->tb_id == RT_TABLE_LOCAL)
-		return false;
+		if (!mvsw_pr_port_dev_lower_find(dev))
+			return false;
+
+		if (netdev_has_any_upper_dev(dev) &&
+		    (netif_is_bridge_port(dev) || netif_is_lag_port(dev)))
+			return false;
+	}
 
 	return true;
 }
 
+static int mvsw_pr_nexthop4_group_get(struct mvsw_pr_switch *sw,
+				      struct mvsw_pr_fib4_entry *fib4_entry,
+				      struct fib_info *fi)
+{
+	struct mvsw_pr_nexthop_group *nh_grp;
+
+	if (!fib4_entry->should_offload)
+		return 0;
+
+	nh_grp = mvsw_pr_nexthop4_group_lookup(sw, fi);
+	if (!nh_grp) {
+		nh_grp = mvsw_pr_nexthop4_group_create(sw, fi);
+		if (IS_ERR(nh_grp))
+			return PTR_ERR(nh_grp);
+	}
+	list_add_tail(&fib4_entry->nexthop_group_node, &nh_grp->fib_list);
+	fib4_entry->nh_grp = nh_grp;
+	return 0;
+}
+
 static struct mvsw_pr_fib4_entry *
 mvsw_pr_fib4_entry_create(struct mvsw_pr_switch *sw,
 			  struct mvsw_pr_fib_node *fib_node,
 			  struct fib_entry_notifier_info *fen_info)
 {
 	struct mvsw_pr_fib4_entry *fib4_entry;
+	struct fib_info *fi = fen_info->fi;
+	struct mvsw_pr_rif *rif;
+	struct net_device *dev;
+	bool should_offload;
 	struct fib_nh *nh;
+	int err;
 
 	fib4_entry = kzalloc(sizeof(*fib4_entry), GFP_KERNEL);
 	if (!fib4_entry)
 		return ERR_PTR(-ENOMEM);
 
 	nh  = fib_info_nh(fen_info->fi, 0);
-	if (nh->fib_nh_gw4)
-		mvsw_pr_router_resolve_nh(sw, fen_info);
+	dev = nh->fib_nh_dev;
 
-	fib4_entry->nh_dev = nh->fib_nh_dev;
+	if (dev) {
+		rif = mvsw_pr_rif_find(sw, dev);
+		should_offload = mvsw_pr_fi_should_offload(fi) && rif;
+		fib4_entry->vid = mvsw_pr_nh_dev_to_vid(sw, dev);
+		fib4_entry->should_offload = should_offload;
+	} else {
+		fib4_entry->vid = 0;
+		fib4_entry->should_offload = false;
+	}
+
+	fib4_entry->fib_node = fib_node;
+	fib4_entry->fi = fi;
 	fib4_entry->tb_id = fen_info->tb_id;
 	fib4_entry->type = fen_info->type;
-	fib4_entry->fi = fen_info->fi;
 
-	fib4_entry->fib_node = fib_node;
-	fib_info_hold(fib4_entry->fi);
+	err = mvsw_pr_nexthop4_group_get(sw, fib4_entry, fi);
+	if (err)
+		goto err_nexthop4_group_get;
+
+	if (fib4_entry->should_offload)
+		list_add(&fib4_entry->rif_head, &rif->fib_list);
 
 	return fib4_entry;
+
+err_nexthop4_group_get:
+	kfree(fib4_entry);
+	return ERR_PTR(err);
 }
 
-static void mvsw_pr_fib4_entry_destroy(struct mvsw_pr_fib4_entry *fib4_entry)
+static void mvsw_pr_fib4_entry_destroy(struct mvsw_pr_switch *sw,
+				       struct mvsw_pr_fib4_entry *fib4_entry)
 {
-	fib_info_put(fib4_entry->fi);
+	if (fib4_entry->should_offload)
+		list_del(&fib4_entry->rif_head);
+
+	mvsw_pr_nexthop4_group_put(sw, fib4_entry);
 	kfree(fib4_entry);
 }
 
+static void
+mvsw_pr_fib_entry_offload_refresh(struct mvsw_pr_fib4_entry *fib4_entry,
+				  bool is_offloaded)
+{
+	struct fib_nh *nh = fib_info_nh(fib4_entry->fi, 0);
+
+	if (is_offloaded)
+		nh->fib_nh_flags |= RTNH_F_OFFLOAD;
+	else
+		nh->fib_nh_flags &= !RTNH_F_OFFLOAD;
+}
+
 static int mvsw_pr_fib_node_entry_offload(struct mvsw_pr_switch *sw,
 					  struct mvsw_pr_fib4_entry *fib4_entry)
 {
 	struct mvsw_pr_fib_node *fib_node = fib4_entry->fib_node;
-	u16 vid = mvsw_pr_nh_dev_to_vid(sw, fib4_entry->nh_dev);
-	struct fib_nh *nh;
+	struct mvsw_pr_nexthop_group *grp = fib4_entry->nh_grp;
+	u16 vid = fib4_entry->vid;
+	int err;
 
 	fib_node->fib4_entry = fib4_entry;
-	nh = fib_info_nh(fib4_entry->fi, 0);
-	nh->fib_nh_flags |= RTNH_F_OFFLOAD;
+	if (!fib4_entry->should_offload)
+		return 0;
+
+	err = mvsw_pr_lpm_update(sw, fib_node->vr->id, fib_node->key.addr,
+				 fib_node->key.prefix_len, vid, grp->grp_id);
+	if (err)
+		return err;
+
+	err = mvsw_pr_nexthop_adjust_fib(sw, fib4_entry);
+	if (err)
+		return err;
+
+	mvsw_pr_nexthop_group_refresh(sw, grp);
+	mvsw_pr_fib_entry_offload_refresh(fib4_entry, true);
+	return err;
+}
+
+static void mvsw_pr_fib_entry_flush(struct mvsw_pr_switch *sw,
+				    struct mvsw_pr_fib4_entry *fib4_entry)
+{
+	struct mvsw_pr_fib_node *fib_node = fib4_entry->fib_node;
+	struct mvsw_pr_nexthop_group *grp = fib4_entry->nh_grp;
 
-	return mvsw_pr_lpm_update(sw, fib_node->vr->id,
-				  fib_node->dst,
-				  fib_node->dst_len,
-				  vid);
+	if (sw->router->aborted) {
+		mvsw_pr_fib_entry_offload_refresh(fib4_entry, false);
+		return;
+	}
+
+	mvsw_pr_lpm_del(sw, fib_node->vr->id, fib_node->key.addr,
+			fib_node->key.prefix_len, grp->grp_id);
 }
 
 static void
 mvsw_pr_fib_node_entry_release(struct mvsw_pr_switch *sw,
 			       struct mvsw_pr_fib_node *fib_node)
 {
-	struct fib_nh *nh;
-
-	if (mvsw_pr_router_is_defalt_gw_prefix
-		(htonl(fib_node->dst), fib_node->dst_len))
-		mvsw_pr_router_release_default_gw(sw, fib_node->vr->id);
+	struct mvsw_pr_fib4_entry *fib4_entry;
 
-	mvsw_pr_lpm_del(sw, fib_node->vr->id,
-			fib_node->dst, fib_node->dst_len);
-	nh = fib_info_nh(fib_node->fib4_entry->fi, 0);
-	nh->fib_nh_flags &= ~RTNH_F_OFFLOAD;
+	fib4_entry = fib_node->fib4_entry;
 	fib_node->fib4_entry = NULL;
+
+	if (!fib4_entry->should_offload)
+		return;
+
+	mvsw_pr_fib_entry_flush(sw, fib4_entry);
 }
 
 static void mvsw_pr_fib4_replace(struct mvsw_pr_switch *sw,
@@ -1393,21 +1944,13 @@ static void mvsw_pr_fib4_replace(struct mvsw_pr_switch *sw,
 				 struct mvsw_pr_fib4_entry *fib4_entry)
 {
 	struct mvsw_pr_fib4_entry *fib4_replaced = fib_node->fib4_entry;
-	struct mvsw_pr_nh *nh_neigh;
-	struct fib_nh *nh;
 
 	fib_node->fib4_entry = fib4_entry;
 	if (!fib4_replaced)
 		return;
 
-	nh_neigh = mvsw_pr_nh_neigh_find(sw, htonl(fib_node->dst));
-	if (nh_neigh)
-		mvsw_pr_nh_destroy(nh_neigh);
-
-	nh = fib_info_nh(fib4_replaced->fi, 0);
-	nh->fib_nh_flags &= ~RTNH_F_OFFLOAD;
-
-	mvsw_pr_fib4_entry_destroy(fib4_replaced);
+	mvsw_pr_fib_entry_offload_refresh(fib4_replaced, false);
+	mvsw_pr_fib4_entry_destroy(sw, fib4_replaced);
 }
 
 static int
@@ -1436,7 +1979,7 @@ mvsw_pr_router_fib4_replace(struct mvsw_pr_switch *sw,
 	}
 
 	if (!mvsw_pr_fib4_allow_replace(fib4_entry)) {
-		mvsw_pr_fib4_entry_destroy(fib4_entry);
+		mvsw_pr_fib4_entry_destroy(sw, fib4_entry);
 		mvsw_pr_fib_node_put(sw, fib_node);
 		return 0;
 	}
@@ -1487,7 +2030,6 @@ static void mvsw_pr_router_fib4_del(struct mvsw_pr_switch *sw,
 {
 	struct mvsw_pr_fib4_entry *fib4_entry;
 	struct mvsw_pr_fib_node *fib_node;
-	struct mvsw_pr_nh *nh_neigh;
 
 	if (sw->router->aborted)
 		return;
@@ -1497,12 +2039,8 @@ static void mvsw_pr_router_fib4_del(struct mvsw_pr_switch *sw,
 		return;
 	fib_node = fib4_entry->fib_node;
 
-	nh_neigh = mvsw_pr_nh_neigh_find(sw, htonl(fen_info->dst));
-	if (nh_neigh)
-		mvsw_pr_nh_destroy(nh_neigh);
-
 	mvsw_pr_fib_node_entry_release(sw, fib_node);
-	mvsw_pr_fib4_entry_destroy(fib4_entry);
+	mvsw_pr_fib4_entry_destroy(sw, fib4_entry);
 	mvsw_pr_fib_node_put(sw, fib_node);
 }
 
@@ -1513,12 +2051,13 @@ static void mvsw_pr_router_fib4_event_work(struct work_struct *work)
 	struct mvsw_pr_switch *sw = fib_work->sw;
 	int err;
 
+	if (sw->router->aborted)
+		goto out;
+
 	rtnl_lock();
 
 	switch (fib_work->event) {
-	case FIB_EVENT_ENTRY_ADD:
 	case FIB_EVENT_ENTRY_REPLACE:
-	case FIB_EVENT_ENTRY_APPEND:
 		err = mvsw_pr_router_fib4_replace(sw,
 						  &fib_work->fen_info);
 		if (err)
@@ -1532,40 +2071,10 @@ static void mvsw_pr_router_fib4_event_work(struct work_struct *work)
 	}
 
 	rtnl_unlock();
+out:
 	kfree(fib_work);
 }
 
-static void mvsw_pr_router_fib4_event(struct mvsw_pr_fib_event_work *fib_work,
-				      struct fib_notifier_info *info)
-{
-	struct fib_entry_notifier_info *fen_info;
-
-	switch (fib_work->event) {
-	case FIB_EVENT_ENTRY_REPLACE: /* fall through */
-	case FIB_EVENT_ENTRY_DEL:
-		fen_info = container_of(info, struct fib_entry_notifier_info,
-					info);
-		fib_work->fen_info = *fen_info;
-		/* Take reference on fib_info to prevent it from being
-		 * freed while work is queued. Release it afterwards.
-		 */
-		fib_info_hold(fib_work->fen_info.fi);
-		break;
-	}
-}
-
-static bool
-mvsw_pr_fen_info_verify(struct fib_entry_notifier_info *fen_info)
-{
-	struct fib_nh *nh;
-
-	nh = fib_info_nh(fen_info->fi, 0);
-	if (!nh->fib_nh_dev || !mvsw_pr_port_dev_lower_find(nh->fib_nh_dev))
-		return false;
-
-	return true;
-}
-
 /* Called with rcu_read_lock() */
 static int mvsw_pr_router_fib_event(struct notifier_block *nb,
 				    unsigned long event, void *ptr)
@@ -1574,6 +2083,7 @@ static int mvsw_pr_router_fib_event(struct notifier_block *nb,
 	struct fib_notifier_info *info = ptr;
 	struct mvsw_pr_fib_event_work *fib_work;
 	struct mvsw_pr_router *router;
+	struct fib_info *fi;
 
 	if (info->family != AF_INET)
 		return NOTIFY_DONE;
@@ -1583,25 +2093,29 @@ static int mvsw_pr_router_fib_event(struct notifier_block *nb,
 		return NOTIFY_DONE;
 
 	switch (event) {
-	case FIB_EVENT_RULE_ADD:
-	case FIB_EVENT_RULE_DEL:
-		/* TODO: */
-		return notifier_from_errno(0);
-	case FIB_EVENT_ENTRY_ADD:
 	case FIB_EVENT_ENTRY_REPLACE:
-	case FIB_EVENT_ENTRY_APPEND:
-
 		if (!fen_info->fi)
 			return NOTIFY_DONE;
 
-		if (fen_info->fi->nh)
+		fi = fen_info->fi;
+		if (fi->nh)
 			return notifier_from_errno(-EINVAL);
 
-		if (fen_info->fi->fib_nh_is_v6)
+		if (fi->fib_nh_is_v6)
 			return notifier_from_errno(-EINVAL);
 
-		if (!mvsw_pr_fen_info_verify(fen_info))
-			return NOTIFY_DONE;
+		if (fib_info_num_path(fi) > MVSW_PR_NHGR_SIZE_MAX) {
+			NL_SET_ERR_MSG_MOD(info->extack,
+					   "Exceeded number of nexthops per route");
+			return notifier_from_errno(-EINVAL);
+		}
+
+		break;
+	case FIB_EVENT_ENTRY_DEL:
+		/* no validation needed at this point */
+		break;
+	default:
+		return NOTIFY_DONE;
 	}
 
 	fib_work = kzalloc(sizeof(*fib_work), GFP_ATOMIC);
@@ -1609,73 +2123,94 @@ static int mvsw_pr_router_fib_event(struct notifier_block *nb,
 		return NOTIFY_BAD;
 
 	INIT_WORK(&fib_work->work, mvsw_pr_router_fib4_event_work);
+	fib_info_hold(fen_info->fi);
+	fib_work->fen_info = *fen_info;
 	fib_work->event = event;
 	fib_work->sw = router->sw;
 
-	mvsw_pr_router_fib4_event(fib_work, info);
 	queue_work(mvsw_r_owq, &fib_work->work);
 
 	return NOTIFY_DONE;
 }
 
-static void mvsw_pr_router_nh_flush(struct mvsw_pr_router *router)
+static void mvsw_pr_router_fib_flush(struct mvsw_pr_router *router)
 {
-	struct mvsw_pr_nh *nh, *tmp;
+	struct mvsw_pr_switch *sw = router->sw;
+	struct mvsw_pr_fib4_entry *fib4_entry;
+	struct mvsw_pr_fib_node *node;
+	struct mvsw_pr_vr *vr, *vr_tmp;
+	struct rhashtable_iter iter;
+
+	list_for_each_entry_safe(vr, vr_tmp, &router->vr_list, router_node) {
+		rhashtable_walk_enter(&vr->ht, &iter);
+		rhashtable_walk_start(&iter);
+		while ((node = rhashtable_walk_next(&iter))) {
+			if (IS_ERR(node))
+				continue;
 
-	list_for_each_entry_safe(nh, tmp, &router->nexthop_list, nh_node) {
-		if (!nh->gw_ip) {
-			mvsw_pr_nh_entry_delete(router->sw, nh->vr_id,
-						nh->n_ip, nh->dst_len, nh->ha);
-			list_del(&nh->nh_node);
+			fib4_entry = node->fib4_entry;
+			mvsw_pr_fib_node_entry_release(sw, node);
+			mvsw_pr_fib4_entry_destroy(sw, fib4_entry);
+			mvsw_pr_fib_node_put(sw, node);
 		}
-
-		nh->n->flags &= ~NTF_OFFLOADED;
+		rhashtable_walk_stop(&iter);
+		rhashtable_walk_exit(&iter);
 	}
 }
 
-static void mvsw_pr_router_lpm_flush(struct mvsw_pr_router *router)
+static void mvsw_pr_router_neigh_flush(struct mvsw_pr_router *router)
 {
-	struct mvsw_pr_fib_node *node, *tmp;
-	struct mvsw_pr_fib4_entry *fib4_entry;
-	struct mvsw_pr_vr *vr;
+	struct mvsw_pr_neigh_entry *neigh_entry;
+	struct mvsw_pr_switch *sw = router->sw;
+	struct rhashtable_iter iter;
 
-	list_for_each_entry(vr, &router->vr_list, router_node) {
-		list_for_each_entry_safe(node, tmp, &vr->fib_list, fib_node) {
-			fib4_entry = node->fib4_entry;
-			mvsw_pr_fib_node_entry_release(router->sw, node);
-			mvsw_pr_fib4_entry_destroy(fib4_entry);
-			mvsw_pr_fib_node_put(router->sw, node);
-		}
+	rhashtable_walk_enter(&router->neigh_ht, &iter);
+	rhashtable_walk_start(&iter);
+	while ((neigh_entry = rhashtable_walk_next(&iter))) {
+		if (IS_ERR(neigh_entry))
+			continue;
+
+		mvsw_pr_neigh_entry_flush(sw, neigh_entry);
 	}
+	rhashtable_walk_stop(&iter);
+	rhashtable_walk_exit(&iter);
 }
 
-static void __mvsw_pr_router_fib_flush(struct mvsw_pr_router *router)
+static void mvsw_pr_router_fib_dump_flush(struct notifier_block *nb)
 {
+	struct mvsw_pr_router *router;
+
 	/* Flush pending FIB notifications and then flush the device's
 	 * table before requesting another dump. The FIB notification
 	 * block is unregistered, so no need to take RTNL.
+	 * No neighbours are expected to be present since FIBs  are not
+	 * registered yet
 	 */
-	mvsw_pr_router_nh_flush(router);
-	mvsw_pr_router_lpm_flush(router);
-}
-
-static void mvsw_pr_router_fib_dump_flush(struct notifier_block *nb)
-{
-	struct mvsw_pr_router *router;
-
 	router = container_of(nb, struct mvsw_pr_router, fib_nb);
-	__mvsw_pr_router_fib_flush(router);
+	flush_workqueue(mvsw_r_owq);
+	flush_workqueue(mvsw_r_wq);
+	mvsw_pr_router_fib_flush(router);
 }
 
 static void mvsw_pr_router_fib_abort(struct mvsw_pr_switch *sw)
 {
 	struct mvsw_pr_router *router = sw->router;
+	struct mvsw_pr_vr *vr, *vr_tmp;
 
 	if (router->aborted)
 		return;
 
+	/* RTNL is already taken, therefore other events won`t
+	 * modify structures that are being flushed
+	 */
 	router->aborted = true;
-	__mvsw_pr_router_fib_flush(router);
+
+	list_for_each_entry_safe(vr, vr_tmp, &router->vr_list, router_node) {
+		mvsw_pr_hw_vr_abort(sw, vr->id);
+	}
+
+	mvsw_pr_router_neigh_flush(router);
+	mvsw_pr_router_fib_flush(router);
 	dev_err(sw->dev->dev, "Abort. HW routing offloading disabled");
 }
 
@@ -1745,7 +2280,9 @@ static int mvsw_pr_port_vrf_join(struct mvsw_pr_switch *sw,
 	if (rif)
 		__mvsw_pr_inetaddr_event(sw, dev, NETDEV_DOWN, extack);
 
-	return __mvsw_pr_inetaddr_event(sw, dev, NETDEV_UP, extack);
+	__mvsw_pr_inetaddr_event(sw, dev, NETDEV_UP, extack);
+	rif = mvsw_pr_rif_find(sw, dev);
+	return mvsw_pr_rif_vr_update(sw, rif, extack);
 }
 
 static void mvsw_pr_port_vrf_leave(struct mvsw_pr_switch *sw,
@@ -1758,6 +2295,8 @@ static void mvsw_pr_port_vrf_leave(struct mvsw_pr_switch *sw,
 		return;
 
 	__mvsw_pr_inetaddr_event(sw, dev, NETDEV_DOWN, NULL);
+	/* Restore rif in the default vrf */
+	__mvsw_pr_inetaddr_event(sw, dev, NETDEV_UP, NULL);
 }
 
 int mvsw_pr_netdevice_vrf_event(struct net_device *dev, unsigned long event,
@@ -1808,6 +2347,21 @@ static int mvsw_pr_rif_macvlan_flush(struct mvsw_pr_rif *rif)
 					     __mvsw_pr_rif_macvlan_flush, rif);
 }
 
+#ifdef CONFIG_IP_ROUTE_MULTIPATH
+static int mvsw_pr_mp_hash_init(struct mvsw_pr_switch *sw)
+{
+	u8 hash_policy;
+
+	hash_policy = init_net.ipv4.sysctl_fib_multipath_hash_policy;
+	return  mvsw_pr_mp4_hash_set(sw, hash_policy);
+}
+#else
+static int mvsw_pr_mp_hash_init(struct mvsw_pr_switch *sw)
+{
+	return 0;
+}
+#endif
+
 static struct notifier_block mvsw_pr_inetaddr_valid_nb __read_mostly = {
 	.notifier_call = mvsw_pr_inetaddr_valid_event,
 };
@@ -1823,6 +2377,15 @@ int mvsw_pr_router_init(struct mvsw_pr_switch *sw)
 	sw->router = router;
 	router->sw = sw;
 
+	err = mvsw_pr_mp_hash_init(sw);
+	if (err)
+		goto err_mp_hash_init;
+
+	err = rhashtable_init(&router->nexthop_group_ht,
+			      &mvsw_pr_nexthop_group_ht_params);
+	if (err)
+		goto err_nexthop_grp_ht_init;
+
 	err = register_inetaddr_validator_notifier(&mvsw_pr_inetaddr_valid_nb);
 	if (err)
 		goto err_register_inetaddr_validator_notifier;
@@ -1835,7 +2398,6 @@ int mvsw_pr_router_init(struct mvsw_pr_switch *sw)
 	INIT_LIST_HEAD(&sw->router->rif_list);
 	INIT_LIST_HEAD(&sw->router->vr_list);
 	INIT_LIST_HEAD(&sw->router->nexthop_neighs_list);
-	INIT_LIST_HEAD(&sw->router->nexthop_list);
 
 	mvsw_r_wq = alloc_workqueue(mvsw_driver_name, 0, 0);
 	if (!mvsw_r_wq) {
@@ -1879,6 +2441,10 @@ err_alloc_workqueue:
 err_register_inetaddr_notifier:
 	unregister_inetaddr_validator_notifier(&mvsw_pr_inetaddr_valid_nb);
 err_register_inetaddr_validator_notifier:
+	rhashtable_destroy(&router->nexthop_group_ht);
+err_nexthop_grp_ht_init:
+	rhashtable_destroy(&router->neigh_ht);
+err_mp_hash_init:
 	kfree(sw->router);
 	return err;
 }
@@ -1895,8 +2461,10 @@ static void mvsw_pr_rifs_fini(struct mvsw_pr_switch *sw)
 {
 	struct mvsw_pr_rif *rif, *tmp;
 
-	list_for_each_entry_safe(rif, tmp, &sw->router->rif_list, router_node)
+	list_for_each_entry_safe(rif, tmp, &sw->router->rif_list, router_node) {
+		rif->is_active = false;
 		mvsw_pr_rif_destroy(rif);
+	}
 }
 
 void mvsw_pr_router_fini(struct mvsw_pr_switch *sw)
@@ -1941,19 +2509,6 @@ static struct mvsw_pr_vr *mvsw_pr_vr_find(struct mvsw_pr_switch *sw, u32 tb_id)
 	return NULL;
 }
 
-static struct mvsw_pr_vr *mvsw_pr_vr_find_by_id(struct mvsw_pr_switch *sw,
-						u16 vr_id)
-{
-	struct mvsw_pr_vr *vr;
-
-	list_for_each_entry(vr, &sw->router->vr_list, router_node) {
-		if (vr->id == vr_id)
-			return vr;
-	}
-
-	return NULL;
-}
-
 static struct mvsw_pr_vr *mvsw_pr_vr_create(struct mvsw_pr_switch *sw,
 					    u32 tb_id,
 					    struct netlink_ext_ack *extack)
@@ -1967,21 +2522,33 @@ static struct mvsw_pr_vr *mvsw_pr_vr_create(struct mvsw_pr_switch *sw,
 		return ERR_PTR(-ENOMEM);
 
 	vr = kzalloc(sizeof(*vr), GFP_KERNEL);
-	if (!vr)
-		return ERR_PTR(-ENOMEM);
+	if (!vr) {
+		err = -ENOMEM;
+		goto err_alloc_vr;
+	}
+
+	err = rhashtable_init(&vr->ht, &mvsw_pr_fib_ht_params);
+	if (err)
+		goto err_rhashtable_init;
 
 	vr->tb_id = tb_id;
 	vr->id = vr_id;
 
 	list_add(&vr->router_node, &sw->router->vr_list);
-	INIT_LIST_HEAD(&vr->fib_list);
 
 	return vr;
+
+err_rhashtable_init:
+	kfree(vr);
+err_alloc_vr:
+	mvsw_pr_hw_vr_delete(sw, vr_id);
+	return ERR_PTR(err);
 }
 
 static void mvsw_pr_vr_destroy(struct mvsw_pr_switch *sw, struct mvsw_pr_vr *vr)
 {
 	mvsw_pr_hw_vr_delete(sw, vr->id);
+	rhashtable_destroy(&vr->ht);
 	list_del(&vr->router_node);
 	kfree(vr);
 }
@@ -2022,6 +2589,7 @@ mvsw_pr_rif_alloc(struct mvsw_pr_switch *sw,
 	}
 
 	rif->sw = sw;
+	rif->vr = vr;
 	rif->dev = params->dev;
 	err = mvsw_pr_rif_iface_init(rif);
 	if (err)
@@ -2029,7 +2597,6 @@ mvsw_pr_rif_alloc(struct mvsw_pr_switch *sw,
 
 	ether_addr_copy(rif->addr, params->dev->dev_addr);
 	rif->mtu = params->dev->mtu;
-	rif->vr_id = vr->id;
 
 	return rif;
 
@@ -2041,9 +2608,8 @@ err_rif_alloc:
 
 static int mvsw_pr_rif_offload(struct mvsw_pr_rif *rif)
 {
-	return mvsw_pr_hw_rif_create(rif->sw, &rif->iface,
-					rif->vr_id, rif->addr,
-					&rif->rif_id);
+	return mvsw_pr_hw_rif_create(rif->sw, &rif->iface, rif->addr,
+				     &rif->rif_id);
 }
 
 static struct mvsw_pr_rif *mvsw_pr_rif_create(struct mvsw_pr_switch *sw,
@@ -2074,11 +2640,12 @@ static struct mvsw_pr_rif *mvsw_pr_rif_create(struct mvsw_pr_switch *sw,
 	}
 
 	vr->ref_cnt++;
-	// dev_hold(rif->dev);
+	dev_hold(rif->dev);
+	INIT_LIST_HEAD(&rif->neigh_list);
+	INIT_LIST_HEAD(&rif->fib_list);
+	INIT_LIST_HEAD(&rif->nexthop_list);
 	list_add(&rif->router_node, &sw->router->rif_list);
 
-	/* TODO: rif->configure() ??? */
-
 	return rif;
 
 err_rif_offload:
@@ -2088,26 +2655,29 @@ err_rif_offload:
 
 static int mvsw_pr_rif_delete(struct mvsw_pr_rif *rif)
 {
-	return mvsw_pr_hw_rif_delete(rif->sw, rif->rif_id, &rif->iface,
-				     rif->vr_id);
+	return mvsw_pr_hw_rif_delete(rif->sw, rif->rif_id, &rif->iface);
 }
 
 static void mvsw_pr_rif_destroy(struct mvsw_pr_rif *rif)
 {
-	struct mvsw_pr_vr *vr;
+	mvsw_pr_rif_macvlan_flush(rif);
+	if (!rif->is_active) {
+		mvsw_pr_rif_delete(rif);
+		list_del(&rif->router_node);
+		dev_put(rif->dev);
+		rif->vr->ref_cnt--;
+		mvsw_pr_vr_put(rif->sw, rif->vr);
+		kfree(rif);
+	}
+}
 
-	MVSW_LOG_ERROR("dev=%s", rif->dev->name);
+static void mvsw_pr_rif_put(struct mvsw_pr_rif *rif)
+{
+	if (!list_empty(&rif->nexthop_list) ||
+	    !list_empty(&rif->neigh_list))
+		return;
 
-	vr = mvsw_pr_vr_find_by_id(rif->sw, rif->vr_id);
-	/* TODO: rif->deconfigure() ??? */
-	mvsw_pr_rif_fdb_op(rif, rif->dev->dev_addr, false);
-	mvsw_pr_rif_macvlan_flush(rif);
-	mvsw_pr_rif_delete(rif);
-	list_del(&rif->router_node);
-	//dev_put(rif->dev);
-	kfree(rif);
-	vr->ref_cnt--;
-	mvsw_pr_vr_put(rif->sw, vr);
+	mvsw_pr_rif_destroy(rif);
 }
 
 void mvsw_pr_rif_enable(struct mvsw_pr_switch *sw,
@@ -2129,3 +2699,50 @@ static int mvsw_pr_rif_update(struct mvsw_pr_rif *rif, char *mac)
 {
 	return mvsw_pr_hw_rif_set(rif->sw, &rif->rif_id, &rif->iface, mac);
 }
+
+static int mvsw_pr_rif_vr_update(struct mvsw_pr_switch *sw,
+				 struct mvsw_pr_rif *rif,
+				 struct netlink_ext_ack *extack)
+{
+	u32 tb_id = l3mdev_fib_table(rif->dev);
+	struct mvsw_pr_vr *vr;
+
+	rif->vr->ref_cnt--;
+	mvsw_pr_vr_put(sw, rif->vr);
+	mvsw_pr_rif_delete(rif);
+	vr = mvsw_pr_vr_get(sw, tb_id ? : RT_TABLE_MAIN, extack);
+	if (IS_ERR(vr))
+		return PTR_ERR(vr);
+	rif->vr = vr;
+	mvsw_pr_rif_iface_init(rif);
+	mvsw_pr_rif_offload(rif);
+	rif->vr->ref_cnt++;
+
+	return 0;
+}
+
+void mvsw_pr_router_lag_member_leave(const struct mvsw_pr_port *port,
+				     const struct net_device *dev)
+{
+	struct mvsw_pr_rif *rif;
+	u16 vr_id;
+
+	rif = mvsw_pr_rif_find(port->sw, dev);
+	if (!rif)
+		return;
+
+	vr_id = mvsw_pr_rif_vr_id(rif);
+	mvsw_pr_lag_member_rif_leave(port, port->lag_id, vr_id);
+}
+
+void mvsw_pr_lag_router_leave(struct mvsw_pr_switch *sw,
+			      struct net_device *lag_dev)
+{
+	struct mvsw_pr_rif *rif;
+
+	rif = mvsw_pr_rif_find(sw, lag_dev);
+	if (rif) {
+		rif->is_active = false;
+		mvsw_pr_rif_flush(rif);
+	}
+}
diff --git a/drivers/net/ethernet/marvell/prestera_sw/prestera_rxtx.c b/drivers/net/ethernet/marvell/prestera_sw/prestera_rxtx.c
index fddf8fc..89f69c5 100644
--- a/drivers/net/ethernet/marvell/prestera_sw/prestera_rxtx.c
+++ b/drivers/net/ethernet/marvell/prestera_sw/prestera_rxtx.c
@@ -20,6 +20,8 @@ enum mvsw_pr_rxtx_type {
 
 static struct mvsw_pr_rxtx *rxtx_registered;
 
+static u64 *cpu_code_stats;
+
 netdev_tx_t mvsw_pr_rxtx_xmit(struct sk_buff *skb,
 			      struct mvsw_pr_rxtx_info *info)
 {
@@ -123,6 +125,8 @@ int mvsw_pr_rxtx_recv_skb(struct mvsw_pr_rxtx *rxtx, struct sk_buff *skb)
 		__vlan_hwaccel_put_tag(skb, htons(ETH_P_8021Q), tci);
 	}
 
+	++cpu_code_stats[dsa.dsa_info.to_cpu.cpu_code];
+
 	return 0;
 }
 
@@ -155,6 +159,12 @@ static int mvsw_pr_rxtx_probe(struct platform_device *pdev)
 	struct mvsw_pr_rxtx *rxtx;
 	int err;
 
+	cpu_code_stats = kzalloc(sizeof(u64) * MVSW_PR_RXTX_CPU_CODE_MAX_NUM,
+				 GFP_KERNEL);
+
+	if (!cpu_code_stats)
+		return -ENOMEM;
+
 	rxtx_type = (enum mvsw_pr_rxtx_type)of_device_get_match_data(dev);
 
 	rxtx = devm_kzalloc(dev, sizeof(*rxtx), GFP_KERNEL);
@@ -188,6 +198,8 @@ static int mvsw_pr_rxtx_remove(struct platform_device *pdev)
 	if (rxtx->ops->rxtx_fini)
 		err = rxtx->ops->rxtx_fini(rxtx);
 
+	kfree(cpu_code_stats);
+
 	rxtx_registered = NULL;
 	return err;
 }
@@ -253,3 +265,8 @@ void mvsw_pr_rxtx_switch_fini(struct mvsw_pr_switch *sw)
 
 	return rxtx_registered->ops->rxtx_switch_fini(rxtx_registered, sw);
 }
+
+u64 mvsw_pr_rxtx_get_cpu_code_stats(u8 cpu_code)
+{
+	return cpu_code_stats[cpu_code];
+}
diff --git a/drivers/net/ethernet/marvell/prestera_sw/prestera_rxtx.h b/drivers/net/ethernet/marvell/prestera_sw/prestera_rxtx.h
index d541ca7..a105225 100644
--- a/drivers/net/ethernet/marvell/prestera_sw/prestera_rxtx.h
+++ b/drivers/net/ethernet/marvell/prestera_sw/prestera_rxtx.h
@@ -9,6 +9,8 @@
 
 #include <linux/netdevice.h>
 
+#define MVSW_PR_RXTX_CPU_CODE_MAX_NUM	256
+
 struct mvsw_pr_switch;
 
 struct mvsw_pr_rxtx_info {
@@ -25,4 +27,6 @@ void mvsw_pr_rxtx_switch_fini(struct mvsw_pr_switch *sw);
 netdev_tx_t mvsw_pr_rxtx_xmit(struct sk_buff *skb,
 			      struct mvsw_pr_rxtx_info *info);
 
+u64 mvsw_pr_rxtx_get_cpu_code_stats(u8 cpu_code);
+
 #endif /* _MVSW_PRESTERA_RXTX_H_ */
diff --git a/drivers/net/ethernet/marvell/prestera_sw/prestera_rxtx_sdma.c b/drivers/net/ethernet/marvell/prestera_sw/prestera_rxtx_sdma.c
index 77dbeb7..9a65f3c 100644
--- a/drivers/net/ethernet/marvell/prestera_sw/prestera_rxtx_sdma.c
+++ b/drivers/net/ethernet/marvell/prestera_sw/prestera_rxtx_sdma.c
@@ -76,6 +76,8 @@ struct mvsw_sdma_buf {
 struct mvsw_sdma_rx_ring {
 	struct mvsw_sdma_buf *bufs;
 	int next_rx;
+	int weight;
+	int recvd;
 };
 
 struct mvsw_sdma_tx_ring {
@@ -93,6 +95,7 @@ struct mvsw_pr_rxtx_sdma {
 	struct mvsw_pr_rxtx *rxtx;
 	struct work_struct tx_work;
 	struct napi_struct rx_napi;
+	int next_rxq;
 	struct net_device napi_dev;
 	/* protect SDMA with concurrrent access from multiple CPUs */
 	spinlock_t tx_lock;
@@ -100,6 +103,10 @@ struct mvsw_pr_rxtx_sdma {
 	u64 dma_mask;
 };
 
+static int prestera_rx_weight_map[SDMA_RX_QUEUE_NUM] = {
+	1, 2, 2, 2, 2, 4, 4, 8
+};
+
 static int mvsw_sdma_buf_desc_alloc(struct mvsw_pr_rxtx_sdma *sdma,
 				    struct mvsw_sdma_buf *buf)
 {
@@ -209,6 +216,23 @@ static struct sk_buff *mvsw_sdma_rx_buf_get(struct mvsw_pr_rxtx_sdma *sdma,
 	return skb_orig;
 }
 
+static void mvsw_sdma_rx_set_next_queue(struct mvsw_pr_rxtx_sdma *sdma, int rxq)
+{
+	sdma->next_rxq = rxq % SDMA_RX_QUEUE_NUM;
+}
+
+static int mvsw_sdma_rx_pick_next_queue(struct mvsw_pr_rxtx_sdma *sdma)
+{
+	struct mvsw_sdma_rx_ring *ring = &sdma->rx_ring[sdma->next_rxq];
+
+	if (ring->recvd >= ring->weight) {
+		mvsw_sdma_rx_set_next_queue(sdma, sdma->next_rxq + 1);
+		ring->recvd = 0;
+	}
+
+	return sdma->next_rxq;
+}
+
 static int mvsw_sdma_rx_poll(struct napi_struct *napi, int budget)
 {
 	unsigned int qmask = GENMASK(SDMA_RX_QUEUE_NUM - 1, 0);
@@ -216,46 +240,50 @@ static int mvsw_sdma_rx_poll(struct napi_struct *napi, int budget)
 	unsigned int rxq_done_map = 0;
 	struct list_head rx_list;
 	int pkts_done = 0;
-	int q;
 
 	INIT_LIST_HEAD(&rx_list);
 
 	sdma = container_of(napi, struct mvsw_pr_rxtx_sdma, rx_napi);
 
 	while (pkts_done < budget && rxq_done_map != qmask) {
-		for (q = 0; q < SDMA_RX_QUEUE_NUM && pkts_done < budget; q++) {
-			struct mvsw_sdma_rx_ring *ring = &sdma->rx_ring[q];
-			int buf_idx = ring->next_rx;
-			struct mvsw_sdma_desc *desc;
-			struct mvsw_sdma_buf *buf;
-			struct sk_buff *skb;
-
-			buf = &ring->bufs[buf_idx];
-			desc = buf->desc;
-
-			if (SDMA_RX_DESC_OWNER(desc) != SDMA_RX_DESC_CPU_OWN) {
-				rxq_done_map |= BIT(q);
-				continue;
-			} else {
-				rxq_done_map &= ~BIT(q);
-			}
+		struct mvsw_sdma_rx_ring *ring;
+		struct mvsw_sdma_desc *desc;
+		struct mvsw_sdma_buf *buf;
+		struct sk_buff *skb;
+		int buf_idx;
+		int rxq;
+
+		rxq = mvsw_sdma_rx_pick_next_queue(sdma);
+		ring = &sdma->rx_ring[rxq];
+
+		buf_idx = ring->next_rx;
+		buf = &ring->bufs[buf_idx];
+		desc = buf->desc;
+
+		if (SDMA_RX_DESC_OWNER(desc) != SDMA_RX_DESC_CPU_OWN) {
+			mvsw_sdma_rx_set_next_queue(sdma, rxq + 1);
+			rxq_done_map |= BIT(rxq);
+			continue;
+		} else {
+			rxq_done_map &= ~BIT(rxq);
+		}
 
-			pkts_done++;
+		ring->recvd++;
+		pkts_done++;
 
-			__skb_trim(buf->skb, SDMA_RX_DESC_PKT_LEN(desc));
+		__skb_trim(buf->skb, SDMA_RX_DESC_PKT_LEN(desc));
 
-			skb = mvsw_sdma_rx_buf_get(sdma, buf);
-			if (!skb)
-				goto rx_reset_buf;
+		skb = mvsw_sdma_rx_buf_get(sdma, buf);
+		if (!skb)
+			goto rx_reset_buf;
 
-			if (unlikely(mvsw_pr_rxtx_recv_skb(sdma->rxtx, skb)))
-				goto rx_reset_buf;
+		if (unlikely(mvsw_pr_rxtx_recv_skb(sdma->rxtx, skb)))
+			goto rx_reset_buf;
 
-			list_add_tail(&skb->list, &rx_list);
+		list_add_tail(&skb->list, &rx_list);
 rx_reset_buf:
-			mvsw_sdma_rx_desc_init(sdma, buf->desc, buf->buf_dma);
-			ring->next_rx = (buf_idx + 1) % SDMA_RX_DESC_PER_Q;
-		}
+		mvsw_sdma_rx_desc_init(sdma, buf->desc, buf->buf_dma);
+		ring->next_rx = (buf_idx + 1) % SDMA_RX_DESC_PER_Q;
 	}
 
 	if (pkts_done < budget && napi_complete_done(napi, pkts_done))
@@ -315,9 +343,12 @@ static int mvsw_sdma_rx_init(struct mvsw_pr_rxtx_sdma *sdma)
 		if (!ring->bufs)
 			return -ENOMEM;
 
-		head = &ring->bufs[0];
+		ring->weight = prestera_rx_weight_map[q];
+		ring->recvd = 0;
 		ring->next_rx = 0;
 
+		head = &ring->bufs[0];
+
 		for (b = 0; b < SDMA_RX_DESC_PER_Q; b++) {
 			struct mvsw_sdma_buf *buf = &ring->bufs[b];
 
diff --git a/drivers/net/ethernet/marvell/prestera_sw/prestera_switchdev.c b/drivers/net/ethernet/marvell/prestera_sw/prestera_switchdev.c
index 7c727e7..d73a254 100644
--- a/drivers/net/ethernet/marvell/prestera_sw/prestera_switchdev.c
+++ b/drivers/net/ethernet/marvell/prestera_sw/prestera_switchdev.c
@@ -136,7 +136,7 @@ u16 mvsw_pr_vlan_dev_vlan_id(struct mvsw_pr_bridge *bridge,
 
 	bridge_dev = mvsw_pr_bridge_device_find(bridge, dev);
 
-	return bridge_dev->bridge_id;
+	return bridge_dev ? bridge_dev->bridge_id : 0;
 }
 
 static struct mvsw_pr_bridge_vlan *
@@ -510,7 +510,7 @@ static int mvsw_pr_port_attr_br_ageing_set(struct mvsw_pr_port *port,
 	int err;
 	struct mvsw_pr_switch *sw = port->sw;
 	unsigned long ageing_jiffies = clock_t_to_jiffies(ageing_clock_t);
-	u32 ageing_time = jiffies_to_msecs(ageing_jiffies) / 1000;
+	u32 ageing_time = jiffies_to_msecs(ageing_jiffies);
 
 	if (switchdev_trans_ph_prepare(trans)) {
 		if (ageing_time < MVSW_PR_MIN_AGEING_TIME ||
@@ -751,8 +751,6 @@ static int mvsw_pr_switchdev_event(struct notifier_block *unused,
 	switch (event) {
 	case SWITCHDEV_FDB_ADD_TO_DEVICE:
 	case SWITCHDEV_FDB_DEL_TO_DEVICE:
-	case SWITCHDEV_FDB_ADD_TO_BRIDGE:
-	case SWITCHDEV_FDB_DEL_TO_BRIDGE:
 		fdb_info = container_of(info,
 					struct switchdev_notifier_fdb_info,
 					info);
@@ -768,6 +766,8 @@ static int mvsw_pr_switchdev_event(struct notifier_block *unused,
 		dev_hold(net_dev);
 
 		break;
+	case SWITCHDEV_FDB_ADD_TO_BRIDGE:
+	case SWITCHDEV_FDB_DEL_TO_BRIDGE:
 	case SWITCHDEV_VXLAN_FDB_ADD_TO_DEVICE:
 	case SWITCHDEV_VXLAN_FDB_DEL_TO_DEVICE:
 	default:
@@ -1181,6 +1181,8 @@ static int mvsw_pr_port_lag_join(struct mvsw_pr_port *port,
 static void mvsw_pr_port_lag_leave(struct mvsw_pr_port *port,
 				   struct net_device *lag_dev)
 {
+	mvsw_pr_router_lag_member_leave(port, lag_dev);
+
 	if (mvsw_pr_lag_member_del(port))
 		return;
 
