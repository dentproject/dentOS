diff --git a/drivers/net/ethernet/marvell/prestera_sw/Makefile b/drivers/net/ethernet/marvell/prestera_sw/Makefile
index ff2d290..e605b0b 100644
--- a/drivers/net/ethernet/marvell/prestera_sw/Makefile
+++ b/drivers/net/ethernet/marvell/prestera_sw/Makefile
@@ -3,6 +3,8 @@
 # Makefile for the Marvell Switch driver.
 #
 
+ccflags-$(CONFIG_MRVL_PRESTERA_USE_INTR_DRIVER)    += -DMRVL_PRESTERA_USE_INTR_DRIVER
+
 obj-$(CONFIG_MRVL_PRESTERA_SW) += prestera_sw.o
 prestera_sw-objs := prestera.o \
 	prestera_hw.o prestera_switchdev.o prestera_fw_log.o \
diff --git a/drivers/net/ethernet/marvell/prestera_sw/netlink.c b/drivers/net/ethernet/marvell/prestera_sw/netlink.c
index 36fda4d..9f8ca24 100644
--- a/drivers/net/ethernet/marvell/prestera_sw/netlink.c
+++ b/drivers/net/ethernet/marvell/prestera_sw/netlink.c
@@ -10,11 +10,25 @@
 #include <net/genetlink.h>
 #include <linux/completion.h>
 #include <linux/types.h>
+#include <linux/pci.h>
+#include <linux/cdev.h>
 
 #include "prestera.h"
 #include "prestera_hw.h"
 #include "prestera_log.h"
 
+#ifdef MVSW_NL_CHRDEV
+#define MVSW_IPC_IOC		'I'
+#define MVSW_IPC_RX_PKT_IOC	_IO(MVSW_IPC_IOC, 0)
+
+#define DRV_NAME	"mvsw_ipc"
+
+static struct class *mvsw_class;
+static struct device *mvsw_device;
+static struct cdev mvsw_cdev;
+static dev_t mvsw_dev;
+#endif /* MVSW_NL_CHRDEV */
+
 /* Protects send_sync function */
 static DEFINE_MUTEX(mvsw_nl_send_mtx);
 static DECLARE_COMPLETION(mvsw_nl_receive_cmpl);
@@ -50,16 +64,31 @@ struct mvsw_nl_event_t {
 	char data[];
 };
 
-static struct work_struct nl_bus_work;
+static struct work_struct mvsw_nl_dev_register_work;
 
-/* TODO: change to use list in case of supporting more than 1 switch device */
-static struct mvsw_pr_device *dev;
 static struct workqueue_struct *mvsw_nl_owq;
+static struct mvsw_pr_device *sw_dev;
+
+#define pp_reg_write(dev, reg, val) \
+	writel(val, (dev)->pp_regs + (reg))
+#define pp_reg_read(dev, reg) \
+	readl((dev)->pp_regs + (reg))
+
+#define MVSW_INTR_CAUSE_SDMA	0x280C
 
 static int _reply_errno;
 static int _reply_msg_size;
 static u8 _reply_msg[MVSW_MSG_MAX_SIZE];
 
+struct mvsw_nl_device {
+	struct mvsw_pr_device *pr_dev;
+	struct list_head head;
+	bool is_registered;
+};
+
+static DEFINE_MUTEX(mvsw_nl_devices_mtx);
+static LIST_HEAD(mvsw_nl_devices);
+
 static const struct genl_multicast_group mvsw_genl_mcgrps[] = {
 	[MVSW_NL_MCGRP_ID] = {.name = MVSW_NL_MCGRP_NAME}
 };
@@ -69,10 +98,54 @@ static const struct nla_policy mvsw_nl_policy[__MVSW_NL_ATTR_MAX] = {
 	[MVSW_NL_ATTR_TLV] = {.type = NLA_UNSPEC},
 };
 
+#ifndef MRVL_PRESTERA_USE_INTR_DRIVER
+static int
+prestera_intr_handler_register(void (*fn)(unsigned int irq, void *arg),
+			       void *arg)
+{
+	return 0;
+}
+
+static
+void prestera_intr_handler_unregister(void (*fn)(unsigned int irq, void *arg),
+				      void *arg)
+{
+}
+#else
+/* these are from CPSS intDriver.c */
+extern
+int prestera_intr_handler_register(void (*fn)(unsigned int irq, void *arg),
+				   void *arg);
+extern
+void prestera_intr_handler_unregister(void (*fn)(unsigned int irq, void *arg),
+				      void *arg);
+#endif
+
 static int mvsw_nl_handle_reply(struct sk_buff *skb, struct genl_info *info);
 static int mvsw_nl_handle_ready(struct sk_buff *skb, struct genl_info *info);
 static int mvsw_nl_handle_event(struct sk_buff *skb, struct genl_info *info);
 
+static int mvsw_nl_send_req(struct mvsw_pr_device *dev,
+			    u8 *in_msg, size_t in_size,
+			    u8 *out_msg, size_t out_size,
+			    unsigned int wait);
+
+static int mvsw_nl_pci_probe(struct pci_dev *pdev,
+			     const struct pci_device_id *id);
+static void mvsw_nl_pci_remove(struct pci_dev *pdev);
+
+static const struct pci_device_id mvsw_nl_pci_devices[] = {
+	{ PCI_DEVICE(PCI_VENDOR_ID_MARVELL, 0xC804) },
+	{ }
+};
+
+static struct pci_driver mvsw_nl_pci_driver = {
+	.name     = MVSW_NL_NAME,
+	.id_table = mvsw_nl_pci_devices,
+	.probe    = mvsw_nl_pci_probe,
+	.remove   = mvsw_nl_pci_remove,
+};
+
 static const struct genl_ops mvsw_genl_ops[] = {
 	{
 	 .cmd = MVSW_NL_TYPE_REPLY,
@@ -101,6 +174,138 @@ static struct genl_family mvsw_genl_family = {
 	.policy = mvsw_nl_policy,
 };
 
+static void mvsw_nl_do_intr(unsigned int irq, void *arg)
+{
+	u32 err_mask = GENMASK(18, 11);
+	u32 rx_mask = GENMASK(9, 2);
+	u32 rx_cause;
+
+	if (!READ_ONCE(sw_dev))
+		return;
+
+	if (likely(sw_dev->recv_pkt)) {
+		rx_cause = pp_reg_read(sw_dev, MVSW_INTR_CAUSE_SDMA);
+
+		if (rx_cause & (rx_mask | err_mask))
+			sw_dev->recv_pkt(sw_dev);
+	}
+}
+
+static int mvsw_nl_pci_probe(struct pci_dev *pdev,
+			     const struct pci_device_id *id)
+{
+	struct mvsw_nl_device *nl_dev;
+	struct mvsw_pr_device *pr_dev;
+	u8 __iomem *pp_addr;
+	int err;
+
+	pr_dev = kzalloc(sizeof(*pr_dev), GFP_KERNEL);
+	if (!pr_dev)
+		return -ENOMEM;
+
+	err = pci_enable_device(pdev);
+	if (err) {
+		dev_err(&pdev->dev, "pci_enable_device failed\n");
+		goto err_pci_enable_device;
+	}
+
+	err = pci_request_regions(pdev, pdev->driver->name);
+	if (err) {
+		dev_err(&pdev->dev, "pci_request_regions failed\n");
+		goto err_pci_request_regions;
+	}
+
+	if (dma_set_mask_and_coherent(&pdev->dev, DMA_BIT_MASK(32))) {
+		dev_err(&pdev->dev, "fail to set DMA mask\n");
+		goto err_dma_mask;
+	}
+	pp_addr = ioremap(pci_resource_start(pdev, 2),
+			  pci_resource_len(pdev, 2));
+	if (!pp_addr) {
+		dev_err(&pdev->dev, "pp regs ioremap failed\n");
+		err = -EIO;
+		goto err_pp_ioremap;
+	}
+
+	pr_dev->send_req = mvsw_nl_send_req;
+	pr_dev->pp_regs = pp_addr;
+	pr_dev->dev = &pdev->dev;
+
+	nl_dev = kzalloc(sizeof(*nl_dev), GFP_KERNEL);
+	if (!nl_dev) {
+		err = -ENOMEM;
+		goto err_nl_dev_alloc;
+	}
+
+	pci_set_drvdata(pdev, nl_dev);
+	nl_dev->is_registered = false;
+	nl_dev->pr_dev = pr_dev;
+
+	mutex_lock(&mvsw_nl_devices_mtx);
+
+	/* register only first device */
+	if (list_empty(&mvsw_nl_devices)) {
+		err = prestera_intr_handler_register(mvsw_nl_do_intr, NULL);
+		if (err) {
+			dev_err(&pdev->dev, "fail register intr handler\n");
+			goto err_intr_register;
+		}
+
+		err = mvsw_pr_device_register(pr_dev);
+		if (err) {
+			dev_err(&pdev->dev, "fail register prestera device\n");
+			mutex_unlock(&mvsw_nl_devices_mtx);
+			goto err_pr_dev_register;
+		};
+
+		nl_dev->is_registered = true;
+
+		/* make sure we assign sw_dev after everything above is done */
+		wmb();
+
+		WRITE_ONCE(sw_dev, pr_dev);
+	}
+
+	list_add(&nl_dev->head, &mvsw_nl_devices);
+
+	mutex_unlock(&mvsw_nl_devices_mtx);
+
+	dev_info(&pdev->dev, "registered new device\n");
+
+	return 0;
+
+err_pr_dev_register:
+	prestera_intr_handler_unregister(mvsw_nl_do_intr, NULL);
+err_intr_register:
+err_nl_dev_alloc:
+err_pp_ioremap:
+	iounmap(pp_addr);
+err_dma_mask:
+	pci_release_regions(pdev);
+err_pci_request_regions:
+	pci_disable_device(pdev);
+err_pci_enable_device:
+	kfree(pr_dev);
+	return err;
+}
+
+static void mvsw_nl_pci_remove(struct pci_dev *pdev)
+{
+	struct mvsw_nl_device *nl_dev = pci_get_drvdata(pdev);
+
+	if (nl_dev->is_registered) {
+		prestera_intr_handler_unregister(mvsw_nl_do_intr, NULL);
+		mvsw_pr_device_unregister(nl_dev->pr_dev);
+		nl_dev->is_registered = false;
+		sw_dev = NULL;
+	}
+
+	iounmap(nl_dev->pr_dev->pp_regs);
+	list_del(&nl_dev->head);
+	kfree(nl_dev->pr_dev);
+	kfree(nl_dev);
+}
+
 static int mvsw_nl_handle_reply(struct sk_buff *skb, struct genl_info *info)
 {
 	size_t size;
@@ -272,34 +477,10 @@ static int mvsw_nl_bus_register(void)
 	return err;
 }
 
-static void mvsw_nl_bus_unregister(void)
+static void mvsw_nl_dev_register_work_fn(struct work_struct *work)
 {
-	genl_unregister_family(&mvsw_genl_family);
-	/* Check if some dev is registered */
-	if (dev) {
-		mvsw_pr_device_unregister(dev);
-		kfree(dev->dev);
-		kfree(dev);
-		dev = NULL;
-	}
-}
-
-static void nl_register_device_work(struct work_struct *work)
-{
-	struct device *nl_device = kzalloc(sizeof(*nl_device), GFP_KERNEL);
-
-	dev = kzalloc(sizeof(*dev), GFP_KERNEL);
-	dev->send_req = mvsw_nl_send_req;
-
-	nl_device->init_name = mvsw_genl_family.name;
-	dev->dev = nl_device;
-
-	if (mvsw_pr_device_register(dev)) {
-		pr_err("Failed registering prestera device\n");
-		kfree(dev->dev);
-		kfree(dev);
-		dev = NULL;
-	}
+	if (pci_register_driver(&mvsw_nl_pci_driver))
+		pr_err("Failed to register PCI driver\n");
 }
 
 static int mvsw_nl_handle_ready(struct sk_buff *skb, struct genl_info *info)
@@ -329,15 +510,15 @@ static int mvsw_nl_handle_ready(struct sk_buff *skb, struct genl_info *info)
 	}
 
 	genlmsg_end(nl_msg, hdr);
-	err = genlmsg_reply(nl_msg, info);
 
+	err = genlmsg_reply(nl_msg, info);
 	if (err) {
 		MVSW_LOG_ERROR("Failed to send ready reply");
 		return 0;
 	}
 
-	INIT_WORK(&nl_bus_work, nl_register_device_work);
-	schedule_work(&nl_bus_work);
+	INIT_WORK(&mvsw_nl_dev_register_work, mvsw_nl_dev_register_work_fn);
+	schedule_work(&mvsw_nl_dev_register_work);
 
 	return 0;
 
@@ -362,10 +543,10 @@ static void mvsw_nl_event_process(struct work_struct *work)
 
 static int mvsw_nl_handle_event(struct sk_buff *skb, struct genl_info *info)
 {
-	size_t len;
 	struct mvsw_nl_event_t *ev;
+	size_t len;
 
-	if (!dev || !dev->recv_msg)
+	if (!sw_dev || !sw_dev->recv_msg)
 		return 0;
 
 	if (!info->attrs[MVSW_NL_ATTR_TLV]) {
@@ -379,9 +560,9 @@ static int mvsw_nl_handle_event(struct sk_buff *skb, struct genl_info *info)
 	if (!ev)
 		return -ENOMEM;
 
-	ev->dev = dev;
-	ev->len = len;
 	memcpy(ev->data, nla_data(info->attrs[MVSW_NL_ATTR_TLV]), len);
+	ev->dev = sw_dev;
+	ev->len = len;
 
 	INIT_WORK(&ev->work, mvsw_nl_event_process);
 	queue_work(mvsw_nl_owq, &ev->work);
@@ -389,42 +570,111 @@ static int mvsw_nl_handle_event(struct sk_buff *skb, struct genl_info *info)
 	return 0;
 }
 
+#ifdef MVSW_NL_CHRDEV
+static int mvsw_ipc_open(struct inode *inode, struct file *f)
+{
+	return 0;
+}
+
+static long mvsw_ipc_ioctl(struct file *f, unsigned int cmd, unsigned long arg)
+{
+	switch (cmd) {
+	case MVSW_IPC_RX_PKT_IOC:
+		if (likely(sw_dev->recv_pkt))
+			sw_dev->recv_pkt(sw_dev);
+		return 0;
+	}
+
+	return -EINVAL;
+}
+
+static const struct file_operations mvsw_ipc_fops = {
+	.owner = THIS_MODULE,
+	.open  = mvsw_ipc_open,
+	.unlocked_ioctl = mvsw_ipc_ioctl,
+	.llseek = noop_llseek,
+};
+#endif
+
 static int __init mvsw_pr_nl_init(void)
 {
 	int err;
 
 	pr_info("Loading Marvell Prestera Netlink Driver\n");
 
-	mvsw_nl_owq = alloc_ordered_workqueue("%s_ordered", 0, MVSW_NL_NAME);
+#ifdef MVSW_NL_CHRDEV
+	err = alloc_chrdev_region(&mvsw_dev, 0, 1, DRV_NAME);
+	if (err)
+		return err;
+
+	cdev_init(&mvsw_cdev, &mvsw_ipc_fops);
+	mvsw_cdev.owner = THIS_MODULE;
 
+	err = cdev_add(&mvsw_cdev, mvsw_dev, 1);
+	if (err)
+		goto err_cdev_add;
+
+	mvsw_class = class_create(THIS_MODULE, DRV_NAME);
+	if (IS_ERR(mvsw_class))
+		goto err_class_create;
+
+	mvsw_device = device_create(mvsw_class, NULL, mvsw_dev, NULL, DRV_NAME);
+	if (IS_ERR(mvsw_device))
+		goto err_device_create;
+#endif
+
+	mvsw_nl_owq = alloc_ordered_workqueue("%s_ordered", 0, MVSW_NL_NAME);
 	if (!mvsw_nl_owq) {
 		err = -ENOMEM;
-		goto exit;
+		goto err_alloc_wq;
 	}
 
 	err = mvsw_nl_bus_register();
+	if (err)
+		goto err_nl_register;
 
-	if (err) {
-		destroy_workqueue(mvsw_nl_owq);
-		goto exit;
-	}
+	return 0;
 
-exit:
-	if (err)
-		pr_err("Loading Marvell Prestera Netlink Driver failed!");
+err_nl_register:
+	destroy_workqueue(mvsw_nl_owq);
+err_alloc_wq:
+#ifdef MVSW_NL_CHRDEV
+	device_destroy(mvsw_class, mvsw_dev);
+err_device_create:
+	class_destroy(mvsw_class);
+err_class_create:
+	cdev_del(&mvsw_cdev);
+err_cdev_add:
+	unregister_chrdev_region(mvsw_dev, 1);
+#endif
 
 	return err;
 }
 
 static void __exit mvsw_pr_nl_exit(void)
 {
-	flush_workqueue(mvsw_nl_owq);
+	struct mvsw_nl_device *dev, *tmp;
+
+	pr_info("Unloading Marvell Prestera Netlink Driver\n");
 
-	mvsw_nl_bus_unregister();
+#ifdef MVSW_NL_CHRDEV
+	device_destroy(mvsw_class, mvsw_dev);
+	class_destroy(mvsw_class);
+	cdev_del(&mvsw_cdev);
 
+	unregister_chrdev_region(mvsw_dev, 1);
+#endif
+
+	flush_workqueue(mvsw_nl_owq);
 	destroy_workqueue(mvsw_nl_owq);
+	genl_unregister_family(&mvsw_genl_family);
 
-	pr_info("Unloading Marvell Prestera Netlink Driver\n");
+	mutex_lock(&mvsw_nl_devices_mtx);
+
+	list_for_each_entry_safe(dev, tmp, &mvsw_nl_devices, head)
+		pci_stop_and_remove_bus_device(to_pci_dev(dev->pr_dev->dev));
+
+	mutex_unlock(&mvsw_nl_devices_mtx);
 }
 
 module_init(mvsw_pr_nl_init);
diff --git a/drivers/net/ethernet/marvell/prestera_sw/prestera.c b/drivers/net/ethernet/marvell/prestera_sw/prestera.c
index 42d43dc..7be4d96 100644
--- a/drivers/net/ethernet/marvell/prestera_sw/prestera.c
+++ b/drivers/net/ethernet/marvell/prestera_sw/prestera.c
@@ -12,6 +12,7 @@
 #include <linux/etherdevice.h>
 #include <linux/ethtool.h>
 #include <linux/jiffies.h>
+#include <linux/if_bridge.h>
 #include <net/switchdev.h>
 
 #include "prestera.h"
@@ -21,6 +22,8 @@
 #include "prestera_rxtx.h"
 #include "prestera_drv_ver.h"
 
+static char base_mac_addr[ETH_ALEN];
+
 #define MVSW_PR_MTU_DEFAULT 1536
 
 #define PORT_STATS_CACHE_TIMEOUT_MS	(msecs_to_jiffies(1000))
@@ -1039,38 +1042,37 @@ static int mvsw_pr_port_autoneg_set(struct mvsw_pr_port *port, bool enable,
 	if (port->caps.type != MVSW_PORT_TYPE_TP)
 		return enable ? -EINVAL : 0;
 
-	if (!enable)
-		goto set_autoneg;
-
-	link_modes = port->caps.supp_link_modes & adver_link_modes;
-	fec = port->caps.supp_fec & adver_fec;
+	if (enable) {
+		link_modes = port->caps.supp_link_modes & adver_link_modes;
+		fec = port->caps.supp_fec & adver_fec;
 
-	if (!link_modes && !fec) {
-		netdev_err(port->net_dev, "Unsupported link mode requested");
-		return -EINVAL;
-	}
+		if (!link_modes && !fec) {
+			netdev_err(port->net_dev, "Unsupported link mode requested");
+			return -EINVAL;
+		}
 
-	if (link_modes && port->adver_link_modes != link_modes) {
-		port->adver_link_modes = link_modes;
-		refresh = true;
-	}
+		if (!link_modes)
+			link_modes = port->adver_link_modes;
+		if (!fec)
+			fec = port->adver_fec;
 
-	if (fec && port->adver_fec != fec) {
-		port->adver_fec = fec;
-		refresh = true;
+		refresh = (port->adver_link_modes != link_modes) ||
+			  (port->adver_fec != fec);
+	} else {
+		link_modes = port->adver_link_modes;
+		fec = port->adver_fec;
 	}
 
-set_autoneg:
 	if (port->autoneg == enable && !refresh)
 		return 0;
 
-	err = mvsw_pr_hw_port_autoneg_set(port, enable,
-					  port->adver_link_modes,
-					  port->adver_fec);
+	err = mvsw_pr_hw_port_autoneg_set(port, enable, link_modes, fec);
 	if (err)
 		return -EINVAL;
 
 	port->autoneg = enable;
+	port->adver_link_modes = link_modes;
+	port->adver_fec = fec;
 	return 0;
 }
 
@@ -1334,6 +1336,35 @@ err_port_allow_untagged_set:
 	return err;
 }
 
+int mvsw_pr_port_vid_stp_set(struct mvsw_pr_port *port, u16 vid, u8 state)
+{
+	u8 hw_state = state;
+
+	switch (state) {
+	case BR_STATE_DISABLED:
+		hw_state = MVSW_STP_DISABLED;
+		break;
+
+	case BR_STATE_BLOCKING:
+	case BR_STATE_LISTENING:
+		hw_state = MVSW_STP_BLOCK_LISTEN;
+		break;
+
+	case BR_STATE_LEARNING:
+		hw_state = MVSW_STP_LEARN;
+		break;
+
+	case BR_STATE_FORWARDING:
+		hw_state = MVSW_STP_FORWARD;
+		break;
+
+	default:
+		return -EINVAL;
+	}
+
+	return mvsw_pr_hw_port_vid_stp_set(port, vid, hw_state);
+}
+
 struct mvsw_pr_port_vlan*
 mvsw_pr_port_vlan_find_by_vid(const struct mvsw_pr_port *port, u16 vid)
 {
@@ -1421,6 +1452,7 @@ static int mvsw_pr_port_create(struct mvsw_pr_switch *sw, u32 id)
 	port->net_dev = net_dev;
 	port->id = id;
 	port->sw = sw;
+	port->lag_id = sw->lag_max;
 
 	err = mvsw_pr_hw_port_info_get(port, &port->fp_id,
 				       &port->hw_id, &port->dev_id);
@@ -1433,8 +1465,7 @@ static int mvsw_pr_port_create(struct mvsw_pr_switch *sw, u32 id)
 
 	net_dev->netdev_ops = &mvsw_pr_netdev_ops;
 	net_dev->ethtool_ops = &mvsw_pr_ethtool_ops;
-	net_dev->features |= NETIF_F_NETNS_LOCAL | NETIF_F_HW_L2FW_DOFFLOAD |
-		NETIF_F_HW_TC;
+	net_dev->features |= NETIF_F_NETNS_LOCAL | NETIF_F_HW_TC;
 	net_dev->hw_features |= NETIF_F_HW_TC;
 	net_dev->ethtool_ops = &mvsw_pr_ethtool_ops;
 	net_dev->netdev_ops = &mvsw_pr_netdev_ops;
@@ -1537,6 +1568,24 @@ int mvsw_pr_switch_ageing_set(struct mvsw_pr_switch *sw, u32 ageing_time)
 	return mvsw_pr_hw_switch_ageing_set(sw, ageing_time);
 }
 
+int mvsw_pr_dev_if_type(const struct net_device *dev)
+{
+	struct macvlan_dev *vlan;
+
+	if (is_vlan_dev(dev) && netif_is_bridge_master(vlan_dev_real_dev(dev)))
+		return MVSW_IF_VID_E;
+	else if (netif_is_bridge_master(dev))
+		return MVSW_IF_VID_E;
+	else if (netif_is_lag_master(dev))
+		return MVSW_IF_LAG_E;
+	else if (netif_is_macvlan(dev)) {
+		vlan = netdev_priv(dev);
+		return mvsw_pr_dev_if_type(vlan->lowerdev);
+	}
+	else
+		return MVSW_IF_PORT_E;
+}
+
 int mvsw_pr_lpm_update(struct mvsw_pr_switch *sw, u16 vr_id, u32 dst,
 		       u32 dst_len, u16 vid)
 {
@@ -1561,11 +1610,12 @@ int mvsw_pr_nh_entry_delete(struct mvsw_pr_switch *sw, u16 vr_id, __be32 dst,
 	return mvsw_pr_hw_nh_entry_del(sw, vr_id, dst, dst_len, mac);
 }
 
-int mvsw_pr_nh_entry_set(struct mvsw_pr_port *port, u16 vr_id, u16 vid,
+int mvsw_pr_nh_entry_set(struct mvsw_pr_switch *sw,
+			 struct mvsw_pr_iface *iface, u16 vr_id,
 			 __be32 dst, u32 dst_len, u8 *mac, u32 *hw_id)
 {
-	return mvsw_pr_hw_nh_entry_set(port, vr_id, vid, dst, dst_len, mac,
-					hw_id);
+	return mvsw_pr_hw_nh_entry_set(sw, iface, vr_id, dst, dst_len,
+				       mac, hw_id);
 }
 
 int mvsw_pr_fdb_flush_vlan(struct mvsw_pr_switch *sw, u16 vid,
@@ -1577,13 +1627,169 @@ int mvsw_pr_fdb_flush_vlan(struct mvsw_pr_switch *sw, u16 vid,
 int mvsw_pr_fdb_flush_port_vlan(struct mvsw_pr_port *port, u16 vid,
 				enum mvsw_pr_fdb_flush_mode mode)
 {
-	return mvsw_pr_hw_fdb_flush_port_vlan(port, vid, mode);
+	if (mvsw_pr_port_is_lag_member(port))
+		return mvsw_pr_hw_fdb_flush_lag_vlan(port->sw, port->lag_id,
+						     vid, mode);
+	else
+		return mvsw_pr_hw_fdb_flush_port_vlan(port, vid, mode);
 }
 
 int mvsw_pr_fdb_flush_port(struct mvsw_pr_port *port,
 			   enum mvsw_pr_fdb_flush_mode mode)
 {
-	return mvsw_pr_hw_fdb_flush_port(port, mode);
+	if (mvsw_pr_port_is_lag_member(port))
+		return mvsw_pr_hw_fdb_flush_lag(port->sw, port->lag_id, mode);
+	else
+		return mvsw_pr_hw_fdb_flush_port(port, mode);
+}
+
+int mvsw_pr_macvlan_add(const struct mvsw_pr_switch *sw, u16 vr_id,
+			const u8 *mac, u16 vid)
+{
+	return mvsw_pr_hw_macvlan_add(sw, vr_id, mac,  vid);
+}
+
+int mvsw_pr_macvlan_del(const struct mvsw_pr_switch *sw, u16 vr_id,
+			const u8 *mac, u16 vid)
+{
+	return mvsw_pr_hw_macvlan_del(sw, vr_id, mac,  vid);
+}
+
+static struct mvsw_pr_lag *
+mvsw_pr_lag_get(struct mvsw_pr_switch *sw, u8 id)
+{
+	return id < sw->lag_max ? &sw->lags[id] : NULL;
+}
+
+static void mvsw_pr_port_lag_create(struct mvsw_pr_switch *sw, u16 lag_id,
+				    struct net_device *lag_dev)
+{
+	INIT_LIST_HEAD(&sw->lags[lag_id].members);
+	sw->lags[lag_id].dev = lag_dev;
+}
+
+static void mvsw_pr_port_lag_destroy(struct mvsw_pr_switch *sw, u16 lag_id)
+{
+	WARN_ON(!list_empty(&sw->lags[lag_id].members));
+	sw->lags[lag_id].dev = NULL;
+	sw->lags[lag_id].member_count = 0;
+}
+
+int mvsw_pr_lag_member_add(struct mvsw_pr_port *port,
+			   struct net_device *lag_dev, u16 lag_id)
+{
+	struct mvsw_pr_switch *sw = port->sw;
+	struct mvsw_pr_lag_member *member;
+	struct mvsw_pr_lag *lag;
+
+	lag = mvsw_pr_lag_get(sw, lag_id);
+
+	if (lag->member_count >= sw->lag_member_max)
+		return -ENOSPC;
+	else if (!lag->member_count)
+		mvsw_pr_port_lag_create(sw, lag_id, lag_dev);
+
+	member = kzalloc(sizeof(*member), GFP_KERNEL);
+	if (!member)
+		return -ENOMEM;
+
+	if (mvsw_pr_hw_lag_member_add(port, lag_id)) {
+		kfree(member);
+		if (!lag->member_count)
+			mvsw_pr_port_lag_destroy(sw, lag_id);
+		return -EBUSY;
+	}
+
+	member->port = port;
+	list_add(&member->list, &lag->members);
+	lag->member_count++;
+	port->lag_id = lag_id;
+	return 0;
+}
+
+int mvsw_pr_lag_member_del(struct mvsw_pr_port *port)
+{
+	struct mvsw_pr_switch *sw = port->sw;
+	struct mvsw_pr_lag_member *member;
+	struct list_head *pos, *n;
+	u16 lag_id = port->lag_id;
+	struct mvsw_pr_lag *lag;
+	int err;
+
+	lag = mvsw_pr_lag_get(sw, lag_id);
+	if (!lag || !lag->member_count)
+		return -EINVAL;
+
+	err = mvsw_pr_hw_lag_member_del(port, lag_id);
+	if (err)
+		return err;
+
+	lag->member_count--;
+	port->lag_id = sw->lag_max;
+
+	list_for_each_safe(pos, n, &lag->members) {
+		member = list_entry(pos, typeof(*member), list);
+		if (member->port->id == port->id) {
+			list_del(&member->list);
+			break;
+		}
+	}
+
+	if (!lag->member_count)
+		mvsw_pr_port_lag_destroy(sw, lag_id);
+
+	return 0;
+}
+
+int mvsw_pr_lag_member_enable(struct mvsw_pr_port *port, bool enable)
+{
+	return mvsw_pr_hw_lag_member_enable(port, port->lag_id, enable);
+}
+
+bool mvsw_pr_port_is_lag_member(const struct mvsw_pr_port *port)
+{
+	return port->lag_id < port->sw->lag_max;
+}
+
+int mvsw_pr_lag_id_find(struct mvsw_pr_switch *sw,
+			struct net_device *lag_dev,
+			u16 *lag_id)
+{
+	struct mvsw_pr_lag *lag;
+	int free_id = -1;
+	int id;
+
+	for (id = 0; id < sw->lag_max; id++) {
+		lag = mvsw_pr_lag_get(sw, id);
+		if (lag->member_count) {
+			if (lag->dev == lag_dev) {
+				*lag_id = id;
+				return 0;
+			}
+		} else if (free_id < 0) {
+			free_id = id;
+		}
+	}
+	if (free_id < 0)
+		return -ENOSPC;
+	*lag_id = free_id;
+	return 0;
+}
+
+static int mvsw_pr_lag_init(struct mvsw_pr_switch *sw)
+{
+	sw->lags = kcalloc(sw->lag_max, sizeof(*sw->lags), GFP_KERNEL);
+	return sw->lags ? 0 : -ENOMEM;
+}
+
+static void mvsw_pr_lag_fini(struct mvsw_pr_switch *sw)
+{
+	u8 idx;
+
+	for (idx = 0; idx < sw->lag_max; idx++)
+		WARN_ON(sw->lags[idx].member_count);
+
+	kfree(sw->lags);
 }
 
 static int mvsw_pr_clear_ports(struct mvsw_pr_switch *sw)
@@ -1608,7 +1814,7 @@ static int mvsw_pr_clear_ports(struct mvsw_pr_switch *sw)
 }
 
 static void mvsw_pr_port_handle_event(struct mvsw_pr_switch *sw,
-				      struct mvsw_pr_event *evt)
+				      struct mvsw_pr_event *evt, void *arg)
 {
 	struct mvsw_pr_port *port;
 	struct delayed_work *caching_dw;
@@ -1634,14 +1840,36 @@ static void mvsw_pr_port_handle_event(struct mvsw_pr_switch *sw,
 	}
 }
 
+static bool mvsw_pr_lag_exists(const struct mvsw_pr_switch *sw, u16 lag_id)
+{
+	return lag_id < sw->lag_max &&
+	       sw->lags[lag_id].member_count != 0;
+}
+
 static void mvsw_pr_fdb_handle_event(struct mvsw_pr_switch *sw,
-				     struct mvsw_pr_event *evt)
+				     struct mvsw_pr_event *evt, void *arg)
 {
 	struct switchdev_notifier_fdb_info info;
+	struct net_device *dev = NULL;
 	struct mvsw_pr_port *port;
+	u16 lag_id;
 
-	port = __find_pr_port(sw, evt->fdb_evt.port_id);
-	if (!port)
+	switch (evt->fdb_evt.type) {
+	case MVSW_PR_FDB_ENTRY_TYPE_REG_PORT:
+		port = __find_pr_port(sw, evt->fdb_evt.dest.port_id);
+		if (port)
+			dev = port->net_dev;
+		break;
+	case MVSW_PR_FDB_ENTRY_TYPE_LAG:
+		lag_id = evt->fdb_evt.dest.lag_id;
+		if (mvsw_pr_lag_exists(sw, lag_id))
+			dev = sw->lags[lag_id].dev;
+		break;
+	default:
+		return;
+	}
+
+	if (!dev)
 		return;
 
 	info.addr = evt->fdb_evt.data.mac;
@@ -1652,11 +1880,11 @@ static void mvsw_pr_fdb_handle_event(struct mvsw_pr_switch *sw,
 	switch (evt->id) {
 	case MVSW_FDB_EVENT_LEARNED:
 		call_switchdev_notifiers(SWITCHDEV_FDB_ADD_TO_BRIDGE,
-					 port->net_dev, &info.info, NULL);
+					 dev, &info.info, NULL);
 		break;
 	case MVSW_FDB_EVENT_AGED:
 		call_switchdev_notifiers(SWITCHDEV_FDB_DEL_TO_BRIDGE,
-					 port->net_dev, &info.info, NULL);
+					 dev, &info.info, NULL);
 		break;
 	}
 	rtnl_unlock();
@@ -1665,25 +1893,31 @@ static void mvsw_pr_fdb_handle_event(struct mvsw_pr_switch *sw,
 int mvsw_pr_fdb_add(struct mvsw_pr_port *port, const unsigned char *mac,
 		    u16 vid, bool dynamic)
 {
-	return mvsw_pr_hw_fdb_add(port, mac, vid, dynamic);
+	if (mvsw_pr_port_is_lag_member(port))
+		return mvsw_pr_hw_lag_fdb_add(port->sw, port->lag_id,
+					      mac, vid, dynamic);
+	else
+		return mvsw_pr_hw_fdb_add(port, mac, vid, dynamic);
 }
 
 int mvsw_pr_fdb_del(struct mvsw_pr_port *port, const unsigned char *mac,
 		    u16 vid)
 {
-	return mvsw_pr_hw_fdb_del(port, mac, vid);
+	if (mvsw_pr_port_is_lag_member(port))
+		return mvsw_pr_hw_lag_fdb_del(port->sw, port->lag_id,
+					      mac, vid);
+	else
+		return mvsw_pr_hw_fdb_del(port, mac, vid);
 }
 
 static void mvsw_pr_fdb_event_handler_unregister(struct mvsw_pr_switch *sw)
 {
-	mvsw_pr_hw_event_handler_unregister(sw, MVSW_EVENT_TYPE_FDB,
-					    mvsw_pr_fdb_handle_event);
+	mvsw_pr_hw_event_handler_unregister(sw, MVSW_EVENT_TYPE_FDB);
 }
 
 static void mvsw_pr_port_event_handler_unregister(struct mvsw_pr_switch *sw)
 {
-	mvsw_pr_hw_event_handler_unregister(sw, MVSW_EVENT_TYPE_PORT,
-					    mvsw_pr_port_handle_event);
+	mvsw_pr_hw_event_handler_unregister(sw, MVSW_EVENT_TYPE_PORT);
 }
 
 static void mvsw_pr_event_handlers_unregister(struct mvsw_pr_switch *sw)
@@ -1695,13 +1929,15 @@ static void mvsw_pr_event_handlers_unregister(struct mvsw_pr_switch *sw)
 static int mvsw_pr_fdb_event_handler_register(struct mvsw_pr_switch *sw)
 {
 	return mvsw_pr_hw_event_handler_register(sw, MVSW_EVENT_TYPE_FDB,
-						 mvsw_pr_fdb_handle_event);
+						 mvsw_pr_fdb_handle_event,
+						 NULL);
 }
 
 static int mvsw_pr_port_event_handler_register(struct mvsw_pr_switch *sw)
 {
 	return mvsw_pr_hw_event_handler_register(sw, MVSW_EVENT_TYPE_PORT,
-						 mvsw_pr_port_handle_event);
+						 mvsw_pr_port_handle_event,
+						 NULL);
 }
 
 static int mvsw_pr_event_handlers_register(struct mvsw_pr_switch *sw)
@@ -1754,8 +1990,18 @@ static int mvsw_pr_init(struct mvsw_pr_switch *sw)
 		return err;
 	}
 
+	memcpy(sw->base_mac, base_mac_addr, sizeof(sw->base_mac));
+
+	err = mvsw_pr_hw_switch_mac_set(sw, sw->base_mac);
+	if (err)
+		return err;
+
 	dev_info(mvsw_dev(sw), "Initialized Switch device\n");
 
+	err = mvsw_pr_lag_init(sw);
+	if (err)
+		return err;
+
 	err = mvsw_pr_switchdev_register(sw);
 	if (err)
 		return err;
@@ -1803,10 +2049,11 @@ static void mvsw_pr_fini(struct mvsw_pr_switch *sw)
 
 	mvsw_pr_fw_log_fini(sw);
 
-	mvsw_pr_rxtx_switch_fini(sw);
 	mvsw_pr_clear_ports(sw);
+	mvsw_pr_rxtx_switch_fini(sw);
 	mvsw_pr_switchdev_unregister(sw);
 	mvsw_pr_acl_fini(sw);
+	mvsw_pr_lag_fini(sw);
 }
 
 int mvsw_pr_device_register(struct mvsw_pr_device *dev)
diff --git a/drivers/net/ethernet/marvell/prestera_sw/prestera.h b/drivers/net/ethernet/marvell/prestera_sw/prestera.h
index 95d63cb..d9894f5 100644
--- a/drivers/net/ethernet/marvell/prestera_sw/prestera.h
+++ b/drivers/net/ethernet/marvell/prestera_sw/prestera.h
@@ -10,6 +10,7 @@
 #include <linux/skbuff.h>
 #include <linux/notifier.h>
 #include <uapi/linux/if_ether.h>
+#include <linux/if_macvlan.h>
 #include <linux/workqueue.h>
 #include <net/pkt_cls.h>
 
@@ -92,6 +93,7 @@ struct mvsw_pr_port {
 	bool autoneg;
 	u64 adver_link_modes;
 	u8 adver_fec;
+	u16 lag_id;
 	struct mvsw_pr_port_caps caps;
 	struct list_head list;
 	struct list_head vlans_list;
@@ -121,6 +123,9 @@ struct mvsw_pr_device {
 	u8 __iomem *pp_regs;
 	void *priv;
 
+	/* called by device driver to handle received packets */
+	void (*recv_pkt)(struct mvsw_pr_device *dev);
+
 	/* called by device driver to pass event up to the higher layer */
 	int (*recv_msg)(struct mvsw_pr_device *dev, u8 *msg, size_t size);
 
@@ -134,11 +139,20 @@ enum mvsw_pr_event_type {
 	MVSW_EVENT_TYPE_UNSPEC,
 	MVSW_EVENT_TYPE_PORT,
 	MVSW_EVENT_TYPE_FDB,
+	MVSW_EVENT_TYPE_RXTX,
 	MVSW_EVENT_TYPE_FW_LOG,
 
 	MVSW_EVENT_TYPE_MAX,
 };
 
+enum mvsw_pr_rxtx_event_id {
+	MVSW_RXTX_EVENT_UNSPEC,
+
+	MVSW_RXTX_EVENT_RCV_PKT,
+
+	MVSW_RXTX_EVENT_MAX,
+};
+
 enum mvsw_pr_port_event_id {
 	MVSW_PORT_EVENT_UNSPEC,
 	MVSW_PORT_EVENT_STATE_CHANGED,
@@ -154,8 +168,18 @@ enum mvsw_pr_fdb_event_id {
 	MVSW_FDB_EVENT_MAX,
 };
 
+enum mvsw_pr_fdb_entry_type {
+	MVSW_PR_FDB_ENTRY_TYPE_REG_PORT,
+	MVSW_PR_FDB_ENTRY_TYPE_LAG,
+	MVSW_PR_FDB_ENTRY_TYPE_MAX
+};
+
 struct mvsw_pr_fdb_event {
-	u32 port_id;
+	enum mvsw_pr_fdb_entry_type type;
+	union {
+		u32 port_id;
+		u16 lag_id;
+	} dest;
 	u32 vid;
 	union {
 		u8 mac[ETH_ALEN];
@@ -183,12 +207,37 @@ struct mvsw_pr_event {
 	};
 };
 
-enum mvsw_pr_rif_type {
-	MVSW_PR_RIF_TYPE_PORT,
-	MVSW_PR_RIF_TYPE_VLAN,
-	MVSW_PR_RIF_TYPE_BRIDGE,
+struct mvsw_pr_lag_member {
+	struct list_head list;
+	struct mvsw_pr_port *port;
+};
+
+struct mvsw_pr_lag {
+	struct net_device *dev;
+	u16 member_count;
+	struct list_head members;
+};
+
+enum mvsw_pr_if_type {
+	/* the interface is of port type (dev,port) */
+	MVSW_IF_PORT_E = 0,
 
-	MVSW_PR_RIF_TYPE_MAX,
+	/* the interface is of lag type (lag-id) */
+	MVSW_IF_LAG_E = 1,
+
+	/* the interface is of Vid type (vlan-id) */
+	MVSW_IF_VID_E = 3,
+};
+
+struct mvsw_pr_iface {
+	enum mvsw_pr_if_type type;
+	struct {
+		u32 hw_dev_num;
+		u32 port_num;
+	} dev_port;
+	u16 lag_id;
+	u16 vlan_id;
+	u32 hw_dev_num;
 };
 
 struct mvsw_pr_bridge;
@@ -205,10 +254,13 @@ struct mvsw_pr_switch {
 	u32 mtu_min;
 	u32 mtu_max;
 	u8 id;
+	u8 lag_max;
+	u8 lag_member_max;
 	struct mvsw_pr_acl *acl;
 	struct mvsw_pr_bridge *bridge;
 	struct mvsw_pr_switchdev *switchdev;
 	struct mvsw_pr_router *router;
+	struct mvsw_pr_lag *lags;
 	struct notifier_block netdevice_nb;
 };
 
@@ -270,6 +322,7 @@ int mvsw_pr_port_learning_set(struct mvsw_pr_port *mvsw_pr_port,
 			      bool learn_enable);
 int mvsw_pr_port_flood_set(struct mvsw_pr_port *mvsw_pr_port, bool flood);
 int mvsw_pr_port_pvid_set(struct mvsw_pr_port *mvsw_pr_port, u16 vid);
+int mvsw_pr_port_vid_stp_set(struct mvsw_pr_port *port, u16 vid, u8 state);
 struct mvsw_pr_port_vlan *
 mvsw_pr_port_vlan_create(struct mvsw_pr_port *mvsw_pr_port, u16 vid,
 			 bool untagged);
@@ -299,6 +352,21 @@ int mvsw_pr_fdb_flush_port_vlan(struct mvsw_pr_port *port, u16 vid,
 				enum mvsw_pr_fdb_flush_mode mode);
 int mvsw_pr_fdb_flush_port(struct mvsw_pr_port *port,
 			   enum mvsw_pr_fdb_flush_mode mode);
+int mvsw_pr_macvlan_add(const struct mvsw_pr_switch *sw, u16 vr_id,
+			const u8 *mac, u16 vid);
+int mvsw_pr_macvlan_del(const struct mvsw_pr_switch *sw, u16 vr_id,
+			const u8 *mac, u16 vid);
+
+int mvsw_pr_lag_member_add(struct mvsw_pr_port *port,
+			   struct net_device *lag_dev, u16 lag_id);
+int mvsw_pr_lag_member_del(struct mvsw_pr_port *port);
+int mvsw_pr_lag_member_enable(struct mvsw_pr_port *port, bool enable);
+bool mvsw_pr_port_is_lag_member(const struct mvsw_pr_port *port);
+int mvsw_pr_lag_id_find(struct mvsw_pr_switch *sw,
+			struct net_device *lag_dev,
+			u16 *lag_id);
+
+int mvsw_pr_dev_if_type(const struct net_device *dev);
 
 /* prestera_flower.c */
 int mvsw_pr_flower_replace(struct mvsw_pr_switch *sw,
@@ -393,12 +461,15 @@ int mvsw_pr_lpm_del(struct mvsw_pr_switch *sw, u16 vr_id, u32 dst,
 		    u32 dst_len);
 int mvsw_pr_nh_entry_add(struct mvsw_pr_switch *sw, u16 vr_id, u16 vid,
 			 __be32 dst, u8 *mac, u32 *hw_id);
-int mvsw_pr_nh_entry_set(struct mvsw_pr_port *port, u16 vr_id, u16 vid,
+int mvsw_pr_nh_entry_set(struct mvsw_pr_switch *sw,
+			 struct mvsw_pr_iface *iface, u16 vr_id,
 			 __be32 dst, u32 dst_len, u8 *mac, u32 *hw_id);
 int mvsw_pr_nh_entry_delete(struct mvsw_pr_switch *sw, u16 vr_id, __be32 dst,
 			    u32 dst_len, u8 *mac);
 
-void mvsw_pr_rif_enable(struct mvsw_pr_switch *sw, struct net_device *dev);
-void mvsw_pr_rif_disable(struct mvsw_pr_switch *sw, struct net_device *dev);
+void mvsw_pr_rif_enable(struct mvsw_pr_switch *sw, struct net_device *dev,
+			bool enable);
+bool mvsw_pr_rif_exists(const struct mvsw_pr_switch *sw,
+			const struct net_device *dev);
 
 #endif /* _MVSW_PRESTERA_H_ */
diff --git a/drivers/net/ethernet/marvell/prestera_sw/prestera_drv_ver.h b/drivers/net/ethernet/marvell/prestera_sw/prestera_drv_ver.h
index a5c0921..dd9b222 100644
--- a/drivers/net/ethernet/marvell/prestera_sw/prestera_drv_ver.h
+++ b/drivers/net/ethernet/marvell/prestera_sw/prestera_drv_ver.h
@@ -12,7 +12,7 @@
 #define PRESTERA_DRV_VER_MAJOR	2
 #define PRESTERA_DRV_VER_MINOR	0
 #define PRESTERA_DRV_VER_PATCH	0
-#define PRESTERA_DRV_VER_EXTRA	-v2.0.1-pvt
+#define PRESTERA_DRV_VER_EXTRA	-v2.1.0
 
 #define PRESTERA_DRV_VER \
 		__stringify(PRESTERA_DRV_VER_MAJOR)  "." \
diff --git a/drivers/net/ethernet/marvell/prestera_sw/prestera_dsa.c b/drivers/net/ethernet/marvell/prestera_sw/prestera_dsa.c
index 653e0b9..ad82cee 100644
--- a/drivers/net/ethernet/marvell/prestera_sw/prestera_dsa.c
+++ b/drivers/net/ethernet/marvell/prestera_sw/prestera_dsa.c
@@ -3,6 +3,7 @@
  * Copyright (c) 2020 Marvell International Ltd. All rights reserved.
  *
  */
+#include "prestera.h"
 #include "prestera_dsa.h"
 
 #include <linux/string.h>
diff --git a/drivers/net/ethernet/marvell/prestera_sw/prestera_dsa.h b/drivers/net/ethernet/marvell/prestera_sw/prestera_dsa.h
index f65d110..fcf5632 100644
--- a/drivers/net/ethernet/marvell/prestera_sw/prestera_dsa.h
+++ b/drivers/net/ethernet/marvell/prestera_sw/prestera_dsa.h
@@ -18,17 +18,6 @@ enum mvsw_pr_dsa_cmd {
 	MVSW_NET_DSA_CMD_FROM_CPU_E,
 };
 
-enum mvsw_pr_if_type {
-	/* the interface is of port type (dev,port) */
-	MVSW_IF_PORT_E = 0,
-
-	/* the interface is of trunk type (trunkId) */
-	MVSW_IF_TRUNK_E = 1,
-
-	/* the interface is of Vid type (vlan-id) */
-	MVSW_IF_VID_E = 3,
-};
-
 struct mvsw_pr_dsa_common {
 	/* the value vlan priority tag (APPLICABLE RANGES: 0..7) */
 	u8 vpt;
@@ -40,17 +29,6 @@ struct mvsw_pr_dsa_common {
 	u16 vid;
 };
 
-struct mvsw_pr_iface_info {
-	enum mvsw_pr_if_type type;
-	struct {
-		u32 hw_dev_num;
-		u32 port_num;
-	} dev_port;
-	u16 trunk_id;
-	u16 vlan_id;
-	u32 hw_dev_num;
-};
-
 struct mvsw_pr_dsa_to_cpu {
 	bool is_tagged;
 	u32 hw_dev_num;
@@ -63,7 +41,7 @@ struct mvsw_pr_dsa_to_cpu {
 };
 
 struct mvsw_pr_dsa_from_cpu {
-	struct mvsw_pr_iface_info dst_iface;	/* vid/port */
+	struct mvsw_pr_iface dst_iface;	/* vid/port */
 	bool egr_filter_en;
 	bool egr_filter_registered;
 	u32 src_id;
diff --git a/drivers/net/ethernet/marvell/prestera_sw/prestera_fw_log.c b/drivers/net/ethernet/marvell/prestera_sw/prestera_fw_log.c
index 8211001..5336b33 100644
--- a/drivers/net/ethernet/marvell/prestera_sw/prestera_fw_log.c
+++ b/drivers/net/ethernet/marvell/prestera_sw/prestera_fw_log.c
@@ -40,7 +40,8 @@
 	(ptr[lib] &= ~(1 << type))
 
 static void mvsw_pr_fw_log_evt_handler(struct mvsw_pr_switch *,
-				       struct mvsw_pr_event *);
+				       struct mvsw_pr_event *,
+				       void *);
 static ssize_t mvsw_pr_fw_log_debugfs_read(struct file *file,
 					   char __user *ubuf,
 					   size_t count, loff_t *ppos);
@@ -104,6 +105,9 @@ enum {
 	FW_LOG_LIB_FLOW_MANAGER,
 	FW_LOG_LIB_BRIDGE_FDB_MANAGER,
 	FW_LOG_LIB_I2C,
+	FW_LOG_LIB_PPU,
+	FW_LOG_LIB_EXACT_MATCH_MANAGER,
+	FW_LOG_LIB_MAC_SEC,
 	FW_LOG_LIB_ALL,
 
 	FW_LOG_LIB_MAX
@@ -189,7 +193,10 @@ static const char *mvsw_pr_fw_log_prv_lib_str_enu[FW_LOG_LIB_MAX] = {
 	[FW_LOG_LIB_TUNNEL] =  "tunnel",
 	[FW_LOG_LIB_VERSION] =  "version",
 	[FW_LOG_LIB_VIRTUAL_TCAM] =  "virtual-tcam",
-	[FW_LOG_LIB_VNT] =  "vnt"
+	[FW_LOG_LIB_VNT] =  "vnt",
+	[FW_LOG_LIB_PPU] = "ppu",
+	[FW_LOG_LIB_EXACT_MATCH_MANAGER] = "exact-match-manager",
+	[FW_LOG_LIB_MAC_SEC] = "mac-sec",
 };
 
 static const char *mvsw_pr_fw_log_prv_type_str_enu[FW_LOG_TYPE_MAX] = {
@@ -201,7 +208,7 @@ static const char *mvsw_pr_fw_log_prv_type_str_enu[FW_LOG_TYPE_MAX] = {
 };
 
 static void mvsw_pr_fw_log_evt_handler(struct mvsw_pr_switch *sw,
-				       struct mvsw_pr_event *evt)
+				       struct mvsw_pr_event *evt, void *arg)
 {
 	u32 log_len = evt->fw_log_evt.log_len;
 	u8 *buf = evt->fw_log_evt.data;
@@ -376,13 +383,13 @@ static inline int mvsw_pr_fw_log_get_lib_from_str(const char *str)
 static int mvsw_pr_fw_log_event_handler_register(struct mvsw_pr_switch *sw)
 {
 	return mvsw_pr_hw_event_handler_register(sw, MVSW_EVENT_TYPE_FW_LOG,
-						 mvsw_pr_fw_log_evt_handler);
+						 mvsw_pr_fw_log_evt_handler,
+						 NULL);
 }
 
 static void mvsw_pr_fw_log_event_handler_unregister(struct mvsw_pr_switch *sw)
 {
-	mvsw_pr_hw_event_handler_unregister(sw, MVSW_EVENT_TYPE_FW_LOG,
-					    mvsw_pr_fw_log_evt_handler);
+	mvsw_pr_hw_event_handler_unregister(sw, MVSW_EVENT_TYPE_FW_LOG);
 }
 
 int mvsw_pr_fw_log_init(struct mvsw_pr_switch *sw)
diff --git a/drivers/net/ethernet/marvell/prestera_sw/prestera_hw.c b/drivers/net/ethernet/marvell/prestera_sw/prestera_hw.c
index 19777c7..81bf0ba 100644
--- a/drivers/net/ethernet/marvell/prestera_sw/prestera_hw.c
+++ b/drivers/net/ethernet/marvell/prestera_sw/prestera_hw.c
@@ -19,87 +19,94 @@
 #define MVSW_PR_MSG_BUFF_CHUNK_SIZE	32	/* bytes */
 
 enum mvsw_msg_type {
-	MVSW_MSG_TYPE_SWITCH_UNSPEC,
-	MVSW_MSG_TYPE_SWITCH_INIT,
-
-	MVSW_MSG_TYPE_AGEING_TIMEOUT_SET,
-
-	MVSW_MSG_TYPE_PORT_ATTR_SET,
-	MVSW_MSG_TYPE_PORT_ATTR_GET,
-	MVSW_MSG_TYPE_PORT_INFO_GET,
-
-	MVSW_MSG_TYPE_VLAN_CREATE,
-	MVSW_MSG_TYPE_VLAN_DELETE,
-	MVSW_MSG_TYPE_VLAN_PORT_SET,
-	MVSW_MSG_TYPE_VLAN_PVID_SET,
-
-	MVSW_MSG_TYPE_FDB_ADD,
-	MVSW_MSG_TYPE_FDB_DELETE,
-	MVSW_MSG_TYPE_FDB_FLUSH_PORT,
-	MVSW_MSG_TYPE_FDB_FLUSH_VLAN,
-	MVSW_MSG_TYPE_FDB_FLUSH_PORT_VLAN,
+	MVSW_MSG_TYPE_SWITCH_INIT = 0x1,
+	MVSW_MSG_TYPE_SWITCH_ATTR_SET = 0x2,
+
+	MVSW_MSG_TYPE_PORT_ATTR_SET = 0x100,
+	MVSW_MSG_TYPE_PORT_ATTR_GET = 0x101,
+	MVSW_MSG_TYPE_PORT_INFO_GET = 0x110,
+
+	MVSW_MSG_TYPE_VLAN_CREATE = 0x200,
+	MVSW_MSG_TYPE_VLAN_DELETE = 0x201,
+	MVSW_MSG_TYPE_VLAN_PORT_SET = 0x202,
+	MVSW_MSG_TYPE_VLAN_PVID_SET = 0x203,
+
+	MVSW_MSG_TYPE_FDB_ADD = 0x300,
+	MVSW_MSG_TYPE_FDB_DELETE = 0x301,
+	MVSW_MSG_TYPE_FDB_FLUSH_PORT = 0x310,
+	MVSW_MSG_TYPE_FDB_FLUSH_VLAN = 0x311,
+	MVSW_MSG_TYPE_FDB_FLUSH_PORT_VLAN = 0x312,
+	MVSW_MSG_TYPE_FDB_MACVLAN_ADD = 0x320,
+	MVSW_MSG_TYPE_FDB_MACVLAN_DEL = 0x321,
 
 	MVSW_MSG_TYPE_LOG_LEVEL_SET,
 
-	MVSW_MSG_TYPE_BRIDGE_CREATE,
-	MVSW_MSG_TYPE_BRIDGE_DELETE,
-	MVSW_MSG_TYPE_BRIDGE_PORT_ADD,
-	MVSW_MSG_TYPE_BRIDGE_PORT_DELETE,
-
-	MVSW_MSG_TYPE_ACL_RULE_ADD,
-	MVSW_MSG_TYPE_ACL_RULE_DELETE,
-	MVSW_MSG_TYPE_ACL_RULE_STATS_GET,
-	MVSW_MSG_TYPE_ACL_RULESET_CREATE,
-	MVSW_MSG_TYPE_ACL_RULESET_DELETE,
-	MVSW_MSG_TYPE_ACL_PORT_BIND,
-	MVSW_MSG_TYPE_ACL_PORT_UNBIND,
-
-	MVSW_MSG_TYPE_RIF_PORT_CREATE,
-	MVSW_MSG_TYPE_RIF_VLAN_CREATE,
-	MVSW_MSG_TYPE_RIF_BRIDGE_CREATE,
-	MVSW_MSG_TYPE_RIF_DELETE,
-	MVSW_MSG_TYPE_RIF_SET,
-
-	MVSW_MSG_TYPE_LPM_UPDATE,
-	MVSW_MSG_TYPE_LPM_DELETE,
-
-	MVSW_MSG_TYPE_NH_ADD,
-	MVSW_MSG_TYPE_NH_DELETE,
-	MVSW_MSG_TYPE_NH_SET,
-	MVSW_MSG_TYPE_NH_GET,
-
-	MVSW_MSG_TYPE_VR_CREATE,
-	MVSW_MSG_TYPE_VR_DELETE,
-
-	MVSW_MSG_TYPE_RXTX_INIT,
-
-	MVSW_MSG_TYPE_ACK,
+	MVSW_MSG_TYPE_BRIDGE_CREATE = 0x400,
+	MVSW_MSG_TYPE_BRIDGE_DELETE = 0x401,
+	MVSW_MSG_TYPE_BRIDGE_PORT_ADD = 0x402,
+	MVSW_MSG_TYPE_BRIDGE_PORT_DELETE = 0x403,
+
+	MVSW_MSG_TYPE_ACL_RULE_ADD = 0x500,
+	MVSW_MSG_TYPE_ACL_RULE_DELETE = 0x501,
+	MVSW_MSG_TYPE_ACL_RULE_STATS_GET = 0x510,
+	MVSW_MSG_TYPE_ACL_RULESET_CREATE = 0x520,
+	MVSW_MSG_TYPE_ACL_RULESET_DELETE = 0x521,
+	MVSW_MSG_TYPE_ACL_PORT_BIND = 0x530,
+	MVSW_MSG_TYPE_ACL_PORT_UNBIND = 0x531,
+
+	MVSW_MSG_TYPE_ROUTER_RIF_CREATE = 0x600,
+	MVSW_MSG_TYPE_ROUTER_RIF_DELETE = 0x601,
+	MVSW_MSG_TYPE_ROUTER_RIF_SET = 0x602,
+	MVSW_MSG_TYPE_ROUTER_LPM_UPDATE = 0x610,
+	MVSW_MSG_TYPE_ROUTER_LPM_DELETE = 0x611,
+	MVSW_MSG_TYPE_ROUTER_NH_ADD = 0x620,
+	MVSW_MSG_TYPE_ROUTER_NH_DELETE = 0x621,
+	MVSW_MSG_TYPE_ROUTER_NH_SET = 0x622,
+	MVSW_MSG_TYPE_ROUTER_NH_GET = 0x623,
+	MVSW_MSG_TYPE_ROUTER_VR_CREATE = 0x630,
+	MVSW_MSG_TYPE_ROUTER_VR_DELETE = 0x631,
+
+	MVSW_MSG_TYPE_RXTX_INIT = 0x800,
+
+	MVSW_MSG_TYPE_LAG_ADD = 0x900,
+	MVSW_MSG_TYPE_LAG_DELETE = 0x901,
+	MVSW_MSG_TYPE_LAG_ENABLE = 0x902,
+	MVSW_MSG_TYPE_LAG_DISABLE = 0x903,
+
+	MVSW_MSG_TYPE_STP_PORT_SET = 0x1000,
+
+	MVSW_MSG_TYPE_ACK = 0x10000,
 	MVSW_MSG_TYPE_MAX
 };
 
 enum mvsw_msg_port_attr {
-	MVSW_MSG_PORT_ATTR_ADMIN_STATE,
-	MVSW_MSG_PORT_ATTR_OPER_STATE,
-	MVSW_MSG_PORT_ATTR_MTU,
-	MVSW_MSG_PORT_ATTR_MAC,
-	MVSW_MSG_PORT_ATTR_SPEED,
-	MVSW_MSG_PORT_ATTR_ACCEPT_FRAME_TYPE,
-	MVSW_MSG_PORT_ATTR_LEARNING,
-	MVSW_MSG_PORT_ATTR_FLOOD,
-	MVSW_MSG_PORT_ATTR_CAPABILITY,
-	MVSW_MSG_PORT_ATTR_REMOTE_CAPABILITY,
-	MVSW_MSG_PORT_ATTR_REMOTE_FC,
-	MVSW_MSG_PORT_ATTR_LINK_MODE,
-	MVSW_MSG_PORT_ATTR_TYPE,
-	MVSW_MSG_PORT_ATTR_FEC,
-	MVSW_MSG_PORT_ATTR_AUTONEG,
-	MVSW_MSG_PORT_ATTR_DUPLEX,
-	MVSW_MSG_PORT_ATTR_STATS,
-	MVSW_MSG_PORT_ATTR_MDIX,
-	MVSW_MSG_PORT_ATTR_AUTONEG_RESTART,
+	MVSW_MSG_PORT_ATTR_ADMIN_STATE = 1,
+	MVSW_MSG_PORT_ATTR_OPER_STATE = 2,
+	MVSW_MSG_PORT_ATTR_MTU = 3,
+	MVSW_MSG_PORT_ATTR_MAC = 4,
+	MVSW_MSG_PORT_ATTR_SPEED = 5,
+	MVSW_MSG_PORT_ATTR_ACCEPT_FRAME_TYPE = 6,
+	MVSW_MSG_PORT_ATTR_LEARNING = 7,
+	MVSW_MSG_PORT_ATTR_FLOOD = 8,
+	MVSW_MSG_PORT_ATTR_CAPABILITY = 9,
+	MVSW_MSG_PORT_ATTR_REMOTE_CAPABILITY = 10,
+	MVSW_MSG_PORT_ATTR_REMOTE_FC = 11,
+	MVSW_MSG_PORT_ATTR_LINK_MODE = 12,
+	MVSW_MSG_PORT_ATTR_TYPE = 13,
+	MVSW_MSG_PORT_ATTR_FEC = 14,
+	MVSW_MSG_PORT_ATTR_AUTONEG = 15,
+	MVSW_MSG_PORT_ATTR_DUPLEX = 16,
+	MVSW_MSG_PORT_ATTR_STATS = 17,
+	MVSW_MSG_PORT_ATTR_MDIX = 18,
+	MVSW_MSG_PORT_ATTR_AUTONEG_RESTART = 19,
 	MVSW_MSG_PORT_ATTR_MAX
 };
 
+enum mvsw_msg_switch_attr {
+	MVSW_MSG_SWITCH_ATTR_MAC = 1,
+	MVSW_MSG_SWITCH_ATTR_AGEING = 2,
+};
+
 enum {
 	MVSW_MSG_ACK_OK,
 	MVSW_MSG_ACK_FAILED,
@@ -154,6 +161,12 @@ enum {
 	MVSW_FC_SYMM_ASYMM,
 };
 
+enum {
+	MVSW_HW_FDB_ENTRY_TYPE_REG_PORT = 0,
+	MVSW_HW_FDB_ENTRY_TYPE_LAG = 1,
+	MVSW_HW_FDB_ENTRY_TYPE_MAX = 2,
+};
+
 struct mvsw_msg_buff {
 	u32 free;
 	u32 total;
@@ -180,10 +193,12 @@ struct mvsw_msg_common_response {
 
 union mvsw_msg_switch_param {
 	u32 ageing_timeout;
+	u8  mac[ETH_ALEN];
 };
 
 struct mvsw_msg_switch_attr_cmd {
 	struct mvsw_msg_cmd cmd;
+	u32 attr;
 	union mvsw_msg_switch_param param;
 } __packed __aligned(4);
 
@@ -192,7 +207,8 @@ struct mvsw_msg_switch_init_ret {
 	u32 port_count;
 	u32 mtu_max;
 	u8  switch_id;
-	u8  mac[ETH_ALEN];
+	u8  lag_max;
+	u8  lag_member_max;
 } __packed __aligned(4);
 
 struct mvsw_msg_port_autoneg_param {
@@ -273,8 +289,14 @@ struct mvsw_msg_vlan_cmd {
 
 struct mvsw_msg_fdb_cmd {
 	struct mvsw_msg_cmd cmd;
-	u32 port;
-	u32 dev;
+	u8 dest_type;
+	union {
+		struct {
+			u32 port;
+			u32 dev;
+		};
+		u16 lag_id;
+	} dest;
 	u8  mac[ETH_ALEN];
 	u16 vid;
 	u8  dynamic;
@@ -338,7 +360,11 @@ union mvsw_msg_event_fdb_param {
 
 struct mvsw_msg_event_fdb {
 	struct mvsw_msg_event id;
-	u32 port_id;
+	u8 dest_type;
+	union {
+		u32 port_id;
+		u16 lag_id;
+	} dest;
 	u32 vid;
 	union mvsw_msg_event_fdb_param param;
 } __packed __aligned(4);
@@ -365,14 +391,39 @@ struct mvsw_msg_bridge_ret {
 	u16 bridge;
 } __packed __aligned(4);
 
-struct mvsw_msg_rif_cmd {
+struct mvsw_msg_macvlan_cmd {
 	struct mvsw_msg_cmd cmd;
-	u16 rif_id;
 	u16 vr_id;
-	u16 bridge;
 	u8 mac[ETH_ALEN];
+	u16 vid;
+} __packed __aligned(4);
+
+struct mvsw_msg_stp_cmd {
+	struct mvsw_msg_cmd cmd;
 	u32 port;
 	u32 dev;
+	u16 vid;
+	u8  state;
+} __packed __aligned(4);
+
+struct mvsw_msg_iface {
+	u8 type;
+	u16 vid;
+	union {
+		struct {
+			u32 dev;
+			u32 port;
+		};
+		u16 lag_id;
+	};
+} __packed __aligned(4);
+
+struct mvsw_msg_rif_cmd {
+	struct mvsw_msg_cmd cmd;
+	struct mvsw_msg_iface iif;
+	u16 rif_id;
+	u16 vr_id;
+	u8 mac[ETH_ALEN];
 	u32 mtu;
 } __packed __aligned(4);
 
@@ -391,12 +442,10 @@ struct mvsw_msg_lpm_cmd {
 
 struct mvsw_msg_nh_cmd {
 	struct mvsw_msg_cmd cmd;
-	u32 port;
-	u32 dev;
+	struct mvsw_msg_iface oif;
 	u32 hw_id;
 	__be32 dst;
 	u32 dst_len;
-	u16 vid;
 	u16 vr_id;
 	u8 mac[ETH_ALEN];
 } __packed __aligned(4);
@@ -427,6 +476,13 @@ struct mvsw_msg_vr_ret {
 	u16 vr_id;
 } __packed __aligned(4);
 
+struct mvsw_msg_lag_cmd {
+	struct mvsw_msg_cmd cmd;
+	u32 port;
+	u32 dev;
+	u16 lag_id;
+} __packed __aligned(4);
+
 #define fw_check_resp(_response)	\
 ({								\
 	int __er = 0;						\
@@ -473,7 +529,10 @@ _response, _wait)						\
 struct mvsw_fw_event_handler {
 	struct list_head list;
 	enum mvsw_pr_event_type type;
-	void (*func)(struct mvsw_pr_switch *sw, struct mvsw_pr_event *evt);
+	void (*func)(struct mvsw_pr_switch *sw,
+		     struct mvsw_pr_event *evt,
+		     void *arg);
+	void *arg;
 };
 
 static int fw_parse_port_evt(u8 *msg, struct mvsw_pr_event *evt)
@@ -494,8 +553,20 @@ static int fw_parse_fdb_evt(u8 *msg, struct mvsw_pr_event *evt)
 {
 	struct mvsw_msg_event_fdb *hw_evt = (struct mvsw_msg_event_fdb *)msg;
 
-	evt->fdb_evt.port_id	= hw_evt->port_id;
-	evt->fdb_evt.vid	= hw_evt->vid;
+	switch (hw_evt->dest_type) {
+	case MVSW_HW_FDB_ENTRY_TYPE_REG_PORT:
+		evt->fdb_evt.type = MVSW_PR_FDB_ENTRY_TYPE_REG_PORT;
+		evt->fdb_evt.dest.port_id = hw_evt->dest.port_id;
+		break;
+	case MVSW_HW_FDB_ENTRY_TYPE_LAG:
+		evt->fdb_evt.type = MVSW_PR_FDB_ENTRY_TYPE_LAG;
+		evt->fdb_evt.dest.lag_id = hw_evt->dest.lag_id;
+		break;
+	default:
+		return -EINVAL;
+	}
+
+	evt->fdb_evt.vid = hw_evt->vid;
 
 	memcpy(&evt->fdb_evt.data, &hw_evt->param, sizeof(u8) * ETH_ALEN);
 
@@ -536,36 +607,65 @@ __find_event_handler(const struct mvsw_pr_switch *sw,
 	return NULL;
 }
 
+static int mvsw_find_event_handler(const struct mvsw_pr_switch *sw,
+				   enum mvsw_pr_event_type type,
+				   struct mvsw_fw_event_handler *eh)
+{
+	struct mvsw_fw_event_handler *tmp;
+	int err = 0;
+
+	rcu_read_lock();
+	tmp = __find_event_handler(sw, type);
+	if (tmp)
+		*eh = *tmp;
+	else
+		err = -EEXIST;
+	rcu_read_unlock();
+
+	return err;
+}
+
 static int fw_event_recv(struct mvsw_pr_device *dev, u8 *buf, size_t size)
 {
-	void (*cb)(struct mvsw_pr_switch *sw, struct mvsw_pr_event *evt) = NULL;
 	struct mvsw_msg_event *msg = (struct mvsw_msg_event *)buf;
 	struct mvsw_pr_switch *sw = dev->priv;
-	struct mvsw_fw_event_handler *eh;
+	struct mvsw_fw_event_handler eh;
 	struct mvsw_pr_event evt;
 	int err;
 
 	if (msg->type >= MVSW_EVENT_TYPE_MAX)
 		return -EINVAL;
 
-	rcu_read_lock();
-	eh = __find_event_handler(sw, msg->type);
-	if (eh)
-		cb = eh->func;
-	rcu_read_unlock();
+	err = mvsw_find_event_handler(sw, msg->type, &eh);
 
-	if (!cb || !fw_event_parsers[msg->type].func)
+	if (err || !fw_event_parsers[msg->type].func)
 		return 0;
 
 	evt.id = msg->id;
 
 	err = fw_event_parsers[msg->type].func(buf, &evt);
 	if (!err)
-		cb(sw, &evt);
+		eh.func(sw, &evt, eh.arg);
 
 	return err;
 }
 
+static void fw_pkt_recv(struct mvsw_pr_device *dev)
+{
+	struct mvsw_pr_switch *sw = dev->priv;
+	struct mvsw_fw_event_handler eh;
+	struct mvsw_pr_event ev;
+	int err;
+
+	ev.id = MVSW_RXTX_EVENT_RCV_PKT;
+
+	err = mvsw_find_event_handler(sw, MVSW_EVENT_TYPE_RXTX, &eh);
+	if (err)
+		return;
+
+	eh.func(sw, &ev, eh.arg);
+}
+
 static struct mvsw_msg_buff *mvsw_msg_buff_create(u16 head_size)
 {
 	struct mvsw_msg_buff *msg_buff;
@@ -709,8 +809,10 @@ int mvsw_pr_hw_switch_init(struct mvsw_pr_switch *sw)
 	sw->port_count = resp.port_count;
 	sw->mtu_min = MVSW_PR_MIN_MTU;
 	sw->mtu_max = resp.mtu_max;
+	sw->lag_max = resp.lag_max;
+	sw->lag_member_max = resp.lag_member_max;
 	sw->dev->recv_msg = fw_event_recv;
-	memcpy(sw->base_mac, resp.mac, ETH_ALEN);
+	sw->dev->recv_pkt = fw_pkt_recv;
 
 	return err;
 }
@@ -719,10 +821,22 @@ int mvsw_pr_hw_switch_ageing_set(const struct mvsw_pr_switch *sw,
 				 u32 ageing_time)
 {
 	struct mvsw_msg_switch_attr_cmd req = {
-		.param = {.ageing_timeout = ageing_time}
+		.param = {.ageing_timeout = ageing_time},
+		.attr = MVSW_MSG_SWITCH_ATTR_AGEING,
+	};
+
+	return fw_send_req(sw, MVSW_MSG_TYPE_SWITCH_ATTR_SET, &req);
+}
+
+int mvsw_pr_hw_switch_mac_set(const struct mvsw_pr_switch *sw, const u8 *mac)
+{
+	struct mvsw_msg_switch_attr_cmd req = {
+		.attr = MVSW_MSG_SWITCH_ATTR_MAC,
 	};
 
-	return fw_send_req(sw, MVSW_MSG_TYPE_AGEING_TIMEOUT_SET, &req);
+	memcpy(req.param.mac, mac, sizeof(req.param.mac));
+
+	return fw_send_req(sw, MVSW_MSG_TYPE_SWITCH_ATTR_SET, &req);
 }
 
 int mvsw_pr_hw_port_state_set(const struct mvsw_pr_port *port,
@@ -861,7 +975,9 @@ int mvsw_pr_hw_port_learning_set(const struct mvsw_pr_port *port, bool enable)
 int mvsw_pr_hw_event_handler_register(struct mvsw_pr_switch *sw,
 				      enum mvsw_pr_event_type type,
 				      void (*cb)(struct mvsw_pr_switch *sw,
-						 struct mvsw_pr_event *evt))
+						 struct mvsw_pr_event *evt,
+						 void *arg),
+				      void *arg)
 {
 	struct mvsw_fw_event_handler *eh;
 
@@ -874,6 +990,7 @@ int mvsw_pr_hw_event_handler_register(struct mvsw_pr_switch *sw,
 
 	eh->type = type;
 	eh->func = cb;
+	eh->arg = arg;
 
 	INIT_LIST_HEAD(&eh->list);
 
@@ -883,9 +1000,7 @@ int mvsw_pr_hw_event_handler_register(struct mvsw_pr_switch *sw,
 }
 
 void mvsw_pr_hw_event_handler_unregister(struct mvsw_pr_switch *sw,
-					 enum mvsw_pr_event_type type,
-					 void (*cb)(struct mvsw_pr_switch *sw,
-						    struct mvsw_pr_event *evt))
+					 enum mvsw_pr_event_type type)
 {
 	struct mvsw_fw_event_handler *eh;
 
@@ -941,6 +1056,18 @@ int mvsw_pr_hw_vlan_port_vid_set(const struct mvsw_pr_port *port, u16 vid)
 	return fw_send_req(port->sw, MVSW_MSG_TYPE_VLAN_PVID_SET, &req);
 }
 
+int mvsw_pr_hw_port_vid_stp_set(struct mvsw_pr_port *port, u16 vid, u8 state)
+{
+	struct mvsw_msg_stp_cmd req = {
+		.port = port->hw_id,
+		.dev = port->dev_id,
+		.vid = vid,
+		.state = state
+	};
+
+	return fw_send_req(port->sw, MVSW_MSG_TYPE_STP_PORT_SET, &req);
+}
+
 int mvsw_pr_hw_port_speed_get(const struct mvsw_pr_port *port, u32 *speed)
 {
 	struct mvsw_msg_port_attr_ret resp;
@@ -977,8 +1104,11 @@ int mvsw_pr_hw_fdb_add(const struct mvsw_pr_port *port,
 		       const unsigned char *mac, u16 vid, bool dynamic)
 {
 	struct mvsw_msg_fdb_cmd req = {
-		.port = port->hw_id,
-		.dev = port->dev_id,
+		.dest_type = MVSW_HW_FDB_ENTRY_TYPE_REG_PORT,
+		.dest = {
+			.dev = port->dev_id,
+			.port = port->hw_id,
+		},
 		.vid = vid,
 		.dynamic = dynamic ? 1 : 0
 	};
@@ -988,12 +1118,30 @@ int mvsw_pr_hw_fdb_add(const struct mvsw_pr_port *port,
 	return fw_send_req(port->sw, MVSW_MSG_TYPE_FDB_ADD, &req);
 }
 
+int mvsw_pr_hw_lag_fdb_add(const struct mvsw_pr_switch *sw, u16 lag_id,
+			   const unsigned char *mac, u16 vid, bool dynamic)
+{
+	struct mvsw_msg_fdb_cmd req = {
+		.dest_type = MVSW_HW_FDB_ENTRY_TYPE_LAG,
+		.dest = { .lag_id = lag_id },
+		.vid = vid,
+		.dynamic = dynamic
+	};
+
+	memcpy(req.mac, mac, sizeof(req.mac));
+
+	return fw_send_req(sw, MVSW_MSG_TYPE_FDB_ADD, &req);
+}
+
 int mvsw_pr_hw_fdb_del(const struct mvsw_pr_port *port,
 		       const unsigned char *mac, u16 vid)
 {
 	struct mvsw_msg_fdb_cmd req = {
-		.port = port->hw_id,
-		.dev = port->dev_id,
+		.dest_type = MVSW_HW_FDB_ENTRY_TYPE_REG_PORT,
+		.dest = {
+			.dev = port->dev_id,
+			.port = port->hw_id,
+		},
 		.vid = vid
 	};
 
@@ -1002,6 +1150,20 @@ int mvsw_pr_hw_fdb_del(const struct mvsw_pr_port *port,
 	return fw_send_req(port->sw, MVSW_MSG_TYPE_FDB_DELETE, &req);
 }
 
+int mvsw_pr_hw_lag_fdb_del(const struct mvsw_pr_switch *sw, u16 lag_id,
+			   const unsigned char *mac, u16 vid)
+{
+	struct mvsw_msg_fdb_cmd req = {
+		.dest_type = MVSW_HW_FDB_ENTRY_TYPE_LAG,
+		.dest = { .lag_id = lag_id },
+		.vid = vid
+	};
+
+	memcpy(req.mac, mac, sizeof(req.mac));
+
+	return fw_send_req(sw, MVSW_MSG_TYPE_FDB_DELETE, &req);
+}
+
 int mvsw_pr_hw_port_cap_get(const struct mvsw_pr_port *port,
 			    struct mvsw_pr_port_caps *caps)
 {
@@ -1311,17 +1473,58 @@ int mvsw_pr_hw_bridge_port_delete(const struct mvsw_pr_port *port,
 	return fw_send_req(port->sw, MVSW_MSG_TYPE_BRIDGE_PORT_DELETE, &req);
 }
 
+int mvsw_pr_hw_macvlan_add(const struct mvsw_pr_switch *sw, u16 vr_id,
+			   const u8 *mac, u16 vid)
+{
+	struct mvsw_msg_macvlan_cmd req = {
+		.vr_id = vr_id,
+		.vid = vid
+	};
+
+	memcpy(req.mac, mac, ETH_ALEN);
+
+	return fw_send_req(sw, MVSW_MSG_TYPE_FDB_MACVLAN_ADD, &req);
+}
+
+int mvsw_pr_hw_macvlan_del(const struct mvsw_pr_switch *sw, u16 vr_id,
+			   const u8 *mac, u16 vid)
+{
+	struct mvsw_msg_macvlan_cmd req = {
+		.vr_id = vr_id,
+		.vid = vid
+	};
+
+	memcpy(req.mac, mac, ETH_ALEN);
+
+	return fw_send_req(sw, MVSW_MSG_TYPE_FDB_MACVLAN_DEL, &req);
+}
+
 int mvsw_pr_hw_fdb_flush_port(const struct mvsw_pr_port *port, u32 mode)
 {
 	struct mvsw_msg_fdb_cmd req = {
-		.port = port->hw_id,
-		.dev = port->dev_id,
+		.dest_type = MVSW_HW_FDB_ENTRY_TYPE_REG_PORT,
+		.dest = {
+			.dev = port->dev_id,
+			.port = port->hw_id,
+		},
 		.flush_mode = mode,
 	};
 
 	return fw_send_req(port->sw, MVSW_MSG_TYPE_FDB_FLUSH_PORT, &req);
 }
 
+int mvsw_pr_hw_fdb_flush_lag(const struct mvsw_pr_switch *sw, u16 lag_id,
+			     u32 mode)
+{
+	struct mvsw_msg_fdb_cmd req = {
+		.dest_type = MVSW_HW_FDB_ENTRY_TYPE_LAG,
+		.dest = { .lag_id = lag_id },
+		.flush_mode = mode,
+	};
+
+	return fw_send_req(sw, MVSW_MSG_TYPE_FDB_FLUSH_PORT, &req);
+}
+
 int mvsw_pr_hw_fdb_flush_vlan(const struct mvsw_pr_switch *sw, u16 vid,
 			      u32 mode)
 {
@@ -1337,8 +1540,11 @@ int mvsw_pr_hw_fdb_flush_port_vlan(const struct mvsw_pr_port *port, u16 vid,
 				   u32 mode)
 {
 	struct mvsw_msg_fdb_cmd req = {
-		.port = port->hw_id,
-		.dev = port->dev_id,
+		.dest_type = MVSW_HW_FDB_ENTRY_TYPE_REG_PORT,
+		.dest = {
+			.dev = port->dev_id,
+			.port = port->hw_id,
+		},
 		.vid = vid,
 		.flush_mode = mode,
 	};
@@ -1346,6 +1552,19 @@ int mvsw_pr_hw_fdb_flush_port_vlan(const struct mvsw_pr_port *port, u16 vid,
 	return fw_send_req(port->sw, MVSW_MSG_TYPE_FDB_FLUSH_PORT_VLAN, &req);
 }
 
+int mvsw_pr_hw_fdb_flush_lag_vlan(const struct mvsw_pr_switch *sw,
+				  u16 lag_id, u16 vid, u32 mode)
+{
+	struct mvsw_msg_fdb_cmd req = {
+		.dest_type = MVSW_HW_FDB_ENTRY_TYPE_LAG,
+		.dest = { .lag_id = lag_id },
+		.vid = vid,
+		.flush_mode = mode,
+	};
+
+	return fw_send_req(sw, MVSW_MSG_TYPE_FDB_FLUSH_PORT_VLAN, &req);
+}
+
 int mvsw_pr_hw_port_link_mode_get(const struct mvsw_pr_port *port,
 				  u32 *mode)
 {
@@ -1380,64 +1599,44 @@ int mvsw_pr_hw_port_link_mode_set(const struct mvsw_pr_port *port,
 	return fw_send_req(port->sw, MVSW_MSG_TYPE_PORT_ATTR_SET, &req);
 }
 
-int mvsw_pr_hw_rif_port_create(const struct mvsw_pr_port *port,
-			       u16 vr_id, u8 *mac, u16 *rif_id)
+static int mvsw_pr_iface_to_msg(struct mvsw_pr_iface *iface,
+				struct mvsw_msg_iface *msg_if)
 {
-	struct mvsw_msg_rif_ret resp;
-	struct mvsw_msg_rif_cmd req = {
-		.vr_id = vr_id,
-		.port = port->hw_id,
-		.dev = port->dev_id,
-		.mtu = port->net_dev->mtu,
-	};
-	int err;
-
-	memcpy(req.mac, port->net_dev->dev_addr, ETH_ALEN);
-
-	err = fw_send_req_resp(port->sw, MVSW_MSG_TYPE_RIF_PORT_CREATE,
-			       &req, &resp);
-	if (err)
-		return err;
+	switch (iface->type) {
+	case MVSW_IF_PORT_E:
+	case MVSW_IF_VID_E:
+		msg_if->port = iface->dev_port.port_num;
+		msg_if->dev = iface->dev_port.hw_dev_num;
+		break;
+	case MVSW_IF_LAG_E:
+		msg_if->lag_id = iface->lag_id;
+		break;
+	default:
+		return -ENOTSUPP;
+	}
 
-	*rif_id = resp.rif_id;
-	return err;
+	msg_if->vid = iface->vlan_id;
+	msg_if->type = iface->type;
+	return 0;
 }
 
-int mvsw_pr_hw_rif_vlan_create(const struct mvsw_pr_switch *sw,
-			       u16 vr_id, u8 *mac, u16 vid, u16 *rif_id)
+int mvsw_pr_hw_rif_create(const struct mvsw_pr_switch *sw,
+			  struct mvsw_pr_iface *iif, u16 vr_id, u8 *mac,
+			  u16 *rif_id)
 {
-	struct mvsw_msg_rif_ret resp;
 	struct mvsw_msg_rif_cmd req = {
 		.vr_id = vr_id,
-		.bridge = vid,
 	};
+	struct mvsw_msg_rif_ret resp;
 	int err;
 
 	memcpy(req.mac, mac, ETH_ALEN);
 
-	err = fw_send_req_resp(sw, MVSW_MSG_TYPE_RIF_VLAN_CREATE,
-			       &req, &resp);
+	err = mvsw_pr_iface_to_msg(iif, &req.iif);
 	if (err)
 		return err;
 
-	*rif_id = resp.rif_id;
-	return err;
-}
-
-int mvsw_pr_hw_rif_bridge_create(const struct mvsw_pr_switch *sw,
-				 u16 vr_id,
-				 u8 *mac, u16 bridge_id, u16 *rif_id)
-{
-	struct mvsw_msg_rif_ret resp;
-	struct mvsw_msg_rif_cmd req = {
-		.vr_id = vr_id,
-		.bridge = bridge_id,
-	};
-	int err;
-
-	memcpy(req.mac, mac, ETH_ALEN);
-
-	err = fw_send_req_resp(sw, MVSW_MSG_TYPE_RIF_BRIDGE_CREATE,
+	err = fw_send_req_resp(sw, MVSW_MSG_TYPE_ROUTER_RIF_CREATE,
 			       &req, &resp);
 	if (err)
 		return err;
@@ -1446,36 +1645,38 @@ int mvsw_pr_hw_rif_bridge_create(const struct mvsw_pr_switch *sw,
 	return err;
 }
 
-int mvsw_pr_hw_rif_delete(const struct mvsw_pr_switch *sw, u16 vr_id,
-			  u16 rif_id, struct mvsw_pr_port *port)
+int mvsw_pr_hw_rif_delete(const struct mvsw_pr_switch *sw, u16 rif_id,
+			  struct mvsw_pr_iface *iif, u16 vr_id)
 {
 	struct mvsw_msg_rif_cmd req = {
 		.rif_id = rif_id,
 		.vr_id = vr_id,
 	};
+	int err;
 
-	if (port) {
-		req.port = port->hw_id;
-		req.dev = port->dev_id;
-	}
+	err = mvsw_pr_iface_to_msg(iif, &req.iif);
+	if (err)
+		return err;
 
-	return fw_send_req(sw, MVSW_MSG_TYPE_RIF_DELETE, &req);
+	return fw_send_req(sw, MVSW_MSG_TYPE_ROUTER_RIF_DELETE, &req);
 }
 
-int mvsw_pr_hw_rif_set(const struct mvsw_pr_switch *sw,
-		       u16 *rif_id, u32 mtu, u8 *mac)
+int mvsw_pr_hw_rif_set(const struct mvsw_pr_switch *sw, u16 *rif_id,
+		       struct mvsw_pr_iface *iif, u8 *mac)
 {
 	struct mvsw_msg_rif_ret resp;
 	struct mvsw_msg_rif_cmd req = {
 		.rif_id = *rif_id,
-		.mtu = mtu,
 	};
 	int err;
 
-	if (mac)
-		memcpy(req.mac, mac, ETH_ALEN);
+	memcpy(req.mac, mac, ETH_ALEN);
 
-	err = fw_send_req_resp(sw, MVSW_MSG_TYPE_RIF_SET, &req, &resp);
+	err = mvsw_pr_iface_to_msg(iif, &req.iif);
+	if (err)
+		return err;
+
+	err = fw_send_req_resp(sw, MVSW_MSG_TYPE_ROUTER_RIF_SET, &req, &resp);
 	if (err)
 		return err;
 
@@ -1489,7 +1690,7 @@ int mvsw_pr_hw_vr_create(const struct mvsw_pr_switch *sw, u16 *vr_id)
 	struct mvsw_msg_vr_ret resp;
 	struct mvsw_msg_vr_cmd req;
 
-	err = fw_send_req_resp(sw, MVSW_MSG_TYPE_VR_CREATE, &req, &resp);
+	err = fw_send_req_resp(sw, MVSW_MSG_TYPE_ROUTER_VR_CREATE, &req, &resp);
 	if (err)
 		return err;
 
@@ -1503,7 +1704,7 @@ int mvsw_pr_hw_vr_delete(const struct mvsw_pr_switch *sw, u16 vr_id)
 		.vr_id = vr_id,
 	};
 
-	return fw_send_req(sw, MVSW_MSG_TYPE_VR_DELETE, &req);
+	return fw_send_req(sw, MVSW_MSG_TYPE_ROUTER_VR_DELETE, &req);
 }
 
 int mvsw_pr_hw_lpm_update(const struct mvsw_pr_switch *sw, u16 vr_id,
@@ -1516,7 +1717,7 @@ int mvsw_pr_hw_lpm_update(const struct mvsw_pr_switch *sw, u16 vr_id,
 		.vr_id = vr_id
 	};
 
-	return fw_send_req(sw, MVSW_MSG_TYPE_LPM_UPDATE, &req);
+	return fw_send_req(sw, MVSW_MSG_TYPE_ROUTER_LPM_UPDATE, &req);
 }
 
 int mvsw_pr_hw_lpm_del(const struct mvsw_pr_switch *sw, u16 vr_id, __be32 dst,
@@ -1528,7 +1729,7 @@ int mvsw_pr_hw_lpm_del(const struct mvsw_pr_switch *sw, u16 vr_id, __be32 dst,
 		.vr_id = vr_id
 	};
 
-	return fw_send_req(sw, MVSW_MSG_TYPE_LPM_DELETE, &req);
+	return fw_send_req(sw, MVSW_MSG_TYPE_ROUTER_LPM_DELETE, &req);
 }
 
 int mvsw_pr_hw_nh_entry_add(const struct mvsw_pr_switch *sw, u16 vr_id, u16 vid,
@@ -1536,15 +1737,15 @@ int mvsw_pr_hw_nh_entry_add(const struct mvsw_pr_switch *sw, u16 vr_id, u16 vid,
 {
 	struct mvsw_msg_nh_ret resp;
 	struct mvsw_msg_nh_cmd req = {
+		.oif = { .vid = vid },
 		.dst = dst,
-		.vid = vid,
 		.vr_id = vr_id,
 	};
 	int err;
 
 	memcpy(req.mac, mac, ETH_ALEN);
 
-	err = fw_send_req_resp(sw, MVSW_MSG_TYPE_NH_ADD, &req, &resp);
+	err = fw_send_req_resp(sw, MVSW_MSG_TYPE_ROUTER_NH_ADD, &req, &resp);
 
 	if (err)
 		return err;
@@ -1564,26 +1765,29 @@ int mvsw_pr_hw_nh_entry_del(const struct mvsw_pr_switch *sw, u16 vr_id,
 
 	memcpy(req.mac, mac, ETH_ALEN);
 
-	return fw_send_req(sw, MVSW_MSG_TYPE_NH_DELETE, &req);
+	return fw_send_req(sw, MVSW_MSG_TYPE_ROUTER_NH_DELETE, &req);
 }
 
-int mvsw_pr_hw_nh_entry_set(const struct mvsw_pr_port *port, u16 vr_id, u16 vid,
+int mvsw_pr_hw_nh_entry_set(struct mvsw_pr_switch *sw,
+			    struct mvsw_pr_iface *oif, u16 vr_id,
 			    __be32 dst, u32 dst_len, u8 *mac, u32 *hw_id)
 {
 	struct mvsw_msg_nh_ret resp;
 	struct mvsw_msg_nh_cmd req = {
-		.port = port->hw_id,
-		.dev = port->dev_id,
 		.dst = dst,
 		.dst_len = dst_len,
 		.vr_id = vr_id,
-		.vid = vid
 	};
 	int err;
 
 	memcpy(req.mac, mac, ETH_ALEN);
 
-	err = fw_send_req_resp(port->sw, MVSW_MSG_TYPE_NH_SET, &req, &resp);
+	err = mvsw_pr_iface_to_msg(oif, &req.oif);
+	if (err)
+		return err;
+
+	err = fw_send_req_resp(sw, MVSW_MSG_TYPE_ROUTER_NH_SET,
+			       &req, &resp);
 	if (err)
 		return err;
 
@@ -1599,7 +1803,7 @@ int mvsw_pr_hw_nh_get(const struct mvsw_pr_switch *sw, u32 hw_id, u8 *is_active)
 	struct mvsw_msg_nh_ret resp;
 	int err;
 
-	err = fw_send_req_resp(sw, MVSW_MSG_TYPE_NH_GET, &req, &resp);
+	err = fw_send_req_resp(sw, MVSW_MSG_TYPE_ROUTER_NH_GET, &req, &resp);
 	if (err)
 		return err;
 
@@ -1855,3 +2059,39 @@ int mvsw_pr_hw_acl_port_unbind(const struct mvsw_pr_port *port, u16 ruleset_id)
 
 	return fw_send_req(port->sw, MVSW_MSG_TYPE_ACL_PORT_UNBIND, &req);
 }
+
+int mvsw_pr_hw_lag_member_add(struct mvsw_pr_port *port, u16 lag_id)
+{
+	struct mvsw_msg_lag_cmd req = {
+		.port = port->hw_id,
+		.dev = port->dev_id,
+		.lag_id = lag_id
+	};
+
+	return fw_send_req(port->sw, MVSW_MSG_TYPE_LAG_ADD, &req);
+}
+
+int mvsw_pr_hw_lag_member_del(struct mvsw_pr_port *port, u16 lag_id)
+{
+	struct mvsw_msg_lag_cmd req = {
+		.port = port->hw_id,
+		.dev = port->dev_id,
+		.lag_id = lag_id
+	};
+
+	return fw_send_req(port->sw, MVSW_MSG_TYPE_LAG_DELETE, &req);
+}
+
+int mvsw_pr_hw_lag_member_enable(struct mvsw_pr_port *port, u16 lag_id,
+				 bool enable)
+{
+	u32 cmd;
+	struct mvsw_msg_lag_cmd req = {
+		.port = port->hw_id,
+		.dev = port->dev_id,
+		.lag_id = lag_id
+	};
+
+	cmd = enable ? MVSW_MSG_TYPE_LAG_ENABLE : MVSW_MSG_TYPE_LAG_DISABLE;
+	return fw_send_req(port->sw, cmd, &req);
+}
diff --git a/drivers/net/ethernet/marvell/prestera_sw/prestera_hw.h b/drivers/net/ethernet/marvell/prestera_sw/prestera_hw.h
index 5300c21..e667570 100644
--- a/drivers/net/ethernet/marvell/prestera_sw/prestera_hw.h
+++ b/drivers/net/ethernet/marvell/prestera_sw/prestera_hw.h
@@ -74,11 +74,19 @@ enum {
 	MVSW_PORT_DUPLEX_FULL
 };
 
+enum {
+	MVSW_STP_DISABLED,
+	MVSW_STP_BLOCK_LISTEN,
+	MVSW_STP_LEARN,
+	MVSW_STP_FORWARD
+};
+
 struct mvsw_pr_switch;
 struct mvsw_pr_port;
 struct mvsw_pr_port_stats;
 struct mvsw_pr_port_caps;
 struct mvsw_pr_acl_rule;
+struct mvsw_pr_iface;
 
 enum mvsw_pr_event_type;
 struct mvsw_pr_event;
@@ -87,6 +95,7 @@ struct mvsw_pr_event;
 int mvsw_pr_hw_switch_init(struct mvsw_pr_switch *sw);
 int mvsw_pr_hw_switch_ageing_set(const struct mvsw_pr_switch *sw,
 				 u32 ageing_time);
+int mvsw_pr_hw_switch_mac_set(const struct mvsw_pr_switch *sw, const u8 *mac);
 
 /* Port API */
 int mvsw_pr_hw_port_info_get(const struct mvsw_pr_port *port,
@@ -144,6 +153,10 @@ int mvsw_pr_hw_fdb_flush_vlan(const struct mvsw_pr_switch *sw, u16 vid,
 			      u32 mode);
 int mvsw_pr_hw_fdb_flush_port_vlan(const struct mvsw_pr_port *port, u16 vid,
 				   u32 mode);
+int mvsw_pr_hw_macvlan_add(const struct mvsw_pr_switch *sw, u16 vr_id,
+			   const u8 *mac, u16 vid);
+int mvsw_pr_hw_macvlan_del(const struct mvsw_pr_switch *sw, u16 vr_id,
+			   const u8 *mac, u16 vid);
 
 /* Bridge API */
 int mvsw_pr_hw_bridge_create(const struct mvsw_pr_switch *sw, u16 *bridge_id);
@@ -152,6 +165,9 @@ int mvsw_pr_hw_bridge_port_add(const struct mvsw_pr_port *port, u16 bridge_id);
 int mvsw_pr_hw_bridge_port_delete(const struct mvsw_pr_port *port,
 				  u16 bridge_id);
 
+/* STP API */
+int mvsw_pr_hw_port_vid_stp_set(struct mvsw_pr_port *port, u16 vid, u8 state);
+
 /* ACL API */
 int mvsw_pr_hw_acl_ruleset_create(const struct mvsw_pr_switch *sw,
 				  u16 *ruleset_id);
@@ -167,17 +183,13 @@ int mvsw_pr_hw_acl_port_bind(const struct mvsw_pr_port *port, u16 ruleset_id);
 int mvsw_pr_hw_acl_port_unbind(const struct mvsw_pr_port *port, u16 ruleset_id);
 
 /* Router API */
-int mvsw_pr_hw_rif_port_create(const struct mvsw_pr_port *port,
-			       u16 vr_id, u8 *mac, u16 *rif_id);
-int mvsw_pr_hw_rif_vlan_create(const struct mvsw_pr_switch *sw,
-			       u16 vr_id, u8 *mac, u16 vid, u16 *rif_id);
-int mvsw_pr_hw_rif_bridge_create(const struct mvsw_pr_switch *sw,
-				 u16 vr_id,
-				 u8 *mac, u16 bridge_id, u16 *rif_id);
-int mvsw_pr_hw_rif_delete(const struct mvsw_pr_switch *sw, u16 vr_id,
-			  u16 rif_id, struct mvsw_pr_port *port);
-int mvsw_pr_hw_rif_set(const struct mvsw_pr_switch *sw,
-		       u16 *rif_id, u32 mtu, u8 *mac);
+int mvsw_pr_hw_rif_create(const struct mvsw_pr_switch *sw,
+			  struct mvsw_pr_iface *iif, u16 vr_id, u8 *mac,
+			  u16 *rif_id);
+int mvsw_pr_hw_rif_delete(const struct mvsw_pr_switch *sw, u16 rif_id,
+			  struct mvsw_pr_iface *iif, u16 vr_id);
+int mvsw_pr_hw_rif_set(const struct mvsw_pr_switch *sw, u16 *rif_id,
+		       struct mvsw_pr_iface *iif, u8 *mac);
 
 /* Virtual Router API */
 int mvsw_pr_hw_vr_create(const struct mvsw_pr_switch *sw, u16 *vr_id);
@@ -194,20 +206,37 @@ int mvsw_pr_hw_nh_entry_add(const struct mvsw_pr_switch *sw, u16 vr_id, u16 vid,
 			    __be32 dst, u8 *mac, u32 *hw_id);
 int mvsw_pr_hw_nh_entry_del(const struct mvsw_pr_switch *sw, u16 vr_id,
 			    __be32 dst, u32 dst_len, u8 *mac);
-int mvsw_pr_hw_nh_entry_set(const struct mvsw_pr_port *port, u16 vr_id, u16 vid,
+int mvsw_pr_hw_nh_entry_set(struct mvsw_pr_switch *sw,
+			    struct mvsw_pr_iface *iif, u16 vr_id,
 			    __be32 dst, u32 dst_len, u8 *mac, u32 *hw_id);
 int mvsw_pr_hw_nh_get(const struct mvsw_pr_switch *sw, u32 hw_id,
 		      u8 *is_valid);
 
+/* LAG API */
+int mvsw_pr_hw_lag_member_add(struct mvsw_pr_port *port, u16 lag_id);
+int mvsw_pr_hw_lag_member_del(struct mvsw_pr_port *port, u16 lag_id);
+int mvsw_pr_hw_lag_member_enable(struct mvsw_pr_port *port, u16 lag_id,
+				 bool enable);
+int mvsw_pr_hw_lag_fdb_add(const struct mvsw_pr_switch *sw, u16 lag_id,
+			   const unsigned char *mac, u16 vid, bool dynamic);
+int mvsw_pr_hw_lag_fdb_del(const struct mvsw_pr_switch *sw, u16 lag_id,
+			   const unsigned char *mac, u16 vid);
+int mvsw_pr_hw_fdb_flush_lag(const struct mvsw_pr_switch *sw, u16 lag_id,
+			     u32 mode);
+int mvsw_pr_hw_fdb_flush_lag_vlan(const struct mvsw_pr_switch *sw,
+				  u16 lag_id, u16 vid, u32 mode);
+
 /* Event handlers */
 int mvsw_pr_hw_event_handler_register(struct mvsw_pr_switch *sw,
 				      enum mvsw_pr_event_type type,
 				      void (*cb)(struct mvsw_pr_switch *sw,
-						 struct mvsw_pr_event *evt));
+						 struct mvsw_pr_event *evt,
+						 void *arg),
+				      void *arg);
+
 void mvsw_pr_hw_event_handler_unregister(struct mvsw_pr_switch *sw,
-					 enum mvsw_pr_event_type type,
-					 void (*cb)(struct mvsw_pr_switch *sw,
-						    struct mvsw_pr_event *evt));
+					 enum mvsw_pr_event_type type);
+
 /* SW Dev Log API */
 int mvsw_pr_hw_fw_log_level_set(const struct mvsw_pr_switch *sw,
 				u32 lib_name, u32 log_level);
diff --git a/drivers/net/ethernet/marvell/prestera_sw/prestera_pci.c b/drivers/net/ethernet/marvell/prestera_sw/prestera_pci.c
index e391187..d4d0e3f 100644
--- a/drivers/net/ethernet/marvell/prestera_sw/prestera_pci.c
+++ b/drivers/net/ethernet/marvell/prestera_sw/prestera_pci.c
@@ -142,6 +142,7 @@ struct mvsw_pr_fw_regs {
 	u32 cmd_rcv_len;
 
 	u32 fw_status;
+	u32 rx_status;
 
 	struct mvsw_pr_fw_evtq_regs evtq_list[MVSW_EVT_QNUM_MAX];
 };
@@ -164,6 +165,7 @@ struct mvsw_pr_fw_regs {
 #define MVSW_CMD_RCV_CTL_REG		MVSW_FW_REG_OFFSET(cmd_rcv_ctl)
 #define MVSW_CMD_RCV_LEN_REG		MVSW_FW_REG_OFFSET(cmd_rcv_len)
 #define MVSW_FW_STATUS_REG		MVSW_FW_REG_OFFSET(fw_status)
+#define MVSW_RX_STATUS_REG		MVSW_FW_REG_OFFSET(rx_status)
 
 /* MVSW_CMD_REQ_CTL_REG flags */
 #define MVSW_CMD_F_REQ_SENT		BIT(0)
@@ -480,6 +482,13 @@ static irqreturn_t mvsw_pci_irq_handler(int irq, void *dev_id)
 {
 	struct mvsw_pr_fw *fw = dev_id;
 
+	if (mvsw_fw_read(fw, MVSW_RX_STATUS_REG)) {
+		if (fw->dev.recv_pkt) {
+			mvsw_fw_write(fw, MVSW_RX_STATUS_REG, 0);
+			fw->dev.recv_pkt(&fw->dev);
+		}
+	}
+
 	queue_work(fw->wq, &fw->evt_work);
 
 	return IRQ_HANDLED;
diff --git a/drivers/net/ethernet/marvell/prestera_sw/prestera_router.c b/drivers/net/ethernet/marvell/prestera_sw/prestera_router.c
index 574e11b..902cfb9 100644
--- a/drivers/net/ethernet/marvell/prestera_sw/prestera_router.c
+++ b/drivers/net/ethernet/marvell/prestera_sw/prestera_router.c
@@ -44,8 +44,8 @@ struct mvsw_pr_router {
 
 struct mvsw_pr_rif {
 	struct net_device *dev;
+	struct mvsw_pr_iface iface;
 	struct list_head router_node;
-	enum mvsw_pr_rif_type type;
 	unsigned char addr[ETH_ALEN];
 	unsigned int mtu;
 	u16 rif_id;
@@ -86,6 +86,7 @@ struct mvsw_pr_vr {
 struct mvsw_pr_nh {
 	struct list_head nh_node;
 	struct neighbour *n;
+	struct mvsw_pr_iface iface;
 	bool connected, has_gw;
 	u8 ha[ETH_ALEN];
 	__be32 gw_ip, n_ip;
@@ -105,6 +106,8 @@ static struct mvsw_pr_vr *mvsw_pr_vr_get(struct mvsw_pr_switch *sw, u32 tb_id,
 					 struct netlink_ext_ack *extack);
 static u16 mvsw_pr_nh_dev_to_vr_id(struct mvsw_pr_switch *sw,
 				   struct net_device *dev);
+static void mvsw_pr_vr_destroy(struct mvsw_pr_switch *sw,
+			       struct mvsw_pr_vr *vr);
 static u32 mvsw_pr_fix_tb_id(u32 tb_id);
 static void mvsw_pr_vr_put(struct mvsw_pr_switch *sw, struct mvsw_pr_vr *vr);
 static struct mvsw_pr_vr *mvsw_pr_vr_find(struct mvsw_pr_switch *sw, u32 tb_id);
@@ -116,24 +119,7 @@ static struct mvsw_pr_rif *mvsw_pr_rif_create(struct mvsw_pr_switch *sw,
 					      *params,
 					      struct netlink_ext_ack *extack);
 static void mvsw_pr_rif_destroy(struct mvsw_pr_rif *rif);
-static int mvsw_pr_rif_edit(struct mvsw_pr_switch *sw, u16 *rif_id, char *mac,
-			    int mtu);
-
-static enum mvsw_pr_rif_type mvsw_pr_dev_rif_type(const struct net_device *dev)
-{
-	enum mvsw_pr_rif_type type;
-
-	if (is_vlan_dev(dev) && netif_is_bridge_master(vlan_dev_real_dev(dev)))
-		type = MVSW_PR_RIF_TYPE_VLAN;
-	else if (netif_is_bridge_master(dev) && br_vlan_enabled(dev))
-		type = MVSW_PR_RIF_TYPE_VLAN;
-	else if (netif_is_bridge_master(dev))
-		type = MVSW_PR_RIF_TYPE_BRIDGE;
-	else
-		type = MVSW_PR_RIF_TYPE_PORT;
-
-	return type;
-}
+static int mvsw_pr_rif_update(struct mvsw_pr_rif *rif, char *mac);
 
 static struct mvsw_pr_nh*
 mvsw_pr_nh_neigh_find(const struct mvsw_pr_switch *sw,
@@ -368,6 +354,7 @@ static u16 mvsw_pr_nh_dev_to_vr_id(struct mvsw_pr_switch *sw,
 static u16 mvsw_pr_nh_dev_to_vid(struct mvsw_pr_switch *sw,
 				 struct net_device *dev)
 {
+	struct macvlan_dev *vlan;
 	u16 vid = 0;
 
 	if (is_vlan_dev(dev) && netif_is_bridge_master(vlan_dev_real_dev(dev)))
@@ -376,6 +363,10 @@ static u16 mvsw_pr_nh_dev_to_vid(struct mvsw_pr_switch *sw,
 		br_vlan_get_pvid(dev, &vid);
 	else if (netif_is_bridge_master(dev))
 		vid = mvsw_pr_vlan_dev_vlan_id(sw->bridge, dev);
+	else if (netif_is_macvlan(dev)) {
+		vlan = netdev_priv(dev);
+		return mvsw_pr_nh_dev_to_vid(sw, vlan->lowerdev);
+	}
 
 	return vid;
 }
@@ -386,6 +377,7 @@ mvsw_pr_nh_dev_to_port(struct mvsw_pr_switch *sw, struct net_device *dev,
 {
 	struct net_device *bridge_dev, *port_dev = dev;
 	u16 vid = mvsw_pr_nh_dev_to_vid(sw, dev);
+	struct macvlan_dev *vlan;
 
 	if (is_vlan_dev(dev) &&
 	    netif_is_bridge_master(vlan_dev_real_dev(dev))) {
@@ -397,6 +389,9 @@ mvsw_pr_nh_dev_to_port(struct mvsw_pr_switch *sw, struct net_device *dev,
 	} else if (netif_is_bridge_master(dev)) {
 		/* vid in .1d bridge is 0 */
 		port_dev = br_fdb_find_port(dev, ha, 0);
+	} else if (netif_is_macvlan(dev)) {
+		vlan = netdev_priv(dev);
+		return mvsw_pr_nh_dev_to_port(sw, vlan->lowerdev, ha);
 	}
 
 	if (!port_dev)
@@ -412,16 +407,14 @@ static int mvsw_pr_router_gw_nh_refresh(struct mvsw_pr_switch *sw,
 	struct neighbour *n = nh->n;
 	int hw_id, err = 0;
 	u8 mac[ETH_ALEN];
-	u16 vid;
 
 	read_lock_bh(&n->lock);
 	memcpy(mac, n->ha, ETH_ALEN);
 	read_unlock_bh(&n->lock);
 
 	port = mvsw_pr_nh_dev_to_port(sw, nh->n->dev, nh->ha);
-	vid = mvsw_pr_nh_dev_to_vid(sw, n->dev);
 
-	err = mvsw_pr_nh_entry_set(port, nh->vr_id, vid,
+	err = mvsw_pr_nh_entry_set(sw, &nh->iface, nh->vr_id,
 				   nh->gw_ip, nh->dst_len,
 				   mac, &hw_id);
 	if (err)
@@ -431,28 +424,18 @@ static int mvsw_pr_router_gw_nh_refresh(struct mvsw_pr_switch *sw,
 	return err;
 }
 
-static int mvsw_pr_router_nhs_refresh(struct mvsw_pr_router *router,
-				      struct mvsw_pr_port *port,
-				      struct neighbour *n)
+static int mvsw_pr_nhs_offload(struct mvsw_pr_switch *sw, struct neighbour *n)
 {
 	struct mvsw_pr_nh *nh;
-	u8 mac[ETH_ALEN];
 	__be32 addr;
-	u16 vid;
 	int hw_id, err = 0;
 
-	read_lock_bh(&n->lock);
-	memcpy(mac, n->ha, ETH_ALEN);
-	read_unlock_bh(&n->lock);
-
-	list_for_each_entry(nh, &router->nexthop_list, nh_node) {
+	list_for_each_entry(nh, &sw->router->nexthop_list, nh_node) {
 		if (nh->n == n) {
 			addr = nh->has_gw ? nh->gw_ip : nh->n_ip;
-			vid = mvsw_pr_nh_dev_to_vid(router->sw, n->dev);
-
-			err = mvsw_pr_nh_entry_set(port, nh->vr_id, vid,
-						   addr, nh->dst_len,
-						   mac, &hw_id);
+			err = mvsw_pr_nh_entry_set(sw, &nh->iface,
+						   nh->vr_id, addr, nh->dst_len,
+						   nh->ha, &hw_id);
 			if (err)
 				return err;
 
@@ -465,14 +448,14 @@ static int mvsw_pr_router_nhs_refresh(struct mvsw_pr_router *router,
 }
 
 static void
-mvsw_pr_router_nhs_release(struct mvsw_pr_router *router,
+mvsw_pr_router_nhs_release(struct mvsw_pr_switch *sw,
 			   struct neighbour *n)
 {
 	struct mvsw_pr_nh *nh, *tmp;
 
-	list_for_each_entry_safe(nh, tmp, &router->nexthop_list, nh_node) {
+	list_for_each_entry_safe(nh, tmp, &sw->router->nexthop_list, nh_node) {
 		if (nh->n == n) {
-			mvsw_pr_nh_release(router->sw, nh);
+			mvsw_pr_nh_release(sw, nh);
 			if (!nh->has_gw)
 				mvsw_pr_nh_destroy(nh);
 		}
@@ -516,27 +499,112 @@ mvsw_pr_router_release_default_gw(struct mvsw_pr_switch *sw, u16 vr_id)
 	}
 }
 
+static int
+mvsw_pr_nh_iface_update(struct mvsw_pr_switch *sw, struct mvsw_pr_nh *nh)
+{
+	struct net_device *dev = nh->n->dev;
+	struct mvsw_pr_iface *iface = &nh->iface;
+	struct mvsw_pr_port *port;
+
+	iface->vlan_id = mvsw_pr_nh_dev_to_vid(sw, dev);
+	iface->type = mvsw_pr_dev_if_type(dev);
+	switch (iface->type) {
+	case MVSW_IF_LAG_E:
+		mvsw_pr_lag_id_find(sw, dev, &iface->lag_id);
+		break;
+	case MVSW_IF_PORT_E:
+	case MVSW_IF_VID_E:
+		port = mvsw_pr_nh_dev_to_port(sw, dev, nh->ha);
+		if (!port)
+			return -1;
+
+		iface->dev_port.hw_dev_num = port->dev_id;
+		iface->dev_port.port_num = port->hw_id;
+		break;
+	default:
+		MVSW_LOG_ERROR("Unsupported nexthop device");
+		return -1;
+	}
+
+	return 0;
+}
+
+static int
+mvsw_pr_rif_iface_init(struct mvsw_pr_rif *rif)
+{
+	struct net_device *dev = rif->dev;
+	struct mvsw_pr_switch *sw = rif->sw;
+	struct mvsw_pr_port *port;
+	int if_type;
+
+	if_type = mvsw_pr_dev_if_type(dev);
+	switch (if_type) {
+	case MVSW_IF_PORT_E:
+		port = netdev_priv(dev);
+		rif->iface.dev_port.hw_dev_num = port->dev_id;
+		rif->iface.dev_port.port_num = port->hw_id;
+		break;
+	case MVSW_IF_LAG_E:
+		mvsw_pr_lag_id_find(sw, dev, &rif->iface.lag_id);
+		break;
+	case MVSW_IF_VID_E:
+		break;
+	default:
+		pr_err("Unsupported rif type");
+		return -EINVAL;
+	}
+
+	rif->iface.type = if_type;
+	rif->iface.vlan_id = mvsw_pr_nh_dev_to_vid(sw, dev);
+
+	return 0;
+}
+
+static int
+mvsw_pr_nhs_update(struct mvsw_pr_switch *sw, struct neighbour *n)
+{
+	struct mvsw_pr_nh *nh;
+	char mac[ETH_ALEN];
+	u16 vr_id;
+	int err;
+
+	read_lock_bh(&n->lock);
+	memcpy(mac, n->ha, ETH_ALEN);
+	read_unlock_bh(&n->lock);
+
+	vr_id = mvsw_pr_nh_dev_to_vr_id(sw, n->dev);
+
+	list_for_each_entry(nh, &sw->router->nexthop_list, nh_node) {
+		if (nh->n == n) {
+			nh->vr_id = vr_id;
+			memcpy(nh->ha, mac, ETH_ALEN);
+			err = mvsw_pr_nh_iface_update(sw, nh);
+			if (err)
+				return err;
+		}
+	}
+
+	return 0;
+}
+
 static void
-mvsw_pr_router_update_resolved_nh(struct mvsw_pr_router *router,
+mvsw_pr_router_update_resolved_nh(struct mvsw_pr_switch *sw,
 				  struct mvsw_pr_nh *nh, bool is_connected)
 {
-	struct mvsw_pr_port *port;
 	int err;
 
 	if (nh && !is_connected) {
-		mvsw_pr_router_nhs_release(router, nh->n);
+		mvsw_pr_router_nhs_release(sw, nh->n);
 	} else if (nh) {
-		read_lock_bh(&nh->n->lock);
-		memcpy(nh->ha, nh->n->ha, ETH_ALEN);
-		read_unlock_bh(&nh->n->lock);
-
-		port = mvsw_pr_nh_dev_to_port(router->sw, nh->n->dev, nh->ha);
-		if (!port)
+		err = mvsw_pr_nhs_update(sw, nh->n);
+		if (err) {
+			pr_err("Failed to update neighbours");
 			return;
+		}
 
-		err = mvsw_pr_router_nhs_refresh(router, port, nh->n);
+		err = mvsw_pr_nhs_offload(sw, nh->n);
 		if (err)
-			pr_err("Failed to update neighbours");
+			pr_err("Failed to offload neighbours");
 
 		nh->connected = is_connected;
 	}
@@ -632,7 +700,7 @@ static void mvsw_pr_router_neigh_event_work(struct work_struct *work)
 		}
 	}
 
-	mvsw_pr_router_update_resolved_nh(sw->router, nh, nh_connected);
+	mvsw_pr_router_update_resolved_nh(sw, nh, nh_connected);
 
 out:
 	neigh_release(n);
@@ -717,9 +785,9 @@ static void mvsw_pr_neigh_fini(struct mvsw_pr_switch *sw)
 	cancel_delayed_work_sync(&sw->router->neighs_update.dw);
 }
 
-static struct mvsw_pr_rif *mvsw_pr_rif_find_by_dev(const struct mvsw_pr_switch
-						   *sw,
-						   const struct net_device *dev)
+static struct mvsw_pr_rif*
+mvsw_pr_rif_find(const struct mvsw_pr_switch *sw,
+		 const struct net_device *dev)
 {
 	struct mvsw_pr_rif *rif;
 
@@ -731,6 +799,12 @@ static struct mvsw_pr_rif *mvsw_pr_rif_find_by_dev(const struct mvsw_pr_switch
 	return NULL;
 }
 
+bool mvsw_pr_rif_exists(const struct mvsw_pr_switch *sw,
+			const struct net_device *dev)
+{
+	return !!mvsw_pr_rif_find(sw, dev);
+}
+
 static int
 mvsw_pr_port_vlan_router_join(struct mvsw_pr_port_vlan *mvsw_pr_port_vlan,
 			      struct net_device *dev,
@@ -746,7 +820,7 @@ mvsw_pr_port_vlan_router_join(struct mvsw_pr_port_vlan *mvsw_pr_port_vlan,
 
 	MVSW_LOG_ERROR("NOT IMPLEMENTED!!!");
 
-	rif = mvsw_pr_rif_find_by_dev(sw, dev);
+	rif = mvsw_pr_rif_find(sw, dev);
 	if (!rif)
 		rif = mvsw_pr_rif_create(sw, &params, extack);
 
@@ -784,7 +858,7 @@ mvsw_pr_port_router_join(struct mvsw_pr_port *mvsw_pr_port,
 	};
 	struct mvsw_pr_rif *rif;
 
-	rif = mvsw_pr_rif_find_by_dev(sw, dev);
+	rif = mvsw_pr_rif_find(sw, dev);
 	if (!rif)
 		rif = mvsw_pr_rif_create(sw, &params, extack);
 
@@ -808,26 +882,94 @@ void mvsw_pr_port_router_leave(struct mvsw_pr_port *mvsw_pr_port)
 	 * - vid learning set (true)
 	 */
 
-	rif = mvsw_pr_rif_find_by_dev(mvsw_pr_port->sw, mvsw_pr_port->net_dev);
+	rif = mvsw_pr_rif_find(mvsw_pr_port->sw, mvsw_pr_port->net_dev);
 	if (rif)
 		mvsw_pr_rif_destroy(rif);
 }
 
+static int mvsw_pr_rif_fdb_op(struct mvsw_pr_rif *rif, const char *mac,
+			      bool adding)
+{
+	if (adding)
+		mvsw_pr_macvlan_add(rif->sw, rif->vr_id, mac,
+				    rif->iface.vlan_id);
+	else
+		mvsw_pr_macvlan_del(rif->sw, rif->vr_id, mac,
+				    rif->iface.vlan_id);
+
+	return 0;
+}
+
+static int mvsw_pr_rif_macvlan_add(struct mvsw_pr_switch *sw,
+				   const struct net_device *macvlan_dev,
+				   struct netlink_ext_ack *extack)
+{
+	struct macvlan_dev *vlan = netdev_priv(macvlan_dev);
+	struct mvsw_pr_rif *rif;
+	int err;
+
+	rif = mvsw_pr_rif_find(sw, vlan->lowerdev);
+	if (!rif) {
+		NL_SET_ERR_MSG_MOD(extack,
+				   "macvlan is only supported on top of RIF");
+		return -EOPNOTSUPP;
+	}
+
+	err = mvsw_pr_rif_fdb_op(rif, macvlan_dev->dev_addr, true);
+	if (err)
+		return err;
+
+	return err;
+}
+
+static void __mvsw_pr_rif_macvlan_del(struct mvsw_pr_switch *sw,
+				      const struct net_device *macvlan_dev)
+{
+	struct macvlan_dev *vlan = netdev_priv(macvlan_dev);
+	struct mvsw_pr_rif *rif;
+
+	rif = mvsw_pr_rif_find(sw, vlan->lowerdev);
+	if (!rif)
+		return;
+
+	mvsw_pr_rif_fdb_op(rif, macvlan_dev->dev_addr,  false);
+}
+
+static void mvsw_pr_rif_macvlan_del(struct mvsw_pr_switch *sw,
+				    const struct net_device *macvlan_dev)
+{
+	__mvsw_pr_rif_macvlan_del(sw, macvlan_dev);
+}
+
+static int mvsw_pr_inetaddr_macvlan_event(struct mvsw_pr_switch *sw,
+					  struct net_device *macvlan_dev,
+					  unsigned long event,
+					  struct netlink_ext_ack *extack)
+{
+	switch (event) {
+	case NETDEV_UP:
+		return mvsw_pr_rif_macvlan_add(sw, macvlan_dev, extack);
+	case NETDEV_DOWN:
+		mvsw_pr_rif_macvlan_del(sw, macvlan_dev);
+		break;
+	}
+
+	return 0;
+}
+
 static int mvsw_pr_router_port_check_rif_addr(struct mvsw_pr_switch *sw,
 					      struct net_device *dev,
 					      const unsigned char *dev_addr,
 					      struct netlink_ext_ack *extack)
 {
-	struct mvsw_pr_rif *rif;
+	if (netif_is_macvlan(dev) || netif_is_l3_master(dev))
+		return 0;
 
-	list_for_each_entry(rif, &sw->router->rif_list, router_node) {
-		if (rif->dev && rif->dev != dev &&
-		    !ether_addr_equal_masked(rif->dev->dev_addr, dev_addr,
-					     mvsw_pr_mac_mask)) {
-			NL_SET_ERR_MSG_MOD(extack,
-					   "RIF MAC must have the same prefix");
-			return -EINVAL;
-		}
+	if (!ether_addr_equal_masked(sw->base_mac, dev_addr,
+				     mvsw_pr_mac_mask)) {
+		NL_SET_ERR_MSG_MOD(extack,
+				   "RIF MAC must have the same prefix");
+		return -EINVAL;
 	}
 
 	return 0;
@@ -873,7 +1015,7 @@ static int mvsw_pr_inetaddr_bridge_event(struct mvsw_pr_switch *sw,
 			return PTR_ERR(rif);
 		break;
 	case NETDEV_DOWN:
-		rif = mvsw_pr_rif_find_by_dev(sw, dev);
+		rif = mvsw_pr_rif_find(sw, dev);
 		mvsw_pr_rif_destroy(rif);
 		break;
 	}
@@ -928,6 +1070,33 @@ static int mvsw_pr_inetaddr_vlan_event(struct mvsw_pr_switch *sw,
 	return 0;
 }
 
+static int mvsw_pr_inetaddr_lag_event(struct mvsw_pr_switch *sw,
+				      struct net_device *lag_dev,
+				      unsigned long event,
+				      struct netlink_ext_ack *extack)
+{
+	struct mvsw_pr_rif_params params = {
+		.dev = lag_dev,
+	};
+	struct mvsw_pr_rif *rif;
+
+	MVSW_LOG_ERROR("lag_dev=%s", lag_dev->name);
+
+	switch (event) {
+	case NETDEV_UP:
+		rif = mvsw_pr_rif_create(sw, &params, extack);
+		if (IS_ERR(rif))
+			return PTR_ERR(rif);
+		break;
+	case NETDEV_DOWN:
+		rif = mvsw_pr_rif_find(sw, lag_dev);
+		mvsw_pr_rif_destroy(rif);
+		break;
+	}
+
+	return 0;
+}
+
 static int __mvsw_pr_inetaddr_event(struct mvsw_pr_switch *sw,
 				    struct net_device *dev,
 				    unsigned long event,
@@ -939,8 +1108,12 @@ static int __mvsw_pr_inetaddr_event(struct mvsw_pr_switch *sw,
 		return mvsw_pr_inetaddr_vlan_event(sw, dev, event, extack);
 	else if (netif_is_bridge_master(dev))
 		return mvsw_pr_inetaddr_bridge_event(sw, dev, event, extack);
-
-	return 0;
+	else if (netif_is_lag_master(dev))
+		return mvsw_pr_inetaddr_lag_event(sw, dev, event, extack);
+	else if (netif_is_macvlan(dev))
+		return mvsw_pr_inetaddr_macvlan_event(sw, dev, event, extack);
+	else
+		return 0;
 }
 
 static bool
@@ -1019,7 +1192,7 @@ static int mvsw_pr_inetaddr_event(struct notifier_block *nb,
 	if (idev && idev->ifa_list)
 		mvsw_pr_rif_neigh_clean(router, ifa);
 
-	rif = mvsw_pr_rif_find_by_dev(router->sw, dev);
+	rif = mvsw_pr_rif_find(router->sw, dev);
 	if (!mvsw_pr_rif_should_config(rif, dev, event))
 		goto out;
 
@@ -1041,7 +1214,7 @@ int mvsw_pr_inetaddr_valid_event(struct notifier_block *unused,
 	if (!sw)
 		goto out;
 
-	rif = mvsw_pr_rif_find_by_dev(sw, dev);
+	rif = mvsw_pr_rif_find(sw, dev);
 	if (!mvsw_pr_rif_should_config(rif, dev, event))
 		goto out;
 
@@ -1507,12 +1680,12 @@ static void mvsw_pr_router_fib_abort(struct mvsw_pr_switch *sw)
 }
 
 static int
-mvsw_pr_router_port_change(struct mvsw_pr_switch *sw, struct mvsw_pr_rif *rif)
+mvsw_pr_router_port_change(struct mvsw_pr_rif *rif)
 {
 	struct net_device *dev = rif->dev;
 	int err;
 
-	err = mvsw_pr_rif_edit(sw, &rif->rif_id, dev->dev_addr, dev->mtu);
+	err = mvsw_pr_rif_update(rif, dev->dev_addr);
 	if (err)
 		return err;
 
@@ -1545,13 +1718,13 @@ int mvsw_pr_netdevice_router_port_event(struct net_device *dev,
 	if (!sw)
 		return 0;
 
-	rif = mvsw_pr_rif_find_by_dev(sw, dev);
+	rif = mvsw_pr_rif_find(sw, dev);
 	if (!rif)
 		return 0;
 
 	switch (event) {
 	case NETDEV_CHANGEADDR:
-		return mvsw_pr_router_port_change(sw, rif);
+		return mvsw_pr_router_port_change(rif);
 	case NETDEV_PRE_CHANGEADDR:
 		return mvsw_pr_router_port_pre_change(rif, ptr);
 	}
@@ -1568,7 +1741,7 @@ static int mvsw_pr_port_vrf_join(struct mvsw_pr_switch *sw,
 	/* If netdev is already associated with a RIF, then we need to
 	 * destroy it and create a new one with the new virtual router ID.
 	 */
-	rif = mvsw_pr_rif_find_by_dev(sw, dev);
+	rif = mvsw_pr_rif_find(sw, dev);
 	if (rif)
 		__mvsw_pr_inetaddr_event(sw, dev, NETDEV_DOWN, extack);
 
@@ -1580,7 +1753,7 @@ static void mvsw_pr_port_vrf_leave(struct mvsw_pr_switch *sw,
 {
 	struct mvsw_pr_rif *rif;
 
-	rif = mvsw_pr_rif_find_by_dev(sw, dev);
+	rif = mvsw_pr_rif_find(sw, dev);
 	if (!rif)
 		return;
 
@@ -1614,6 +1787,27 @@ int mvsw_pr_netdevice_vrf_event(struct net_device *dev, unsigned long event,
 	return err;
 }
 
+static int __mvsw_pr_rif_macvlan_flush(struct net_device *dev, void *data)
+{
+	struct mvsw_pr_rif *rif = data;
+
+	if (!netif_is_macvlan(dev))
+		return 0;
+
+	return mvsw_pr_rif_fdb_op(rif, dev->dev_addr, false);
+}
+
+static int mvsw_pr_rif_macvlan_flush(struct mvsw_pr_rif *rif)
+{
+	if (!netif_is_macvlan_port(rif->dev))
+		return 0;
+
+	netdev_warn(rif->dev,
+		    "Router interface is deleted. Upper macvlans will not work\n");
+	return netdev_walk_all_upper_dev_rcu(rif->dev,
+					     __mvsw_pr_rif_macvlan_flush, rif);
+}
+
 static struct notifier_block mvsw_pr_inetaddr_valid_nb __read_mostly = {
 	.notifier_call = mvsw_pr_inetaddr_valid_event,
 };
@@ -1689,11 +1883,29 @@ err_register_inetaddr_validator_notifier:
 	return err;
 }
 
+static void mvsw_pr_vrs_fini(struct mvsw_pr_switch *sw)
+{
+	struct mvsw_pr_vr *vr, *tmp;
+
+	list_for_each_entry_safe(vr, tmp, &sw->router->vr_list, router_node)
+		mvsw_pr_vr_destroy(sw, vr);
+}
+
+static void mvsw_pr_rifs_fini(struct mvsw_pr_switch *sw)
+{
+	struct mvsw_pr_rif *rif, *tmp;
+
+	list_for_each_entry_safe(rif, tmp, &sw->router->rif_list, router_node)
+		mvsw_pr_rif_destroy(rif);
+}
+
 void mvsw_pr_router_fini(struct mvsw_pr_switch *sw)
 {
 	unregister_fib_notifier(&init_net, &sw->router->fib_nb);
 	unregister_netevent_notifier(&sw->router->netevent_nb);
 	mvsw_pr_neigh_fini(sw);
+	mvsw_pr_rifs_fini(sw);
+	mvsw_pr_vrs_fini(sw);
 	unregister_inetaddr_notifier(&sw->router->inetaddr_nb);
 	unregister_inetaddr_validator_notifier(&mvsw_pr_inetaddr_valid_nb);
 
@@ -1800,77 +2012,38 @@ mvsw_pr_rif_alloc(struct mvsw_pr_switch *sw,
 		  struct mvsw_pr_vr *vr,
 		  const struct mvsw_pr_rif_params *params)
 {
-	enum mvsw_pr_rif_type type;
 	struct mvsw_pr_rif *rif;
 	int err;
 
-	type = mvsw_pr_dev_rif_type(params->dev);
 	rif = kzalloc(sizeof(*rif), GFP_KERNEL);
 	if (!rif) {
 		err = -ENOMEM;
 		goto err_rif_alloc;
 	}
 
-	if (params->dev) {
-		ether_addr_copy(rif->addr, params->dev->dev_addr);
-		rif->mtu = params->dev->mtu;
-		rif->dev = params->dev;
-	}
-	rif->type = type;
-	rif->vr_id = vr->id;
 	rif->sw = sw;
+	rif->dev = params->dev;
+	err = mvsw_pr_rif_iface_init(rif);
+	if (err)
+		goto err_rif_iface_init;
+
+	ether_addr_copy(rif->addr, params->dev->dev_addr);
+	rif->mtu = params->dev->mtu;
+	rif->vr_id = vr->id;
 
 	return rif;
 
+err_rif_iface_init:
+	kfree(rif);
 err_rif_alloc:
 	return ERR_PTR(err);
 }
 
-static int mvsw_pr_rif_offload(struct mvsw_pr_switch *sw,
-			       struct mvsw_pr_rif *rif)
+static int mvsw_pr_rif_offload(struct mvsw_pr_rif *rif)
 {
-	struct mvsw_pr_port *mvsw_pr_port;
-	u16 rif_id;
-	u16 vid;
-	int err = 0;
-
-	switch (rif->type) {
-	case MVSW_PR_RIF_TYPE_PORT:
-		mvsw_pr_port = netdev_priv(rif->dev);
-		err = mvsw_pr_hw_rif_port_create(mvsw_pr_port,
-						 rif->vr_id,
-						 rif->addr,
-						 &rif_id);
-		if (err)
-			return err;
-
-		break;
-	case MVSW_PR_RIF_TYPE_VLAN:
-		vid = mvsw_pr_nh_dev_to_vid(sw, rif->dev);
-		err = mvsw_pr_hw_rif_vlan_create(sw, rif->vr_id,
-						 rif->dev->dev_addr,
-						 vid, &rif_id);
-		if (err)
-			return err;
-
-		break;
-	case MVSW_PR_RIF_TYPE_BRIDGE:
-		vid = mvsw_pr_vlan_dev_vlan_id(sw->bridge,
-					       rif->dev);
-		err = mvsw_pr_hw_rif_bridge_create(sw, rif->vr_id,
-						   rif->dev->dev_addr,
-						   vid, &rif_id);
-		if (err)
-			return err;
-
-		break;
-	default:
-		WARN_ON(1);
-		return -EINVAL;
-	}
-
-	rif->rif_id = rif_id;
-	return err;
+	return mvsw_pr_hw_rif_create(rif->sw, &rif->iface,
+					rif->vr_id, rif->addr,
+					&rif->rif_id);
 }
 
 static struct mvsw_pr_rif *mvsw_pr_rif_create(struct mvsw_pr_switch *sw,
@@ -1893,7 +2066,7 @@ static struct mvsw_pr_rif *mvsw_pr_rif_create(struct mvsw_pr_switch *sw,
 		return rif;
 	}
 
-	err = mvsw_pr_rif_offload(sw, rif);
+	err = mvsw_pr_rif_offload(rif);
 	if (err)  {
 		NL_SET_ERR_MSG_MOD(extack,
 				   "Exceeded number of supported rifs");
@@ -1913,6 +2086,12 @@ err_rif_offload:
 	return ERR_PTR(err);
 }
 
+static int mvsw_pr_rif_delete(struct mvsw_pr_rif *rif)
+{
+	return mvsw_pr_hw_rif_delete(rif->sw, rif->rif_id, &rif->iface,
+				     rif->vr_id);
+}
+
 static void mvsw_pr_rif_destroy(struct mvsw_pr_rif *rif)
 {
 	struct mvsw_pr_vr *vr;
@@ -1921,8 +2100,9 @@ static void mvsw_pr_rif_destroy(struct mvsw_pr_rif *rif)
 
 	vr = mvsw_pr_vr_find_by_id(rif->sw, rif->vr_id);
 	/* TODO: rif->deconfigure() ??? */
-	mvsw_pr_hw_rif_delete(rif->sw, rif->vr_id, rif->rif_id,
-			      mvsw_pr_port_dev_lower_find(rif->dev));
+	mvsw_pr_rif_fdb_op(rif, rif->dev->dev_addr, false);
+	mvsw_pr_rif_macvlan_flush(rif);
+	mvsw_pr_rif_delete(rif);
 	list_del(&rif->router_node);
 	//dev_put(rif->dev);
 	kfree(rif);
@@ -1930,27 +2110,22 @@ static void mvsw_pr_rif_destroy(struct mvsw_pr_rif *rif)
 	mvsw_pr_vr_put(rif->sw, vr);
 }
 
-void mvsw_pr_rif_disable(struct mvsw_pr_switch *sw, struct net_device *dev)
+void mvsw_pr_rif_enable(struct mvsw_pr_switch *sw,
+			struct net_device *dev, bool enable)
 {
 	struct mvsw_pr_rif *rif;
 
-	rif = mvsw_pr_rif_find_by_dev(sw, dev);
-	if (rif)
-		mvsw_pr_hw_rif_delete(sw, rif->vr_id, rif->rif_id,
-				      mvsw_pr_port_dev_lower_find(rif->dev));
-}
-
-void mvsw_pr_rif_enable(struct mvsw_pr_switch *sw, struct net_device *dev)
-{
-	struct mvsw_pr_rif *rif;
+	rif = mvsw_pr_rif_find(sw, dev);
+	if (!rif)
+		return;
 
-	rif = mvsw_pr_rif_find_by_dev(sw, dev);
-	if (rif)
-		mvsw_pr_rif_offload(rif->sw, rif);
+	if (enable)
+		mvsw_pr_rif_offload(rif);
+	else
+		mvsw_pr_rif_delete(rif);
 }
 
-static int mvsw_pr_rif_edit(struct mvsw_pr_switch *sw, u16 *rif_id,
-			    char *mac, int mtu)
+static int mvsw_pr_rif_update(struct mvsw_pr_rif *rif, char *mac)
 {
-	return mvsw_pr_hw_rif_set(sw, rif_id, mtu, mac);
+	return mvsw_pr_hw_rif_set(rif->sw, &rif->rif_id, &rif->iface, mac);
 }
diff --git a/drivers/net/ethernet/marvell/prestera_sw/prestera_rxtx.c b/drivers/net/ethernet/marvell/prestera_sw/prestera_rxtx.c
index acab6d4..fddf8fc 100644
--- a/drivers/net/ethernet/marvell/prestera_sw/prestera_rxtx.c
+++ b/drivers/net/ethernet/marvell/prestera_sw/prestera_rxtx.c
@@ -227,7 +227,7 @@ void mvsw_pr_rxtx_fini(void)
 	platform_driver_unregister(&mvsw_pr_rxtx_driver);
 }
 
-int mvsw_pr_rxtx_switch_init(const struct mvsw_pr_switch *sw)
+int mvsw_pr_rxtx_switch_init(struct mvsw_pr_switch *sw)
 {
 	int err;
 
@@ -246,7 +246,7 @@ int mvsw_pr_rxtx_switch_init(const struct mvsw_pr_switch *sw)
 	return 0;
 }
 
-void mvsw_pr_rxtx_switch_fini(const struct mvsw_pr_switch *sw)
+void mvsw_pr_rxtx_switch_fini(struct mvsw_pr_switch *sw)
 {
 	if (!rxtx_registered || !rxtx_registered->ops->rxtx_switch_init)
 		return;
diff --git a/drivers/net/ethernet/marvell/prestera_sw/prestera_rxtx.h b/drivers/net/ethernet/marvell/prestera_sw/prestera_rxtx.h
index 0566bc1..d541ca7 100644
--- a/drivers/net/ethernet/marvell/prestera_sw/prestera_rxtx.h
+++ b/drivers/net/ethernet/marvell/prestera_sw/prestera_rxtx.h
@@ -19,8 +19,8 @@ struct mvsw_pr_rxtx_info {
 int mvsw_pr_rxtx_init(void);
 void mvsw_pr_rxtx_fini(void);
 
-int mvsw_pr_rxtx_switch_init(const struct mvsw_pr_switch *sw);
-void mvsw_pr_rxtx_switch_fini(const struct mvsw_pr_switch *sw);
+int mvsw_pr_rxtx_switch_init(struct mvsw_pr_switch *sw);
+void mvsw_pr_rxtx_switch_fini(struct mvsw_pr_switch *sw);
 
 netdev_tx_t mvsw_pr_rxtx_xmit(struct sk_buff *skb,
 			      struct mvsw_pr_rxtx_info *info);
diff --git a/drivers/net/ethernet/marvell/prestera_sw/prestera_rxtx_eth.c b/drivers/net/ethernet/marvell/prestera_sw/prestera_rxtx_eth.c
index 2983276..10722b8 100644
--- a/drivers/net/ethernet/marvell/prestera_sw/prestera_rxtx_eth.c
+++ b/drivers/net/ethernet/marvell/prestera_sw/prestera_rxtx_eth.c
@@ -95,7 +95,7 @@ int mvsw_pr_rxtx_eth_fini(struct mvsw_pr_rxtx *rxtx)
 }
 
 int mvsw_pr_rxtx_eth_switch_init(struct mvsw_pr_rxtx *rxtx,
-				 const struct mvsw_pr_switch *sw)
+				 struct mvsw_pr_switch *sw)
 {
 	int err;
 
@@ -107,7 +107,7 @@ int mvsw_pr_rxtx_eth_switch_init(struct mvsw_pr_rxtx *rxtx,
 }
 
 void mvsw_pr_rxtx_eth_switch_fini(struct mvsw_pr_rxtx *rxtx,
-				  const struct mvsw_pr_switch *sw)
+				  struct mvsw_pr_switch *sw)
 {
 }
 
diff --git a/drivers/net/ethernet/marvell/prestera_sw/prestera_rxtx_priv.h b/drivers/net/ethernet/marvell/prestera_sw/prestera_rxtx_priv.h
index 66a5267..13527b9 100644
--- a/drivers/net/ethernet/marvell/prestera_sw/prestera_rxtx_priv.h
+++ b/drivers/net/ethernet/marvell/prestera_sw/prestera_rxtx_priv.h
@@ -19,9 +19,9 @@ struct mvsw_pr_rxtx_ops {
 	int (*rxtx_fini)(struct mvsw_pr_rxtx *rxtx);
 
 	int (*rxtx_switch_init)(struct mvsw_pr_rxtx *rxtx,
-				const struct mvsw_pr_switch *sw);
+				struct mvsw_pr_switch *sw);
 	void (*rxtx_switch_fini)(struct mvsw_pr_rxtx *rxtx,
-				 const struct mvsw_pr_switch *sw);
+				 struct mvsw_pr_switch *sw);
 
 	netdev_tx_t (*rxtx_xmit)(struct mvsw_pr_rxtx *rxtx,
 				 struct sk_buff *skb);
@@ -45,17 +45,17 @@ netdev_tx_t mvsw_pr_rxtx_eth_xmit(struct mvsw_pr_rxtx *rxtx,
 int mvsw_pr_rxtx_mvpp_init(struct mvsw_pr_rxtx *rxtx);
 int mvsw_pr_rxtx_mvpp_fini(struct mvsw_pr_rxtx *rxtx);
 int mvsw_pr_rxtx_eth_switch_init(struct mvsw_pr_rxtx *rxtx,
-				 const struct mvsw_pr_switch *sw);
+				 struct mvsw_pr_switch *sw);
 void mvsw_pr_rxtx_eth_switch_fini(struct mvsw_pr_rxtx *rxtx,
-				  const struct mvsw_pr_switch *sw);
+				  struct mvsw_pr_switch *sw);
 netdev_tx_t mvsw_pr_rxtx_mvpp_xmit(struct mvsw_pr_rxtx *rxtx,
 				   struct sk_buff *skb);
 
 int mvsw_pr_rxtx_sdma_init(struct mvsw_pr_rxtx *rxtx);
 int mvsw_pr_rxtx_sdma_fini(struct mvsw_pr_rxtx *rxtx);
 int mvsw_pr_rxtx_sdma_switch_init(struct mvsw_pr_rxtx *rxtx,
-				  const struct mvsw_pr_switch *sw);
+				  struct mvsw_pr_switch *sw);
 void mvsw_pr_rxtx_sdma_switch_fini(struct mvsw_pr_rxtx *rxtx,
-				   const struct mvsw_pr_switch *sw);
+				   struct mvsw_pr_switch *sw);
 netdev_tx_t mvsw_pr_rxtx_sdma_xmit(struct mvsw_pr_rxtx *rxtx,
 				   struct sk_buff *skb);
diff --git a/drivers/net/ethernet/marvell/prestera_sw/prestera_rxtx_sdma.c b/drivers/net/ethernet/marvell/prestera_sw/prestera_rxtx_sdma.c
index 1fb8ee3..77dbeb7 100644
--- a/drivers/net/ethernet/marvell/prestera_sw/prestera_rxtx_sdma.c
+++ b/drivers/net/ethernet/marvell/prestera_sw/prestera_rxtx_sdma.c
@@ -27,7 +27,7 @@ struct mvsw_sdma_desc {
 	((le32_to_cpu((desc)->word2) >> 16) & 0x3FFF)
 
 #define SDMA_RX_DESC_OWNER(desc) \
-	(le32_to_cpu(((desc)->word1)) >> 31)
+	((le32_to_cpu((desc)->word1) & BIT(31)) >> 31)
 
 #define SDMA_RX_DESC_CPU_OWN	0
 #define SDMA_RX_DESC_DMA_OWN	1
@@ -36,13 +36,18 @@ struct mvsw_sdma_desc {
 
 #define SDMA_RX_DESC_PER_Q	1000
 
-#define SDMA_TX_DESC_NUM	1000
+#define SDMA_TX_DESC_PER_Q	1000
+#define SDMA_TX_MAX_BURST	32
 
-#define SDMA_TX_DESC_OWNER(desc)	(le32_to_cpu((desc)->word1) >> 31)
+#define SDMA_TX_DESC_OWNER(desc) \
+	((le32_to_cpu((desc)->word1) & BIT(31)) >> 31)
 
 #define SDMA_TX_DESC_CPU_OWN	0
 #define SDMA_TX_DESC_DMA_OWN	1
 
+#define SDMA_TX_DESC_IS_SENT(desc) \
+	(SDMA_TX_DESC_OWNER(desc) == SDMA_TX_DESC_CPU_OWN)
+
 #define SDMA_TX_DESC_LAST	BIT(20)
 #define SDMA_TX_DESC_FIRST	BIT(21)
 #define SDMA_TX_DESC_SINGLE	(SDMA_TX_DESC_FIRST | SDMA_TX_DESC_LAST)
@@ -53,6 +58,7 @@ struct mvsw_sdma_desc {
 #define mvsw_reg_read(sw, reg) \
 	readl((sw)->dev->pp_regs + (reg))
 
+#define SDMA_RX_INTR_MASK_REG		0x2814
 #define SDMA_RX_QUEUE_STATUS_REG	0x2680
 #define SDMA_RX_QUEUE_DESC_REG(n)	(0x260C + (n) * 16)
 
@@ -61,7 +67,6 @@ struct mvsw_sdma_desc {
 
 struct mvsw_sdma_buf {
 	struct mvsw_sdma_desc *desc;
-	struct list_head list;
 	dma_addr_t desc_dma;
 	struct sk_buff *skb;
 	dma_addr_t buf_dma;
@@ -70,11 +75,14 @@ struct mvsw_sdma_buf {
 
 struct mvsw_sdma_rx_ring {
 	struct mvsw_sdma_buf *bufs;
+	int next_rx;
 };
 
 struct mvsw_sdma_tx_ring {
 	struct mvsw_sdma_buf *bufs;
 	int next_tx;
+	int max_burst;
+	int burst;
 };
 
 struct mvsw_pr_rxtx_sdma {
@@ -83,10 +91,11 @@ struct mvsw_pr_rxtx_sdma {
 	const struct mvsw_pr_switch *sw;
 	struct dma_pool *desc_pool;
 	struct mvsw_pr_rxtx *rxtx;
-	struct workqueue_struct *rxwq;
-	struct work_struct rx_work;
 	struct work_struct tx_work;
-	bool finish_rx;
+	struct napi_struct rx_napi;
+	struct net_device napi_dev;
+	/* protect SDMA with concurrrent access from multiple CPUs */
+	spinlock_t tx_lock;
 	u32 map_addr;
 	u64 dma_mask;
 };
@@ -150,8 +159,7 @@ static int mvsw_sdma_rx_dma_alloc(struct mvsw_pr_rxtx_sdma *sdma,
 {
 	struct device *dev = sdma->sw->dev->dev;
 
-	buf->skb = __netdev_alloc_skb_ip_align(NULL, SDMA_BUFF_SIZE_MAX,
-					       GFP_DMA | GFP_KERNEL);
+	buf->skb = alloc_skb(SDMA_BUFF_SIZE_MAX, GFP_DMA | GFP_ATOMIC);
 	if (!buf->skb)
 		return -ENOMEM;
 
@@ -190,8 +198,7 @@ static struct sk_buff *mvsw_sdma_rx_buf_get(struct mvsw_pr_rxtx_sdma *sdma,
 		buf->buf_dma = buf_dma;
 		buf->skb = skb_orig;
 
-		skb = __netdev_alloc_skb_ip_align(NULL, SDMA_BUFF_SIZE_MAX,
-						  GFP_KERNEL);
+		skb = alloc_skb(SDMA_BUFF_SIZE_MAX, GFP_ATOMIC);
 		if (!skb)
 			return NULL;
 
@@ -202,25 +209,38 @@ static struct sk_buff *mvsw_sdma_rx_buf_get(struct mvsw_pr_rxtx_sdma *sdma,
 	return skb_orig;
 }
 
-static void mvsw_sdma_rx_work_fn(struct work_struct *work)
+static int mvsw_sdma_rx_poll(struct napi_struct *napi, int budget)
 {
+	unsigned int qmask = GENMASK(SDMA_RX_QUEUE_NUM - 1, 0);
 	struct mvsw_pr_rxtx_sdma *sdma;
-	int q, b;
+	unsigned int rxq_done_map = 0;
+	struct list_head rx_list;
+	int pkts_done = 0;
+	int q;
+
+	INIT_LIST_HEAD(&rx_list);
 
-	sdma = container_of(work, struct mvsw_pr_rxtx_sdma, rx_work);
+	sdma = container_of(napi, struct mvsw_pr_rxtx_sdma, rx_napi);
 
-	for (b = 0; b < SDMA_RX_DESC_PER_Q; b++) {
-		for (q = 0; q < SDMA_RX_QUEUE_NUM; q++) {
+	while (pkts_done < budget && rxq_done_map != qmask) {
+		for (q = 0; q < SDMA_RX_QUEUE_NUM && pkts_done < budget; q++) {
 			struct mvsw_sdma_rx_ring *ring = &sdma->rx_ring[q];
-			struct mvsw_sdma_buf *buf = &ring->bufs[b];
-			struct mvsw_sdma_desc *desc = buf->desc;
+			int buf_idx = ring->next_rx;
+			struct mvsw_sdma_desc *desc;
+			struct mvsw_sdma_buf *buf;
 			struct sk_buff *skb;
 
-			if (sdma->finish_rx)
-				return;
+			buf = &ring->bufs[buf_idx];
+			desc = buf->desc;
 
-			if (SDMA_RX_DESC_OWNER(desc) != SDMA_RX_DESC_CPU_OWN)
+			if (SDMA_RX_DESC_OWNER(desc) != SDMA_RX_DESC_CPU_OWN) {
+				rxq_done_map |= BIT(q);
 				continue;
+			} else {
+				rxq_done_map &= ~BIT(q);
+			}
+
+			pkts_done++;
 
 			__skb_trim(buf->skb, SDMA_RX_DESC_PKT_LEN(desc));
 
@@ -231,15 +251,19 @@ static void mvsw_sdma_rx_work_fn(struct work_struct *work)
 			if (unlikely(mvsw_pr_rxtx_recv_skb(sdma->rxtx, skb)))
 				goto rx_reset_buf;
 
-			netif_rx_ni(skb);
+			list_add_tail(&skb->list, &rx_list);
 rx_reset_buf:
 			mvsw_sdma_rx_desc_init(sdma, buf->desc, buf->buf_dma);
+			ring->next_rx = (buf_idx + 1) % SDMA_RX_DESC_PER_Q;
 		}
 	}
 
-	/* TODO: remove it when rx interrupt will be supported */
-	if (!sdma->finish_rx)
-		queue_work(sdma->rxwq, work);
+	if (pkts_done < budget && napi_complete_done(napi, pkts_done))
+		mvsw_reg_write(sdma->sw, SDMA_RX_INTR_MASK_REG, 0xff << 2);
+
+	netif_receive_skb_list(&rx_list);
+
+	return pkts_done;
 }
 
 static void mvsw_sdma_rx_fini(struct mvsw_pr_rxtx_sdma *sdma)
@@ -249,11 +273,6 @@ static void mvsw_sdma_rx_fini(struct mvsw_pr_rxtx_sdma *sdma)
 	/* disable all rx queues */
 	mvsw_reg_write(sdma->sw, SDMA_RX_QUEUE_STATUS_REG, 0xff00);
 
-	sdma->finish_rx = true;
-
-	flush_workqueue(sdma->rxwq);
-	destroy_workqueue(sdma->rxwq);
-
 	for (q = 0; q < SDMA_RX_QUEUE_NUM; q++) {
 		struct mvsw_sdma_rx_ring *ring = &sdma->rx_ring[q];
 
@@ -284,12 +303,6 @@ static int mvsw_sdma_rx_init(struct mvsw_pr_rxtx_sdma *sdma)
 	int q, b;
 	int err;
 
-	sdma->rxwq = alloc_workqueue("prestera_sdma_wq", WQ_HIGHPRI, 1);
-	if (!sdma->rxwq)
-		return -ENOMEM;
-
-	INIT_WORK(&sdma->rx_work, mvsw_sdma_rx_work_fn);
-
 	/* disable all rx queues */
 	mvsw_reg_write(sdma->sw, SDMA_RX_QUEUE_STATUS_REG, 0xff00);
 
@@ -303,6 +316,7 @@ static int mvsw_sdma_rx_init(struct mvsw_pr_rxtx_sdma *sdma)
 			return -ENOMEM;
 
 		head = &ring->bufs[0];
+		ring->next_rx = 0;
 
 		for (b = 0; b < SDMA_RX_DESC_PER_Q; b++) {
 			struct mvsw_sdma_buf *buf = &ring->bufs[b];
@@ -336,8 +350,6 @@ static int mvsw_sdma_rx_init(struct mvsw_pr_rxtx_sdma *sdma)
 	wmb();
 	mvsw_reg_write(sdma->sw, SDMA_RX_QUEUE_STATUS_REG, 0xff);
 
-	/* TODO: remove it when rx interrupt will be supported */
-	queue_work(sdma->rxwq, &sdma->rx_work);
 	return 0;
 }
 
@@ -378,8 +390,65 @@ static void mvsw_sdma_tx_desc_xmit(struct mvsw_sdma_desc *desc)
 	desc->word1 = cpu_to_le32(word);
 }
 
+static int mvsw_sdma_tx_buf_map(struct mvsw_pr_rxtx_sdma *sdma,
+				struct mvsw_sdma_buf *buf,
+				struct sk_buff *skb)
+{
+	struct device *dma_dev = sdma->sw->dev->dev;
+	struct sk_buff *new_skb;
+	size_t len = skb->len;
+	dma_addr_t dma;
+
+	dma = dma_map_single(dma_dev, skb->data, len, DMA_TO_DEVICE);
+	if (!dma_mapping_error(dma_dev, dma) && dma + len <= sdma->dma_mask) {
+		buf->buf_dma = dma;
+		buf->skb = skb;
+		return 0;
+	}
+
+	if (!dma_mapping_error(dma_dev, dma))
+		dma_unmap_single(dma_dev, dma, len, DMA_TO_DEVICE);
+
+	new_skb = alloc_skb(len, GFP_ATOMIC | GFP_DMA);
+	if (!new_skb)
+		goto err_alloc_skb;
+
+	dma = dma_map_single(dma_dev, new_skb->data, len, DMA_TO_DEVICE);
+	if (dma_mapping_error(dma_dev, dma))
+		goto err_dma_map;
+	if (dma + len > sdma->dma_mask)
+		goto err_dma_range;
+
+	skb_copy_from_linear_data(skb, skb_put(new_skb, len), len);
+
+	dev_consume_skb_any(skb);
+
+	buf->skb = new_skb;
+	buf->buf_dma = dma;
+
+	return 0;
+
+err_dma_range:
+	dma_unmap_single(dma_dev, dma, len, DMA_TO_DEVICE);
+err_dma_map:
+	dev_kfree_skb(new_skb);
+err_alloc_skb:
+	dev_kfree_skb(skb);
+
+	return -ENOMEM;
+}
+
+static void mvsw_sdma_tx_buf_unmap(struct mvsw_pr_rxtx_sdma *sdma,
+				   struct mvsw_sdma_buf *buf)
+{
+	struct device *dma_dev = sdma->sw->dev->dev;
+
+	dma_unmap_single(dma_dev, buf->buf_dma, buf->skb->len, DMA_TO_DEVICE);
+}
+
 static void mvsw_sdma_tx_recycle_work_fn(struct work_struct *work)
 {
+	struct mvsw_sdma_tx_ring *tx_ring;
 	struct mvsw_pr_rxtx_sdma *sdma;
 	struct device *dma_dev;
 	int b;
@@ -387,23 +456,24 @@ static void mvsw_sdma_tx_recycle_work_fn(struct work_struct *work)
 	sdma = container_of(work, struct mvsw_pr_rxtx_sdma, tx_work);
 
 	dma_dev = sdma->sw->dev->dev;
+	tx_ring = &sdma->tx_ring;
 
-	for (b = 0; b < SDMA_TX_DESC_NUM; b++) {
-		struct mvsw_sdma_buf *buf = &sdma->tx_ring.bufs[b];
+	for (b = 0; b < SDMA_TX_DESC_PER_Q; b++) {
+		struct mvsw_sdma_buf *buf = &tx_ring->bufs[b];
 
-		if (!buf->skb)
-			continue;
 		if (!buf->is_used)
 			continue;
-		if (SDMA_TX_DESC_OWNER(buf->desc) == SDMA_TX_DESC_DMA_OWN)
+
+		if (!SDMA_TX_DESC_IS_SENT(buf->desc))
 			continue;
 
-		dma_unmap_single(dma_dev, buf->buf_dma, buf->skb->len,
-				 DMA_TO_DEVICE);
+		mvsw_sdma_tx_buf_unmap(sdma, buf);
 		dev_consume_skb_any(buf->skb);
 		buf->skb = NULL;
+
 		/* make sure everything is cleaned up */
 		wmb();
+
 		buf->is_used = false;
 	}
 }
@@ -415,17 +485,22 @@ static int mvsw_sdma_tx_init(struct mvsw_pr_rxtx_sdma *sdma)
 	int err;
 	int b;
 
+	spin_lock_init(&sdma->tx_lock);
+
 	INIT_WORK(&sdma->tx_work, mvsw_sdma_tx_recycle_work_fn);
 
-	tx_ring->bufs = kmalloc_array(SDMA_TX_DESC_NUM, sizeof(*head),
+	tx_ring->bufs = kmalloc_array(SDMA_TX_DESC_PER_Q, sizeof(*head),
 				      GFP_KERNEL);
 	if (!tx_ring->bufs)
 		return -ENOMEM;
 
 	head = &tx_ring->bufs[0];
+
+	tx_ring->max_burst = SDMA_TX_MAX_BURST;
+	tx_ring->burst = tx_ring->max_burst;
 	tx_ring->next_tx = 0;
 
-	for (b = 0; b < SDMA_TX_DESC_NUM; b++) {
+	for (b = 0; b < SDMA_TX_DESC_PER_Q; b++) {
 		struct mvsw_sdma_buf *buf = &tx_ring->bufs[b];
 
 		err = mvsw_sdma_buf_desc_alloc(sdma, buf);
@@ -435,6 +510,7 @@ static int mvsw_sdma_tx_init(struct mvsw_pr_rxtx_sdma *sdma)
 		mvsw_sdma_tx_desc_init(sdma, buf->desc);
 
 		buf->is_used = false;
+		buf->skb = NULL;
 
 		if (b == 0)
 			continue;
@@ -442,7 +518,7 @@ static int mvsw_sdma_tx_init(struct mvsw_pr_rxtx_sdma *sdma)
 		mvsw_sdma_tx_desc_set_next(sdma, tx_ring->bufs[b - 1].desc,
 					   buf->desc_dma);
 
-		if (b == SDMA_TX_DESC_NUM - 1)
+		if (b == SDMA_TX_DESC_PER_Q - 1)
 			mvsw_sdma_tx_desc_set_next(sdma, buf->desc,
 						   head->desc_dma);
 	}
@@ -465,7 +541,7 @@ static void mvsw_sdma_tx_fini(struct mvsw_pr_rxtx_sdma *sdma)
 	if (!ring->bufs)
 		return;
 
-	for (b = 0; b < SDMA_TX_DESC_NUM; b++) {
+	for (b = 0; b < SDMA_TX_DESC_PER_Q; b++) {
 		struct mvsw_sdma_buf *buf = &ring->bufs[b];
 
 		if (buf->desc)
@@ -502,8 +578,20 @@ int mvsw_pr_rxtx_sdma_fini(struct mvsw_pr_rxtx *rxtx)
 	return 0;
 }
 
+static void mvsw_rxtx_handle_event(struct mvsw_pr_switch *sw,
+				   struct mvsw_pr_event *evt, void *arg)
+{
+	struct mvsw_pr_rxtx_sdma *sdma = arg;
+
+	if (evt->id != MVSW_RXTX_EVENT_RCV_PKT)
+		return;
+
+	mvsw_reg_write(sdma->sw, SDMA_RX_INTR_MASK_REG, 0);
+	napi_schedule(&sdma->rx_napi);
+}
+
 int mvsw_pr_rxtx_sdma_switch_init(struct mvsw_pr_rxtx *rxtx,
-				  const struct mvsw_pr_switch *sw)
+				  struct mvsw_pr_switch *sw)
 {
 	struct mvsw_pr_rxtx_sdma *sdma = rxtx->priv;
 	int err;
@@ -534,8 +622,19 @@ int mvsw_pr_rxtx_sdma_switch_init(struct mvsw_pr_rxtx *rxtx,
 		goto err_tx_init;
 	}
 
+	err = mvsw_pr_hw_event_handler_register(sw, MVSW_EVENT_TYPE_RXTX,
+						mvsw_rxtx_handle_event, sdma);
+	if (err)
+		goto err_evt_register;
+
+	init_dummy_netdev(&sdma->napi_dev);
+
+	netif_napi_add(&sdma->napi_dev, &sdma->rx_napi, mvsw_sdma_rx_poll, 64);
+	napi_enable(&sdma->rx_napi);
+
 	return 0;
 
+err_evt_register:
 err_tx_init:
 	mvsw_sdma_tx_fini(sdma);
 err_rx_init:
@@ -546,61 +645,37 @@ err_rx_init:
 }
 
 void mvsw_pr_rxtx_sdma_switch_fini(struct mvsw_pr_rxtx *rxtx,
-				   const struct mvsw_pr_switch *sw)
+				   struct mvsw_pr_switch *sw)
 {
 	struct mvsw_pr_rxtx_sdma *sdma = rxtx->priv;
 
+	mvsw_pr_hw_event_handler_unregister(sw, MVSW_EVENT_TYPE_RXTX);
+	napi_disable(&sdma->rx_napi);
+	netif_napi_del(&sdma->rx_napi);
 	mvsw_sdma_rx_fini(sdma);
 	mvsw_sdma_tx_fini(sdma);
 	dma_pool_destroy(sdma->desc_pool);
 }
 
-static int mvsw_sdma_tx_buf_map(struct mvsw_pr_rxtx_sdma *sdma,
-				struct mvsw_sdma_buf *buf,
-				struct sk_buff *skb)
+static int mvsw_sdma_wait_tx(struct mvsw_pr_rxtx_sdma *sdma,
+			     struct mvsw_sdma_tx_ring *tx_ring)
 {
-	struct device *dma_dev = sdma->sw->dev->dev;
-	struct sk_buff *new_skb;
-	size_t len = skb->len;
-	dma_addr_t dma;
-
-	dma = dma_map_single(dma_dev, skb->data, len, DMA_TO_DEVICE);
-	if (!dma_mapping_error(dma_dev, dma) && dma + len <= sdma->dma_mask) {
-		buf->buf_dma = dma;
-		buf->skb = skb;
-		return 0;
-	}
-
-	if (!dma_mapping_error(dma_dev, dma))
-		dma_unmap_single(dma_dev, dma, len, DMA_TO_DEVICE);
-
-	new_skb = alloc_skb(len, GFP_ATOMIC | GFP_DMA);
-	if (!new_skb)
-		goto err_alloc_skb;
+	int tx_retry_num = 10 * tx_ring->max_burst;
 
-	dma = dma_map_single(dma_dev, new_skb->data, len, DMA_TO_DEVICE);
-	if (dma_mapping_error(dma_dev, dma))
-		goto err_dma_map;
-	if (dma + len > sdma->dma_mask)
-		goto err_dma_range;
-
-	skb_copy_from_linear_data(skb, skb_put(new_skb, len), len);
-
-	dev_consume_skb_any(skb);
+	while (--tx_retry_num) {
+		if (!(mvsw_reg_read(sdma->sw, SDMA_TX_QUEUE_START_REG) & 1))
+			return 0;
 
-	buf->skb = new_skb;
-	buf->buf_dma = dma;
-
-	return 0;
+		udelay(5);
+	}
 
-err_dma_range:
-	dma_unmap_single(dma_dev, dma, len, DMA_TO_DEVICE);
-err_dma_map:
-	dev_kfree_skb(new_skb);
-err_alloc_skb:
-	dev_kfree_skb(skb);
+	return -EBUSY;
+}
 
-	return -ENOMEM;
+static void mvsw_sdma_start_tx(struct mvsw_pr_rxtx_sdma *sdma)
+{
+	mvsw_reg_write(sdma->sw, SDMA_TX_QUEUE_START_REG, 1);
+	schedule_work(&sdma->tx_work);
 }
 
 netdev_tx_t mvsw_pr_rxtx_sdma_xmit(struct mvsw_pr_rxtx *rxtx,
@@ -608,42 +683,56 @@ netdev_tx_t mvsw_pr_rxtx_sdma_xmit(struct mvsw_pr_rxtx *rxtx,
 {
 	struct mvsw_pr_rxtx_sdma *sdma = rxtx->priv;
 	struct device *dma_dev = sdma->sw->dev->dev;
+	struct mvsw_sdma_tx_ring *tx_ring;
 	struct net_device *dev = skb->dev;
 	struct mvsw_sdma_buf *buf;
-	unsigned int len;
 	int err;
 
-	buf = &sdma->tx_ring.bufs[sdma->tx_ring.next_tx];
+	spin_lock(&sdma->tx_lock);
+
+	tx_ring = &sdma->tx_ring;
+
+	buf = &tx_ring->bufs[tx_ring->next_tx];
 	if (buf->is_used) {
 		schedule_work(&sdma->tx_work);
 		goto drop_skb;
 	}
 
-	if (skb_pad(skb, ETH_ZLEN))
+	if (unlikely(skb_put_padto(skb, ETH_ZLEN)))
 		goto drop_skb_nofree;
 
 	err = mvsw_sdma_tx_buf_map(sdma, buf, skb);
 	if (err)
 		goto drop_skb;
 
-	len = max_t(unsigned int, skb->len, ETH_ZLEN);
+	mvsw_sdma_tx_desc_set_buf(sdma, buf->desc, buf->buf_dma, skb->len);
 
-	mvsw_sdma_tx_desc_set_buf(sdma, buf->desc, buf->buf_dma, len);
+	dma_sync_single_for_device(dma_dev, buf->buf_dma, skb->len,
+				   DMA_TO_DEVICE);
 
-	dma_sync_single_for_device(dma_dev, buf->buf_dma, len, DMA_TO_DEVICE);
+	if (!tx_ring->burst--) {
+		tx_ring->burst = tx_ring->max_burst;
 
-	mvsw_sdma_tx_desc_xmit(buf->desc);
+		err = mvsw_sdma_wait_tx(sdma, tx_ring);
+		if (err)
+			goto drop_skb_unmap;
+	}
 
-	sdma->tx_ring.next_tx = (sdma->tx_ring.next_tx + 1) % SDMA_TX_DESC_NUM;
+	tx_ring->next_tx = (tx_ring->next_tx + 1) % SDMA_TX_DESC_PER_Q;
+	mvsw_sdma_tx_desc_xmit(buf->desc);
 	buf->is_used = true;
 
-	mvsw_reg_write(sdma->sw, SDMA_TX_QUEUE_START_REG, 1);
-	schedule_work(&sdma->tx_work);
-	return NETDEV_TX_OK;
+	mvsw_sdma_start_tx(sdma);
+
+	goto tx_done;
 
+drop_skb_unmap:
+	mvsw_sdma_tx_buf_unmap(sdma, buf);
 drop_skb:
 	dev_consume_skb_any(skb);
 drop_skb_nofree:
 	dev->stats.tx_dropped++;
+tx_done:
+	spin_unlock(&sdma->tx_lock);
 	return NETDEV_TX_OK;
 }
diff --git a/drivers/net/ethernet/marvell/prestera_sw/prestera_switchdev.c b/drivers/net/ethernet/marvell/prestera_sw/prestera_switchdev.c
index 427409c..7c727e7 100644
--- a/drivers/net/ethernet/marvell/prestera_sw/prestera_switchdev.c
+++ b/drivers/net/ethernet/marvell/prestera_sw/prestera_switchdev.c
@@ -14,6 +14,8 @@
 
 #include "prestera.h"
 
+#define MVSW_PR_VID_ALL (0xffff)
+
 struct mvsw_pr_bridge {
 	struct mvsw_pr_switch *sw;
 	u32 ageing_time;
@@ -200,6 +202,10 @@ mvsw_pr_port_vlan_bridge_join(struct mvsw_pr_port_vlan *port_vlan,
 	if (err)
 		goto err_port_learning_set;
 
+	err = mvsw_pr_port_vid_stp_set(port, vid, br_port->stp_state);
+	if (err)
+		goto err_port_vid_stp_set;
+
 	br_vlan = mvsw_pr_bridge_vlan_get(br_port, vid);
 	if (!br_vlan) {
 		err = -ENOMEM;
@@ -214,6 +220,8 @@ mvsw_pr_port_vlan_bridge_join(struct mvsw_pr_port_vlan *port_vlan,
 	return 0;
 
 err_bridge_vlan_get:
+	mvsw_pr_port_vid_stp_set(port, vid, BR_STATE_FORWARDING);
+err_port_vid_stp_set:
 	mvsw_pr_port_learning_set(port, false);
 err_port_learning_set:
 	return err;
@@ -245,11 +253,12 @@ void
 mvsw_pr_port_vlan_bridge_leave(struct mvsw_pr_port_vlan *port_vlan)
 {
 	struct mvsw_pr_port *port = port_vlan->mvsw_pr_port;
+	u32 mode = MVSW_PR_FDB_FLUSH_MODE_DYNAMIC;
 	struct mvsw_pr_bridge_vlan *br_vlan;
 	struct mvsw_pr_bridge_port *br_port;
-	int port_count;
 	u16 vid = port_vlan->vid;
 	bool last_port, last_vlan;
+	int port_count;
 
 	br_port = port_vlan->bridge_port;
 	last_vlan = list_is_singular(&br_port->vlan_list);
@@ -257,18 +266,16 @@ mvsw_pr_port_vlan_bridge_leave(struct mvsw_pr_port_vlan *port_vlan)
 	    mvsw_pr_bridge_vlan_port_count_get(br_port->bridge_device, vid);
 	br_vlan = mvsw_pr_bridge_vlan_find(br_port, vid);
 	last_port = port_count == 1;
-	if (last_vlan) {
-		mvsw_pr_fdb_flush_port(port, MVSW_PR_FDB_FLUSH_MODE_DYNAMIC);
-	} else if (last_port) {
-		mvsw_pr_fdb_flush_vlan(port->sw, vid,
-				       MVSW_PR_FDB_FLUSH_MODE_DYNAMIC);
-	} else {
-		mvsw_pr_fdb_flush_port_vlan(port, vid,
-					    MVSW_PR_FDB_FLUSH_MODE_DYNAMIC);
-	}
+	if (last_vlan)
+		mvsw_pr_fdb_flush_port(port, mode);
+	else if (last_port)
+		mvsw_pr_fdb_flush_vlan(port->sw, vid, mode);
+	else
+		mvsw_pr_fdb_flush_port_vlan(port, vid, mode);
 
 	list_del(&port_vlan->bridge_vlan_node);
 	mvsw_pr_bridge_vlan_put(br_vlan);
+	mvsw_pr_port_vid_stp_set(port, vid, BR_STATE_FORWARDING);
 	mvsw_pr_bridge_port_put(port->sw->bridge, br_port);
 	port_vlan->bridge_port = NULL;
 }
@@ -361,7 +368,7 @@ static int mvsw_pr_port_vlans_add(struct mvsw_pr_port *port,
 	}
 
 	if (list_is_singular(&bridge_device->port_list))
-		mvsw_pr_rif_enable(port->sw, bridge_device->dev);
+		mvsw_pr_rif_enable(port->sw, bridge_device->dev, true);
 
 	return 0;
 }
@@ -520,6 +527,72 @@ static int mvsw_pr_port_attr_br_ageing_set(struct mvsw_pr_port *port,
 	return err;
 }
 
+static int
+mvsw_pr_port_bridge_vlan_stp_set(struct mvsw_pr_port *port,
+				 struct mvsw_pr_bridge_vlan *br_vlan,
+				 u8 state)
+{
+	struct mvsw_pr_port_vlan *port_vlan;
+
+	list_for_each_entry(port_vlan, &br_vlan->port_vlan_list,
+			    bridge_vlan_node) {
+		if (port_vlan->mvsw_pr_port != port)
+			continue;
+		return mvsw_pr_port_vid_stp_set(port, br_vlan->vid, state);
+	}
+
+	return 0;
+}
+
+static int mvsw_pr_port_attr_stp_state_set(struct mvsw_pr_port *port,
+					   struct switchdev_trans *trans,
+					   struct net_device *orig_dev,
+					   u8 state)
+{
+	struct mvsw_pr_bridge_port *br_port;
+	struct mvsw_pr_bridge_vlan *br_vlan;
+	int err;
+	u16 vid;
+
+	if (switchdev_trans_ph_prepare(trans))
+		return 0;
+
+	br_port = mvsw_pr_bridge_port_find(port->sw->bridge, orig_dev);
+	if (!br_port)
+		return 0;
+
+	if (!br_port->bridge_device->vlan_enabled) {
+		vid = br_port->bridge_device->bridge_id;
+		err = mvsw_pr_port_vid_stp_set(port, vid, state);
+		if (err)
+			goto err_port_bridge_stp_set;
+	} else {
+		list_for_each_entry(br_vlan, &br_port->vlan_list,
+				    bridge_port_node) {
+			err = mvsw_pr_port_bridge_vlan_stp_set(port, br_vlan,
+							       state);
+			if (err)
+				goto err_port_bridge_vlan_stp_set;
+		}
+	}
+
+	br_port->stp_state = state;
+
+	return 0;
+
+err_port_bridge_vlan_stp_set:
+	list_for_each_entry_continue_reverse(br_vlan, &br_port->vlan_list,
+					     bridge_port_node)
+		mvsw_pr_port_bridge_vlan_stp_set(port, br_vlan,
+						 br_port->stp_state);
+	return err;
+
+err_port_bridge_stp_set:
+	mvsw_pr_port_vid_stp_set(port, vid, br_port->stp_state);
+
+	return err;
+}
+
 static int mvsw_pr_port_obj_attr_set(struct net_device *dev,
 				     const struct switchdev_attr *attr,
 				     struct switchdev_trans *trans)
@@ -529,7 +602,9 @@ static int mvsw_pr_port_obj_attr_set(struct net_device *dev,
 
 	switch (attr->id) {
 	case SWITCHDEV_ATTR_ID_PORT_STP_STATE:
-		err = -EOPNOTSUPP;
+		err = mvsw_pr_port_attr_stp_state_set(port, trans,
+						      attr->orig_dev,
+						      attr->u.stp_state);
 		break;
 	case SWITCHDEV_ATTR_ID_PORT_PRE_BRIDGE_FLAGS:
 		if (attr->u.brport_flags &
@@ -896,7 +971,7 @@ static void mvsw_pr_bridge_port_put(struct mvsw_pr_bridge *bridge,
 	bridge_device = br_port->bridge_device;
 	mvsw_pr_bridge_port_destroy(br_port);
 	if (list_empty(&bridge_device->port_list))
-		mvsw_pr_rif_disable(bridge->sw, bridge_device->dev);
+		mvsw_pr_rif_enable(bridge->sw, bridge_device->dev, false);
 	mvsw_pr_bridge_device_put(bridge, bridge_device);
 }
 
@@ -941,7 +1016,7 @@ mvsw_pr_bridge_8021d_port_join(struct mvsw_pr_bridge_device *bridge_device,
 		goto err_port_learning_set;
 
 	if (list_is_singular(&bridge_device->port_list))
-		mvsw_pr_rif_enable(port->sw, bridge_device->dev);
+		mvsw_pr_rif_enable(port->sw, bridge_device->dev, true);
 
 	return err;
 
@@ -976,6 +1051,10 @@ static int mvsw_pr_port_bridge_join(struct mvsw_pr_port *port,
 						     port, extack);
 	}
 
+	/* Enslaved port is not usable as a router interface */
+	if (mvsw_pr_rif_exists(sw, port->net_dev))
+		mvsw_pr_rif_enable(sw, port->net_dev, false);
+
 	if (err)
 		goto err_port_join;
 
@@ -1026,7 +1105,86 @@ static void mvsw_pr_port_bridge_leave(struct mvsw_pr_port *port,
 
 	mvsw_pr_port_learning_set(port, false);
 	mvsw_pr_port_flood_set(port, false);
+	mvsw_pr_port_vid_stp_set(port, MVSW_PR_VID_ALL, BR_STATE_FORWARDING);
 	mvsw_pr_bridge_port_put(sw->bridge, br_port);
+
+	/* Offload rif that was previosly disabled */
+	if (mvsw_pr_rif_exists(sw, port->net_dev))
+		mvsw_pr_rif_enable(sw, port->net_dev, true);
+
+}
+
+static bool
+mvsw_pr_lag_master_check(struct mvsw_pr_switch *sw,
+			 struct net_device *lag_dev,
+			 struct netdev_lag_upper_info *upper_info,
+			 struct netlink_ext_ack *ext_ack)
+{
+	u16 lag_id;
+
+	if (mvsw_pr_lag_id_find(sw, lag_dev, &lag_id)) {
+		NL_SET_ERR_MSG_MOD(ext_ack,
+				   "Exceeded max supported LAG devices");
+		return false;
+	}
+	if (upper_info->tx_type != NETDEV_LAG_TX_TYPE_HASH) {
+		NL_SET_ERR_MSG_MOD(ext_ack, "Unsupported LAG Tx type");
+		return false;
+	}
+	return true;
+}
+
+static void mvsw_pr_port_lag_clean(struct mvsw_pr_port *port,
+				   struct net_device *lag_dev)
+{
+	struct net_device *br_dev = netdev_master_upper_dev_get(lag_dev);
+	struct mvsw_pr_port_vlan *port_vlan, *tmp;
+	struct net_device *upper_dev;
+	struct list_head *iter;
+
+	list_for_each_entry_safe(port_vlan, tmp, &port->vlans_list, list) {
+		mvsw_pr_port_vlan_bridge_leave(port_vlan);
+		mvsw_pr_port_vlan_destroy(port_vlan);
+	}
+
+	if (netif_is_bridge_port(lag_dev))
+		mvsw_pr_port_bridge_leave(port, lag_dev, br_dev);
+
+	netdev_for_each_upper_dev_rcu(lag_dev, upper_dev, iter) {
+		if (!netif_is_bridge_port(upper_dev))
+			continue;
+		br_dev = netdev_master_upper_dev_get(upper_dev);
+		mvsw_pr_port_bridge_leave(port, upper_dev, br_dev);
+	}
+
+	mvsw_pr_port_pvid_set(port, MVSW_PR_DEFAULT_VID);
+}
+
+static int mvsw_pr_port_lag_join(struct mvsw_pr_port *port,
+				 struct net_device *lag_dev)
+{
+	u16 lag_id;
+	int err;
+
+	err = mvsw_pr_lag_id_find(port->sw, lag_dev, &lag_id);
+	if (err)
+		return err;
+
+	err = mvsw_pr_lag_member_add(port, lag_dev, lag_id);
+		return err;
+
+	/* TODO: Port should no be longer usable as a router interface */
+
+	return 0;
+}
+
+static void mvsw_pr_port_lag_leave(struct mvsw_pr_port *port,
+				   struct net_device *lag_dev)
+{
+	if (mvsw_pr_lag_member_del(port))
+		return;
+
+	mvsw_pr_port_lag_clean(port, lag_dev);
 }
 
 static int mvsw_pr_netdevice_port_upper_event(struct net_device *lower_dev,
@@ -1048,7 +1206,9 @@ static int mvsw_pr_netdevice_port_upper_event(struct net_device *lower_dev,
 	switch (event) {
 	case NETDEV_PRECHANGEUPPER:
 		upper_dev = info->upper_dev;
-		if (!netif_is_bridge_master(upper_dev)) {
+		if (!netif_is_bridge_master(upper_dev) &&
+		    !netif_is_lag_master(upper_dev) &&
+		    !netif_is_macvlan(upper_dev)) {
 			NL_SET_ERR_MSG_MOD(extack, "Unknown upper device type");
 			return -EINVAL;
 		}
@@ -1061,6 +1221,27 @@ static int mvsw_pr_netdevice_port_upper_event(struct net_device *lower_dev,
 					   "Enslaving a port to a device that already has an upper device is not supported");
 			return -EINVAL;
 		}
+		if (netif_is_lag_master(upper_dev) &&
+		    !mvsw_pr_lag_master_check(sw, upper_dev,
+					      info->upper_info, extack))
+			return -EINVAL;
+		if (netif_is_lag_master(upper_dev) && vlan_uses_dev(dev)) {
+			NL_SET_ERR_MSG_MOD(extack,
+					   "Master device is a LAG master and port has a VLAN");
+			return -EINVAL;
+		}
+		if (netif_is_lag_port(dev) && is_vlan_dev(upper_dev) &&
+		    !netif_is_lag_master(vlan_dev_real_dev(upper_dev))) {
+			NL_SET_ERR_MSG_MOD(extack,
+					   "Can not put a VLAN on a LAG port");
+			return -EINVAL;
+		}
+		if (netif_is_macvlan(upper_dev) &&
+		    !mvsw_pr_rif_exists(sw, lower_dev)) {
+			NL_SET_ERR_MSG_MOD(extack,
+					   "macvlan is only supported on top of router interfaces");
+			return -EOPNOTSUPP;
+		}
 		break;
 	case NETDEV_CHANGEUPPER:
 		upper_dev = info->upper_dev;
@@ -1074,6 +1255,13 @@ static int mvsw_pr_netdevice_port_upper_event(struct net_device *lower_dev,
 				mvsw_pr_port_bridge_leave(port,
 							  lower_dev,
 							  upper_dev);
+		} else if (netif_is_lag_master(upper_dev)) {
+			if (info->linking)
+				err = mvsw_pr_port_lag_join(port,
+							    upper_dev);
+			else
+				mvsw_pr_port_lag_leave(port,
+						       upper_dev);
 		}
 		break;
 	}
@@ -1081,6 +1269,26 @@ static int mvsw_pr_netdevice_port_upper_event(struct net_device *lower_dev,
 	return err;
 }
 
+static int mvsw_pr_netdevice_port_lower_event(struct net_device *dev,
+					      unsigned long event, void *ptr)
+{
+	struct netdev_notifier_changelowerstate_info *info = ptr;
+	struct netdev_lag_lower_state_info *lower_state_info;
+	struct mvsw_pr_port *port = netdev_priv(dev);
+	bool enabled;
+
+	if (event != NETDEV_CHANGELOWERSTATE)
+		return 0;
+	if (!netif_is_lag_port(dev))
+		return 0;
+	if (!mvsw_pr_port_is_lag_member(port))
+		return 0;
+
+	lower_state_info = info->lower_state_info;
+	enabled = lower_state_info->tx_enabled;
+	return mvsw_pr_lag_member_enable(port, enabled);
+}
+
 static int mvsw_pr_netdevice_port_event(struct net_device *lower_dev,
 					struct net_device *port_dev,
 					unsigned long event, void *ptr)
@@ -1091,7 +1299,8 @@ static int mvsw_pr_netdevice_port_event(struct net_device *lower_dev,
 		return mvsw_pr_netdevice_port_upper_event(lower_dev, port_dev,
 							  event, ptr);
 	case NETDEV_CHANGELOWERSTATE:
-		break;
+		return mvsw_pr_netdevice_port_lower_event(port_dev,
+							  event, ptr);
 	}
 
 	return 0;
@@ -1103,6 +1312,7 @@ static int mvsw_pr_netdevice_bridge_event(struct net_device *br_dev,
 	struct mvsw_pr_switch *sw = mvsw_pr_switch_get(br_dev);
 	struct netdev_notifier_changeupper_info *info = ptr;
 	struct netlink_ext_ack *extack;
+	struct net_device *upper_dev;
 
 	if (!sw)
 		return 0;
@@ -1111,7 +1321,19 @@ static int mvsw_pr_netdevice_bridge_event(struct net_device *br_dev,
 
 	switch (event) {
 	case NETDEV_PRECHANGEUPPER:
-		/* TODO:  */
+		upper_dev = info->upper_dev;
+		if (!is_vlan_dev(upper_dev) && !netif_is_macvlan(upper_dev)) {
+			NL_SET_ERR_MSG_MOD(extack, "Unknown upper device type");
+			return -EOPNOTSUPP;
+		}
+		if (!info->linking)
+			break;
+		if (netif_is_macvlan(upper_dev) &&
+		    !mvsw_pr_rif_exists(sw, br_dev)) {
+			NL_SET_ERR_MSG_MOD(extack,
+					   "macvlan is only supported on top of router interfaces");
+			return -EOPNOTSUPP;
+		}
 		break;
 	case NETDEV_CHANGEUPPER:
 		/* TODO:  */
@@ -1121,6 +1343,23 @@ static int mvsw_pr_netdevice_bridge_event(struct net_device *br_dev,
 	return 0;
 }
 
+static int mvsw_pr_netdevice_macvlan_event(struct net_device *macvlan_dev,
+					   unsigned long event, void *ptr)
+{
+	struct mvsw_pr_switch *sw = mvsw_pr_switch_get(macvlan_dev);
+	struct netdev_notifier_changeupper_info *info = ptr;
+	struct netlink_ext_ack *extack;
+
+	if (!sw || event != NETDEV_PRECHANGEUPPER)
+		return 0;
+
+	extack = netdev_notifier_info_to_extack(&info->info);
+
+	NL_SET_ERR_MSG_MOD(extack, "Unknown upper device type");
+
+	return -EOPNOTSUPP;
+}
+
 static bool mvsw_pr_is_vrf_event(unsigned long event, void *ptr)
 {
 	struct netdev_notifier_changeupper_info *info = ptr;
@@ -1131,6 +1370,25 @@ static bool mvsw_pr_is_vrf_event(unsigned long event, void *ptr)
 	return netif_is_l3_master(info->upper_dev);
 }
 
+static int mvsw_pr_netdevice_lag_event(struct net_device *lag_dev,
+				       unsigned long event, void *ptr)
+{
+	struct net_device *dev;
+	struct list_head *iter;
+	int err;
+
+	netdev_for_each_lower_dev(lag_dev, dev, iter) {
+		if (mvsw_pr_netdev_check(dev)) {
+			err = mvsw_pr_netdevice_port_event(lag_dev, dev, event,
+							   ptr);
+			if (err)
+				return err;
+		}
+	}
+
+	return 0;
+}
+
 static int mvsw_pr_netdevice_event(struct notifier_block *nb,
 				   unsigned long event, void *ptr)
 {
@@ -1149,6 +1407,10 @@ static int mvsw_pr_netdevice_event(struct notifier_block *nb,
 		err = mvsw_pr_netdevice_port_event(dev, dev, event, ptr);
 	else if (netif_is_bridge_master(dev))
 		err = mvsw_pr_netdevice_bridge_event(dev, event, ptr);
+	else if (netif_is_lag_master(dev))
+		err = mvsw_pr_netdevice_lag_event(dev, event, ptr);
+	else if (netif_is_macvlan(dev))
+		err = mvsw_pr_netdevice_macvlan_event(dev, event, ptr);
 
 	return notifier_from_errno(err);
 }
