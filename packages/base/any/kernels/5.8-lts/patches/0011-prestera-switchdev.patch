diff --git a/drivers/net/ethernet/marvell/Kconfig b/drivers/net/ethernet/marvell/Kconfig
index 3d5caea..4e48aad 100644
--- a/drivers/net/ethernet/marvell/Kconfig
+++ b/drivers/net/ethernet/marvell/Kconfig
@@ -171,5 +171,6 @@ config SKY2_DEBUG
 
 
 source "drivers/net/ethernet/marvell/octeontx2/Kconfig"
+source "drivers/net/ethernet/marvell/prestera_sw/Kconfig"
 
 endif # NET_VENDOR_MARVELL
diff --git a/drivers/net/ethernet/marvell/Makefile b/drivers/net/ethernet/marvell/Makefile
index 89dea72..b7b5a81 100644
--- a/drivers/net/ethernet/marvell/Makefile
+++ b/drivers/net/ethernet/marvell/Makefile
@@ -11,4 +11,5 @@ obj-$(CONFIG_MVPP2) += mvpp2/
 obj-$(CONFIG_PXA168_ETH) += pxa168_eth.o
 obj-$(CONFIG_SKGE) += skge.o
 obj-$(CONFIG_SKY2) += sky2.o
+obj-$(CONFIG_MRVL_PRESTERA_SW) += prestera_sw/
 obj-y		+= octeontx2/
diff --git a/drivers/net/ethernet/marvell/prestera_sw/Kconfig b/drivers/net/ethernet/marvell/prestera_sw/Kconfig
new file mode 100644
index 0000000..2d7eefe
--- /dev/null
+++ b/drivers/net/ethernet/marvell/prestera_sw/Kconfig
@@ -0,0 +1,38 @@
+# SPDX-License-Identifier: GPL-2.0-only
+#
+# Marvell switch drivers configuration
+#
+
+config MRVL_PRESTERA_SW
+	tristate "Marvell Technologies Prestera switchdev support"
+	depends on NET_SWITCHDEV
+	depends on BRIDGE
+	default m
+	---help---
+	  This driver supports Marvell Technologies Prestera Switchdev
+	  Ethernet Switch ASICs.
+
+	  To compile this driver as a module, choose M here: the
+	  module will be called prestera_sw.
+
+config MRVL_PRESTERA_NL
+	tristate "Marvell's Prestera Switchdev IPC Netlink kernel module"
+	depends on MRVL_PRESTERA_SW
+	default m
+	---help---
+	  This driver supports Marvell's Prestera Switchdev IPC
+          Netlink kernel module
+
+	  To compile this driver as a module, choose M here: the
+	  module will be called prestera_nl.
+
+config MRVL_PRESTERA_PCI
+	tristate "Marvell's Prestera Switchdev IPC PCI kernel module"
+	depends on MRVL_PRESTERA_SW
+	default m
+	---help---
+	  This driver supports Marvell's Prestera Switchdev IPC
+          PCI kernel module
+
+	  To compile this driver as a module, choose M here: the
+	  module will be called prestera_nl.
diff --git a/drivers/net/ethernet/marvell/prestera_sw/Makefile b/drivers/net/ethernet/marvell/prestera_sw/Makefile
new file mode 100644
index 0000000..ff2d290
--- /dev/null
+++ b/drivers/net/ethernet/marvell/prestera_sw/Makefile
@@ -0,0 +1,25 @@
+# SPDX-License-Identifier: GPL-2.0
+#
+# Makefile for the Marvell Switch driver.
+#
+
+obj-$(CONFIG_MRVL_PRESTERA_SW) += prestera_sw.o
+prestera_sw-objs := prestera.o \
+	prestera_hw.o prestera_switchdev.o prestera_fw_log.o \
+	prestera_rxtx.o prestera_rxtx_eth.o prestera_rxtx_mvpp.o \
+	prestera_rxtx_sdma.o prestera_dsa.o prestera_router.o \
+	prestera_acl.o prestera_flower.o
+
+prestera_sw-$(CONFIG_MRVL_PRESTERA_DEBUG) += prestera_log.o
+ccflags-$(CONFIG_MRVL_PRESTERA_DEBUG) += -DCONFIG_MRVL_PRESTERA_DEBUG
+
+obj-$(CONFIG_MRVL_PRESTERA_NL) += prestera_nl.o
+prestera_nl-objs := netlink.o
+
+obj-$(CONFIG_MRVL_PRESTERA_PCI) += prestera_pci.o
+
+# testing infrastructure
+obj-$(CONFIG_MRVL_PRESTERA_TESTS) += prestera_tests.o
+prestera_tests-objs := tests/prestera_tests.o tests/prestera_ipc_tests.o
+CFLAGS_prestera_ipc_tests.o += -I$(src)
+CFLAGS_prestera_tests.o += -I$(src) -I$(src)/tests
diff --git a/drivers/net/ethernet/marvell/prestera_sw/netlink.c b/drivers/net/ethernet/marvell/prestera_sw/netlink.c
new file mode 100644
index 0000000..36fda4d
--- /dev/null
+++ b/drivers/net/ethernet/marvell/prestera_sw/netlink.c
@@ -0,0 +1,435 @@
+/* SPDX-License-Identifier: BSD-3-Clause OR GPL-2.0
+ *
+ * Copyright (c) 2019-2020 Marvell International Ltd. All rights reserved.
+ *
+ */
+
+#include <linux/module.h>
+#include <linux/netlink.h>
+#include <linux/version.h>
+#include <net/genetlink.h>
+#include <linux/completion.h>
+#include <linux/types.h>
+
+#include "prestera.h"
+#include "prestera_hw.h"
+#include "prestera_log.h"
+
+/* Protects send_sync function */
+static DEFINE_MUTEX(mvsw_nl_send_mtx);
+static DECLARE_COMPLETION(mvsw_nl_receive_cmpl);
+
+#define NL_SYNC_TIMEOUT 500000
+
+#define MVSW_NL_MCGRP_NAME	"pgrp_nl"
+#define MVSW_NL_MCGRP_ID	0
+
+#define MVSW_NL_NAME    "prestera_nl"
+#define MVSW_NL_VERSION 0x1
+
+enum {
+	MVSW_NL_TYPE_NEW_MSG,
+	MVSW_NL_TYPE_REPLY,
+	MVSW_NL_TYPE_READY,
+	MVSW_NL_TYPE_EVENT,
+	MVSW_NL_TYPE_MAX,
+};
+
+enum {
+	MVSW_NL_ATTR_UNSPEC,
+	MVSW_NL_ATTR_TLV,
+	MVSW_NL_ATTR_READY_ACK,
+	__MVSW_NL_ATTR_MAX,
+	MVSW_NL_ATTR_MAX = __MVSW_NL_ATTR_MAX - 1
+};
+
+struct mvsw_nl_event_t {
+	struct work_struct work;
+	struct mvsw_pr_device *dev;
+	size_t len;
+	char data[];
+};
+
+static struct work_struct nl_bus_work;
+
+/* TODO: change to use list in case of supporting more than 1 switch device */
+static struct mvsw_pr_device *dev;
+static struct workqueue_struct *mvsw_nl_owq;
+
+static int _reply_errno;
+static int _reply_msg_size;
+static u8 _reply_msg[MVSW_MSG_MAX_SIZE];
+
+static const struct genl_multicast_group mvsw_genl_mcgrps[] = {
+	[MVSW_NL_MCGRP_ID] = {.name = MVSW_NL_MCGRP_NAME}
+};
+
+static const struct nla_policy mvsw_nl_policy[__MVSW_NL_ATTR_MAX] = {
+	[MVSW_NL_ATTR_UNSPEC] = {.type = NLA_UNSPEC},
+	[MVSW_NL_ATTR_TLV] = {.type = NLA_UNSPEC},
+};
+
+static int mvsw_nl_handle_reply(struct sk_buff *skb, struct genl_info *info);
+static int mvsw_nl_handle_ready(struct sk_buff *skb, struct genl_info *info);
+static int mvsw_nl_handle_event(struct sk_buff *skb, struct genl_info *info);
+
+static const struct genl_ops mvsw_genl_ops[] = {
+	{
+	 .cmd = MVSW_NL_TYPE_REPLY,
+	 .validate = GENL_DONT_VALIDATE_STRICT | GENL_DONT_VALIDATE_DUMP,
+	 .doit = mvsw_nl_handle_reply},
+	{
+	 .cmd = MVSW_NL_TYPE_READY,
+	 .validate = GENL_DONT_VALIDATE_STRICT | GENL_DONT_VALIDATE_DUMP,
+	 .doit = mvsw_nl_handle_ready},
+	{
+	 .cmd = MVSW_NL_TYPE_EVENT,
+	 .validate = GENL_DONT_VALIDATE_STRICT | GENL_DONT_VALIDATE_DUMP,
+	 .doit = mvsw_nl_handle_event},
+};
+
+static struct genl_family mvsw_genl_family = {
+	.name = MVSW_NL_NAME,
+	.version = MVSW_NL_VERSION,
+	.maxattr = MVSW_NL_ATTR_MAX,
+	.module = THIS_MODULE,
+	.ops = mvsw_genl_ops,
+	.n_ops = ARRAY_SIZE(mvsw_genl_ops),
+	.netnsok = true,
+	.mcgrps = mvsw_genl_mcgrps,
+	.n_mcgrps = ARRAY_SIZE(mvsw_genl_mcgrps),
+	.policy = mvsw_nl_policy,
+};
+
+static int mvsw_nl_handle_reply(struct sk_buff *skb, struct genl_info *info)
+{
+	size_t size;
+
+	if (!info->attrs[MVSW_NL_ATTR_TLV]) {
+		MVSW_LOG_ERROR("Reply msg does not containt required nl attr");
+		_reply_errno = -EINVAL;
+		goto handler_reply_unlock;
+	}
+
+	size = nla_len(info->attrs[MVSW_NL_ATTR_TLV]);
+	if (size >= MVSW_MSG_MAX_SIZE) {
+		MVSW_LOG_ERROR("NL recv buffer overflow");
+		_reply_errno = -EMSGSIZE;
+		goto handler_reply_unlock;
+	}
+
+	nla_memcpy(_reply_msg, info->attrs[MVSW_NL_ATTR_TLV], size);
+	_reply_msg_size = size;
+	_reply_errno = 0;
+
+handler_reply_unlock:
+	complete(&mvsw_nl_receive_cmpl);
+	return 0;
+}
+
+static int mvsw_nl_get_reply(unsigned int timeout, u8 *msg, size_t size,
+			     size_t *recv_bytes)
+{
+	int err = 0;
+
+	err = wait_for_completion_timeout(&mvsw_nl_receive_cmpl,
+					  usecs_to_jiffies(timeout));
+	if (!err) {
+		MVSW_LOG_ERROR("Reply msg is missed");
+		return err;
+	}
+
+	if (_reply_msg_size > size) {
+		MVSW_LOG_ERROR("Reply msg overflow");
+		return -EMSGSIZE;
+	}
+
+	memcpy(msg, _reply_msg, _reply_msg_size);
+	*recv_bytes = _reply_msg_size;
+	err = _reply_errno;
+
+	return err;
+}
+
+static int mvsw_nl_send_sync(unsigned int wait, u8 *in_msg, size_t in_size,
+			     u8 *out_msg, size_t out_size)
+{
+	size_t out_data_size;
+	int err = 0;
+	struct sk_buff *nl_msg;
+	void *hdr;
+
+	if (!wait)
+		wait = NL_SYNC_TIMEOUT;
+
+	nl_msg = nlmsg_new(NLMSG_DEFAULT_SIZE, GFP_KERNEL);
+	if (!nl_msg) {
+		MVSW_LOG_ERROR("Failed to allocate a new nl msg");
+		return -ENOMEM;
+	}
+
+	hdr = genlmsg_put(nl_msg, 0, 0, &mvsw_genl_family, 0,
+			  MVSW_NL_TYPE_NEW_MSG);
+	if (!hdr) {
+		MVSW_LOG_ERROR("Failed to put cmd to nl msg");
+		err = -EMSGSIZE;
+		goto nl_send_err;
+	}
+
+	err = nla_put(nl_msg, MVSW_NL_ATTR_TLV, in_size, in_msg);
+	if (err) {
+		MVSW_LOG_ERROR("Failed to put data to nl msg");
+		goto nl_send_err;
+	}
+
+	mutex_lock(&mvsw_nl_send_mtx);
+
+	genlmsg_end(nl_msg, hdr);
+
+	err = genlmsg_multicast(&mvsw_genl_family, nl_msg, 0,
+				MVSW_NL_MCGRP_ID, GFP_ATOMIC);
+	if (err) {
+		mutex_unlock(&mvsw_nl_send_mtx);
+		MVSW_LOG_ERROR("Failed to send genl msg");
+		return err;
+	}
+
+	err = mvsw_nl_get_reply(wait, out_msg, out_size, &out_data_size);
+	if (err)
+		MVSW_LOG_ERROR("Failed to receive msg reply");
+
+	mutex_unlock(&mvsw_nl_send_mtx);
+	return err;
+
+nl_send_err:
+	genlmsg_cancel(nl_msg, hdr);
+	nlmsg_free(nl_msg);
+	return err;
+}
+
+/* lets keep this code a bit in case async mechanism will be used */
+#ifdef MVSW_PR_FW_USE_ASYNC_IPC
+static int mvsw_nl_send_async(u8 *in_msg, size_t in_size)
+{
+	int err = 0;
+	struct sk_buff *nl_msg;
+	void *hdr;
+
+	nl_msg = nlmsg_new(NLMSG_DEFAULT_SIZE, GFP_KERNEL);
+	if (!nl_msg) {
+		MVSW_LOG_ERROR("Failed to allocate a new nl msg");
+		return -ENOMEM;
+	}
+
+	hdr = genlmsg_put(nl_msg, 0, 0, &mvsw_genl_family, 0,
+			  MVSW_NL_TYPE_NEW_MSG);
+	if (!hdr) {
+		MVSW_LOG_ERROR("Failed to put cmd to nl msg");
+		err = -EMSGSIZE;
+		goto async_nl_send_err;
+	}
+
+	err = nla_put(nl_msg, MVSW_NL_ATTR_TLV, in_size, in_msg);
+	if (err) {
+		MVSW_LOG_ERROR("Failed to put data to nl msg");
+		goto async_nl_send_err;
+	}
+
+	genlmsg_end(nl_msg, hdr);
+
+	err = genlmsg_multicast(&mvsw_genl_family, nl_msg, 0,
+				MVSW_NL_MCGRP_ID, GFP_ATOMIC);
+	if (err)
+		MVSW_LOG_ERROR("Failed to send genl msg");
+
+	return err;
+
+async_nl_send_err:
+	genlmsg_cancel(nl_msg, hdr);
+	nlmsg_free(nl_msg);
+	return err;
+}
+#endif
+
+static int mvsw_nl_send_req(struct mvsw_pr_device *dev,
+			    u8 *in_msg, size_t in_size,
+			    u8 *out_msg, size_t out_size,
+			    unsigned int wait)
+{
+	return mvsw_nl_send_sync(wait, in_msg, in_size, out_msg, out_size);
+}
+
+static int mvsw_nl_bus_register(void)
+{
+	int err;
+
+	err = genl_register_family(&mvsw_genl_family);
+	if (err) {
+		pr_err("Failed to initialize netlink channel\n");
+		return err;
+	}
+
+	return err;
+}
+
+static void mvsw_nl_bus_unregister(void)
+{
+	genl_unregister_family(&mvsw_genl_family);
+	/* Check if some dev is registered */
+	if (dev) {
+		mvsw_pr_device_unregister(dev);
+		kfree(dev->dev);
+		kfree(dev);
+		dev = NULL;
+	}
+}
+
+static void nl_register_device_work(struct work_struct *work)
+{
+	struct device *nl_device = kzalloc(sizeof(*nl_device), GFP_KERNEL);
+
+	dev = kzalloc(sizeof(*dev), GFP_KERNEL);
+	dev->send_req = mvsw_nl_send_req;
+
+	nl_device->init_name = mvsw_genl_family.name;
+	dev->dev = nl_device;
+
+	if (mvsw_pr_device_register(dev)) {
+		pr_err("Failed registering prestera device\n");
+		kfree(dev->dev);
+		kfree(dev);
+		dev = NULL;
+	}
+}
+
+static int mvsw_nl_handle_ready(struct sk_buff *skb, struct genl_info *info)
+{
+	int err;
+	struct sk_buff *nl_msg;
+	void *hdr;
+
+	nl_msg = nlmsg_new(NLMSG_DEFAULT_SIZE, GFP_KERNEL);
+	if (!nl_msg) {
+		MVSW_LOG_ERROR("Failed to allocate a new nl msg");
+		return -ENOMEM;
+	}
+
+	hdr = genlmsg_put_reply(nl_msg, info, &mvsw_genl_family, 0,
+				MVSW_NL_TYPE_READY);
+	if (!hdr) {
+		MVSW_LOG_ERROR("Failed to put cmd to nl msg");
+		err = -EMSGSIZE;
+		goto ready_reply_err;
+	}
+
+	err = nla_put_flag(nl_msg, MVSW_NL_ATTR_READY_ACK);
+	if (err) {
+		MVSW_LOG_ERROR("Failed to put ready ack");
+		goto ready_reply_err;
+	}
+
+	genlmsg_end(nl_msg, hdr);
+	err = genlmsg_reply(nl_msg, info);
+
+	if (err) {
+		MVSW_LOG_ERROR("Failed to send ready reply");
+		return 0;
+	}
+
+	INIT_WORK(&nl_bus_work, nl_register_device_work);
+	schedule_work(&nl_bus_work);
+
+	return 0;
+
+ready_reply_err:
+	genlmsg_cancel(nl_msg, hdr);
+	nlmsg_free(nl_msg);
+	return 0;
+}
+
+static void mvsw_nl_event_process(struct work_struct *work)
+{
+	int err;
+	struct mvsw_nl_event_t *ev = container_of(work,
+						  struct mvsw_nl_event_t, work);
+
+	err = ev->dev->recv_msg(ev->dev, ev->data, ev->len);
+	if (err)
+		pr_err("recv_msg failed with errno %d", err);
+
+	kfree(ev);
+}
+
+static int mvsw_nl_handle_event(struct sk_buff *skb, struct genl_info *info)
+{
+	size_t len;
+	struct mvsw_nl_event_t *ev;
+
+	if (!dev || !dev->recv_msg)
+		return 0;
+
+	if (!info->attrs[MVSW_NL_ATTR_TLV]) {
+		MVSW_LOG_ERROR("Msg missing MVSW_NL_ATTR_TLV attr");
+		return -EINVAL;
+	}
+
+	len = nla_len(info->attrs[MVSW_NL_ATTR_TLV]);
+
+	ev = kmalloc(sizeof(*ev) + len, GFP_KERNEL);
+	if (!ev)
+		return -ENOMEM;
+
+	ev->dev = dev;
+	ev->len = len;
+	memcpy(ev->data, nla_data(info->attrs[MVSW_NL_ATTR_TLV]), len);
+
+	INIT_WORK(&ev->work, mvsw_nl_event_process);
+	queue_work(mvsw_nl_owq, &ev->work);
+
+	return 0;
+}
+
+static int __init mvsw_pr_nl_init(void)
+{
+	int err;
+
+	pr_info("Loading Marvell Prestera Netlink Driver\n");
+
+	mvsw_nl_owq = alloc_ordered_workqueue("%s_ordered", 0, MVSW_NL_NAME);
+
+	if (!mvsw_nl_owq) {
+		err = -ENOMEM;
+		goto exit;
+	}
+
+	err = mvsw_nl_bus_register();
+
+	if (err) {
+		destroy_workqueue(mvsw_nl_owq);
+		goto exit;
+	}
+
+exit:
+	if (err)
+		pr_err("Loading Marvell Prestera Netlink Driver failed!");
+
+	return err;
+}
+
+static void __exit mvsw_pr_nl_exit(void)
+{
+	flush_workqueue(mvsw_nl_owq);
+
+	mvsw_nl_bus_unregister();
+
+	destroy_workqueue(mvsw_nl_owq);
+
+	pr_info("Unloading Marvell Prestera Netlink Driver\n");
+}
+
+module_init(mvsw_pr_nl_init);
+module_exit(mvsw_pr_nl_exit);
+
+MODULE_AUTHOR("Marvell Semi.");
+MODULE_LICENSE("GPL");
+MODULE_DESCRIPTION("Marvell Prestera netlink driver");
diff --git a/drivers/net/ethernet/marvell/prestera_sw/prestera.c b/drivers/net/ethernet/marvell/prestera_sw/prestera.c
new file mode 100644
index 0000000..42d43dc
--- /dev/null
+++ b/drivers/net/ethernet/marvell/prestera_sw/prestera.c
@@ -0,0 +1,1881 @@
+/* SPDX-License-Identifier: BSD-3-Clause OR GPL-2.0
+ *
+ * Copyright (c) 2019-2020 Marvell International Ltd. All rights reserved.
+ *
+ */
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/list.h>
+#include <linux/netdevice.h>
+#include <linux/netdev_features.h>
+#include <linux/inetdevice.h>
+#include <linux/etherdevice.h>
+#include <linux/ethtool.h>
+#include <linux/jiffies.h>
+#include <net/switchdev.h>
+
+#include "prestera.h"
+#include "prestera_hw.h"
+#include "prestera_fw_log.h"
+#include "prestera_dsa.h"
+#include "prestera_rxtx.h"
+#include "prestera_drv_ver.h"
+
+#define MVSW_PR_MTU_DEFAULT 1536
+
+#define PORT_STATS_CACHE_TIMEOUT_MS	(msecs_to_jiffies(1000))
+#define PORT_STATS_CNT	(sizeof(struct mvsw_pr_port_stats) / sizeof(u64))
+#define PORT_STATS_IDX(name) \
+	(offsetof(struct mvsw_pr_port_stats, name) / sizeof(u64))
+#define PORT_STATS_FIELD(name)	\
+	[PORT_STATS_IDX(name)] = __stringify(name)
+
+static struct list_head switches_registered;
+
+static const char mvsw_driver_kind[] = "prestera_sw";
+static const char mvsw_driver_name[] = "mvsw_switchdev";
+static const char mvsw_driver_version[] = PRESTERA_DRV_VER;
+
+#define mvsw_dev(sw)		((sw)->dev->dev)
+#define mvsw_dev_name(sw)	dev_name((sw)->dev->dev)
+
+static struct workqueue_struct *mvsw_pr_wq;
+
+struct mvsw_pr_link_mode {
+	enum ethtool_link_mode_bit_indices eth_mode;
+	u32 speed;
+	u64 pr_mask;
+	u8 duplex;
+	u8 port_type;
+};
+
+static const struct mvsw_pr_link_mode
+mvsw_pr_link_modes[MVSW_LINK_MODE_MAX] = {
+	[MVSW_LINK_MODE_10baseT_Half_BIT] = {
+		.eth_mode =  ETHTOOL_LINK_MODE_10baseT_Half_BIT,
+		.speed = 10,
+		.pr_mask = 1 << MVSW_LINK_MODE_10baseT_Half_BIT,
+		.duplex = MVSW_PORT_DUPLEX_HALF,
+		.port_type = MVSW_PORT_TYPE_TP,
+	},
+	[MVSW_LINK_MODE_10baseT_Full_BIT] = {
+		.eth_mode =  ETHTOOL_LINK_MODE_10baseT_Full_BIT,
+		.speed = 10,
+		.pr_mask = 1 << MVSW_LINK_MODE_10baseT_Full_BIT,
+		.duplex = MVSW_PORT_DUPLEX_FULL,
+		.port_type = MVSW_PORT_TYPE_TP,
+	},
+	[MVSW_LINK_MODE_100baseT_Half_BIT] = {
+		.eth_mode =  ETHTOOL_LINK_MODE_100baseT_Half_BIT,
+		.speed = 100,
+		.pr_mask = 1 << MVSW_LINK_MODE_100baseT_Half_BIT,
+		.duplex = MVSW_PORT_DUPLEX_HALF,
+		.port_type = MVSW_PORT_TYPE_TP,
+	},
+	[MVSW_LINK_MODE_100baseT_Full_BIT] = {
+		.eth_mode =  ETHTOOL_LINK_MODE_100baseT_Full_BIT,
+		.speed = 100,
+		.pr_mask = 1 << MVSW_LINK_MODE_100baseT_Full_BIT,
+		.duplex = MVSW_PORT_DUPLEX_FULL,
+		.port_type = MVSW_PORT_TYPE_TP,
+	},
+	[MVSW_LINK_MODE_1000baseT_Half_BIT] = {
+		.eth_mode =  ETHTOOL_LINK_MODE_1000baseT_Half_BIT,
+		.speed = 1000,
+		.pr_mask = 1 << MVSW_LINK_MODE_1000baseT_Half_BIT,
+		.duplex = MVSW_PORT_DUPLEX_HALF,
+		.port_type = MVSW_PORT_TYPE_TP,
+	},
+	[MVSW_LINK_MODE_1000baseT_Full_BIT] = {
+		.eth_mode =  ETHTOOL_LINK_MODE_1000baseT_Full_BIT,
+		.speed = 1000,
+		.pr_mask = 1 << MVSW_LINK_MODE_1000baseT_Full_BIT,
+		.duplex = MVSW_PORT_DUPLEX_FULL,
+		.port_type = MVSW_PORT_TYPE_TP,
+	},
+	[MVSW_LINK_MODE_1000baseX_Full_BIT] = {
+		.eth_mode = ETHTOOL_LINK_MODE_1000baseX_Full_BIT,
+		.speed = 1000,
+		.pr_mask = 1 << MVSW_LINK_MODE_1000baseX_Full_BIT,
+		.duplex = MVSW_PORT_DUPLEX_FULL,
+		.port_type = MVSW_PORT_TYPE_FIBRE,
+	},
+	[MVSW_LINK_MODE_1000baseKX_Full_BIT] = {
+		.eth_mode = ETHTOOL_LINK_MODE_1000baseKX_Full_BIT,
+		.speed = 1000,
+		.pr_mask = 1 << MVSW_LINK_MODE_1000baseKX_Full_BIT,
+		.duplex = MVSW_PORT_DUPLEX_FULL,
+		.port_type = MVSW_PORT_TYPE_TP,
+	},
+	[MVSW_LINK_MODE_2500baseX_Full_BIT] = {
+		.eth_mode =  ETHTOOL_LINK_MODE_2500baseX_Full_BIT,
+		.speed = 2500,
+		.pr_mask = 1 << MVSW_LINK_MODE_2500baseX_Full_BIT,
+		.duplex = MVSW_PORT_DUPLEX_FULL,
+	},
+	[MVSW_LINK_MODE_10GbaseKR_Full_BIT] = {
+		.eth_mode = ETHTOOL_LINK_MODE_10000baseKR_Full_BIT,
+		.speed = 10000,
+		.pr_mask = 1 << MVSW_LINK_MODE_10GbaseKR_Full_BIT,
+		.duplex = MVSW_PORT_DUPLEX_FULL,
+		.port_type = MVSW_PORT_TYPE_TP,
+	},
+	[MVSW_LINK_MODE_10GbaseSR_Full_BIT] = {
+		.eth_mode = ETHTOOL_LINK_MODE_10000baseSR_Full_BIT,
+		.speed = 10000,
+		.pr_mask = 1 << MVSW_LINK_MODE_10GbaseSR_Full_BIT,
+		.duplex = MVSW_PORT_DUPLEX_FULL,
+		.port_type = MVSW_PORT_TYPE_FIBRE,
+	},
+	[MVSW_LINK_MODE_10GbaseLR_Full_BIT] = {
+		.eth_mode = ETHTOOL_LINK_MODE_10000baseLR_Full_BIT,
+		.speed = 10000,
+		.pr_mask = 1 << MVSW_LINK_MODE_10GbaseLR_Full_BIT,
+		.duplex = MVSW_PORT_DUPLEX_FULL,
+		.port_type = MVSW_PORT_TYPE_FIBRE,
+	},
+	[MVSW_LINK_MODE_20GbaseKR2_Full_BIT] = {
+		.eth_mode = ETHTOOL_LINK_MODE_20000baseKR2_Full_BIT,
+		.speed = 20000,
+		.pr_mask = 1 << MVSW_LINK_MODE_20GbaseKR2_Full_BIT,
+		.duplex = MVSW_PORT_DUPLEX_FULL,
+		.port_type = MVSW_PORT_TYPE_TP,
+	},
+	[MVSW_LINK_MODE_25GbaseCR_Full_BIT] = {
+		.eth_mode = ETHTOOL_LINK_MODE_25000baseCR_Full_BIT,
+		.speed = 25000,
+		.pr_mask = 1 << MVSW_LINK_MODE_25GbaseCR_Full_BIT,
+		.duplex = MVSW_PORT_DUPLEX_FULL,
+		.port_type = MVSW_PORT_TYPE_DA,
+	},
+	[MVSW_LINK_MODE_25GbaseKR_Full_BIT] = {
+		.eth_mode = ETHTOOL_LINK_MODE_25000baseKR_Full_BIT,
+		.speed = 25000,
+		.pr_mask = 1 << MVSW_LINK_MODE_25GbaseKR_Full_BIT,
+		.duplex = MVSW_PORT_DUPLEX_FULL,
+		.port_type = MVSW_PORT_TYPE_TP,
+	},
+	[MVSW_LINK_MODE_25GbaseSR_Full_BIT] = {
+		.eth_mode = ETHTOOL_LINK_MODE_25000baseSR_Full_BIT,
+		.speed = 25000,
+		.pr_mask = 1 << MVSW_LINK_MODE_25GbaseSR_Full_BIT,
+		.duplex = MVSW_PORT_DUPLEX_FULL,
+		.port_type = MVSW_PORT_TYPE_FIBRE,
+	},
+	[MVSW_LINK_MODE_40GbaseKR4_Full_BIT] = {
+		.eth_mode = ETHTOOL_LINK_MODE_40000baseKR4_Full_BIT,
+		.speed = 40000,
+		.pr_mask = 1 << MVSW_LINK_MODE_40GbaseKR4_Full_BIT,
+		.duplex = MVSW_PORT_DUPLEX_FULL,
+		.port_type = MVSW_PORT_TYPE_TP,
+	},
+	[MVSW_LINK_MODE_40GbaseCR4_Full_BIT] = {
+		.eth_mode = ETHTOOL_LINK_MODE_40000baseCR4_Full_BIT,
+		.speed = 40000,
+		.pr_mask = 1 << MVSW_LINK_MODE_40GbaseCR4_Full_BIT,
+		.duplex = MVSW_PORT_DUPLEX_FULL,
+		.port_type = MVSW_PORT_TYPE_DA,
+	},
+	[MVSW_LINK_MODE_40GbaseSR4_Full_BIT] = {
+		.eth_mode = ETHTOOL_LINK_MODE_40000baseSR4_Full_BIT,
+		.speed = 40000,
+		.pr_mask = 1 << MVSW_LINK_MODE_40GbaseSR4_Full_BIT,
+		.duplex = MVSW_PORT_DUPLEX_FULL,
+		.port_type = MVSW_PORT_TYPE_FIBRE,
+	},
+	[MVSW_LINK_MODE_50GbaseCR2_Full_BIT] = {
+		.eth_mode = ETHTOOL_LINK_MODE_50000baseCR2_Full_BIT,
+		.speed = 50000,
+		.pr_mask = 1 << MVSW_LINK_MODE_50GbaseCR2_Full_BIT,
+		.duplex = MVSW_PORT_DUPLEX_FULL,
+		.port_type = MVSW_PORT_TYPE_DA,
+	},
+	[MVSW_LINK_MODE_50GbaseKR2_Full_BIT] = {
+		.eth_mode = ETHTOOL_LINK_MODE_50000baseKR2_Full_BIT,
+		.speed = 50000,
+		.pr_mask = 1 << MVSW_LINK_MODE_50GbaseKR2_Full_BIT,
+		.duplex = MVSW_PORT_DUPLEX_FULL,
+		.port_type = MVSW_PORT_TYPE_TP,
+	},
+	[MVSW_LINK_MODE_50GbaseSR2_Full_BIT] = {
+		.eth_mode = ETHTOOL_LINK_MODE_50000baseSR2_Full_BIT,
+		.speed = 50000,
+		.pr_mask = 1 << MVSW_LINK_MODE_50GbaseSR2_Full_BIT,
+		.duplex = MVSW_PORT_DUPLEX_FULL,
+		.port_type = MVSW_PORT_TYPE_FIBRE,
+	},
+	[MVSW_LINK_MODE_100GbaseKR4_Full_BIT] = {
+		.eth_mode = ETHTOOL_LINK_MODE_100000baseKR4_Full_BIT,
+		.speed = 100000,
+		.pr_mask = 1 << MVSW_LINK_MODE_100GbaseKR4_Full_BIT,
+		.duplex = MVSW_PORT_DUPLEX_FULL,
+		.port_type = MVSW_PORT_TYPE_TP,
+	},
+	[MVSW_LINK_MODE_100GbaseSR4_Full_BIT] = {
+		.eth_mode = ETHTOOL_LINK_MODE_100000baseSR4_Full_BIT,
+		.speed = 100000,
+		.pr_mask = 1 << MVSW_LINK_MODE_100GbaseSR4_Full_BIT,
+		.duplex = MVSW_PORT_DUPLEX_FULL,
+		.port_type = MVSW_PORT_TYPE_FIBRE,
+	},
+	[MVSW_LINK_MODE_100GbaseCR4_Full_BIT] = {
+		.eth_mode = ETHTOOL_LINK_MODE_100000baseCR4_Full_BIT,
+		.speed = 100000,
+		.pr_mask = 1 << MVSW_LINK_MODE_100GbaseCR4_Full_BIT,
+		.duplex = MVSW_PORT_DUPLEX_FULL,
+		.port_type = MVSW_PORT_TYPE_DA,
+	}
+};
+
+struct mvsw_pr_fec {
+	u32 eth_fec;
+	enum ethtool_link_mode_bit_indices eth_mode;
+	u8 pr_fec;
+};
+
+static const struct mvsw_pr_fec mvsw_pr_fec_caps[MVSW_PORT_FEC_MAX] = {
+	[MVSW_PORT_FEC_OFF_BIT] = {
+		.eth_fec = ETHTOOL_FEC_OFF,
+		.eth_mode = ETHTOOL_LINK_MODE_FEC_NONE_BIT,
+		.pr_fec = 1 << MVSW_PORT_FEC_OFF_BIT,
+	},
+	[MVSW_PORT_FEC_BASER_BIT] = {
+		.eth_fec = ETHTOOL_FEC_BASER,
+		.eth_mode = ETHTOOL_LINK_MODE_FEC_BASER_BIT,
+		.pr_fec = 1 << MVSW_PORT_FEC_BASER_BIT,
+	},
+	[MVSW_PORT_FEC_RS_BIT] = {
+		.eth_fec = ETHTOOL_FEC_RS,
+		.eth_mode = ETHTOOL_LINK_MODE_FEC_RS_BIT,
+		.pr_fec = 1 << MVSW_PORT_FEC_RS_BIT,
+	}
+};
+
+struct mvsw_pr_port_type {
+	enum ethtool_link_mode_bit_indices eth_mode;
+	u8 eth_type;
+};
+
+static const struct mvsw_pr_port_type
+mvsw_pr_port_types[MVSW_PORT_TYPE_MAX] = {
+	[MVSW_PORT_TYPE_NONE] = {
+		.eth_mode = __ETHTOOL_LINK_MODE_MASK_NBITS,
+		.eth_type = PORT_NONE,
+	},
+	[MVSW_PORT_TYPE_TP] = {
+		.eth_mode = ETHTOOL_LINK_MODE_TP_BIT,
+		.eth_type = PORT_TP,
+	},
+	[MVSW_PORT_TYPE_AUI] = {
+		.eth_mode = ETHTOOL_LINK_MODE_AUI_BIT,
+		.eth_type = PORT_AUI,
+	},
+	[MVSW_PORT_TYPE_MII] = {
+		.eth_mode = ETHTOOL_LINK_MODE_MII_BIT,
+		.eth_type = PORT_MII,
+	},
+	[MVSW_PORT_TYPE_FIBRE] = {
+		.eth_mode = ETHTOOL_LINK_MODE_FIBRE_BIT,
+		.eth_type = PORT_FIBRE,
+	},
+	[MVSW_PORT_TYPE_BNC] = {
+		.eth_mode = ETHTOOL_LINK_MODE_BNC_BIT,
+		.eth_type = PORT_BNC,
+	},
+	[MVSW_PORT_TYPE_DA] = {
+		.eth_mode = ETHTOOL_LINK_MODE_TP_BIT,
+		.eth_type = PORT_TP,
+	},
+	[MVSW_PORT_TYPE_OTHER] = {
+		.eth_mode = __ETHTOOL_LINK_MODE_MASK_NBITS,
+		.eth_type = PORT_OTHER,
+	}
+};
+
+static const char mvsw_pr_port_cnt_name[PORT_STATS_CNT][ETH_GSTRING_LEN] = {
+	PORT_STATS_FIELD(good_octets_received),
+	PORT_STATS_FIELD(bad_octets_received),
+	PORT_STATS_FIELD(mac_trans_error),
+	PORT_STATS_FIELD(broadcast_frames_received),
+	PORT_STATS_FIELD(multicast_frames_received),
+	PORT_STATS_FIELD(frames_64_octets),
+	PORT_STATS_FIELD(frames_65_to_127_octets),
+	PORT_STATS_FIELD(frames_128_to_255_octets),
+	PORT_STATS_FIELD(frames_256_to_511_octets),
+	PORT_STATS_FIELD(frames_512_to_1023_octets),
+	PORT_STATS_FIELD(frames_1024_to_max_octets),
+	PORT_STATS_FIELD(excessive_collision),
+	PORT_STATS_FIELD(multicast_frames_sent),
+	PORT_STATS_FIELD(broadcast_frames_sent),
+	PORT_STATS_FIELD(fc_sent),
+	PORT_STATS_FIELD(fc_received),
+	PORT_STATS_FIELD(buffer_overrun),
+	PORT_STATS_FIELD(undersize),
+	PORT_STATS_FIELD(fragments),
+	PORT_STATS_FIELD(oversize),
+	PORT_STATS_FIELD(jabber),
+	PORT_STATS_FIELD(rx_error_frame_received),
+	PORT_STATS_FIELD(bad_crc),
+	PORT_STATS_FIELD(collisions),
+	PORT_STATS_FIELD(late_collision),
+	PORT_STATS_FIELD(unicast_frames_received),
+	PORT_STATS_FIELD(unicast_frames_sent),
+	PORT_STATS_FIELD(sent_multiple),
+	PORT_STATS_FIELD(sent_deferred),
+	PORT_STATS_FIELD(good_octets_sent),
+};
+
+static LIST_HEAD(mvsw_pr_block_cb_list);
+
+static struct mvsw_pr_port *__find_pr_port(const struct mvsw_pr_switch *sw,
+					   u32 port_id)
+{
+	struct mvsw_pr_port *port;
+
+	list_for_each_entry(port, &sw->port_list, list) {
+		if (port->id == port_id)
+			return port;
+	}
+
+	return NULL;
+}
+
+static int mvsw_pr_port_state_set(struct net_device *dev, bool is_up)
+{
+	struct mvsw_pr_port *port = netdev_priv(dev);
+	int err;
+
+	if (!is_up)
+		netif_stop_queue(dev);
+
+	err = mvsw_pr_hw_port_state_set(port, is_up);
+
+	if (is_up && !err)
+		netif_start_queue(dev);
+
+	return err;
+}
+
+static int mvsw_pr_port_get_port_parent_id(struct net_device *dev,
+					   struct netdev_phys_item_id *ppid)
+{
+	const struct mvsw_pr_port *port = netdev_priv(dev);
+
+	ppid->id_len = sizeof(port->sw->id);
+
+	memcpy(&ppid->id, &port->sw->id, ppid->id_len);
+	return 0;
+}
+
+static int mvsw_pr_port_get_phys_port_name(struct net_device *dev,
+					   char *buf, size_t len)
+{
+	const struct mvsw_pr_port *port = netdev_priv(dev);
+
+	snprintf(buf, len, "%u", port->fp_id);
+	return 0;
+}
+
+static int mvsw_pr_port_open(struct net_device *dev)
+{
+	return mvsw_pr_port_state_set(dev, true);
+}
+
+static int mvsw_pr_port_close(struct net_device *dev)
+{
+	return mvsw_pr_port_state_set(dev, false);
+}
+
+static netdev_tx_t mvsw_pr_port_xmit(struct sk_buff *skb,
+				     struct net_device *dev)
+{
+	struct mvsw_pr_port *port = netdev_priv(dev);
+	struct mvsw_pr_rxtx_info rxtx_info = {
+		.port_id = port->id
+	};
+
+	return mvsw_pr_rxtx_xmit(skb, &rxtx_info);
+}
+
+/* TC flower */
+static int
+mvsw_pr_setup_tc_cls_flower(struct mvsw_pr_acl_block *acl_block,
+			    struct flow_cls_offload *f)
+{
+	struct mvsw_pr_switch *sw = mvsw_pr_acl_block_sw(acl_block);
+
+	if (f->common.chain_index != 0)
+		return -EOPNOTSUPP;
+
+	switch (f->command) {
+	case FLOW_CLS_REPLACE:
+		return mvsw_pr_flower_replace(sw, acl_block, f);
+	case FLOW_CLS_DESTROY:
+		mvsw_pr_flower_destroy(sw, acl_block, f);
+		return 0;
+	case FLOW_CLS_STATS:
+		return mvsw_pr_flower_stats(sw, acl_block, f);
+	default:
+		return -EOPNOTSUPP;
+	}
+}
+
+static int mvsw_pr_setup_tc_block_cb_flower(enum tc_setup_type type,
+					    void *type_data, void *cb_priv)
+{
+	struct mvsw_pr_acl_block *acl_block = cb_priv;
+
+	switch (type) {
+	case TC_SETUP_CLSFLOWER:
+		if (mvsw_pr_acl_block_disabled(acl_block))
+			return -EOPNOTSUPP;
+		return mvsw_pr_setup_tc_cls_flower(acl_block, type_data);
+	default:
+		return -EOPNOTSUPP;
+	}
+}
+
+static void mvsw_pr_tc_block_flower_release(void *cb_priv)
+{
+	struct mvsw_pr_acl_block *acl_block = cb_priv;
+
+	mvsw_pr_acl_block_destroy(acl_block);
+}
+
+static int
+mvsw_pr_setup_tc_block_flower_bind(struct mvsw_pr_port *port,
+				   struct flow_block_offload *f)
+{
+	struct mvsw_pr_switch *sw = port->sw;
+	struct mvsw_pr_acl_block *acl_block;
+	struct flow_block_cb *block_cb;
+	bool register_block = false;
+	bool disable_block = false;
+	int err;
+
+	block_cb = flow_block_cb_lookup(f->block,
+					mvsw_pr_setup_tc_block_cb_flower, sw);
+	if (!block_cb) {
+		acl_block = mvsw_pr_acl_block_create(sw, f->net);
+		if (!acl_block)
+			return -ENOMEM;
+		block_cb = flow_block_cb_alloc(mvsw_pr_setup_tc_block_cb_flower,
+					       sw, acl_block,
+					       mvsw_pr_tc_block_flower_release);
+		if (IS_ERR(block_cb)) {
+			mvsw_pr_acl_block_destroy(acl_block);
+			err = PTR_ERR(block_cb);
+			goto err_cb_register;
+		}
+		register_block = true;
+	} else {
+		acl_block = flow_block_cb_priv(block_cb);
+	}
+	flow_block_cb_incref(block_cb);
+
+	if (!tc_can_offload(port->net_dev)) {
+		if (mvsw_pr_acl_block_rule_count(acl_block)) {
+			err = -EOPNOTSUPP;
+			goto err_block_bind;
+		}
+
+		disable_block = true;
+	}
+
+	err = mvsw_pr_acl_block_bind(sw, acl_block, port);
+	if (err)
+		goto err_block_bind;
+
+	if (register_block) {
+		flow_block_cb_add(block_cb, f);
+		list_add_tail(&block_cb->driver_list, &mvsw_pr_block_cb_list);
+	}
+
+	if (disable_block)
+		mvsw_pr_acl_block_disable_inc(acl_block);
+
+	port->acl_block = acl_block;
+	return 0;
+
+err_block_bind:
+	if (!flow_block_cb_decref(block_cb))
+		flow_block_cb_free(block_cb);
+err_cb_register:
+	return err;
+}
+
+static void
+mvsw_pr_setup_tc_block_flower_unbind(struct mvsw_pr_port *port,
+				     struct flow_block_offload *f)
+{
+	struct mvsw_pr_switch *sw = port->sw;
+	struct mvsw_pr_acl_block *acl_block;
+	struct flow_block_cb *block_cb;
+	int err;
+
+	block_cb = flow_block_cb_lookup(f->block,
+					mvsw_pr_setup_tc_block_cb_flower, sw);
+	if (!block_cb)
+		return;
+
+	acl_block = flow_block_cb_priv(block_cb);
+
+	if (!tc_can_offload(port->net_dev))
+		mvsw_pr_acl_block_disable_dec(acl_block);
+
+	err = mvsw_pr_acl_block_unbind(sw, acl_block, port);
+	if (!err && !flow_block_cb_decref(block_cb)) {
+		flow_block_cb_remove(block_cb, f);
+		list_del(&block_cb->driver_list);
+	}
+	port->acl_block = NULL;
+}
+
+static int mvsw_sp_setup_tc_block(struct mvsw_pr_port *port,
+				  struct flow_block_offload *f)
+{
+	if (f->binder_type != FLOW_BLOCK_BINDER_TYPE_CLSACT_INGRESS)
+		return -EOPNOTSUPP;
+
+	f->driver_block_list = &mvsw_pr_block_cb_list;
+
+	switch (f->command) {
+	case FLOW_BLOCK_BIND:
+		return mvsw_pr_setup_tc_block_flower_bind(port, f);
+	case FLOW_BLOCK_UNBIND:
+		mvsw_pr_setup_tc_block_flower_unbind(port, f);
+		return 0;
+	default:
+		return -EOPNOTSUPP;
+	}
+}
+
+static int mvsw_pr_setup_tc(struct net_device *dev, enum tc_setup_type type,
+			    void *type_data)
+{
+	struct mvsw_pr_port *port = netdev_priv(dev);
+
+	switch (type) {
+	case TC_SETUP_BLOCK:
+		return mvsw_sp_setup_tc_block(port, type_data);
+	default:
+		return -EOPNOTSUPP;
+	}
+}
+
+static void mvsw_pr_set_rx_mode(struct net_device *dev)
+{
+	/* TO DO: add implementation */
+}
+
+static int mvsw_is_valid_mac_addr(struct mvsw_pr_port *port, u8 *addr)
+{
+	int err;
+
+	if (!is_valid_ether_addr(addr))
+		return -EADDRNOTAVAIL;
+
+	err = memcmp(port->sw->base_mac, addr, ETH_ALEN - 1);
+	if (err)
+		return -EINVAL;
+
+	return 0;
+}
+
+static int mvsw_pr_port_set_mac_address(struct net_device *dev, void *p)
+{
+	struct mvsw_pr_port *port = netdev_priv(dev);
+	struct sockaddr *addr = p;
+	int err;
+
+	err = mvsw_is_valid_mac_addr(port, addr->sa_data);
+	if (err)
+		return err;
+
+	err = mvsw_pr_hw_port_mac_set(port, addr->sa_data);
+	if (!err)
+		memcpy(dev->dev_addr, addr->sa_data, dev->addr_len);
+
+	return err;
+}
+
+static int mvsw_pr_port_change_mtu(struct net_device *dev, int mtu)
+{
+	struct mvsw_pr_port *port = netdev_priv(dev);
+	int err;
+
+	if (port->sw->mtu_min <= mtu && mtu <= port->sw->mtu_max)
+		err = mvsw_pr_hw_port_mtu_set(port, mtu);
+	else
+		err = -EINVAL;
+
+	if (!err)
+		dev->mtu = mtu;
+
+	return err;
+}
+
+static void mvsw_pr_port_get_stats64(struct net_device *dev,
+				     struct rtnl_link_stats64 *stats)
+{
+	struct mvsw_pr_port *port = netdev_priv(dev);
+	struct mvsw_pr_port_stats *port_stats = &port->cached_hw_stats.stats;
+
+	stats->rx_packets =	port_stats->broadcast_frames_received +
+				port_stats->multicast_frames_received +
+				port_stats->unicast_frames_received;
+
+	stats->tx_packets =	port_stats->broadcast_frames_sent +
+				port_stats->multicast_frames_sent +
+				port_stats->unicast_frames_sent;
+
+	stats->rx_bytes = port_stats->good_octets_received;
+
+	stats->tx_bytes = port_stats->good_octets_sent;
+
+	stats->rx_errors = port_stats->rx_error_frame_received;
+	stats->tx_errors = port_stats->mac_trans_error;
+
+	stats->rx_dropped = port_stats->buffer_overrun;
+	stats->tx_dropped = 0;
+
+	stats->multicast = port_stats->multicast_frames_received;
+	stats->collisions = port_stats->excessive_collision;
+
+	stats->rx_crc_errors = port_stats->bad_crc;
+}
+
+static void mvsw_pr_port_get_hw_stats(struct mvsw_pr_port *port)
+{
+	mvsw_pr_hw_port_stats_get(port, &port->cached_hw_stats.stats);
+}
+
+static void update_stats_cache(struct work_struct *work)
+{
+	struct mvsw_pr_port *port =
+		container_of(work, struct mvsw_pr_port,
+			     cached_hw_stats.caching_dw.work);
+
+	mvsw_pr_port_get_hw_stats(port);
+
+	queue_delayed_work(mvsw_pr_wq, &port->cached_hw_stats.caching_dw,
+			   PORT_STATS_CACHE_TIMEOUT_MS);
+}
+
+static bool mvsw_pr_port_has_offload_stats(const struct net_device *dev,
+					   int attr_id)
+{
+	/* TO DO: add implementation */
+	return false;
+}
+
+static int mvsw_pr_port_get_offload_stats(int attr_id,
+					  const struct net_device *dev,
+					  void *sp)
+{
+	/* TO DO: add implementation */
+	return 0;
+}
+
+static int mvsw_pr_feature_hw_tc(struct net_device *dev, bool enable)
+{
+	struct mvsw_pr_port *port = netdev_priv(dev);
+
+	if (!enable) {
+		if (mvsw_pr_acl_block_rule_count(port->acl_block)) {
+			netdev_err(dev, "Active offloaded tc filters, can't turn hw_tc_offload off\n");
+			return -EINVAL;
+		}
+		mvsw_pr_acl_block_disable_inc(port->acl_block);
+	} else {
+		mvsw_pr_acl_block_disable_dec(port->acl_block);
+	}
+	return 0;
+}
+
+static int
+mvsw_pr_handle_feature(struct net_device *dev,
+		       netdev_features_t wanted_features,
+		       netdev_features_t feature,
+		       int (*feature_handler)(struct net_device *dev,
+					      bool enable))
+{
+	netdev_features_t changes = wanted_features ^ dev->features;
+	bool enable = !!(wanted_features & feature);
+	int err;
+
+	if (!(changes & feature))
+		return 0;
+
+	err = feature_handler(dev, enable);
+	if (err) {
+		netdev_err(dev, "%s feature %pNF failed, err %d\n",
+			   enable ? "Enable" : "Disable", &feature, err);
+		return err;
+	}
+
+	if (enable)
+		dev->features |= feature;
+	else
+		dev->features &= ~feature;
+
+	return 0;
+}
+
+static int mvsw_pr_set_features(struct net_device *dev,
+				netdev_features_t features)
+{
+	netdev_features_t oper_features = dev->features;
+	int err = 0;
+
+	err |= mvsw_pr_handle_feature(dev, features, NETIF_F_HW_TC,
+				       mvsw_pr_feature_hw_tc);
+
+	if (err) {
+		dev->features = oper_features;
+		return -EINVAL;
+	}
+
+	return 0;
+}
+
+static void mvsw_pr_port_get_drvinfo(struct net_device *dev,
+				     struct ethtool_drvinfo *drvinfo)
+{
+	struct mvsw_pr_port *port = netdev_priv(dev);
+	struct mvsw_pr_switch *sw = port->sw;
+
+	strlcpy(drvinfo->driver, mvsw_driver_kind, sizeof(drvinfo->driver));
+	strlcpy(drvinfo->bus_info, mvsw_dev_name(sw), sizeof(drvinfo->bus_info));
+	snprintf(drvinfo->fw_version, sizeof(drvinfo->fw_version),
+		 "%d.%d.%d",
+		 sw->dev->fw_rev.maj,
+		 sw->dev->fw_rev.min,
+		 sw->dev->fw_rev.sub);
+}
+
+static const struct net_device_ops mvsw_pr_netdev_ops = {
+	.ndo_open = mvsw_pr_port_open,
+	.ndo_stop = mvsw_pr_port_close,
+	.ndo_start_xmit = mvsw_pr_port_xmit,
+	.ndo_setup_tc = mvsw_pr_setup_tc,
+	.ndo_change_mtu = mvsw_pr_port_change_mtu,
+	.ndo_set_rx_mode = mvsw_pr_set_rx_mode,
+	.ndo_get_stats64 = mvsw_pr_port_get_stats64,
+	.ndo_set_features = mvsw_pr_set_features,
+	.ndo_set_mac_address = mvsw_pr_port_set_mac_address,
+	.ndo_has_offload_stats = mvsw_pr_port_has_offload_stats,
+	.ndo_get_offload_stats = mvsw_pr_port_get_offload_stats,
+	.ndo_get_phys_port_name = mvsw_pr_port_get_phys_port_name,
+	.ndo_get_port_parent_id = mvsw_pr_port_get_port_parent_id
+};
+
+bool mvsw_pr_netdev_check(const struct net_device *dev)
+{
+	return dev->netdev_ops == &mvsw_pr_netdev_ops;
+}
+
+static int mvsw_pr_lower_dev_walk(struct net_device *lower_dev, void *data)
+{
+	struct mvsw_pr_port **pport = data;
+
+	if (mvsw_pr_netdev_check(lower_dev)) {
+		*pport = netdev_priv(lower_dev);
+		return 1;
+	}
+
+	return 0;
+}
+
+struct mvsw_pr_port *mvsw_pr_port_dev_lower_find(struct net_device *dev)
+{
+	struct mvsw_pr_port *port;
+
+	if (mvsw_pr_netdev_check(dev))
+		return netdev_priv(dev);
+
+	port = NULL;
+	netdev_walk_all_lower_dev(dev, mvsw_pr_lower_dev_walk, &port);
+
+	return port;
+}
+
+struct mvsw_pr_switch *mvsw_pr_switch_get(struct net_device *dev)
+{
+	struct mvsw_pr_port *port;
+
+	port = mvsw_pr_port_dev_lower_find(dev);
+	return port ? port->sw : NULL;
+}
+
+static void mvsw_modes_to_eth(unsigned long *eth_modes, u64 link_modes, u8 fec,
+			      u8 type)
+{
+	u32 mode;
+
+	for (mode = 0; mode < MVSW_LINK_MODE_MAX; mode++) {
+		if ((mvsw_pr_link_modes[mode].pr_mask & link_modes) == 0)
+			continue;
+		if (type != MVSW_PORT_TYPE_NONE &&
+		    mvsw_pr_link_modes[mode].port_type != type)
+			continue;
+		__set_bit(mvsw_pr_link_modes[mode].eth_mode, eth_modes);
+	}
+
+	for (mode = 0; mode < MVSW_PORT_FEC_MAX; mode++) {
+		if ((mvsw_pr_fec_caps[mode].pr_fec & fec) == 0)
+			continue;
+		__set_bit(mvsw_pr_fec_caps[mode].eth_mode, eth_modes);
+	}
+}
+
+static void mvsw_modes_from_eth(const unsigned long *eth_modes, u64 *link_modes,
+				u8 *fec, u8 type)
+{
+	u32 mode;
+
+	for (mode = 0; mode < MVSW_LINK_MODE_MAX; mode++) {
+		if (!test_bit(mvsw_pr_link_modes[mode].eth_mode, eth_modes))
+			continue;
+		if (mvsw_pr_link_modes[mode].port_type != type)
+			continue;
+		*link_modes |= mvsw_pr_link_modes[mode].pr_mask;
+	}
+
+	for (mode = 0; mode < MVSW_PORT_FEC_MAX; mode++) {
+		if (!test_bit(mvsw_pr_fec_caps[mode].eth_mode, eth_modes))
+			continue;
+		*fec |= mvsw_pr_fec_caps[mode].pr_fec;
+	}
+}
+
+static void mvsw_pr_port_supp_types_get(struct ethtool_link_ksettings *ecmd,
+					struct mvsw_pr_port *port)
+{
+	u32 mode;
+	u8 ptype;
+
+	for (mode = 0; mode < MVSW_LINK_MODE_MAX; mode++) {
+		if ((mvsw_pr_link_modes[mode].pr_mask &
+		    port->caps.supp_link_modes) == 0)
+			continue;
+		ptype = mvsw_pr_link_modes[mode].port_type;
+		__set_bit(mvsw_pr_port_types[ptype].eth_mode,
+			  ecmd->link_modes.supported);
+	}
+}
+
+static void mvsw_pr_port_speed_get(struct ethtool_link_ksettings *ecmd,
+				   struct mvsw_pr_port *port)
+{
+	u32 speed;
+	int err;
+
+	err = mvsw_pr_hw_port_speed_get(port, &speed);
+	ecmd->base.speed = !err ? speed : SPEED_UNKNOWN;
+}
+
+static int mvsw_pr_port_link_mode_set(struct mvsw_pr_port *port,
+				      u32 speed, u8 duplex, u8 type)
+{
+	u32 new_mode = MVSW_LINK_MODE_MAX;
+	u32 mode;
+
+	for (mode = 0; mode < MVSW_LINK_MODE_MAX; mode++) {
+		if (speed != mvsw_pr_link_modes[mode].speed)
+			continue;
+		if (duplex != mvsw_pr_link_modes[mode].duplex)
+			continue;
+		if (!(mvsw_pr_link_modes[mode].pr_mask &
+		    port->caps.supp_link_modes))
+			continue;
+		if (type != mvsw_pr_link_modes[mode].port_type)
+			continue;
+
+		new_mode = mode;
+		break;
+	}
+
+	if (new_mode == MVSW_LINK_MODE_MAX) {
+		netdev_err(port->net_dev, "Unsupported speed/duplex requested");
+		return -EINVAL;
+	}
+
+	return mvsw_pr_hw_port_link_mode_set(port, new_mode);
+}
+
+static int mvsw_pr_port_speed_duplex_set(const struct ethtool_link_ksettings
+					 *ecmd, struct mvsw_pr_port *port)
+{
+	int err;
+	u8 duplex;
+	u32 speed;
+	u32 curr_mode;
+
+	err = mvsw_pr_hw_port_link_mode_get(port, &curr_mode);
+	if (err || curr_mode >= MVSW_LINK_MODE_MAX)
+		return -EINVAL;
+
+	if (ecmd->base.duplex != DUPLEX_UNKNOWN)
+		duplex = ecmd->base.duplex == DUPLEX_FULL ?
+			 MVSW_PORT_DUPLEX_FULL : MVSW_PORT_DUPLEX_HALF;
+	else
+		duplex = mvsw_pr_link_modes[curr_mode].duplex;
+
+	if (ecmd->base.speed != SPEED_UNKNOWN)
+		speed = ecmd->base.speed;
+	else
+		speed = mvsw_pr_link_modes[curr_mode].speed;
+
+	return mvsw_pr_port_link_mode_set(port, speed, duplex, port->caps.type);
+}
+
+static u8 mvsw_pr_port_type_get(struct mvsw_pr_port *port)
+{
+	if (port->caps.type < MVSW_PORT_TYPE_MAX)
+		return mvsw_pr_port_types[port->caps.type].eth_type;
+	return PORT_OTHER;
+}
+
+static int mvsw_pr_port_type_set(const struct ethtool_link_ksettings *ecmd,
+				 struct mvsw_pr_port *port)
+{
+	int err;
+	u32 type, mode;
+	u32 new_mode = MVSW_LINK_MODE_MAX;
+
+	for (type = 0; type < MVSW_PORT_TYPE_MAX; type++) {
+		if (mvsw_pr_port_types[type].eth_type == ecmd->base.port &&
+		    test_bit(mvsw_pr_port_types[type].eth_mode,
+			     ecmd->link_modes.supported)) {
+			break;
+		}
+	}
+
+	if (type == port->caps.type)
+		return 0;
+
+	if (type != port->caps.type && ecmd->base.autoneg == AUTONEG_ENABLE)
+		return -EINVAL;
+
+	if (type == MVSW_PORT_TYPE_MAX) {
+		pr_err("Unsupported port type requested\n");
+		return -EINVAL;
+	}
+
+	for (mode = 0; mode < MVSW_LINK_MODE_MAX; mode++) {
+		if ((mvsw_pr_link_modes[mode].pr_mask &
+		    port->caps.supp_link_modes) &&
+		    type == mvsw_pr_link_modes[mode].port_type) {
+			new_mode = mode;
+		}
+	}
+
+	if (new_mode < MVSW_LINK_MODE_MAX)
+		err = mvsw_pr_hw_port_link_mode_set(port, new_mode);
+	else
+		err = -EINVAL;
+
+	if (!err) {
+		port->caps.type = type;
+		port->autoneg = false;
+	}
+
+	return err;
+}
+
+static void mvsw_pr_port_remote_cap_get(struct ethtool_link_ksettings *ecmd,
+					struct mvsw_pr_port *port)
+{
+	u64 bitmap;
+	bool pause;
+	bool asym_pause;
+
+	if (!mvsw_pr_hw_port_remote_cap_get(port, &bitmap)) {
+		mvsw_modes_to_eth(ecmd->link_modes.lp_advertising,
+				  bitmap, 0, MVSW_PORT_TYPE_NONE);
+
+		if (!bitmap_empty(ecmd->link_modes.lp_advertising,
+				  __ETHTOOL_LINK_MODE_MASK_NBITS)) {
+			ethtool_link_ksettings_add_link_mode(ecmd,
+							     lp_advertising,
+							     Autoneg);
+		}
+	}
+
+	if (mvsw_pr_hw_port_remote_fc_get(port, &pause, &asym_pause))
+		return;
+	if (pause)
+		ethtool_link_ksettings_add_link_mode(ecmd,
+						     lp_advertising,
+						     Pause);
+	if (asym_pause)
+		ethtool_link_ksettings_add_link_mode(ecmd,
+						     lp_advertising,
+						     Asym_Pause);
+}
+
+static void mvsw_pr_port_duplex_get(struct ethtool_link_ksettings *ecmd,
+				    struct mvsw_pr_port *port)
+{
+	u8 duplex;
+
+	if (!mvsw_pr_hw_port_duplex_get(port, &duplex)) {
+		ecmd->base.duplex = duplex == MVSW_PORT_DUPLEX_FULL ?
+				    DUPLEX_FULL : DUPLEX_HALF;
+	} else {
+		ecmd->base.duplex = DUPLEX_UNKNOWN;
+	}
+}
+
+static int mvsw_pr_port_autoneg_set(struct mvsw_pr_port *port, bool enable,
+				    u64 adver_link_modes, u8 adver_fec)
+{
+	u64 link_modes;
+	bool refresh = false;
+	int err = 0;
+	u8 fec;
+
+	if (port->caps.type != MVSW_PORT_TYPE_TP)
+		return enable ? -EINVAL : 0;
+
+	if (!enable)
+		goto set_autoneg;
+
+	link_modes = port->caps.supp_link_modes & adver_link_modes;
+	fec = port->caps.supp_fec & adver_fec;
+
+	if (!link_modes && !fec) {
+		netdev_err(port->net_dev, "Unsupported link mode requested");
+		return -EINVAL;
+	}
+
+	if (link_modes && port->adver_link_modes != link_modes) {
+		port->adver_link_modes = link_modes;
+		refresh = true;
+	}
+
+	if (fec && port->adver_fec != fec) {
+		port->adver_fec = fec;
+		refresh = true;
+	}
+
+set_autoneg:
+	if (port->autoneg == enable && !refresh)
+		return 0;
+
+	err = mvsw_pr_hw_port_autoneg_set(port, enable,
+					  port->adver_link_modes,
+					  port->adver_fec);
+	if (err)
+		return -EINVAL;
+
+	port->autoneg = enable;
+	return 0;
+}
+
+static int mvsw_pr_port_nway_reset(struct net_device *dev)
+{
+	struct mvsw_pr_port *port = netdev_priv(dev);
+
+	if (netif_running(dev) &&
+	    port->caps.transceiver == MVSW_PORT_TRANSCEIVER_COPPER &&
+	    port->caps.type == MVSW_PORT_TYPE_TP)
+		return mvsw_pr_hw_port_autoneg_restart(port);
+
+	return -EINVAL;
+}
+
+static int mvsw_pr_port_mdix_set(const struct ethtool_link_ksettings *ecmd,
+				 struct mvsw_pr_port *port)
+{
+	if (ecmd->base.eth_tp_mdix_ctrl != ETH_TP_MDI_INVALID &&
+	    port->caps.transceiver == MVSW_PORT_TRANSCEIVER_COPPER &&
+	    port->caps.type == MVSW_PORT_TYPE_TP)
+		return mvsw_pr_hw_port_mdix_set(port,
+						ecmd->base.eth_tp_mdix_ctrl);
+	return 0;
+}
+
+static int mvsw_pr_port_get_link_ksettings(struct net_device *dev,
+					   struct ethtool_link_ksettings *ecmd)
+{
+	struct mvsw_pr_port *port = netdev_priv(dev);
+
+	ethtool_link_ksettings_zero_link_mode(ecmd, supported);
+	ethtool_link_ksettings_zero_link_mode(ecmd, advertising);
+	ethtool_link_ksettings_zero_link_mode(ecmd, lp_advertising);
+
+	ecmd->base.autoneg = port->autoneg ? AUTONEG_ENABLE : AUTONEG_DISABLE;
+
+	if (port->caps.type == MVSW_PORT_TYPE_TP) {
+		ethtool_link_ksettings_add_link_mode(ecmd, supported, Autoneg);
+		if (netif_running(dev) &&
+		    (port->autoneg ||
+		     port->caps.transceiver == MVSW_PORT_TRANSCEIVER_COPPER))
+			ethtool_link_ksettings_add_link_mode(ecmd, advertising,
+							     Autoneg);
+	}
+
+	mvsw_modes_to_eth(ecmd->link_modes.supported,
+			  port->caps.supp_link_modes,
+			  port->caps.supp_fec,
+			  port->caps.type);
+
+	mvsw_pr_port_supp_types_get(ecmd, port);
+
+	if (netif_carrier_ok(dev)) {
+		mvsw_pr_port_speed_get(ecmd, port);
+		mvsw_pr_port_duplex_get(ecmd, port);
+	} else {
+		ecmd->base.speed = SPEED_UNKNOWN;
+		ecmd->base.duplex = DUPLEX_UNKNOWN;
+	}
+
+	ecmd->base.port = mvsw_pr_port_type_get(port);
+
+	if (port->autoneg) {
+		if (netif_running(dev))
+			mvsw_modes_to_eth(ecmd->link_modes.advertising,
+					  port->adver_link_modes,
+					  port->adver_fec,
+					  port->caps.type);
+
+		if (netif_carrier_ok(dev) &&
+		    port->caps.transceiver == MVSW_PORT_TRANSCEIVER_COPPER)
+			mvsw_pr_port_remote_cap_get(ecmd, port);
+	}
+
+	if (port->caps.type == MVSW_PORT_TYPE_TP &&
+	    port->caps.transceiver == MVSW_PORT_TRANSCEIVER_COPPER)
+		mvsw_pr_hw_port_mdix_get(port, &ecmd->base.eth_tp_mdix,
+					 &ecmd->base.eth_tp_mdix_ctrl);
+
+	return 0;
+}
+
+static int mvsw_pr_port_set_link_ksettings(struct net_device *dev,
+					   const struct ethtool_link_ksettings
+					   *ecmd)
+{
+	struct mvsw_pr_port *port = netdev_priv(dev);
+	u64 adver_modes = 0;
+	u8 adver_fec = 0;
+	int err;
+
+	err = mvsw_pr_port_type_set(ecmd, port);
+	if (err)
+		return err;
+
+	if (port->caps.transceiver == MVSW_PORT_TRANSCEIVER_COPPER) {
+		err = mvsw_pr_port_mdix_set(ecmd, port);
+		if (err)
+			return err;
+	}
+
+	mvsw_modes_from_eth(ecmd->link_modes.advertising, &adver_modes,
+			    &adver_fec, port->caps.type);
+
+	err = mvsw_pr_port_autoneg_set(port,
+				       ecmd->base.autoneg == AUTONEG_ENABLE,
+				       adver_modes, adver_fec);
+	if (err)
+		return err;
+
+	if (ecmd->base.autoneg == AUTONEG_DISABLE) {
+		err = mvsw_pr_port_speed_duplex_set(ecmd, port);
+		if (err)
+			return err;
+	}
+
+	return 0;
+}
+
+static int mvsw_pr_port_get_fecparam(struct net_device *dev,
+				     struct ethtool_fecparam *fecparam)
+{
+	struct mvsw_pr_port *port = netdev_priv(dev);
+	u32 mode;
+	u8 active;
+	int err;
+
+	err = mvsw_pr_hw_port_fec_get(port, &active);
+	if (err)
+		return err;
+
+	fecparam->fec = 0;
+	for (mode = 0; mode < MVSW_PORT_FEC_MAX; mode++) {
+		if ((mvsw_pr_fec_caps[mode].pr_fec & port->caps.supp_fec) == 0)
+			continue;
+		fecparam->fec |= mvsw_pr_fec_caps[mode].eth_fec;
+	}
+
+	if (active < MVSW_PORT_FEC_MAX)
+		fecparam->active_fec = mvsw_pr_fec_caps[active].eth_fec;
+	else
+		fecparam->active_fec = ETHTOOL_FEC_AUTO;
+
+	return 0;
+}
+
+static int mvsw_pr_port_set_fecparam(struct net_device *dev,
+				     struct ethtool_fecparam *fecparam)
+{
+	struct mvsw_pr_port *port = netdev_priv(dev);
+	u8 fec, active;
+	u32 mode;
+	int err;
+
+	if (port->autoneg) {
+		netdev_err(dev, "FEC set is not allowed while autoneg is on\n");
+		return -EINVAL;
+	}
+
+	err = mvsw_pr_hw_port_fec_get(port, &active);
+	if (err)
+		return err;
+
+	fec = MVSW_PORT_FEC_MAX;
+	for (mode = 0; mode < MVSW_PORT_FEC_MAX; mode++) {
+		if ((mvsw_pr_fec_caps[mode].eth_fec & fecparam->fec) &&
+		    (mvsw_pr_fec_caps[mode].pr_fec & port->caps.supp_fec)) {
+			fec = mode;
+			break;
+		}
+	}
+
+	if (fec == active)
+		return 0;
+
+	if (fec == MVSW_PORT_FEC_MAX) {
+		netdev_err(dev, "Unsupported FEC requested");
+		return -EINVAL;
+	}
+
+	return mvsw_pr_hw_port_fec_set(port, fec);
+}
+
+static void mvsw_pr_port_get_ethtool_stats(struct net_device *dev,
+					   struct ethtool_stats *stats,
+					   u64 *data)
+{
+	struct mvsw_pr_port *port = netdev_priv(dev);
+	struct mvsw_pr_port_stats *port_stats = &port->cached_hw_stats.stats;
+
+	memcpy((u8 *)data, port_stats, sizeof(*port_stats));
+}
+
+static void mvsw_pr_port_get_strings(struct net_device *dev,
+				     u32 stringset, u8 *data)
+{
+	if (stringset != ETH_SS_STATS)
+		return;
+
+	memcpy(data, *mvsw_pr_port_cnt_name, sizeof(mvsw_pr_port_cnt_name));
+}
+
+static int mvsw_pr_port_get_sset_count(struct net_device *dev, int sset)
+{
+	switch (sset) {
+	case ETH_SS_STATS:
+		return PORT_STATS_CNT;
+	default:
+		return -EOPNOTSUPP;
+	}
+}
+
+static const struct ethtool_ops mvsw_pr_ethtool_ops = {
+	.get_drvinfo = mvsw_pr_port_get_drvinfo,
+	.get_link_ksettings = mvsw_pr_port_get_link_ksettings,
+	.set_link_ksettings = mvsw_pr_port_set_link_ksettings,
+	.get_fecparam = mvsw_pr_port_get_fecparam,
+	.set_fecparam = mvsw_pr_port_set_fecparam,
+	.get_sset_count = mvsw_pr_port_get_sset_count,
+	.get_strings = mvsw_pr_port_get_strings,
+	.get_ethtool_stats = mvsw_pr_port_get_ethtool_stats,
+	.get_link = ethtool_op_get_link,
+	.nway_reset = mvsw_pr_port_nway_reset
+};
+
+int mvsw_pr_port_learning_set(struct mvsw_pr_port *port, bool learn)
+{
+	return mvsw_pr_hw_port_learning_set(port, learn);
+}
+
+int mvsw_pr_port_flood_set(struct mvsw_pr_port *port, bool flood)
+{
+	return mvsw_pr_hw_port_flood_set(port, flood);
+}
+
+int mvsw_pr_port_pvid_set(struct mvsw_pr_port *port, u16 vid)
+{
+	int err;
+
+	if (!vid) {
+		err = mvsw_pr_hw_port_accept_frame_type_set
+		    (port, MVSW_ACCEPT_FRAME_TYPE_TAGGED);
+		if (err)
+			return err;
+	} else {
+		err = mvsw_pr_hw_vlan_port_vid_set(port, vid);
+		if (err)
+			return err;
+		err = mvsw_pr_hw_port_accept_frame_type_set
+		    (port, MVSW_ACCEPT_FRAME_TYPE_ALL);
+		if (err)
+			goto err_port_allow_untagged_set;
+	}
+
+	port->pvid = vid;
+	return 0;
+
+err_port_allow_untagged_set:
+	mvsw_pr_hw_vlan_port_vid_set(port, port->pvid);
+	return err;
+}
+
+struct mvsw_pr_port_vlan*
+mvsw_pr_port_vlan_find_by_vid(const struct mvsw_pr_port *port, u16 vid)
+{
+	struct mvsw_pr_port_vlan *port_vlan;
+
+	list_for_each_entry(port_vlan, &port->vlans_list, list) {
+		if (port_vlan->vid == vid)
+			return port_vlan;
+	}
+
+	return NULL;
+}
+
+struct mvsw_pr_port_vlan*
+mvsw_pr_port_vlan_create(struct mvsw_pr_port *port, u16 vid, bool untagged)
+{
+	struct mvsw_pr_port_vlan *port_vlan;
+	int err;
+
+	port_vlan = mvsw_pr_port_vlan_find_by_vid(port, vid);
+	if (port_vlan)
+		return ERR_PTR(-EEXIST);
+
+	err = mvsw_pr_port_vlan_set(port, vid, true, untagged);
+	if (err)
+		return ERR_PTR(err);
+
+	port_vlan = kzalloc(sizeof(*port_vlan), GFP_KERNEL);
+	if (!port_vlan) {
+		err = -ENOMEM;
+		goto err_port_vlan_alloc;
+	}
+
+	port_vlan->mvsw_pr_port = port;
+	port_vlan->vid = vid;
+
+	list_add(&port_vlan->list, &port->vlans_list);
+
+	return port_vlan;
+
+err_port_vlan_alloc:
+	mvsw_pr_port_vlan_set(port, vid, false, false);
+	return ERR_PTR(err);
+}
+
+static void
+mvsw_pr_port_vlan_cleanup(struct mvsw_pr_port_vlan *port_vlan)
+{
+	if (port_vlan->bridge_port)
+		mvsw_pr_port_vlan_bridge_leave(port_vlan);
+}
+
+void mvsw_pr_port_vlan_destroy(struct mvsw_pr_port_vlan *port_vlan)
+{
+	struct mvsw_pr_port *port = port_vlan->mvsw_pr_port;
+	u16 vid = port_vlan->vid;
+
+	mvsw_pr_port_vlan_cleanup(port_vlan);
+	list_del(&port_vlan->list);
+	kfree(port_vlan);
+	mvsw_pr_hw_vlan_port_set(port, vid, false, false);
+}
+
+int mvsw_pr_port_vlan_set(struct mvsw_pr_port *port, u16 vid,
+			  bool is_member, bool untagged)
+{
+	return mvsw_pr_hw_vlan_port_set(port, vid, is_member, untagged);
+}
+
+static int mvsw_pr_port_create(struct mvsw_pr_switch *sw, u32 id)
+{
+	struct net_device *net_dev;
+	struct mvsw_pr_port *port;
+	char *mac;
+	int err;
+
+	net_dev = alloc_etherdev(sizeof(*port));
+	if (!net_dev)
+		return -ENOMEM;
+
+	port = netdev_priv(net_dev);
+
+	INIT_LIST_HEAD(&port->vlans_list);
+	port->pvid = MVSW_PR_DEFAULT_VID;
+	port->net_dev = net_dev;
+	port->id = id;
+	port->sw = sw;
+
+	err = mvsw_pr_hw_port_info_get(port, &port->fp_id,
+				       &port->hw_id, &port->dev_id);
+	if (err) {
+		dev_err(mvsw_dev(sw), "Failed to get port(%u) info\n", id);
+		goto err_register_netdev;
+	}
+
+	net_dev->needed_headroom = MVSW_PR_DSA_HLEN + 4;
+
+	net_dev->netdev_ops = &mvsw_pr_netdev_ops;
+	net_dev->ethtool_ops = &mvsw_pr_ethtool_ops;
+	net_dev->features |= NETIF_F_NETNS_LOCAL | NETIF_F_HW_L2FW_DOFFLOAD |
+		NETIF_F_HW_TC;
+	net_dev->hw_features |= NETIF_F_HW_TC;
+	net_dev->ethtool_ops = &mvsw_pr_ethtool_ops;
+	net_dev->netdev_ops = &mvsw_pr_netdev_ops;
+
+	netif_carrier_off(net_dev);
+
+	net_dev->mtu = min_t(unsigned int, sw->mtu_max, MVSW_PR_MTU_DEFAULT);
+	net_dev->min_mtu = sw->mtu_min;
+	net_dev->max_mtu = sw->mtu_max;
+
+	err = mvsw_pr_hw_port_mtu_set(port, net_dev->mtu);
+	if (err) {
+		dev_err(mvsw_dev(sw), "Failed to set port(%u) mtu\n", id);
+		goto err_register_netdev;
+	}
+
+	/* Only 0xFF mac addrs are supported */
+	if (port->fp_id >= 0xFF)
+		goto err_register_netdev;
+
+	mac = net_dev->dev_addr;
+	memcpy(mac, sw->base_mac, net_dev->addr_len - 1);
+	mac[net_dev->addr_len - 1] = (char)port->fp_id;
+
+	err = mvsw_pr_hw_port_mac_set(port, mac);
+	if (err) {
+		dev_err(mvsw_dev(sw), "Failed to set port(%u) mac addr\n", id);
+		goto err_register_netdev;
+	}
+
+	err = mvsw_pr_hw_port_cap_get(port, &port->caps);
+	if (err) {
+		dev_err(mvsw_dev(sw), "Failed to get port(%u) caps\n", id);
+		goto err_register_netdev;
+	}
+
+	port->adver_link_modes = 0;
+	port->adver_fec = 1 << MVSW_PORT_FEC_OFF_BIT;
+	port->autoneg = false;
+	mvsw_pr_port_autoneg_set(port, true, port->caps.supp_link_modes,
+				 port->caps.supp_fec);
+
+	err = mvsw_pr_hw_port_state_set(port, false);
+	if (err) {
+		dev_err(mvsw_dev(sw), "Failed to set port(%u) down\n", id);
+		goto err_register_netdev;
+	}
+
+	INIT_DELAYED_WORK(&port->cached_hw_stats.caching_dw,
+			  &update_stats_cache);
+
+	err = register_netdev(net_dev);
+	if (err)
+		goto err_register_netdev;
+
+	list_add(&port->list, &sw->port_list);
+
+	return 0;
+
+err_register_netdev:
+	free_netdev(net_dev);
+	return err;
+}
+
+static void mvsw_pr_port_vlan_flush(struct mvsw_pr_port *port,
+				    bool flush_default)
+{
+	struct mvsw_pr_port_vlan *port_vlan, *tmp;
+
+	list_for_each_entry_safe(port_vlan, tmp, &port->vlans_list, list) {
+		if (!flush_default && port_vlan->vid == MVSW_PR_DEFAULT_VID)
+			continue;
+
+		mvsw_pr_port_vlan_destroy(port_vlan);
+	}
+}
+
+int mvsw_pr_8021d_bridge_create(struct mvsw_pr_switch *sw, u16 *bridge_id)
+{
+	return mvsw_pr_hw_bridge_create(sw, bridge_id);
+}
+
+int mvsw_pr_8021d_bridge_delete(struct mvsw_pr_switch *sw, u16 bridge_id)
+{
+	return mvsw_pr_hw_bridge_delete(sw, bridge_id);
+}
+
+int mvsw_pr_8021d_bridge_port_add(struct mvsw_pr_port *port, u16 bridge_id)
+{
+	return mvsw_pr_hw_bridge_port_add(port, bridge_id);
+}
+
+int mvsw_pr_8021d_bridge_port_delete(struct mvsw_pr_port *port, u16 bridge_id)
+{
+	return mvsw_pr_hw_bridge_port_delete(port, bridge_id);
+}
+
+int mvsw_pr_switch_ageing_set(struct mvsw_pr_switch *sw, u32 ageing_time)
+{
+	return mvsw_pr_hw_switch_ageing_set(sw, ageing_time);
+}
+
+int mvsw_pr_lpm_update(struct mvsw_pr_switch *sw, u16 vr_id, u32 dst,
+		       u32 dst_len, u16 vid)
+{
+	return mvsw_pr_hw_lpm_update(sw, vr_id, htonl(dst), dst_len, vid);
+}
+
+int mvsw_pr_lpm_del(struct mvsw_pr_switch *sw, u16 vr_id, u32 dst,
+		    u32 dst_len)
+{
+	return mvsw_pr_hw_lpm_del(sw, vr_id, htonl(dst), dst_len);
+}
+
+int mvsw_pr_nh_entry_add(struct mvsw_pr_switch *sw, u16 vr_id, u16 vid,
+			 __be32 dst, u8 *mac, u32 *hw_id)
+{
+	return mvsw_pr_hw_nh_entry_add(sw, vr_id, vid, dst, mac, hw_id);
+}
+
+int mvsw_pr_nh_entry_delete(struct mvsw_pr_switch *sw, u16 vr_id, __be32 dst,
+			    u32 dst_len, u8 *mac)
+{
+	return mvsw_pr_hw_nh_entry_del(sw, vr_id, dst, dst_len, mac);
+}
+
+int mvsw_pr_nh_entry_set(struct mvsw_pr_port *port, u16 vr_id, u16 vid,
+			 __be32 dst, u32 dst_len, u8 *mac, u32 *hw_id)
+{
+	return mvsw_pr_hw_nh_entry_set(port, vr_id, vid, dst, dst_len, mac,
+					hw_id);
+}
+
+int mvsw_pr_fdb_flush_vlan(struct mvsw_pr_switch *sw, u16 vid,
+			   enum mvsw_pr_fdb_flush_mode mode)
+{
+	return mvsw_pr_hw_fdb_flush_vlan(sw, vid, mode);
+}
+
+int mvsw_pr_fdb_flush_port_vlan(struct mvsw_pr_port *port, u16 vid,
+				enum mvsw_pr_fdb_flush_mode mode)
+{
+	return mvsw_pr_hw_fdb_flush_port_vlan(port, vid, mode);
+}
+
+int mvsw_pr_fdb_flush_port(struct mvsw_pr_port *port,
+			   enum mvsw_pr_fdb_flush_mode mode)
+{
+	return mvsw_pr_hw_fdb_flush_port(port, mode);
+}
+
+static int mvsw_pr_clear_ports(struct mvsw_pr_switch *sw)
+{
+	struct net_device *net_dev;
+	struct list_head *pos, *n;
+	struct mvsw_pr_port *port;
+
+	list_for_each_safe(pos, n, &sw->port_list) {
+		port = list_entry(pos, typeof(*port), list);
+		net_dev = port->net_dev;
+
+		cancel_delayed_work_sync(&port->cached_hw_stats.caching_dw);
+		unregister_netdev(net_dev);
+		mvsw_pr_port_vlan_flush(port, true);
+		WARN_ON_ONCE(!list_empty(&port->vlans_list));
+		mvsw_pr_port_router_leave(port);
+		free_netdev(net_dev);
+		list_del(pos);
+	}
+	return (!list_empty(&sw->port_list));
+}
+
+static void mvsw_pr_port_handle_event(struct mvsw_pr_switch *sw,
+				      struct mvsw_pr_event *evt)
+{
+	struct mvsw_pr_port *port;
+	struct delayed_work *caching_dw;
+
+	port = __find_pr_port(sw, evt->port_evt.port_id);
+	if (!port)
+		return;
+
+	caching_dw = &port->cached_hw_stats.caching_dw;
+
+	switch (evt->id) {
+	case MVSW_PORT_EVENT_STATE_CHANGED:
+		if (evt->port_evt.data.oper_state) {
+			netif_carrier_on(port->net_dev);
+			if (!delayed_work_pending(caching_dw))
+				queue_delayed_work(mvsw_pr_wq, caching_dw, 0);
+		} else {
+			netif_carrier_off(port->net_dev);
+			if (delayed_work_pending(caching_dw))
+				cancel_delayed_work(caching_dw);
+		}
+		break;
+	}
+}
+
+static void mvsw_pr_fdb_handle_event(struct mvsw_pr_switch *sw,
+				     struct mvsw_pr_event *evt)
+{
+	struct switchdev_notifier_fdb_info info;
+	struct mvsw_pr_port *port;
+
+	port = __find_pr_port(sw, evt->fdb_evt.port_id);
+	if (!port)
+		return;
+
+	info.addr = evt->fdb_evt.data.mac;
+	info.vid = evt->fdb_evt.vid;
+	info.offloaded = true;
+
+	rtnl_lock();
+	switch (evt->id) {
+	case MVSW_FDB_EVENT_LEARNED:
+		call_switchdev_notifiers(SWITCHDEV_FDB_ADD_TO_BRIDGE,
+					 port->net_dev, &info.info, NULL);
+		break;
+	case MVSW_FDB_EVENT_AGED:
+		call_switchdev_notifiers(SWITCHDEV_FDB_DEL_TO_BRIDGE,
+					 port->net_dev, &info.info, NULL);
+		break;
+	}
+	rtnl_unlock();
+}
+
+int mvsw_pr_fdb_add(struct mvsw_pr_port *port, const unsigned char *mac,
+		    u16 vid, bool dynamic)
+{
+	return mvsw_pr_hw_fdb_add(port, mac, vid, dynamic);
+}
+
+int mvsw_pr_fdb_del(struct mvsw_pr_port *port, const unsigned char *mac,
+		    u16 vid)
+{
+	return mvsw_pr_hw_fdb_del(port, mac, vid);
+}
+
+static void mvsw_pr_fdb_event_handler_unregister(struct mvsw_pr_switch *sw)
+{
+	mvsw_pr_hw_event_handler_unregister(sw, MVSW_EVENT_TYPE_FDB,
+					    mvsw_pr_fdb_handle_event);
+}
+
+static void mvsw_pr_port_event_handler_unregister(struct mvsw_pr_switch *sw)
+{
+	mvsw_pr_hw_event_handler_unregister(sw, MVSW_EVENT_TYPE_PORT,
+					    mvsw_pr_port_handle_event);
+}
+
+static void mvsw_pr_event_handlers_unregister(struct mvsw_pr_switch *sw)
+{
+	mvsw_pr_fdb_event_handler_unregister(sw);
+	mvsw_pr_port_event_handler_unregister(sw);
+}
+
+static int mvsw_pr_fdb_event_handler_register(struct mvsw_pr_switch *sw)
+{
+	return mvsw_pr_hw_event_handler_register(sw, MVSW_EVENT_TYPE_FDB,
+						 mvsw_pr_fdb_handle_event);
+}
+
+static int mvsw_pr_port_event_handler_register(struct mvsw_pr_switch *sw)
+{
+	return mvsw_pr_hw_event_handler_register(sw, MVSW_EVENT_TYPE_PORT,
+						 mvsw_pr_port_handle_event);
+}
+
+static int mvsw_pr_event_handlers_register(struct mvsw_pr_switch *sw)
+{
+	int err;
+
+	err = mvsw_pr_port_event_handler_register(sw);
+	if (err)
+		return err;
+
+	err = mvsw_pr_fdb_event_handler_register(sw);
+	if (err)
+		goto err_fdb_handler_register;
+
+	return 0;
+
+err_fdb_handler_register:
+	mvsw_pr_port_event_handler_unregister(sw);
+	return err;
+}
+
+int mvsw_pr_schedule_dw(struct delayed_work *dwork, unsigned long delay)
+{
+	return queue_delayed_work(mvsw_pr_wq, dwork, delay);
+}
+
+const struct mvsw_pr_port *mvsw_pr_port_find(u32 dev_hw_id, u32 port_hw_id)
+{
+	struct mvsw_pr_port *port = NULL;
+	struct mvsw_pr_switch *sw;
+
+	list_for_each_entry(sw, &switches_registered, list) {
+		list_for_each_entry(port, &sw->port_list, list) {
+			if (port->hw_id == port_hw_id &&
+			    port->dev_id == dev_hw_id)
+				return port;
+		}
+	}
+	return NULL;
+}
+
+static int mvsw_pr_init(struct mvsw_pr_switch *sw)
+{
+	u32 port;
+	int err;
+
+	err = mvsw_pr_hw_switch_init(sw);
+	if (err) {
+		dev_err(mvsw_dev(sw), "Failed to init Switch device\n");
+		return err;
+	}
+
+	dev_info(mvsw_dev(sw), "Initialized Switch device\n");
+
+	err = mvsw_pr_switchdev_register(sw);
+	if (err)
+		return err;
+
+	INIT_LIST_HEAD(&sw->port_list);
+
+	for (port = 0; port < sw->port_count; port++) {
+		err = mvsw_pr_port_create(sw, port);
+		if (err)
+			goto err_ports_init;
+	}
+
+	err = mvsw_pr_rxtx_switch_init(sw);
+	if (err)
+		goto err_rxtx_init;
+
+	err = mvsw_pr_event_handlers_register(sw);
+	if (err)
+		goto err_event_handlers;
+
+	err = mvsw_pr_fw_log_init(sw);
+	if (err)
+		goto err_fw_log_init;
+
+	err = mvsw_pr_acl_init(sw);
+	if (err)
+		goto err_acl_init;
+
+	return 0;
+
+err_acl_init:
+err_fw_log_init:
+	mvsw_pr_event_handlers_unregister(sw);
+err_event_handlers:
+	mvsw_pr_rxtx_switch_fini(sw);
+err_rxtx_init:
+err_ports_init:
+	mvsw_pr_clear_ports(sw);
+	return err;
+}
+
+static void mvsw_pr_fini(struct mvsw_pr_switch *sw)
+{
+	mvsw_pr_event_handlers_unregister(sw);
+
+	mvsw_pr_fw_log_fini(sw);
+
+	mvsw_pr_rxtx_switch_fini(sw);
+	mvsw_pr_clear_ports(sw);
+	mvsw_pr_switchdev_unregister(sw);
+	mvsw_pr_acl_fini(sw);
+}
+
+int mvsw_pr_device_register(struct mvsw_pr_device *dev)
+{
+	struct mvsw_pr_switch *sw;
+	int err;
+
+	sw = kzalloc(sizeof(*sw), GFP_KERNEL);
+	if (!sw)
+		return -ENOMEM;
+
+	dev->priv = sw;
+	sw->dev = dev;
+
+	err = mvsw_pr_init(sw);
+	if (err) {
+		kfree(sw);
+		return err;
+	}
+
+	list_add(&sw->list, &switches_registered);
+
+	return 0;
+}
+EXPORT_SYMBOL(mvsw_pr_device_register);
+
+void mvsw_pr_device_unregister(struct mvsw_pr_device *dev)
+{
+	struct mvsw_pr_switch *sw = dev->priv;
+
+	list_del(&sw->list);
+	mvsw_pr_fini(sw);
+	kfree(sw);
+}
+EXPORT_SYMBOL(mvsw_pr_device_unregister);
+
+static int __init mvsw_pr_module_init(void)
+{
+	int err;
+
+	INIT_LIST_HEAD(&switches_registered);
+
+	mvsw_pr_wq = alloc_workqueue(mvsw_driver_name, 0, 0);
+	if (!mvsw_pr_wq)
+		return -ENOMEM;
+
+	err = mvsw_pr_rxtx_init();
+	if (err) {
+		pr_err("failed to initialize prestera rxtx\n");
+		destroy_workqueue(mvsw_pr_wq);
+		return err;
+	}
+
+	pr_info("Loading Marvell Prestera Switch Driver\n");
+	return 0;
+}
+
+static void __exit mvsw_pr_module_exit(void)
+{
+	destroy_workqueue(mvsw_pr_wq);
+	mvsw_pr_rxtx_fini();
+
+	pr_info("Unloading Marvell Prestera Switch Driver\n");
+}
+
+module_init(mvsw_pr_module_init);
+module_exit(mvsw_pr_module_exit);
+
+MODULE_AUTHOR("Marvell Semi.");
+MODULE_LICENSE("GPL");
+MODULE_DESCRIPTION("Marvell Prestera switch driver");
+MODULE_VERSION(PRESTERA_DRV_VER);
diff --git a/drivers/net/ethernet/marvell/prestera_sw/prestera.h b/drivers/net/ethernet/marvell/prestera_sw/prestera.h
new file mode 100644
index 0000000..95d63cb
--- /dev/null
+++ b/drivers/net/ethernet/marvell/prestera_sw/prestera.h
@@ -0,0 +1,404 @@
+/* SPDX-License-Identifier: BSD-3-Clause OR GPL-2.0
+ *
+ * Copyright (c) 2019-2020 Marvell International Ltd. All rights reserved.
+ *
+ */
+
+#ifndef _MVSW_PRESTERA_H_
+#define _MVSW_PRESTERA_H_
+
+#include <linux/skbuff.h>
+#include <linux/notifier.h>
+#include <uapi/linux/if_ether.h>
+#include <linux/workqueue.h>
+#include <net/pkt_cls.h>
+
+#define MVSW_MSG_MAX_SIZE 1500
+
+#define MVSW_PR_DEFAULT_VID 1
+
+#define MVSW_PR_MIN_AGEING_TIME 10
+#define MVSW_PR_MAX_AGEING_TIME 1000000
+#define MVSW_PR_DEFAULT_AGEING_TIME 300
+
+struct mvsw_fw_rev {
+	u16 maj;
+	u16 min;
+	u16 sub;
+};
+
+struct mvsw_pr_bridge_port;
+struct mvsw_pr_acl;
+struct mvsw_pr_acl_block;
+struct mvsw_pr_acl_rule;
+struct mvsw_pr_acl_ruleset;
+
+struct mvsw_pr_port_vlan {
+	struct list_head list;
+	struct mvsw_pr_port *mvsw_pr_port;
+	u16 vid;
+	struct mvsw_pr_bridge_port *bridge_port;
+	struct list_head bridge_vlan_node;
+};
+
+struct mvsw_pr_port_stats {
+	u64 good_octets_received;
+	u64 bad_octets_received;
+	u64 mac_trans_error;
+	u64 broadcast_frames_received;
+	u64 multicast_frames_received;
+	u64 frames_64_octets;
+	u64 frames_65_to_127_octets;
+	u64 frames_128_to_255_octets;
+	u64 frames_256_to_511_octets;
+	u64 frames_512_to_1023_octets;
+	u64 frames_1024_to_max_octets;
+	u64 excessive_collision;
+	u64 multicast_frames_sent;
+	u64 broadcast_frames_sent;
+	u64 fc_sent;
+	u64 fc_received;
+	u64 buffer_overrun;
+	u64 undersize;
+	u64 fragments;
+	u64 oversize;
+	u64 jabber;
+	u64 rx_error_frame_received;
+	u64 bad_crc;
+	u64 collisions;
+	u64 late_collision;
+	u64 unicast_frames_received;
+	u64 unicast_frames_sent;
+	u64 sent_multiple;
+	u64 sent_deferred;
+	u64 good_octets_sent;
+};
+
+struct mvsw_pr_port_caps {
+	u64 supp_link_modes;
+	u8 supp_fec;
+	u8 type;
+	u8 transceiver;
+};
+
+struct mvsw_pr_port {
+	struct net_device *net_dev;
+	struct mvsw_pr_switch *sw;
+	u32 id;
+	u32 hw_id;
+	u32 dev_id;
+	u16 fp_id;
+	u16 pvid;
+	bool autoneg;
+	u64 adver_link_modes;
+	u8 adver_fec;
+	struct mvsw_pr_port_caps caps;
+	struct list_head list;
+	struct list_head vlans_list;
+	struct {
+		struct mvsw_pr_port_stats stats;
+		struct delayed_work caching_dw;
+	} cached_hw_stats;
+	struct mvsw_pr_acl_block *acl_block;
+};
+
+struct mvsw_pr_switchdev {
+	struct mvsw_pr_switch *sw;
+	struct notifier_block swdev_n;
+	struct notifier_block swdev_blocking_n;
+};
+
+struct mvsw_pr_fib {
+	struct mvsw_pr_switch *sw;
+	struct notifier_block fib_nb;
+	struct notifier_block netevent_nb;
+};
+
+struct mvsw_pr_device {
+	struct device *dev;
+	struct mvsw_fw_rev fw_rev;
+	struct workqueue_struct *dev_wq;
+	u8 __iomem *pp_regs;
+	void *priv;
+
+	/* called by device driver to pass event up to the higher layer */
+	int (*recv_msg)(struct mvsw_pr_device *dev, u8 *msg, size_t size);
+
+	/* called by higher layer to send request to the firmware */
+	int (*send_req)(struct mvsw_pr_device *dev, u8 *in_msg,
+			size_t in_size, u8 *out_msg, size_t out_size,
+			unsigned int wait);
+};
+
+enum mvsw_pr_event_type {
+	MVSW_EVENT_TYPE_UNSPEC,
+	MVSW_EVENT_TYPE_PORT,
+	MVSW_EVENT_TYPE_FDB,
+	MVSW_EVENT_TYPE_FW_LOG,
+
+	MVSW_EVENT_TYPE_MAX,
+};
+
+enum mvsw_pr_port_event_id {
+	MVSW_PORT_EVENT_UNSPEC,
+	MVSW_PORT_EVENT_STATE_CHANGED,
+
+	MVSW_PORT_EVENT_MAX,
+};
+
+enum mvsw_pr_fdb_event_id {
+	MVSW_FDB_EVENT_UNSPEC,
+	MVSW_FDB_EVENT_LEARNED,
+	MVSW_FDB_EVENT_AGED,
+
+	MVSW_FDB_EVENT_MAX,
+};
+
+struct mvsw_pr_fdb_event {
+	u32 port_id;
+	u32 vid;
+	union {
+		u8 mac[ETH_ALEN];
+	} data;
+};
+
+struct mvsw_pr_port_event {
+	u32 port_id;
+	union {
+		u32 oper_state;
+	} data;
+};
+
+struct mvsw_pr_fw_log_event {
+	u32 log_len;
+	u8 *data;
+};
+
+struct mvsw_pr_event {
+	u16 id;
+	union {
+		struct mvsw_pr_port_event port_evt;
+		struct mvsw_pr_fdb_event fdb_evt;
+		struct mvsw_pr_fw_log_event fw_log_evt;
+	};
+};
+
+enum mvsw_pr_rif_type {
+	MVSW_PR_RIF_TYPE_PORT,
+	MVSW_PR_RIF_TYPE_VLAN,
+	MVSW_PR_RIF_TYPE_BRIDGE,
+
+	MVSW_PR_RIF_TYPE_MAX,
+};
+
+struct mvsw_pr_bridge;
+struct mvsw_pr_router;
+struct mvsw_pr_rif;
+
+struct mvsw_pr_switch {
+	struct list_head list;
+	struct mvsw_pr_device *dev;
+	struct list_head event_handlers;
+	char base_mac[ETH_ALEN];
+	struct list_head port_list;
+	u32 port_count;
+	u32 mtu_min;
+	u32 mtu_max;
+	u8 id;
+	struct mvsw_pr_acl *acl;
+	struct mvsw_pr_bridge *bridge;
+	struct mvsw_pr_switchdev *switchdev;
+	struct mvsw_pr_router *router;
+	struct notifier_block netdevice_nb;
+};
+
+enum mvsw_pr_fdb_flush_mode {
+	MVSW_PR_FDB_FLUSH_MODE_DYNAMIC = BIT(0),
+	MVSW_PR_FDB_FLUSH_MODE_STATIC = BIT(1),
+	MVSW_PR_FDB_FLUSH_MODE_ALL = MVSW_PR_FDB_FLUSH_MODE_DYNAMIC
+				   | MVSW_PR_FDB_FLUSH_MODE_STATIC,
+};
+
+enum mvsw_pr_acl_rule_match_entry_type {
+	MVSW_ACL_RULE_MATCH_ENTRY_TYPE_ETH_TYPE = 1,
+	MVSW_ACL_RULE_MATCH_ENTRY_TYPE_ETH_DMAC,
+	MVSW_ACL_RULE_MATCH_ENTRY_TYPE_ETH_SMAC,
+	MVSW_ACL_RULE_MATCH_ENTRY_TYPE_IP_PROTO,
+	MVSW_ACL_RULE_MATCH_ENTRY_TYPE_PORT,
+	MVSW_ACL_RULE_MATCH_ENTRY_TYPE_IP_SRC,
+	MVSW_ACL_RULE_MATCH_ENTRY_TYPE_IP_DST,
+	MVSW_ACL_RULE_MATCH_ENTRY_TYPE_L4_PORT_SRC,
+	MVSW_ACL_RULE_MATCH_ENTRY_TYPE_L4_PORT_DST,
+	MVSW_ACL_RULE_MATCH_ENTRY_TYPE_L4_PORT_RANGE_SRC,
+	MVSW_ACL_RULE_MATCH_ENTRY_TYPE_L4_PORT_RANGE_DST,
+	MVSW_ACL_RULE_MATCH_ENTRY_TYPE_VLAN_ID,
+	MVSW_ACL_RULE_MATCH_ENTRY_TYPE_VLAN_TPID
+};
+
+enum mvsw_pr_acl_rule_action {
+	MVSW_ACL_RULE_ACTION_ACCEPT = BIT(0),
+	MVSW_ACL_RULE_ACTION_DROP = BIT(1),
+	MVSW_ACL_RULE_ACTION_TRAP = BIT(2)
+};
+
+struct mvsw_pr_acl_rule_match_entry {
+	struct list_head list;
+	enum mvsw_pr_acl_rule_match_entry_type type;
+	union {
+		struct {
+			u8 key, mask;
+		} u8;
+		struct {
+			u16 key, mask;
+		} u16;
+		struct {
+			u32 key, mask;
+		} u32;
+		struct {
+			u64 key, mask;
+		} u64;
+		struct {
+			u8 key[ETH_ALEN];
+			u8 mask[ETH_ALEN];
+		} mac;
+	} keymask;
+};
+
+int mvsw_pr_switch_ageing_set(struct mvsw_pr_switch *sw, u32 ageing_time);
+
+int mvsw_pr_port_learning_set(struct mvsw_pr_port *mvsw_pr_port,
+			      bool learn_enable);
+int mvsw_pr_port_flood_set(struct mvsw_pr_port *mvsw_pr_port, bool flood);
+int mvsw_pr_port_pvid_set(struct mvsw_pr_port *mvsw_pr_port, u16 vid);
+struct mvsw_pr_port_vlan *
+mvsw_pr_port_vlan_create(struct mvsw_pr_port *mvsw_pr_port, u16 vid,
+			 bool untagged);
+void mvsw_pr_port_vlan_destroy(struct mvsw_pr_port_vlan *mvsw_pr_port_vlan);
+int mvsw_pr_port_vlan_set(struct mvsw_pr_port *mvsw_pr_port, u16 vid,
+			  bool is_member, bool untagged);
+
+struct mvsw_pr_bridge_device *
+mvsw_pr_bridge_device_find(const struct mvsw_pr_bridge *bridge,
+			   const struct net_device *br_dev);
+u16 mvsw_pr_vlan_dev_vlan_id(struct mvsw_pr_bridge *bridge,
+			     struct net_device *dev);
+int mvsw_pr_8021d_bridge_create(struct mvsw_pr_switch *sw, u16 *bridge_id);
+int mvsw_pr_8021d_bridge_delete(struct mvsw_pr_switch *sw, u16 bridge_id);
+int mvsw_pr_8021d_bridge_port_add(struct mvsw_pr_port *mvsw_pr_port,
+				  u16 bridge_id);
+int mvsw_pr_8021d_bridge_port_delete(struct mvsw_pr_port *mvsw_pr_port,
+				     u16 bridge_id);
+
+int mvsw_pr_fdb_add(struct mvsw_pr_port *mvsw_pr_port, const unsigned char *mac,
+		    u16 vid, bool dynamic);
+int mvsw_pr_fdb_del(struct mvsw_pr_port *mvsw_pr_port, const unsigned char *mac,
+		    u16 vid);
+int mvsw_pr_fdb_flush_vlan(struct mvsw_pr_switch *sw, u16 vid,
+			   enum mvsw_pr_fdb_flush_mode mode);
+int mvsw_pr_fdb_flush_port_vlan(struct mvsw_pr_port *port, u16 vid,
+				enum mvsw_pr_fdb_flush_mode mode);
+int mvsw_pr_fdb_flush_port(struct mvsw_pr_port *port,
+			   enum mvsw_pr_fdb_flush_mode mode);
+
+/* prestera_flower.c */
+int mvsw_pr_flower_replace(struct mvsw_pr_switch *sw,
+			   struct mvsw_pr_acl_block *block,
+			   struct flow_cls_offload *f);
+void mvsw_pr_flower_destroy(struct mvsw_pr_switch *sw,
+			    struct mvsw_pr_acl_block *block,
+			    struct flow_cls_offload *f);
+int mvsw_pr_flower_stats(struct mvsw_pr_switch *sw,
+			 struct mvsw_pr_acl_block *block,
+			 struct flow_cls_offload *f);
+
+/* prestera_acl.c */
+int mvsw_pr_acl_init(struct mvsw_pr_switch *sw);
+void mvsw_pr_acl_fini(struct mvsw_pr_switch *sw);
+struct mvsw_pr_acl_block *
+mvsw_pr_acl_block_create(struct mvsw_pr_switch *sw,
+			 struct net *net);
+void mvsw_pr_acl_block_destroy(struct mvsw_pr_acl_block *block);
+struct net *mvsw_pr_acl_block_net(struct mvsw_pr_acl_block *block);
+struct mvsw_pr_switch *mvsw_pr_acl_block_sw(struct mvsw_pr_acl_block *block);
+unsigned int mvsw_pr_acl_block_rule_count(struct mvsw_pr_acl_block *block);
+void mvsw_pr_acl_block_disable_inc(struct mvsw_pr_acl_block *block);
+void mvsw_pr_acl_block_disable_dec(struct mvsw_pr_acl_block *block);
+bool mvsw_pr_acl_block_disabled(const struct mvsw_pr_acl_block *block);
+int mvsw_pr_acl_block_bind(struct mvsw_pr_switch *sw,
+			   struct mvsw_pr_acl_block *block,
+			   struct mvsw_pr_port *port);
+int mvsw_pr_acl_block_unbind(struct mvsw_pr_switch *sw,
+			     struct mvsw_pr_acl_block *block,
+			     struct mvsw_pr_port *port);
+struct mvsw_pr_acl_ruleset *
+mvsw_pr_acl_block_ruleset_get(struct mvsw_pr_acl_block *block);
+struct mvsw_pr_acl_rule *
+mvsw_pr_acl_rule_create(struct mvsw_pr_acl_block *block,
+			unsigned long cookie);
+u8 mvsw_pr_acl_rule_actions_get(struct mvsw_pr_acl_rule *rule);
+u32 mvsw_pr_acl_rule_priority_get(struct mvsw_pr_acl_rule *rule);
+u16 mvsw_pr_acl_rule_ruleset_id_get(const struct mvsw_pr_acl_rule *rule);
+void mvsw_pr_acl_rule_actions_set(struct mvsw_pr_acl_rule *rule,
+				  u8 actions);
+void mvsw_pr_acl_rule_priority_set(struct mvsw_pr_acl_rule *rule,
+				   u32 priority);
+struct list_head *
+mvsw_pr_acl_rule_match_list_get(struct mvsw_pr_acl_rule *rule);
+void mvsw_pr_acl_rule_match_add(struct mvsw_pr_acl_rule *rule,
+				struct mvsw_pr_acl_rule_match_entry *entry);
+void mvsw_pr_acl_rule_destroy(struct mvsw_pr_acl_rule *rule);
+struct mvsw_pr_acl_rule *
+mvsw_pr_acl_rule_lookup(struct mvsw_pr_acl_ruleset *ruleset,
+			unsigned long cookie);
+int mvsw_pr_acl_rule_add(struct mvsw_pr_switch *sw,
+			 struct mvsw_pr_acl_rule *rule);
+void mvsw_pr_acl_rule_del(struct mvsw_pr_switch *sw,
+			  struct mvsw_pr_acl_rule *rule);
+int mvsw_pr_acl_rule_get_stats(struct mvsw_pr_switch *sw,
+			       struct mvsw_pr_acl_rule *rule,
+			       u64 *packets, u64 *bytes, u64 *last_use);
+
+/* VLAN API */
+struct mvsw_pr_port_vlan *
+mvsw_pr_port_vlan_find_by_vid(const struct mvsw_pr_port *mvsw_pr_port, u16 vid);
+void
+mvsw_pr_port_vlan_bridge_leave(struct mvsw_pr_port_vlan *mvsw_pr_port_vlan);
+
+int mvsw_pr_switchdev_register(struct mvsw_pr_switch *sw);
+void mvsw_pr_switchdev_unregister(struct mvsw_pr_switch *sw);
+
+int mvsw_pr_device_register(struct mvsw_pr_device *dev);
+void mvsw_pr_device_unregister(struct mvsw_pr_device *dev);
+
+bool mvsw_pr_netdev_check(const struct net_device *dev);
+struct mvsw_pr_switch *mvsw_pr_switch_get(struct net_device *dev);
+struct mvsw_pr_port *mvsw_pr_port_dev_lower_find(struct net_device *dev);
+
+const struct mvsw_pr_port *mvsw_pr_port_find(u32 dev_hw_id, u32 port_hw_id);
+int mvsw_pr_schedule_dw(struct delayed_work *dwork, unsigned long delay);
+
+/* prestera_router.c */
+int mvsw_pr_router_init(struct mvsw_pr_switch *sw);
+void mvsw_pr_router_fini(struct mvsw_pr_switch *sw);
+int mvsw_pr_netdevice_router_port_event(struct net_device *dev,
+					unsigned long event, void *ptr);
+int mvsw_pr_inetaddr_valid_event(struct notifier_block *unused,
+				 unsigned long event, void *ptr);
+int mvsw_pr_netdevice_vrf_event(struct net_device *dev, unsigned long event,
+				struct netdev_notifier_changeupper_info *info);
+void mvsw_pr_port_router_leave(struct mvsw_pr_port *mvsw_pr_port);
+int mvsw_pr_lpm_update(struct mvsw_pr_switch *sw, u16 vr_id, u32 dst,
+		       u32 dst_len, u16 vid);
+int mvsw_pr_lpm_del(struct mvsw_pr_switch *sw, u16 vr_id, u32 dst,
+		    u32 dst_len);
+int mvsw_pr_nh_entry_add(struct mvsw_pr_switch *sw, u16 vr_id, u16 vid,
+			 __be32 dst, u8 *mac, u32 *hw_id);
+int mvsw_pr_nh_entry_set(struct mvsw_pr_port *port, u16 vr_id, u16 vid,
+			 __be32 dst, u32 dst_len, u8 *mac, u32 *hw_id);
+int mvsw_pr_nh_entry_delete(struct mvsw_pr_switch *sw, u16 vr_id, __be32 dst,
+			    u32 dst_len, u8 *mac);
+
+void mvsw_pr_rif_enable(struct mvsw_pr_switch *sw, struct net_device *dev);
+void mvsw_pr_rif_disable(struct mvsw_pr_switch *sw, struct net_device *dev);
+
+#endif /* _MVSW_PRESTERA_H_ */
diff --git a/drivers/net/ethernet/marvell/prestera_sw/prestera_acl.c b/drivers/net/ethernet/marvell/prestera_sw/prestera_acl.c
new file mode 100644
index 0000000..3525cf8
--- /dev/null
+++ b/drivers/net/ethernet/marvell/prestera_sw/prestera_acl.c
@@ -0,0 +1,376 @@
+// SPDX-License-Identifier: BSD-3-Clause OR GPL-2.0
+//
+// Copyright (c) 2020 Marvell International Ltd. All rights reserved.
+//
+
+#include "prestera.h"
+#include "prestera_hw.h"
+
+#define MVSW_ACL_RULE_ACTION_DROP	BIT(0)
+#define MVSW_ACL_RULE_ACTION_ACCEPT	BIT(1)
+#define MVSW_ACL_RULE_ACTION_TRAP	BIT(2)
+
+struct mvsw_pr_acl {
+	struct mvsw_pr_switch *sw;
+	struct list_head rules;
+};
+
+struct mvsw_pr_acl_block_binding {
+	struct list_head list;
+	struct mvsw_pr_port *port;
+};
+
+struct mvsw_pr_acl_ruleset {
+	struct rhashtable rule_ht;
+	struct mvsw_pr_switch *sw;
+	u16 id;
+};
+
+struct mvsw_pr_acl_block {
+	struct list_head binding_list;
+	struct mvsw_pr_switch *sw;
+	unsigned int rule_count;
+	unsigned int disable_count;
+	struct net *net;
+	struct mvsw_pr_acl_ruleset *ruleset;
+};
+
+struct mvsw_pr_acl_rule {
+	struct rhash_head ht_node; /* Member of acl HT */
+	struct list_head list;
+	struct list_head match_list;
+	struct mvsw_pr_acl_block *block;
+	unsigned long cookie;
+	u32 priority;
+	u8 actions;
+	u32 id;
+};
+
+static const struct rhashtable_params mvsw_pr_acl_rule_ht_params = {
+	.key_len = sizeof(unsigned long),
+	.key_offset = offsetof(struct mvsw_pr_acl_rule, cookie),
+	.head_offset = offsetof(struct mvsw_pr_acl_rule, ht_node),
+	.automatic_shrinking = true,
+};
+
+static struct mvsw_pr_acl_ruleset *
+mvsw_pr_acl_ruleset_create(struct mvsw_pr_switch *sw)
+{
+	int err;
+	struct mvsw_pr_acl_ruleset *ruleset;
+
+	ruleset = kzalloc(sizeof(*ruleset), GFP_KERNEL);
+	if (!ruleset)
+		return ERR_PTR(-ENOMEM);
+
+	err = rhashtable_init(&ruleset->rule_ht, &mvsw_pr_acl_rule_ht_params);
+	if (err)
+		goto err_rhashtable_init;
+
+	err = mvsw_pr_hw_acl_ruleset_create(sw, &ruleset->id);
+	if (err)
+		goto err_ruleset_create;
+
+	ruleset->sw = sw;
+
+	return ruleset;
+
+err_ruleset_create:
+	rhashtable_destroy(&ruleset->rule_ht);
+err_rhashtable_init:
+	kfree(ruleset);
+	return ERR_PTR(err);
+}
+
+static void mvsw_pr_acl_ruleset_destroy(struct mvsw_pr_acl_ruleset *ruleset)
+{
+	mvsw_pr_hw_acl_ruleset_del(ruleset->sw, ruleset->id);
+	rhashtable_destroy(&ruleset->rule_ht);
+	kfree(ruleset);
+}
+
+struct mvsw_pr_acl_block *
+mvsw_pr_acl_block_create(struct mvsw_pr_switch *sw,
+			 struct net *net)
+{
+	struct mvsw_pr_acl_block *block;
+
+	block = kzalloc(sizeof(*block), GFP_KERNEL);
+	if (!block)
+		return NULL;
+	INIT_LIST_HEAD(&block->binding_list);
+	block->net = net;
+	block->sw = sw;
+
+	block->ruleset = mvsw_pr_acl_ruleset_create(sw);
+	if (IS_ERR(block->ruleset)) {
+		kfree(block);
+		return NULL;
+	}
+
+	return block;
+}
+
+void mvsw_pr_acl_block_destroy(struct mvsw_pr_acl_block *block)
+{
+	mvsw_pr_acl_ruleset_destroy(block->ruleset);
+	WARN_ON(!list_empty(&block->binding_list));
+	kfree(block);
+}
+
+static struct mvsw_pr_acl_block_binding *
+mvsw_pr_acl_block_lookup(struct mvsw_pr_acl_block *block,
+			 struct mvsw_pr_port *port)
+{
+	struct mvsw_pr_acl_block_binding *binding;
+
+	list_for_each_entry(binding, &block->binding_list, list)
+		if (binding->port == port)
+			return binding;
+
+	return NULL;
+}
+
+unsigned int mvsw_pr_acl_block_rule_count(struct mvsw_pr_acl_block *block)
+{
+	return block ? block->rule_count : 0;
+}
+
+void mvsw_pr_acl_block_disable_inc(struct mvsw_pr_acl_block *block)
+{
+	if (block)
+		block->disable_count++;
+}
+
+void mvsw_pr_acl_block_disable_dec(struct mvsw_pr_acl_block *block)
+{
+	if (block)
+		block->disable_count--;
+}
+
+bool mvsw_pr_acl_block_disabled(const struct mvsw_pr_acl_block *block)
+{
+	return block->disable_count;
+}
+
+int mvsw_pr_acl_block_bind(struct mvsw_pr_switch *sw,
+			   struct mvsw_pr_acl_block *block,
+			   struct mvsw_pr_port *port)
+{
+	struct mvsw_pr_acl_block_binding *binding;
+	int err;
+
+	if (WARN_ON(mvsw_pr_acl_block_lookup(block, port)))
+		return -EEXIST;
+
+	binding = kzalloc(sizeof(*binding), GFP_KERNEL);
+	if (!binding)
+		return -ENOMEM;
+	binding->port = port;
+
+	err = mvsw_pr_hw_acl_port_bind(port, block->ruleset->id);
+	if (err)
+		goto err_rules_bind;
+
+	list_add(&binding->list, &block->binding_list);
+	return 0;
+
+err_rules_bind:
+	kfree(binding);
+	return err;
+}
+
+int mvsw_pr_acl_block_unbind(struct mvsw_pr_switch *sw,
+			     struct mvsw_pr_acl_block *block,
+			     struct mvsw_pr_port *port)
+{
+	struct mvsw_pr_acl_block_binding *binding;
+
+	binding = mvsw_pr_acl_block_lookup(block, port);
+	if (!binding)
+		return -ENOENT;
+
+	list_del(&binding->list);
+
+	mvsw_pr_hw_acl_port_unbind(port, block->ruleset->id);
+
+	kfree(binding);
+	return 0;
+}
+
+struct mvsw_pr_acl_ruleset *
+mvsw_pr_acl_block_ruleset_get(struct mvsw_pr_acl_block *block)
+{
+	return block->ruleset;
+}
+
+u16 mvsw_pr_acl_rule_ruleset_id_get(const struct mvsw_pr_acl_rule *rule)
+{
+	return rule->block->ruleset->id;
+}
+
+struct net *mvsw_pr_acl_block_net(struct mvsw_pr_acl_block *block)
+{
+	return block->net;
+}
+
+struct mvsw_pr_switch *mvsw_pr_acl_block_sw(struct mvsw_pr_acl_block *block)
+{
+	return block->sw;
+}
+
+struct mvsw_pr_acl_rule *
+mvsw_pr_acl_rule_lookup(struct mvsw_pr_acl_ruleset *ruleset,
+			unsigned long cookie)
+{
+	return rhashtable_lookup_fast(&ruleset->rule_ht, &cookie,
+				      mvsw_pr_acl_rule_ht_params);
+}
+
+struct mvsw_pr_acl_rule *
+mvsw_pr_acl_rule_create(struct mvsw_pr_acl_block *block,
+			unsigned long cookie)
+{
+	struct mvsw_pr_acl_rule *rule;
+
+	rule = kzalloc(sizeof(*rule), GFP_KERNEL);
+	if (!rule)
+		return ERR_PTR(-ENOMEM);
+
+	INIT_LIST_HEAD(&rule->match_list);
+	rule->cookie = cookie;
+	rule->block = block;
+
+	return rule;
+}
+
+struct list_head *
+mvsw_pr_acl_rule_match_list_get(struct mvsw_pr_acl_rule *rule)
+{
+	return &rule->match_list;
+}
+
+u8 mvsw_pr_acl_rule_actions_get(struct mvsw_pr_acl_rule *rule)
+{
+	return rule->actions;
+}
+
+u32 mvsw_pr_acl_rule_priority_get(struct mvsw_pr_acl_rule *rule)
+{
+	return rule->priority;
+}
+
+void mvsw_pr_acl_rule_actions_set(struct mvsw_pr_acl_rule *rule,
+				  u8 actions)
+{
+	rule->actions = actions;
+}
+
+void mvsw_pr_acl_rule_priority_set(struct mvsw_pr_acl_rule *rule,
+				   u32 priority)
+{
+	rule->priority = priority;
+}
+
+void mvsw_pr_acl_rule_match_add(struct mvsw_pr_acl_rule *rule,
+				struct mvsw_pr_acl_rule_match_entry *entry)
+{
+	list_add(&entry->list, &rule->match_list);
+}
+
+void mvsw_pr_acl_rule_destroy(struct mvsw_pr_acl_rule *rule)
+{
+	struct mvsw_pr_acl_rule_match_entry *m_entry;
+	struct list_head *pos, *n;
+
+	list_for_each_safe(pos, n, &rule->match_list) {
+		m_entry = list_entry(pos, typeof(*m_entry), list);
+		list_del(pos);
+		kfree(m_entry);
+	}
+	kfree(rule);
+}
+
+int mvsw_pr_acl_rule_add(struct mvsw_pr_switch *sw,
+			 struct mvsw_pr_acl_rule *rule)
+{
+	int err;
+	u32 rule_id;
+
+	/* try to add rule to hash table first */
+	err = rhashtable_insert_fast(&rule->block->ruleset->rule_ht,
+				     &rule->ht_node,
+				     mvsw_pr_acl_rule_ht_params);
+	if (err)
+		return err;
+
+	/* add rule to hw */
+	err = mvsw_pr_hw_acl_rule_add(sw, rule, &rule_id);
+	if (err)
+		goto err_rule_add;
+
+	rule->id = rule_id;
+
+	list_add_tail(&rule->list, &sw->acl->rules);
+	rule->block->rule_count++;
+
+	return 0;
+
+err_rule_add:
+	rhashtable_remove_fast(&rule->block->ruleset->rule_ht, &rule->ht_node,
+			       mvsw_pr_acl_rule_ht_params);
+	return err;
+}
+
+void mvsw_pr_acl_rule_del(struct mvsw_pr_switch *sw,
+			  struct mvsw_pr_acl_rule *rule)
+{
+	rhashtable_remove_fast(&rule->block->ruleset->rule_ht, &rule->ht_node,
+			       mvsw_pr_acl_rule_ht_params);
+	rule->block->rule_count--;
+	list_del(&rule->list);
+	mvsw_pr_hw_acl_rule_del(sw, rule->id);
+}
+
+int mvsw_pr_acl_rule_get_stats(struct mvsw_pr_switch *sw,
+			       struct mvsw_pr_acl_rule *rule,
+			       u64 *packets, u64 *bytes, u64 *last_use)
+{
+	u64 current_packets;
+	u64 current_bytes;
+	int err;
+
+	err = mvsw_pr_hw_acl_rule_stats_get(sw, rule->id, &current_packets,
+					    &current_bytes);
+	if (err)
+		return err;
+
+	*packets = current_packets;
+	*bytes = current_bytes;
+	*last_use = jiffies;
+
+	return 0;
+}
+
+int mvsw_pr_acl_init(struct mvsw_pr_switch *sw)
+{
+	struct mvsw_pr_acl *acl;
+
+	acl = kzalloc(sizeof(*acl), GFP_KERNEL);
+	if (!acl)
+		return -ENOMEM;
+
+	INIT_LIST_HEAD(&acl->rules);
+	sw->acl = acl;
+	acl->sw = sw;
+
+	return 0;
+}
+
+void mvsw_pr_acl_fini(struct mvsw_pr_switch *sw)
+{
+	struct mvsw_pr_acl *acl = sw->acl;
+
+	WARN_ON(!list_empty(&acl->rules));
+	kfree(acl);
+}
diff --git a/drivers/net/ethernet/marvell/prestera_sw/prestera_drv_ver.h b/drivers/net/ethernet/marvell/prestera_sw/prestera_drv_ver.h
new file mode 100644
index 0000000..5d4df8e
--- /dev/null
+++ b/drivers/net/ethernet/marvell/prestera_sw/prestera_drv_ver.h
@@ -0,0 +1,23 @@
+/* SPDX-License-Identifier: BSD-3-Clause OR GPL-2.0
+ *
+ * Copyright (c) 2019-2020 Marvell International Ltd. All rights reserved.
+ *
+ */
+#ifndef _PRESTERA_DRV_VER_H_
+#define _PRESTERA_DRV_VER_H_
+
+#include <linux/stringify.h>
+
+/* Prestera driver version */
+#define PRESTERA_DRV_VER_MAJOR	2
+#define PRESTERA_DRV_VER_MINOR	0
+#define PRESTERA_DRV_VER_PATCH	0
+#define PRESTERA_DRV_VER_EXTRA	-v2.0.1-pvt
+
+#define PRESTERA_DRV_VER \
+		__stringify(PRESTERA_DRV_VER_MAJOR)  "." \
+		__stringify(PRESTERA_DRV_VER_MINOR)  "." \
+		__stringify(PRESTERA_DRV_VER_PATCH)  \
+		__stringify(PRESTERA_DRV_VER_EXTRA)
+
+#endif  /* _PRESTERA_DRV_VER_H_ */
diff --git a/drivers/net/ethernet/marvell/prestera_sw/prestera_dsa.c b/drivers/net/ethernet/marvell/prestera_sw/prestera_dsa.c
new file mode 100644
index 0000000..653e0b9
--- /dev/null
+++ b/drivers/net/ethernet/marvell/prestera_sw/prestera_dsa.c
@@ -0,0 +1,309 @@
+// SPDX-License-Identifier: BSD-3-Clause OR GPL-2.0
+/*
+ * Copyright (c) 2020 Marvell International Ltd. All rights reserved.
+ *
+ */
+#include "prestera_dsa.h"
+
+#include <linux/string.h>
+#include <linux/bitops.h>
+#include <linux/bitfield.h>
+#include <linux/errno.h>
+
+#define W0_MASK_IS_TAGGED	BIT(29)
+
+/* TrgDev[4:0] = {Word0[28:24]} */
+#define W0_MASK_HW_DEV_NUM	GENMASK(28, 24)
+
+/* SrcPort/TrgPort extended to 8b
+ * SrcPort/TrgPort[7:0] = {Word2[20], Word1[11:10], Word0[23:19]}
+ */
+#define W0_MASK_IFACE_PORT_NUM	GENMASK(23, 19)
+
+/* bits 30:31 - TagCommand 1 = FROM_CPU */
+#define W0_MASK_DSA_CMD		GENMASK(31, 30)
+
+/* bits 13:15 -- UP */
+#define W0_MASK_VPT		GENMASK(15, 13)
+
+#define W0_MASK_EXT_BIT		BIT(12)
+#define W0_MASK_OPCODE		GENMASK(18, 16)
+
+/* bit 16 - CFI */
+#define W0_MASK_CFI_BIT		BIT(16)
+
+/* bits 0:11 -- VID */
+#define W0_MASK_VID		GENMASK(11, 0)
+
+#define W1_MASK_SRC_IS_TARNK	BIT(27)
+
+/* SrcPort/TrgPort extended to 8b
+ * SrcPort/TrgPort[7:0] = {Word2[20], Word1[11:10], Word0[23:19]}
+ */
+#define W1_MASK_IFACE_PORT_NUM	GENMASK(11, 10)
+
+#define W1_MASK_EXT_BIT		BIT(31)
+#define W1_MASK_CFI_BIT		BIT(30)
+
+/* bit 30 -- EgressFilterEn */
+#define W1_MASK_EGR_FILTER_EN	BIT(30)
+
+/* bit 28 -- egrFilterRegistered */
+#define W1_MASK_EGR_FILTER_REG	BIT(28)
+
+/* bits 20-24 -- Src-ID */
+#define W1_MASK_SRC_ID		GENMASK(24, 20)
+
+/* bits 15-19 -- SrcDev */
+#define W1_MASK_SRC_DEV		GENMASK(19, 15)
+
+/* SrcTrunk is extended to 12b
+ * SrcTrunk[11:0] = {Word2[14:3]
+ */
+#define W2_MASK_SRC_TRANK_ID	GENMASK(14, 3)
+
+/* SRCePort[16:0]/TRGePort[16:0]/ = {Word2[19:3]} */
+#define W2_MASK_IFACE_EPORT	GENMASK(19, 3)
+
+/* SrcPort/TrgPort extended to 8b
+ * SrcPort/TrgPort[7:0] = {Word2[20], Word1[11:10], Word0[23:19]}
+ */
+#define W2_MASK_IFACE_PORT_NUM	BIT(20)
+
+#define W2_MASK_EXT_BIT		BIT(31)
+
+/* 5b SrcID is extended to 12 bits
+ * SrcID[11:0] = {Word2[27:21], Word1[24:20]}
+ */
+#define W2_MASK_SRC_ID		GENMASK(27, 21)
+
+/* 5b SrcDev is extended to 12b
+ * SrcDev[11:0] = {Word2[20:14], Word1[19:15]}
+ */
+#define W2_MASK_SRC_DEV		GENMASK(20, 14)
+
+/* trgHwDev and trgPort
+ * TrgDev[11:5] = {Word3[6:0]}
+ */
+#define W3_MASK_HW_DEV_NUM	GENMASK(6, 0)
+
+/* VID becomes 16b eVLAN. eVLAN[15:0] = {Word3[30:27], Word0[11:0]} */
+#define W3_MASK_VID		GENMASK(30, 27)
+
+/* TRGePort[16:0] = {Word3[23:7]} */
+#define W3_MASK_DST_EPORT	GENMASK(23, 7)
+
+#define DEV_NUM_MASK		GENMASK(11, 5)
+#define VID_MASK		GENMASK(15, 12)
+
+static int net_if_dsa_to_cpu_parse(const u32 *words_ptr,
+				   struct mvsw_pr_dsa *dsa_info_ptr)
+{
+	u32 get_value;	/* used to get needed bits from the DSA */
+	struct mvsw_pr_dsa_to_cpu *to_cpu_ptr;
+
+	to_cpu_ptr = &dsa_info_ptr->dsa_info.to_cpu;
+	to_cpu_ptr->is_tagged =
+	    (bool)FIELD_GET(W0_MASK_IS_TAGGED, words_ptr[0]);
+	to_cpu_ptr->hw_dev_num = FIELD_GET(W0_MASK_HW_DEV_NUM, words_ptr[0]);
+	to_cpu_ptr->src_is_trunk =
+	    (bool)FIELD_GET(W1_MASK_SRC_IS_TARNK, words_ptr[1]);
+
+	/* set hw dev num */
+	get_value = FIELD_GET(W3_MASK_HW_DEV_NUM, words_ptr[3]);
+	to_cpu_ptr->hw_dev_num &= W3_MASK_HW_DEV_NUM;
+	to_cpu_ptr->hw_dev_num |= FIELD_PREP(DEV_NUM_MASK, get_value);
+
+	if (to_cpu_ptr->src_is_trunk) {
+		to_cpu_ptr->iface.src_trunk_id =
+		    (u16)FIELD_GET(W2_MASK_SRC_TRANK_ID, words_ptr[2]);
+	} else {
+		/* When to_cpu_ptr->is_egress_pipe = false:
+		 *   this field indicates the source ePort number assigned by
+		 *   the ingress device.
+		 * When to_cpu_ptr->is_egress_pipe = true:
+		 *   this field indicates the target ePort number assigned by
+		 *   the ingress device.
+		 */
+		to_cpu_ptr->iface.eport =
+		    FIELD_GET(W2_MASK_IFACE_EPORT, words_ptr[2]);
+	}
+	to_cpu_ptr->iface.port_num =
+	    (FIELD_GET(W0_MASK_IFACE_PORT_NUM, words_ptr[0]) << 0) |
+	    (FIELD_GET(W1_MASK_IFACE_PORT_NUM, words_ptr[1]) << 5) |
+	    (FIELD_GET(W2_MASK_IFACE_PORT_NUM, words_ptr[2]) << 7);
+
+	return 0;
+}
+
+int mvsw_pr_dsa_parse(const u8 *dsa_bytes_ptr, struct mvsw_pr_dsa *dsa_info_ptr)
+{
+	u32 get_value;		/* used to get needed bits from the DSA */
+	u32 words_ptr[4] = { 0 };	/* DSA tag can be up to 4 words */
+	u32 *dsa_words_ptr = (u32 *)dsa_bytes_ptr;
+
+	/* sanity */
+	if (unlikely(!dsa_info_ptr || !dsa_bytes_ptr))
+		return -EINVAL;
+
+	/* zero results */
+	memset(dsa_info_ptr, 0, sizeof(struct mvsw_pr_dsa));
+
+	/* copy the data of the first word */
+	words_ptr[0] = ntohl((__force __be32)dsa_words_ptr[0]);
+
+	/* set the common parameters */
+	dsa_info_ptr->dsa_cmd =
+	    (enum mvsw_pr_dsa_cmd)FIELD_GET(W0_MASK_DSA_CMD, words_ptr[0]);
+
+	/* vid & vlan prio */
+	dsa_info_ptr->common_params.vid =
+	    (u16)FIELD_GET(W0_MASK_VID, words_ptr[0]);
+	dsa_info_ptr->common_params.vpt =
+	    (u8)FIELD_GET(W0_MASK_VPT, words_ptr[0]);
+
+	/* only to CPU is supported */
+	if (unlikely(dsa_info_ptr->dsa_cmd != MVSW_NET_DSA_CMD_TO_CPU_E))
+		return -EINVAL;
+
+	/* check extended bit */
+	if (FIELD_GET(W0_MASK_EXT_BIT, words_ptr[0]) == 0)
+		/* 1 words DSA tag is not supported */
+		return -EINVAL;
+
+	/* check that the "old" cpu opcode is set the 0xF
+	 * (with the extended bit)
+	 */
+	if (FIELD_GET(W0_MASK_OPCODE, words_ptr[0]) != 0x07)
+		return -EINVAL;
+
+	/* copy the data of the second word */
+	words_ptr[1] = ntohl((__force __be32)dsa_words_ptr[1]);
+
+	/* check the extended bit */
+	if (FIELD_GET(W1_MASK_EXT_BIT, words_ptr[1]) == 0)
+		/* 2 words DSA tag is not supported */
+		return -EINVAL;
+
+	/* copy the data of the third word */
+	words_ptr[2] = ntohl((__force __be32)dsa_words_ptr[2]);
+
+	/* check the extended bit */
+	if (FIELD_GET(W2_MASK_EXT_BIT, words_ptr[1]) == 0)
+		/* 3 words DSA tag is not supported */
+		return -EINVAL;
+
+	/* copy the data of the forth word */
+	words_ptr[3] = ntohl((__force __be32)dsa_words_ptr[3]);
+
+	/* VID */
+	get_value = FIELD_GET(W3_MASK_VID, words_ptr[3]);
+	dsa_info_ptr->common_params.vid &= ~VID_MASK;
+	dsa_info_ptr->common_params.vid |= FIELD_PREP(VID_MASK, get_value);
+
+	dsa_info_ptr->common_params.cfi_bit =
+	    (u8)FIELD_GET(W1_MASK_CFI_BIT, words_ptr[1]);
+
+	return net_if_dsa_to_cpu_parse(words_ptr, dsa_info_ptr);
+}
+
+static int net_if_dsa_tag_from_cpu_build(const struct mvsw_pr_dsa *dsa_info_ptr,
+					 u32 *words_ptr)
+{
+	u32 trg_hw_dev = 0;
+	u32 trg_port = 0;
+	const struct mvsw_pr_dsa_from_cpu *from_cpu_ptr =
+	    &dsa_info_ptr->dsa_info.from_cpu;
+
+	if (unlikely(from_cpu_ptr->dst_iface.type != MVSW_IF_PORT_E))
+		/* only sending to port interface is supported */
+		return -EINVAL;
+
+	words_ptr[0] |=
+	    FIELD_PREP(W0_MASK_DSA_CMD, MVSW_NET_DSA_CMD_FROM_CPU_E);
+
+	trg_hw_dev = from_cpu_ptr->dst_iface.dev_port.hw_dev_num;
+	trg_port = from_cpu_ptr->dst_iface.dev_port.port_num;
+
+	if (trg_hw_dev >= BIT(12))
+		return -EINVAL;
+
+	if (trg_port >= BIT(8) || trg_port >= BIT(10))
+		return -EINVAL;
+
+	words_ptr[0] |= FIELD_PREP(W0_MASK_HW_DEV_NUM, trg_hw_dev);
+	words_ptr[3] |= FIELD_PREP(W3_MASK_HW_DEV_NUM, (trg_hw_dev >> 5));
+
+	if (dsa_info_ptr->common_params.cfi_bit == 1)
+		words_ptr[0] |= FIELD_PREP(W0_MASK_CFI_BIT, 1);
+
+	words_ptr[0] |= FIELD_PREP(W0_MASK_VPT,
+				   dsa_info_ptr->common_params.vpt);
+	words_ptr[0] |= FIELD_PREP(W0_MASK_VID,
+				   dsa_info_ptr->common_params.vid);
+
+	/* set extended bits */
+	words_ptr[0] |= FIELD_PREP(W0_MASK_EXT_BIT, 1);
+	words_ptr[1] |= FIELD_PREP(W1_MASK_EXT_BIT, 1);
+	words_ptr[2] |= FIELD_PREP(W2_MASK_EXT_BIT, 1);
+
+	if (from_cpu_ptr->egr_filter_en)
+		words_ptr[1] |= FIELD_PREP(W1_MASK_EGR_FILTER_EN, 1);
+
+	if (from_cpu_ptr->egr_filter_registered)
+		words_ptr[1] |= FIELD_PREP(W1_MASK_EGR_FILTER_REG, 1);
+
+	/* check src_id & src_hw_dev */
+	if (from_cpu_ptr->src_id >= BIT(12) ||
+	    from_cpu_ptr->src_hw_dev >= BIT(12)) {
+		return -EINVAL;
+	}
+
+	words_ptr[1] |= FIELD_PREP(W1_MASK_SRC_ID, from_cpu_ptr->src_id);
+	words_ptr[1] |= FIELD_PREP(W1_MASK_SRC_DEV, from_cpu_ptr->src_hw_dev);
+
+	words_ptr[2] |= FIELD_PREP(W2_MASK_SRC_ID, from_cpu_ptr->src_id >> 5);
+	words_ptr[2] |= FIELD_PREP(W2_MASK_SRC_DEV,
+				   from_cpu_ptr->src_hw_dev >> 5);
+
+	/* bits 0:9 -- reserved with value 0 */
+	if (from_cpu_ptr->dst_eport >= BIT(17))
+		return -EINVAL;
+
+	words_ptr[3] |= FIELD_PREP(W3_MASK_DST_EPORT, from_cpu_ptr->dst_eport);
+	words_ptr[3] |= FIELD_PREP(W3_MASK_VID,
+				   (dsa_info_ptr->common_params.vid >> 12));
+
+	return 0;
+}
+
+int mvsw_pr_dsa_build(const struct mvsw_pr_dsa *dsa_info_ptr,
+		      u8 *dsa_bytes_ptr)
+{
+	int rc;
+	u32 words_ptr[4] = { 0 };	/* 4 words of DSA tag */
+	__be32 *dsa_words_ptr = (__be32 *)dsa_bytes_ptr;
+
+	if (unlikely(!dsa_info_ptr || !dsa_bytes_ptr))
+		return -EINVAL;
+
+	if (dsa_info_ptr->common_params.cfi_bit >= BIT(1) ||
+	    dsa_info_ptr->common_params.vpt >= BIT(3)) {
+		return -EINVAL;
+	}
+
+	if (unlikely(dsa_info_ptr->dsa_cmd != MVSW_NET_DSA_CMD_FROM_CPU_E))
+		return -EINVAL;
+
+	/* build form CPU DSA tag */
+	rc = net_if_dsa_tag_from_cpu_build(dsa_info_ptr, words_ptr);
+	if (rc != 0)
+		return rc;
+
+	dsa_words_ptr[0] = htonl(words_ptr[0]);
+	dsa_words_ptr[1] = htonl(words_ptr[1]);
+	dsa_words_ptr[2] = htonl(words_ptr[2]);
+	dsa_words_ptr[3] = htonl(words_ptr[3]);
+
+	return 0;
+}
diff --git a/drivers/net/ethernet/marvell/prestera_sw/prestera_dsa.h b/drivers/net/ethernet/marvell/prestera_sw/prestera_dsa.h
new file mode 100644
index 0000000..f65d110
--- /dev/null
+++ b/drivers/net/ethernet/marvell/prestera_sw/prestera_dsa.h
@@ -0,0 +1,88 @@
+/* SPDX-License-Identifier: BSD-3-Clause OR GPL-2.0
+ *
+ * Copyright (c) 2020 Marvell International Ltd. All rights reserved.
+ *
+ */
+#ifndef _MVSW_PRESTERA_DSA_H_
+#define _MVSW_PRESTERA_DSA_H_
+
+#include <linux/types.h>
+
+#define MVSW_PR_DSA_HLEN	16
+
+enum mvsw_pr_dsa_cmd {
+	/* DSA command is "To CPU" */
+	MVSW_NET_DSA_CMD_TO_CPU_E = 0,
+
+	/* DSA command is "FROM CPU" */
+	MVSW_NET_DSA_CMD_FROM_CPU_E,
+};
+
+enum mvsw_pr_if_type {
+	/* the interface is of port type (dev,port) */
+	MVSW_IF_PORT_E = 0,
+
+	/* the interface is of trunk type (trunkId) */
+	MVSW_IF_TRUNK_E = 1,
+
+	/* the interface is of Vid type (vlan-id) */
+	MVSW_IF_VID_E = 3,
+};
+
+struct mvsw_pr_dsa_common {
+	/* the value vlan priority tag (APPLICABLE RANGES: 0..7) */
+	u8 vpt;
+
+	/* CFI bit of the vlan tag (APPLICABLE RANGES: 0..1) */
+	u8 cfi_bit;
+
+	/* Vlan id */
+	u16 vid;
+};
+
+struct mvsw_pr_iface_info {
+	enum mvsw_pr_if_type type;
+	struct {
+		u32 hw_dev_num;
+		u32 port_num;
+	} dev_port;
+	u16 trunk_id;
+	u16 vlan_id;
+	u32 hw_dev_num;
+};
+
+struct mvsw_pr_dsa_to_cpu {
+	bool is_tagged;
+	u32 hw_dev_num;
+	bool src_is_trunk;
+	struct {
+		u16 src_trunk_id;
+		u32 port_num;
+		u32 eport;
+	} iface;
+};
+
+struct mvsw_pr_dsa_from_cpu {
+	struct mvsw_pr_iface_info dst_iface;	/* vid/port */
+	bool egr_filter_en;
+	bool egr_filter_registered;
+	u32 src_id;
+	u32 src_hw_dev;
+	u32 dst_eport;	/* for port but not for vid */
+};
+
+struct mvsw_pr_dsa {
+	struct mvsw_pr_dsa_common common_params;
+	enum mvsw_pr_dsa_cmd dsa_cmd;
+	union {
+		struct mvsw_pr_dsa_to_cpu to_cpu;
+		struct mvsw_pr_dsa_from_cpu from_cpu;
+	} dsa_info;
+};
+
+int mvsw_pr_dsa_parse(const u8 *dsa_bytes_ptr,
+		      struct mvsw_pr_dsa *dsa_info_ptr);
+int mvsw_pr_dsa_build(const struct mvsw_pr_dsa *dsa_info_ptr,
+		      u8 *dsa_bytes_ptr);
+
+#endif /* _MVSW_PRESTERA_DSA_H_ */
diff --git a/drivers/net/ethernet/marvell/prestera_sw/prestera_flower.c b/drivers/net/ethernet/marvell/prestera_sw/prestera_flower.c
new file mode 100644
index 0000000..d66d256
--- /dev/null
+++ b/drivers/net/ethernet/marvell/prestera_sw/prestera_flower.c
@@ -0,0 +1,373 @@
+// SPDX-License-Identifier: BSD-3-Clause OR GPL-2.0
+//
+// Copyright (c) 2020 Marvell International Ltd. All rights reserved.
+//
+
+#include "prestera.h"
+#include "prestera_hw.h"
+
+static int mvsw_pr_flower_parse_actions(struct mvsw_pr_switch *sw,
+					struct mvsw_pr_acl_block *block,
+					struct mvsw_pr_acl_rule *rule,
+					struct flow_action *flow_action,
+					struct netlink_ext_ack *extack)
+{
+	const struct flow_action_entry *act;
+	u8 actions = 0;
+	int i;
+
+	if (!flow_action_has_entries(flow_action))
+		return 0;
+
+	flow_action_for_each(i, act, flow_action) {
+		switch (act->id) {
+		case FLOW_ACTION_ACCEPT:
+			actions |= MVSW_ACL_RULE_ACTION_ACCEPT;
+			break;
+		case FLOW_ACTION_DROP:
+			actions |= MVSW_ACL_RULE_ACTION_DROP;
+			break;
+		case FLOW_ACTION_TRAP:
+			actions |= MVSW_ACL_RULE_ACTION_TRAP;
+			break;
+		default:
+			NL_SET_ERR_MSG_MOD(extack, "Unsupported action");
+			pr_err("Unsupported action\n");
+			return -EOPNOTSUPP;
+		}
+	}
+	mvsw_pr_acl_rule_actions_set(rule, actions);
+
+	return 0;
+}
+
+static int mvsw_pr_flower_parse_meta(struct mvsw_pr_acl_rule *rule,
+				     struct flow_cls_offload *f,
+				     struct mvsw_pr_acl_block *block)
+{
+	struct flow_rule *f_rule = flow_cls_offload_flow_rule(f);
+	struct mvsw_pr_acl_rule_match_entry *m_entry;
+	struct mvsw_pr_port *port;
+	struct net_device *ingress_dev;
+	struct flow_match_meta match;
+
+	flow_rule_match_meta(f_rule, &match);
+	if (match.mask->ingress_ifindex != 0xFFFFFFFF) {
+		NL_SET_ERR_MSG_MOD(f->common.extack,
+				   "Unsupported ingress ifindex mask");
+		return -EINVAL;
+	}
+
+	ingress_dev = __dev_get_by_index(mvsw_pr_acl_block_net(block),
+					 match.key->ingress_ifindex);
+	if (!ingress_dev) {
+		NL_SET_ERR_MSG_MOD(f->common.extack,
+				   "Can't find specified ingress port to match on");
+		return -EINVAL;
+	}
+
+	if (!mvsw_pr_netdev_check(ingress_dev)) {
+		NL_SET_ERR_MSG_MOD(f->common.extack,
+				   "Can't match on switchdev ingress port");
+		return -EINVAL;
+	}
+	port = netdev_priv(ingress_dev);
+
+	/* add port key,mask */
+	m_entry = kmalloc(sizeof(*m_entry), GFP_KERNEL);
+	if (!m_entry)
+		return -ENOMEM;
+
+	m_entry->type = MVSW_ACL_RULE_MATCH_ENTRY_TYPE_PORT;
+	m_entry->keymask.u64.key = port->hw_id | ((u64)port->dev_id << 32);
+	m_entry->keymask.u64.mask = ~(u64)0;
+	mvsw_pr_acl_rule_match_add(rule, m_entry);
+
+	return 0;
+}
+
+static int mvsw_pr_flower_parse(struct mvsw_pr_switch *sw,
+				struct mvsw_pr_acl_block *block,
+				struct mvsw_pr_acl_rule *rule,
+				struct flow_cls_offload *f)
+{
+	struct flow_rule *f_rule = flow_cls_offload_flow_rule(f);
+	struct flow_dissector *dissector = f_rule->match.dissector;
+	struct mvsw_pr_acl_rule_match_entry *m_entry;
+	u16 n_proto_mask = 0;
+	u16 n_proto_key = 0;
+	u16 addr_type = 0;
+	u8 ip_proto = 0;
+	int err;
+
+	if (dissector->used_keys &
+	    ~(BIT(FLOW_DISSECTOR_KEY_META) |
+	      BIT(FLOW_DISSECTOR_KEY_CONTROL) |
+	      BIT(FLOW_DISSECTOR_KEY_BASIC) |
+	      BIT(FLOW_DISSECTOR_KEY_ETH_ADDRS) |
+	      BIT(FLOW_DISSECTOR_KEY_IPV4_ADDRS) |
+	      BIT(FLOW_DISSECTOR_KEY_IPV6_ADDRS) |
+	      BIT(FLOW_DISSECTOR_KEY_PORTS) |
+	      BIT(FLOW_DISSECTOR_KEY_PORTS_RANGE) |
+	      BIT(FLOW_DISSECTOR_KEY_VLAN))) {
+		NL_SET_ERR_MSG_MOD(f->common.extack, "Unsupported key");
+		return -EOPNOTSUPP;
+	}
+
+	mvsw_pr_acl_rule_priority_set(rule, f->common.prio);
+
+	if (flow_rule_match_key(f_rule, FLOW_DISSECTOR_KEY_META)) {
+		err = mvsw_pr_flower_parse_meta(rule, f, block);
+		if (err)
+			return err;
+	}
+
+	if (flow_rule_match_key(f_rule, FLOW_DISSECTOR_KEY_CONTROL)) {
+		struct flow_match_control match;
+
+		flow_rule_match_control(f_rule, &match);
+		addr_type = match.key->addr_type;
+	}
+
+	if (flow_rule_match_key(f_rule, FLOW_DISSECTOR_KEY_BASIC)) {
+		struct flow_match_basic match;
+
+		flow_rule_match_basic(f_rule, &match);
+		n_proto_key = ntohs(match.key->n_proto);
+		n_proto_mask = ntohs(match.mask->n_proto);
+
+		if (n_proto_key == ETH_P_ALL) {
+			n_proto_key = 0;
+			n_proto_mask = 0;
+		}
+
+		/* add eth type key,mask */
+		m_entry = kmalloc(sizeof(*m_entry), GFP_KERNEL);
+		if (!m_entry)
+			return -ENOMEM;
+
+		m_entry->type = MVSW_ACL_RULE_MATCH_ENTRY_TYPE_ETH_TYPE;
+		m_entry->keymask.u16.key = n_proto_key;
+		m_entry->keymask.u16.mask = n_proto_mask;
+		mvsw_pr_acl_rule_match_add(rule, m_entry);
+
+		/* add ip proto key,mask */
+		m_entry = kmalloc(sizeof(*m_entry), GFP_KERNEL);
+		if (!m_entry)
+			return -ENOMEM;
+
+		m_entry->type = MVSW_ACL_RULE_MATCH_ENTRY_TYPE_IP_PROTO;
+		m_entry->keymask.u8.key = match.key->ip_proto;
+		m_entry->keymask.u8.mask = match.mask->ip_proto;
+		mvsw_pr_acl_rule_match_add(rule, m_entry);
+		ip_proto = match.key->ip_proto;
+	}
+
+	if (flow_rule_match_key(f_rule, FLOW_DISSECTOR_KEY_ETH_ADDRS)) {
+		struct flow_match_eth_addrs match;
+
+		flow_rule_match_eth_addrs(f_rule, &match);
+
+		/* add ethernet dst key,mask */
+		m_entry = kmalloc(sizeof(*m_entry), GFP_KERNEL);
+		if (!m_entry)
+			return -ENOMEM;
+
+		m_entry->type = MVSW_ACL_RULE_MATCH_ENTRY_TYPE_ETH_DMAC;
+		memcpy(&m_entry->keymask.mac.key,
+		       &match.key->dst, sizeof(match.key->dst));
+		memcpy(&m_entry->keymask.mac.mask,
+		       &match.mask->dst, sizeof(match.mask->dst));
+		mvsw_pr_acl_rule_match_add(rule, m_entry);
+
+		/* add ethernet src key,mask */
+		m_entry = kmalloc(sizeof(*m_entry), GFP_KERNEL);
+		if (!m_entry)
+			return -ENOMEM;
+
+		m_entry->type = MVSW_ACL_RULE_MATCH_ENTRY_TYPE_ETH_SMAC;
+		memcpy(&m_entry->keymask.mac.key,
+		       &match.key->src, sizeof(match.key->src));
+		memcpy(&m_entry->keymask.mac.mask,
+		       &match.mask->src, sizeof(match.mask->src));
+		mvsw_pr_acl_rule_match_add(rule, m_entry);
+	}
+
+	if (addr_type == FLOW_DISSECTOR_KEY_IPV4_ADDRS) {
+		struct flow_match_ipv4_addrs match;
+
+		flow_rule_match_ipv4_addrs(f_rule, &match);
+
+		m_entry = kmalloc(sizeof(*m_entry), GFP_KERNEL);
+		if (!m_entry)
+			return -ENOMEM;
+
+		m_entry->type = MVSW_ACL_RULE_MATCH_ENTRY_TYPE_IP_SRC;
+		memcpy(&m_entry->keymask.u32.key,
+		       &match.key->src, sizeof(match.key->src));
+		memcpy(&m_entry->keymask.u32.mask,
+		       &match.mask->src, sizeof(match.mask->src));
+		mvsw_pr_acl_rule_match_add(rule, m_entry);
+
+		m_entry = kmalloc(sizeof(*m_entry), GFP_KERNEL);
+		if (!m_entry)
+			return -ENOMEM;
+
+		m_entry->type = MVSW_ACL_RULE_MATCH_ENTRY_TYPE_IP_DST;
+		memcpy(&m_entry->keymask.u32.key,
+		       &match.key->dst, sizeof(match.key->dst));
+		memcpy(&m_entry->keymask.u32.mask,
+		       &match.mask->dst, sizeof(match.mask->dst));
+		mvsw_pr_acl_rule_match_add(rule, m_entry);
+	}
+
+	if (flow_rule_match_key(f_rule, FLOW_DISSECTOR_KEY_PORTS)) {
+		struct flow_match_ports match;
+
+		if (ip_proto != IPPROTO_TCP && ip_proto != IPPROTO_UDP) {
+			NL_SET_ERR_MSG_MOD
+			    (f->common.extack,
+			     "Only UDP and TCP keys are supported");
+			return -EINVAL;
+		}
+
+		flow_rule_match_ports(f_rule, &match);
+
+		m_entry = kmalloc(sizeof(*m_entry), GFP_KERNEL);
+		if (!m_entry)
+			return -ENOMEM;
+		m_entry->type = MVSW_ACL_RULE_MATCH_ENTRY_TYPE_L4_PORT_SRC;
+		m_entry->keymask.u16.key = ntohs(match.key->src);
+		m_entry->keymask.u16.mask = ntohs(match.mask->src);
+		mvsw_pr_acl_rule_match_add(rule, m_entry);
+
+		m_entry = kmalloc(sizeof(*m_entry), GFP_KERNEL);
+		if (!m_entry)
+			return -ENOMEM;
+		m_entry->type = MVSW_ACL_RULE_MATCH_ENTRY_TYPE_L4_PORT_DST;
+		m_entry->keymask.u16.key = ntohs(match.key->dst);
+		m_entry->keymask.u16.mask = ntohs(match.mask->dst);
+		mvsw_pr_acl_rule_match_add(rule, m_entry);
+	}
+
+	if (flow_rule_match_key(f_rule, FLOW_DISSECTOR_KEY_PORTS_RANGE)) {
+		struct flow_match_ports_range match;
+
+		flow_rule_match_ports_range(f_rule, &match);
+
+		m_entry = kmalloc(sizeof(*m_entry), GFP_KERNEL);
+		if (!m_entry)
+			return -ENOMEM;
+		m_entry->type =
+			MVSW_ACL_RULE_MATCH_ENTRY_TYPE_L4_PORT_RANGE_SRC;
+		m_entry->keymask.u32.key = ntohs(match.key->tp_min.src) |
+				(u32)ntohs(match.key->tp_max.src) << 16;
+		m_entry->keymask.u32.mask = ntohs(match.mask->tp_min.src) |
+				(u32)ntohs(match.mask->tp_max.src) << 16;
+		mvsw_pr_acl_rule_match_add(rule, m_entry);
+
+		m_entry = kmalloc(sizeof(*m_entry), GFP_KERNEL);
+		if (!m_entry)
+			return -ENOMEM;
+		m_entry->type =
+			MVSW_ACL_RULE_MATCH_ENTRY_TYPE_L4_PORT_RANGE_DST;
+		m_entry->keymask.u32.key = ntohs(match.key->tp_min.dst) |
+				(u32)ntohs(match.key->tp_max.dst) << 16;
+		m_entry->keymask.u32.mask = ntohs(match.mask->tp_min.dst) |
+				(u32)ntohs(match.mask->tp_max.dst) << 16;
+		mvsw_pr_acl_rule_match_add(rule, m_entry);
+	}
+
+	if (flow_rule_match_key(f_rule, FLOW_DISSECTOR_KEY_VLAN)) {
+		struct flow_match_vlan match;
+
+		flow_rule_match_vlan(f_rule, &match);
+
+		if (match.mask->vlan_id != 0) {
+			m_entry = kmalloc(sizeof(*m_entry), GFP_KERNEL);
+			if (!m_entry)
+				return -ENOMEM;
+			m_entry->type = MVSW_ACL_RULE_MATCH_ENTRY_TYPE_VLAN_ID;
+			m_entry->keymask.u16.key = match.key->vlan_id;
+			m_entry->keymask.u16.mask = match.mask->vlan_id;
+			mvsw_pr_acl_rule_match_add(rule, m_entry);
+		}
+
+		m_entry = kmalloc(sizeof(*m_entry), GFP_KERNEL);
+		if (!m_entry)
+			return -ENOMEM;
+		m_entry->type = MVSW_ACL_RULE_MATCH_ENTRY_TYPE_VLAN_TPID;
+		m_entry->keymask.u16.key = ntohs(match.key->vlan_tpid);
+		m_entry->keymask.u16.mask = ntohs(match.mask->vlan_tpid);
+		mvsw_pr_acl_rule_match_add(rule, m_entry);
+	}
+
+	return mvsw_pr_flower_parse_actions(sw, block, rule,
+					    &f->rule->action,
+					    f->common.extack);
+}
+
+int mvsw_pr_flower_replace(struct mvsw_pr_switch *sw,
+			   struct mvsw_pr_acl_block *block,
+			   struct flow_cls_offload *f)
+{
+	struct mvsw_pr_acl_rule *rule;
+	int err;
+
+	rule = mvsw_pr_acl_rule_create(block, f->cookie);
+	if (IS_ERR(rule))
+		return PTR_ERR(rule);
+
+	err = mvsw_pr_flower_parse(sw, block, rule, f);
+	if (err)
+		goto err_flower_parse;
+
+	err = mvsw_pr_acl_rule_add(sw, rule);
+	if (err)
+		goto err_rule_add;
+
+	return 0;
+
+err_rule_add:
+err_flower_parse:
+	mvsw_pr_acl_rule_destroy(rule);
+	return err;
+}
+
+void mvsw_pr_flower_destroy(struct mvsw_pr_switch *sw,
+			    struct mvsw_pr_acl_block *block,
+			    struct flow_cls_offload *f)
+{
+	struct mvsw_pr_acl_rule *rule;
+
+	rule = mvsw_pr_acl_rule_lookup(mvsw_pr_acl_block_ruleset_get(block),
+				       f->cookie);
+	if (rule) {
+		mvsw_pr_acl_rule_del(sw, rule);
+		mvsw_pr_acl_rule_destroy(rule);
+	}
+}
+
+int mvsw_pr_flower_stats(struct mvsw_pr_switch *sw,
+			 struct mvsw_pr_acl_block *block,
+			 struct flow_cls_offload *f)
+{
+	struct mvsw_pr_acl_rule *rule;
+	u64 packets;
+	u64 lastuse;
+	u64 bytes;
+	int err;
+
+	rule = mvsw_pr_acl_rule_lookup(mvsw_pr_acl_block_ruleset_get(block),
+				       f->cookie);
+	if (!rule)
+		return -EINVAL;
+
+	err = mvsw_pr_acl_rule_get_stats(sw, rule, &packets, &bytes,
+					 &lastuse);
+	if (err)
+		return err;
+
+	flow_stats_update(&f->stats, bytes, packets, lastuse);
+	return 0;
+}
diff --git a/drivers/net/ethernet/marvell/prestera_sw/prestera_fw_log.c b/drivers/net/ethernet/marvell/prestera_sw/prestera_fw_log.c
new file mode 100644
index 0000000..8211001
--- /dev/null
+++ b/drivers/net/ethernet/marvell/prestera_sw/prestera_fw_log.c
@@ -0,0 +1,436 @@
+/* SPDX-License-Identifier: BSD-3-Clause OR GPL-2.0
+ *
+ * Copyright (c) 2019-2020 Marvell International Ltd. All rights reserved.
+ *
+ */
+#include <linux/sysfs.h>
+#include <linux/fs.h>
+#include <linux/etherdevice.h>
+#include <linux/string.h>
+#include <linux/ctype.h>
+#include <linux/debugfs.h>
+
+#include "prestera.h"
+#include "prestera_hw.h"
+#include "prestera_log.h"
+#include "prestera_fw_log.h"
+
+#define FW_LOG_DBGFS_CFG_DIR "mvsw_pr_fw_log"
+#define FW_LOG_DBGFS_CFG_NAME "cfg"
+#define FW_LOG_DBGFS_MAX_STR_LEN 64
+#define FW_LOG_PR_LOG_PREFIX "[mvsw_pr_fw_log]"
+#define FW_LOG_PR_LIB_SIZE 32
+#define FW_LOG_PR_READ_BUF_STATIC_SIZE 4095
+#define MVSW_FW_LOG_INFO(fmt, ...) \
+	pr_info(fmt, ##__VA_ARGS__)
+
+#define FW_LOG_SETTINGS_GET(ptr, lib, type)				\
+	(((ptr[lib] & (1 << type)) >> type) & 1)
+
+#define FW_LOG_SETTINGS_TOG_TYPE(ptr, lib, type)			\
+	(ptr[lib] ^= (1 << type))
+
+#define FW_LOG_SETTINGS_CLR(ptr)					\
+	memset(ptr, 0, FW_LOG_LIB_MAX * 2)
+
+#define FW_LOG_SETTINGS_CLR_LIB(ptr, lib)				\
+	(ptr[lib] = 0)
+
+#define FW_LOG_SETTINGS_CLR_TYPE(ptr, lib, type)			\
+	(ptr[lib] &= ~(1 << type))
+
+static void mvsw_pr_fw_log_evt_handler(struct mvsw_pr_switch *,
+				       struct mvsw_pr_event *);
+static ssize_t mvsw_pr_fw_log_debugfs_read(struct file *file,
+					   char __user *ubuf,
+					   size_t count, loff_t *ppos);
+static ssize_t mvsw_pr_fw_log_debugfs_write(struct file *file,
+					    const char __user *ubuf,
+					    size_t count, loff_t *ppos);
+static inline int mvsw_pr_fw_log_get_type_from_str(const char *str);
+static inline int mvsw_pr_fw_log_get_lib_from_str(const char *str);
+
+static int mvsw_pr_fw_log_event_handler_register(struct mvsw_pr_switch *sw);
+static void mvsw_pr_fw_log_event_handler_unregister(struct mvsw_pr_switch *sw);
+
+enum {
+	FW_LOG_LIB_BRIDGE,
+	FW_LOG_LIB_CNC,
+	FW_LOG_LIB_CONFIG,
+	FW_LOG_LIB_COS,
+	FW_LOG_LIB_HW_INIT,
+	FW_LOG_LIB_CSCD,
+	FW_LOG_LIB_CUT_THROUGH,
+	FW_LOG_LIB_DIAG,
+	FW_LOG_LIB_FABRIC,
+	FW_LOG_LIB_IP,
+	FW_LOG_LIB_IPFIX,
+	FW_LOG_LIB_IP_LPM,
+	FW_LOG_LIB_L2_MLL,
+	FW_LOG_LIB_LOGICAL_TARGET,
+	FW_LOG_LIB_LPM,
+	FW_LOG_LIB_MIRROR,
+	FW_LOG_LIB_MULTI_PORT_GROUP,
+	FW_LOG_LIB_NETWORK_IF,
+	FW_LOG_LIB_NST,
+	FW_LOG_LIB_OAM,
+	FW_LOG_LIB_PCL,
+	FW_LOG_LIB_PHY,
+	FW_LOG_LIB_POLICER,
+	FW_LOG_LIB_PORT,
+	FW_LOG_LIB_PROTECTION,
+	FW_LOG_LIB_PTP,
+	FW_LOG_LIB_SYSTEM_RECOVERY,
+	FW_LOG_LIB_TCAM,
+	FW_LOG_LIB_TM_GLUE,
+	FW_LOG_LIB_TRUNK,
+	FW_LOG_LIB_TTI,
+	FW_LOG_LIB_TUNNEL,
+	FW_LOG_LIB_VNT,
+	FW_LOG_LIB_RESOURCE_MANAGER,
+	FW_LOG_LIB_VERSION,
+	FW_LOG_LIB_TM,
+	FW_LOG_LIB_SMI,
+	FW_LOG_LIB_INIT,
+	FW_LOG_LIB_DRAGONITE,
+	FW_LOG_LIB_VIRTUAL_TCAM,
+	FW_LOG_LIB_INGRESS,
+	FW_LOG_LIB_EGRESS,
+	FW_LOG_LIB_LATENCY_MONITORING,
+	FW_LOG_LIB_TAM,
+	FW_LOG_LIB_EXACT_MATCH,
+	FW_LOG_LIB_PHA,
+	FW_LOG_LIB_PACKET_ANALYZER,
+	FW_LOG_LIB_FLOW_MANAGER,
+	FW_LOG_LIB_BRIDGE_FDB_MANAGER,
+	FW_LOG_LIB_I2C,
+	FW_LOG_LIB_ALL,
+
+	FW_LOG_LIB_MAX
+};
+
+enum {
+	FW_LOG_TYPE_INFO,
+	FW_LOG_TYPE_ENTRY_LEVEL_FUNCTION,
+	FW_LOG_TYPE_ERROR,
+	FW_LOG_TYPE_ALL,
+	FW_LOG_TYPE_NONE,
+
+	FW_LOG_TYPE_MAX
+};
+
+struct mvsw_pr_fw_log_prv_debugfs {
+	struct dentry *cfg_dir;
+	struct dentry *cfg;
+	const struct file_operations cfg_fops;
+	char *read_buf;
+	ssize_t read_buf_size;
+};
+
+static u16 fw_log_lib_type_settings[FW_LOG_LIB_MAX] = { 0 };
+
+static struct mvsw_pr_fw_log_prv_debugfs fw_log_debugfs_handle = {
+	.cfg_dir = NULL,
+	.cfg_fops = {
+		.read = mvsw_pr_fw_log_debugfs_read,
+		.write = mvsw_pr_fw_log_debugfs_write,
+		.open = simple_open,
+		.llseek = default_llseek,
+	}
+};
+
+static const char *mvsw_pr_fw_log_prv_lib_str_enu[FW_LOG_LIB_MAX] = {
+	[FW_LOG_LIB_ALL] =  "all",
+	[FW_LOG_LIB_BRIDGE] =  "bridge",
+	[FW_LOG_LIB_CNC] =  "cnc",
+	[FW_LOG_LIB_CONFIG] =  "config",
+	[FW_LOG_LIB_COS] =  "cos",
+	[FW_LOG_LIB_CSCD] =  "cscd",
+	[FW_LOG_LIB_CUT_THROUGH] =  "cut-through",
+	[FW_LOG_LIB_DIAG] =  "diag",
+	[FW_LOG_LIB_DRAGONITE] =  "dragonite",
+	[FW_LOG_LIB_EGRESS] =  "egress",
+	[FW_LOG_LIB_EXACT_MATCH] =  "exact-match",
+	[FW_LOG_LIB_FABRIC] =  "fabric",
+	[FW_LOG_LIB_BRIDGE_FDB_MANAGER] =  "fdb-manager",
+	[FW_LOG_LIB_FLOW_MANAGER] =  "flow-manager",
+	[FW_LOG_LIB_HW_INIT] =  "hw-init",
+	[FW_LOG_LIB_I2C] =  "I2C_UNDEFINED",
+	[FW_LOG_LIB_INGRESS] =  "ingress",
+	[FW_LOG_LIB_INIT] =  "init",
+	[FW_LOG_LIB_IPFIX] =  "ipfix",
+	[FW_LOG_LIB_IP] =  "ip",
+	[FW_LOG_LIB_IP_LPM] =  "ip-lpm",
+	[FW_LOG_LIB_L2_MLL] =  "l2-mll",
+	[FW_LOG_LIB_LATENCY_MONITORING] =  "latency-monitoring",
+	[FW_LOG_LIB_LOGICAL_TARGET] =  "logical-target",
+	[FW_LOG_LIB_LPM] =  "lpm",
+	[FW_LOG_LIB_MIRROR] =  "mirror",
+	[FW_LOG_LIB_MULTI_PORT_GROUP] =  "multi-port-group",
+	[FW_LOG_LIB_NETWORK_IF] =  "network-if",
+	[FW_LOG_LIB_NST] =  "nst",
+	[FW_LOG_LIB_OAM] =  "oam",
+	[FW_LOG_LIB_PACKET_ANALYZER] =  "packet-analyzer",
+	[FW_LOG_LIB_PCL] =  "pcl",
+	[FW_LOG_LIB_PHA] =  "pha",
+	[FW_LOG_LIB_PHY] =  "phy",
+	[FW_LOG_LIB_POLICER] =  "policer",
+	[FW_LOG_LIB_PROTECTION] =  "protection",
+	[FW_LOG_LIB_PTP] =  "ptp",
+	[FW_LOG_LIB_RESOURCE_MANAGER] =  "resource-manager",
+	[FW_LOG_LIB_SMI] =  "smi",
+	[FW_LOG_LIB_SYSTEM_RECOVERY] =  "system-recovery",
+	[FW_LOG_LIB_TAM] =  "tam",
+	[FW_LOG_LIB_TCAM] =  "tcam",
+	[FW_LOG_LIB_TM] =  "tm",
+	[FW_LOG_LIB_TM_GLUE] =  "tm-glue",
+	[FW_LOG_LIB_TRUNK] =  "trunk",
+	[FW_LOG_LIB_TTI] =  "tti",
+	[FW_LOG_LIB_TUNNEL] =  "tunnel",
+	[FW_LOG_LIB_VERSION] =  "version",
+	[FW_LOG_LIB_VIRTUAL_TCAM] =  "virtual-tcam",
+	[FW_LOG_LIB_VNT] =  "vnt"
+};
+
+static const char *mvsw_pr_fw_log_prv_type_str_enu[FW_LOG_TYPE_MAX] = {
+	[FW_LOG_TYPE_INFO] = "info",
+	[FW_LOG_TYPE_ENTRY_LEVEL_FUNCTION] = "entry-level-function",
+	[FW_LOG_TYPE_ERROR] = "error",
+	[FW_LOG_TYPE_ALL] = "all",
+	[FW_LOG_TYPE_NONE]  = "none",
+};
+
+static void mvsw_pr_fw_log_evt_handler(struct mvsw_pr_switch *sw,
+				       struct mvsw_pr_event *evt)
+{
+	u32 log_len = evt->fw_log_evt.log_len;
+	u8 *buf = evt->fw_log_evt.data;
+
+	buf[log_len] = '\0';
+
+	MVSW_FW_LOG_INFO(FW_LOG_PR_LOG_PREFIX "%s\n", buf);
+}
+
+static ssize_t mvsw_pr_fw_log_debugfs_read(struct file *file,
+					   char __user *ubuf,
+					   size_t count, loff_t *ppos)
+{
+	char *buf = fw_log_debugfs_handle.read_buf;
+	int lib, type;
+
+	memset(fw_log_debugfs_handle.read_buf, 0,
+	       fw_log_debugfs_handle.read_buf_size);
+
+	for (type = 0; type < FW_LOG_TYPE_MAX; ++type) {
+		if (type == FW_LOG_TYPE_NONE || type == FW_LOG_TYPE_ALL)
+			continue;
+		strcat(buf, mvsw_pr_fw_log_prv_type_str_enu[type]);
+		strcat(buf, "\t");
+	}
+
+	strcat(buf, "\n");
+
+	for (lib = 0; lib < FW_LOG_LIB_MAX; ++lib) {
+		if (lib == FW_LOG_LIB_MAX ||
+		    !mvsw_pr_fw_log_prv_lib_str_enu[lib])
+			continue;
+		strcat(buf, mvsw_pr_fw_log_prv_lib_str_enu[lib]);
+
+		for (type = 0; type < FW_LOG_TYPE_MAX; ++type) {
+			if (type == FW_LOG_TYPE_NONE || type == FW_LOG_TYPE_ALL)
+				continue;
+			strcat(buf, "\t");
+			strcat(buf,
+			       FW_LOG_SETTINGS_GET(fw_log_lib_type_settings,
+						   lib, type) ? "+" : "-");
+		}
+		strcat(buf, "\n");
+	}
+
+	return simple_read_from_buffer(ubuf, count, ppos, buf, strlen(buf));
+}
+
+static ssize_t mvsw_pr_fw_log_debugfs_write(struct file *file,
+					    const char __user *ubuf,
+					    size_t count, loff_t *ppos)
+{
+	u8 tmp_buf[FW_LOG_DBGFS_MAX_STR_LEN] = { 0 };
+	u8 lib_str[FW_LOG_PR_LIB_SIZE] = { 0 };
+	u8 type_str[FW_LOG_PR_LIB_SIZE] = { 0 };
+	u8 *ppos_lib, *ppos_type;
+	int err;
+	int lib, type;
+	int lib_name;
+	int log_type;
+	ssize_t len_to_copy = count - 1;
+	struct mvsw_pr_switch *sw = file->private_data;
+	char *end = tmp_buf;
+
+	if (len_to_copy > FW_LOG_DBGFS_MAX_STR_LEN) {
+		MVSW_LOG_ERROR("Len is > than max(%zu vs max possible %d)\n",
+			       count, FW_LOG_DBGFS_MAX_STR_LEN);
+		return -EMSGSIZE;
+	}
+
+	err = copy_from_user(tmp_buf, ubuf, len_to_copy);
+	if (err) {
+		err = -EINVAL;
+		goto error;
+	}
+
+	ppos_lib  = strsep(&end, " \t");
+	ppos_type = strsep(&end, " \t\0");
+
+	if (!ppos_lib || !ppos_type) {
+		err = -EINVAL;
+		goto error;
+	}
+
+	strcpy(lib_str, ppos_lib);
+
+	strcpy(type_str, ppos_type);
+
+	if (iscntrl(lib_str[0]) || isspace(lib_str[0]) || lib_str[0] == '\0' ||
+	    iscntrl(type_str[0]) || isspace(type_str[0]) ||
+	    type_str[0] == '\0') {
+		err = -EINVAL;
+		goto error;
+	}
+
+	lib_name = mvsw_pr_fw_log_get_lib_from_str(lib_str);
+	log_type = mvsw_pr_fw_log_get_type_from_str(type_str);
+
+	if (lib_name >= FW_LOG_LIB_MAX || log_type >= FW_LOG_TYPE_MAX) {
+		err = -EINVAL;
+		goto error;
+	}
+
+	if (lib_name == FW_LOG_LIB_ALL && log_type == FW_LOG_TYPE_NONE) {
+		FW_LOG_SETTINGS_CLR(fw_log_lib_type_settings);
+	} else if (lib_name == FW_LOG_LIB_ALL && log_type == FW_LOG_TYPE_ALL) {
+		FW_LOG_SETTINGS_CLR(fw_log_lib_type_settings);
+		for (lib = 0; lib < FW_LOG_LIB_MAX; ++lib) {
+			for (type = 0; type < FW_LOG_TYPE_ALL; ++type) {
+				FW_LOG_SETTINGS_TOG_TYPE
+				    (fw_log_lib_type_settings, lib, type);
+			}
+		}
+	} else if (lib_name == FW_LOG_LIB_ALL) {
+		for (lib = 0; lib < FW_LOG_LIB_MAX; ++lib) {
+			FW_LOG_SETTINGS_CLR_TYPE(fw_log_lib_type_settings,
+						 lib, log_type);
+			FW_LOG_SETTINGS_TOG_TYPE(fw_log_lib_type_settings,
+						 lib, log_type);
+		}
+	} else if (log_type == FW_LOG_TYPE_ALL) {
+		FW_LOG_SETTINGS_CLR_LIB(fw_log_lib_type_settings, lib_name);
+		for (type = 0; type < FW_LOG_TYPE_ALL; ++type) {
+			FW_LOG_SETTINGS_TOG_TYPE(fw_log_lib_type_settings,
+						 lib_name, type);
+		}
+	} else {
+		FW_LOG_SETTINGS_TOG_TYPE(fw_log_lib_type_settings,
+					 lib_name, log_type);
+	}
+
+	mvsw_pr_hw_fw_log_level_set(sw, lib_name, log_type);
+
+	return count;
+
+error:
+	MVSW_LOG_ERROR("Invalid str received, make sure request is valid\n");
+	MVSW_LOG_ERROR("Valid fmt consists of: \"lib type\" string, e.g:\n");
+	MVSW_LOG_ERROR("\"phy error\" for 'phy' lib 'error' logs enabled\n");
+
+	return err;
+}
+
+static inline int mvsw_pr_fw_log_get_type_from_str(const char *str)
+{
+	int i;
+
+	for (i = 0; i < FW_LOG_TYPE_MAX; ++i) {
+		if (!mvsw_pr_fw_log_prv_type_str_enu[i])
+			continue;
+		if (strcmp(mvsw_pr_fw_log_prv_type_str_enu[i], str) == 0)
+			return i;
+	}
+
+	return FW_LOG_TYPE_MAX;
+}
+
+static inline int mvsw_pr_fw_log_get_lib_from_str(const char *str)
+{
+	int i;
+
+	for (i = 0; i < FW_LOG_LIB_MAX; ++i) {
+		if (!mvsw_pr_fw_log_prv_lib_str_enu[i])
+			continue;
+		if (strcmp(mvsw_pr_fw_log_prv_lib_str_enu[i], str) == 0)
+			return i;
+	}
+
+	return FW_LOG_LIB_MAX;
+}
+
+static int mvsw_pr_fw_log_event_handler_register(struct mvsw_pr_switch *sw)
+{
+	return mvsw_pr_hw_event_handler_register(sw, MVSW_EVENT_TYPE_FW_LOG,
+						 mvsw_pr_fw_log_evt_handler);
+}
+
+static void mvsw_pr_fw_log_event_handler_unregister(struct mvsw_pr_switch *sw)
+{
+	mvsw_pr_hw_event_handler_unregister(sw, MVSW_EVENT_TYPE_FW_LOG,
+					    mvsw_pr_fw_log_evt_handler);
+}
+
+int mvsw_pr_fw_log_init(struct mvsw_pr_switch *sw)
+{
+	fw_log_debugfs_handle.cfg_dir =
+		debugfs_create_dir(FW_LOG_DBGFS_CFG_DIR, NULL);
+
+	if (!fw_log_debugfs_handle.cfg_dir) {
+		MVSW_LOG_ERROR("Failed to create debugfs dir entry");
+		return -1;
+	}
+
+	fw_log_debugfs_handle.cfg =
+		debugfs_create_file(FW_LOG_DBGFS_CFG_NAME, 0644,
+				    fw_log_debugfs_handle.cfg_dir, sw,
+				    &fw_log_debugfs_handle.cfg_fops);
+
+	if (!fw_log_debugfs_handle.cfg) {
+		MVSW_LOG_ERROR("Failed to create debugfs dir entry");
+		debugfs_remove(fw_log_debugfs_handle.cfg_dir);
+		return -1;
+	}
+
+	if (mvsw_pr_fw_log_event_handler_register(sw))
+		goto error;
+
+	fw_log_debugfs_handle.read_buf_size = FW_LOG_PR_READ_BUF_STATIC_SIZE;
+
+	// +1 for null term char '\0'
+	fw_log_debugfs_handle.read_buf =
+		kzalloc(fw_log_debugfs_handle.read_buf_size + 1, GFP_KERNEL);
+
+	if (!fw_log_debugfs_handle.read_buf)
+		goto error;
+
+	return 0;
+error:
+	debugfs_remove(fw_log_debugfs_handle.cfg);
+	debugfs_remove(fw_log_debugfs_handle.cfg_dir);
+	return -1;
+}
+
+void mvsw_pr_fw_log_fini(struct mvsw_pr_switch *sw)
+{
+	mvsw_pr_fw_log_event_handler_unregister(sw);
+
+	kfree(fw_log_debugfs_handle.read_buf);
+
+	debugfs_remove(fw_log_debugfs_handle.cfg);
+	debugfs_remove(fw_log_debugfs_handle.cfg_dir);
+}
diff --git a/drivers/net/ethernet/marvell/prestera_sw/prestera_fw_log.h b/drivers/net/ethernet/marvell/prestera_sw/prestera_fw_log.h
new file mode 100644
index 0000000..ccd5514
--- /dev/null
+++ b/drivers/net/ethernet/marvell/prestera_sw/prestera_fw_log.h
@@ -0,0 +1,15 @@
+/* SPDX-License-Identifier: BSD-3-Clause OR GPL-2.0
+ *
+ * Copyright (c) 2019-2020 Marvell International Ltd. All rights reserved.
+ *
+ */
+
+#ifndef _MVSW_PRESTERA_FW_LOG_H_
+#define _MVSW_PRESTERA_FW_LOG_H_
+
+#include "prestera.h"
+
+int  mvsw_pr_fw_log_init(struct mvsw_pr_switch *sw);
+void mvsw_pr_fw_log_fini(struct mvsw_pr_switch *sw);
+
+#endif /* _MVSW_PRESTERA_FW_LOG_H_ */
diff --git a/drivers/net/ethernet/marvell/prestera_sw/prestera_hw.c b/drivers/net/ethernet/marvell/prestera_sw/prestera_hw.c
new file mode 100644
index 0000000..19777c7
--- /dev/null
+++ b/drivers/net/ethernet/marvell/prestera_sw/prestera_hw.c
@@ -0,0 +1,1857 @@
+/* SPDX-License-Identifier: BSD-3-Clause OR GPL-2.0
+ *
+ * Copyright (c) 2019-2020 Marvell International Ltd. All rights reserved.
+ *
+ */
+#include <linux/etherdevice.h>
+#include <linux/ethtool.h>
+#include <linux/netdevice.h>
+#include <linux/list.h>
+
+#include "prestera_hw.h"
+#include "prestera.h"
+#include "prestera_log.h"
+#include "prestera_fw_log.h"
+#include "prestera_rxtx.h"
+
+#define MVSW_PR_INIT_TIMEOUT 30000000	/* 30sec */
+#define MVSW_PR_MIN_MTU 64
+#define MVSW_PR_MSG_BUFF_CHUNK_SIZE	32	/* bytes */
+
+enum mvsw_msg_type {
+	MVSW_MSG_TYPE_SWITCH_UNSPEC,
+	MVSW_MSG_TYPE_SWITCH_INIT,
+
+	MVSW_MSG_TYPE_AGEING_TIMEOUT_SET,
+
+	MVSW_MSG_TYPE_PORT_ATTR_SET,
+	MVSW_MSG_TYPE_PORT_ATTR_GET,
+	MVSW_MSG_TYPE_PORT_INFO_GET,
+
+	MVSW_MSG_TYPE_VLAN_CREATE,
+	MVSW_MSG_TYPE_VLAN_DELETE,
+	MVSW_MSG_TYPE_VLAN_PORT_SET,
+	MVSW_MSG_TYPE_VLAN_PVID_SET,
+
+	MVSW_MSG_TYPE_FDB_ADD,
+	MVSW_MSG_TYPE_FDB_DELETE,
+	MVSW_MSG_TYPE_FDB_FLUSH_PORT,
+	MVSW_MSG_TYPE_FDB_FLUSH_VLAN,
+	MVSW_MSG_TYPE_FDB_FLUSH_PORT_VLAN,
+
+	MVSW_MSG_TYPE_LOG_LEVEL_SET,
+
+	MVSW_MSG_TYPE_BRIDGE_CREATE,
+	MVSW_MSG_TYPE_BRIDGE_DELETE,
+	MVSW_MSG_TYPE_BRIDGE_PORT_ADD,
+	MVSW_MSG_TYPE_BRIDGE_PORT_DELETE,
+
+	MVSW_MSG_TYPE_ACL_RULE_ADD,
+	MVSW_MSG_TYPE_ACL_RULE_DELETE,
+	MVSW_MSG_TYPE_ACL_RULE_STATS_GET,
+	MVSW_MSG_TYPE_ACL_RULESET_CREATE,
+	MVSW_MSG_TYPE_ACL_RULESET_DELETE,
+	MVSW_MSG_TYPE_ACL_PORT_BIND,
+	MVSW_MSG_TYPE_ACL_PORT_UNBIND,
+
+	MVSW_MSG_TYPE_RIF_PORT_CREATE,
+	MVSW_MSG_TYPE_RIF_VLAN_CREATE,
+	MVSW_MSG_TYPE_RIF_BRIDGE_CREATE,
+	MVSW_MSG_TYPE_RIF_DELETE,
+	MVSW_MSG_TYPE_RIF_SET,
+
+	MVSW_MSG_TYPE_LPM_UPDATE,
+	MVSW_MSG_TYPE_LPM_DELETE,
+
+	MVSW_MSG_TYPE_NH_ADD,
+	MVSW_MSG_TYPE_NH_DELETE,
+	MVSW_MSG_TYPE_NH_SET,
+	MVSW_MSG_TYPE_NH_GET,
+
+	MVSW_MSG_TYPE_VR_CREATE,
+	MVSW_MSG_TYPE_VR_DELETE,
+
+	MVSW_MSG_TYPE_RXTX_INIT,
+
+	MVSW_MSG_TYPE_ACK,
+	MVSW_MSG_TYPE_MAX
+};
+
+enum mvsw_msg_port_attr {
+	MVSW_MSG_PORT_ATTR_ADMIN_STATE,
+	MVSW_MSG_PORT_ATTR_OPER_STATE,
+	MVSW_MSG_PORT_ATTR_MTU,
+	MVSW_MSG_PORT_ATTR_MAC,
+	MVSW_MSG_PORT_ATTR_SPEED,
+	MVSW_MSG_PORT_ATTR_ACCEPT_FRAME_TYPE,
+	MVSW_MSG_PORT_ATTR_LEARNING,
+	MVSW_MSG_PORT_ATTR_FLOOD,
+	MVSW_MSG_PORT_ATTR_CAPABILITY,
+	MVSW_MSG_PORT_ATTR_REMOTE_CAPABILITY,
+	MVSW_MSG_PORT_ATTR_REMOTE_FC,
+	MVSW_MSG_PORT_ATTR_LINK_MODE,
+	MVSW_MSG_PORT_ATTR_TYPE,
+	MVSW_MSG_PORT_ATTR_FEC,
+	MVSW_MSG_PORT_ATTR_AUTONEG,
+	MVSW_MSG_PORT_ATTR_DUPLEX,
+	MVSW_MSG_PORT_ATTR_STATS,
+	MVSW_MSG_PORT_ATTR_MDIX,
+	MVSW_MSG_PORT_ATTR_AUTONEG_RESTART,
+	MVSW_MSG_PORT_ATTR_MAX
+};
+
+enum {
+	MVSW_MSG_ACK_OK,
+	MVSW_MSG_ACK_FAILED,
+	MVSW_MSG_ACK_MAX
+};
+
+enum {
+	MVSW_PORT_TP_NA,
+	MVSW_PORT_TP_MDI,
+	MVSW_PORT_TP_MDIX,
+	MVSW_PORT_TP_AUTO
+};
+
+enum {
+	MVSW_PORT_GOOD_OCTETS_RCV_CNT,
+	MVSW_PORT_BAD_OCTETS_RCV_CNT,
+	MVSW_PORT_MAC_TRANSMIT_ERR_CNT,
+	MVSW_PORT_BRDC_PKTS_RCV_CNT,
+	MVSW_PORT_MC_PKTS_RCV_CNT,
+	MVSW_PORT_PKTS_64_OCTETS_CNT,
+	MVSW_PORT_PKTS_65TO127_OCTETS_CNT,
+	MVSW_PORT_PKTS_128TO255_OCTETS_CNT,
+	MVSW_PORT_PKTS_256TO511_OCTETS_CNT,
+	MVSW_PORT_PKTS_512TO1023_OCTETS_CNT,
+	MVSW_PORT_PKTS_1024TOMAX_OCTETS_CNT,
+	MVSW_PORT_EXCESSIVE_COLLISIONS_CNT,
+	MVSW_PORT_MC_PKTS_SENT_CNT,
+	MVSW_PORT_BRDC_PKTS_SENT_CNT,
+	MVSW_PORT_FC_SENT_CNT,
+	MVSW_PORT_GOOD_FC_RCV_CNT,
+	MVSW_PORT_DROP_EVENTS_CNT,
+	MVSW_PORT_UNDERSIZE_PKTS_CNT,
+	MVSW_PORT_FRAGMENTS_PKTS_CNT,
+	MVSW_PORT_OVERSIZE_PKTS_CNT,
+	MVSW_PORT_JABBER_PKTS_CNT,
+	MVSW_PORT_MAC_RCV_ERROR_CNT,
+	MVSW_PORT_BAD_CRC_CNT,
+	MVSW_PORT_COLLISIONS_CNT,
+	MVSW_PORT_LATE_COLLISIONS_CNT,
+	MVSW_PORT_GOOD_UC_PKTS_RCV_CNT,
+	MVSW_PORT_GOOD_UC_PKTS_SENT_CNT,
+	MVSW_PORT_MULTIPLE_PKTS_SENT_CNT,
+	MVSW_PORT_DEFERRED_PKTS_SENT_CNT,
+	MVSW_PORT_GOOD_OCTETS_SENT_CNT,
+	MVSW_PORT_CNT_MAX,
+};
+
+enum {
+	MVSW_FC_NONE,
+	MVSW_FC_SYMMETRIC,
+	MVSW_FC_ASYMMETRIC,
+	MVSW_FC_SYMM_ASYMM,
+};
+
+struct mvsw_msg_buff {
+	u32 free;
+	u32 total;
+	u32 used;
+	void *data;
+};
+
+struct mvsw_msg_cmd {
+	u32 type;
+} __packed __aligned(4);
+
+struct mvsw_msg_ret {
+	struct mvsw_msg_cmd cmd;
+	u32 status;
+} __packed __aligned(4);
+
+struct mvsw_msg_common_request {
+	struct mvsw_msg_cmd cmd;
+} __packed __aligned(4);
+
+struct mvsw_msg_common_response {
+	struct mvsw_msg_ret ret;
+} __packed __aligned(4);
+
+union mvsw_msg_switch_param {
+	u32 ageing_timeout;
+};
+
+struct mvsw_msg_switch_attr_cmd {
+	struct mvsw_msg_cmd cmd;
+	union mvsw_msg_switch_param param;
+} __packed __aligned(4);
+
+struct mvsw_msg_switch_init_ret {
+	struct mvsw_msg_ret ret;
+	u32 port_count;
+	u32 mtu_max;
+	u8  switch_id;
+	u8  mac[ETH_ALEN];
+} __packed __aligned(4);
+
+struct mvsw_msg_port_autoneg_param {
+	u64 link_mode;
+	u8  enable;
+	u8  fec;
+};
+
+struct mvsw_msg_port_cap_param {
+	u64 link_mode;
+	u8  type;
+	u8  fec;
+	u8  transceiver;
+};
+
+struct mvsw_msg_port_mdix_param {
+	u8 status;
+	u8 admin_mode;
+};
+
+union mvsw_msg_port_param {
+	u8  admin_state;
+	u8  oper_state;
+	u32 mtu;
+	u8  mac[ETH_ALEN];
+	u8  accept_frm_type;
+	u8  learning;
+	u32 speed;
+	u8  flood;
+	u32 link_mode;
+	u8  type;
+	u8  duplex;
+	u8  fec;
+	u8  fc;
+	struct mvsw_msg_port_mdix_param mdix;
+	struct mvsw_msg_port_autoneg_param autoneg;
+	struct mvsw_msg_port_cap_param cap;
+};
+
+struct mvsw_msg_port_attr_cmd {
+	struct mvsw_msg_cmd cmd;
+	u32 attr;
+	u32 port;
+	u32 dev;
+	union mvsw_msg_port_param param;
+} __packed __aligned(4);
+
+struct mvsw_msg_port_attr_ret {
+	struct mvsw_msg_ret ret;
+	union mvsw_msg_port_param param;
+} __packed __aligned(4);
+
+struct mvsw_msg_port_stats_ret {
+	struct mvsw_msg_ret ret;
+	u64 stats[MVSW_PORT_CNT_MAX];
+} __packed __aligned(4);
+
+struct mvsw_msg_port_info_cmd {
+	struct mvsw_msg_cmd cmd;
+	u32 port;
+} __packed __aligned(4);
+
+struct mvsw_msg_port_info_ret {
+	struct mvsw_msg_ret ret;
+	u32 hw_id;
+	u32 dev_id;
+	u16 fp_id;
+} __packed __aligned(4);
+
+struct mvsw_msg_vlan_cmd {
+	struct mvsw_msg_cmd cmd;
+	u32 port;
+	u32 dev;
+	u16 vid;
+	u8  is_member;
+	u8  is_tagged;
+} __packed __aligned(4);
+
+struct mvsw_msg_fdb_cmd {
+	struct mvsw_msg_cmd cmd;
+	u32 port;
+	u32 dev;
+	u8  mac[ETH_ALEN];
+	u16 vid;
+	u8  dynamic;
+	u32 flush_mode;
+} __packed __aligned(4);
+
+struct mvsw_msg_log_cmd {
+	struct mvsw_msg_cmd cmd;
+	u32 lib;
+	u32 level;
+} __packed __aligned(4);
+
+struct mvsw_msg_acl_rule_cmd {
+	struct mvsw_msg_cmd cmd;
+	u32 id;
+	u16 ruleset_id;
+} __packed __aligned(4);
+
+struct mvsw_msg_acl_rule_ret {
+	struct mvsw_msg_ret ret;
+	u32 id;
+} __packed __aligned(4);
+
+struct mvsw_msg_acl_rule_stats_ret {
+	struct mvsw_msg_ret ret;
+	u64 packets;
+	u64 bytes;
+} __packed __aligned(4);
+
+struct mvsw_msg_acl_ruleset_bind_cmd {
+	struct mvsw_msg_cmd cmd;
+	u32 port;
+	u32 dev;
+	u16 ruleset_id;
+} __packed __aligned(4);
+
+struct mvsw_msg_acl_ruleset_cmd {
+	struct mvsw_msg_cmd cmd;
+	u16 id;
+} __packed __aligned(4);
+
+struct mvsw_msg_acl_ruleset_ret {
+	struct mvsw_msg_ret ret;
+	u16 id;
+} __packed __aligned(4);
+
+struct mvsw_msg_event {
+	u16 type;
+	u16 id;
+} __packed __aligned(4);
+
+struct mvsw_msg_event_log {
+	struct mvsw_msg_event id;
+	u32 log_string_size;
+	u8 log_string[0];
+} __packed __aligned(4);
+
+union mvsw_msg_event_fdb_param {
+	u8 mac[ETH_ALEN];
+};
+
+struct mvsw_msg_event_fdb {
+	struct mvsw_msg_event id;
+	u32 port_id;
+	u32 vid;
+	union mvsw_msg_event_fdb_param param;
+} __packed __aligned(4);
+
+union mvsw_msg_event_port_param {
+	u32 oper_state;
+};
+
+struct mvsw_msg_event_port {
+	struct mvsw_msg_event id;
+	u32 port_id;
+	union mvsw_msg_event_port_param param;
+} __packed __aligned(4);
+
+struct mvsw_msg_bridge_cmd {
+	struct mvsw_msg_cmd cmd;
+	u32 port;
+	u32 dev;
+	u16 bridge;
+} __packed __aligned(4);
+
+struct mvsw_msg_bridge_ret {
+	struct mvsw_msg_ret ret;
+	u16 bridge;
+} __packed __aligned(4);
+
+struct mvsw_msg_rif_cmd {
+	struct mvsw_msg_cmd cmd;
+	u16 rif_id;
+	u16 vr_id;
+	u16 bridge;
+	u8 mac[ETH_ALEN];
+	u32 port;
+	u32 dev;
+	u32 mtu;
+} __packed __aligned(4);
+
+struct mvsw_msg_rif_ret {
+	struct mvsw_msg_ret ret;
+	u16 rif_id;
+} __packed __aligned(4);
+
+struct mvsw_msg_lpm_cmd {
+	struct mvsw_msg_cmd cmd;
+	__be32 dst;
+	u32 dst_len;
+	u16 vid;
+	u16 vr_id;
+} __packed __aligned(4);
+
+struct mvsw_msg_nh_cmd {
+	struct mvsw_msg_cmd cmd;
+	u32 port;
+	u32 dev;
+	u32 hw_id;
+	__be32 dst;
+	u32 dst_len;
+	u16 vid;
+	u16 vr_id;
+	u8 mac[ETH_ALEN];
+} __packed __aligned(4);
+
+struct mvsw_msg_nh_ret {
+	struct mvsw_msg_ret ret;
+	u32 hw_id;
+	u8 is_active;
+} __packed __aligned(4);
+
+struct mvsw_msg_rxtx_cmd {
+	struct mvsw_msg_cmd cmd;
+	u8 use_sdma;
+} __packed __aligned(4);
+
+struct mvsw_msg_rxtx_ret {
+	struct mvsw_msg_ret ret;
+	u32 map_addr;
+} __packed __aligned(4);
+
+struct mvsw_msg_vr_cmd {
+	struct mvsw_msg_cmd cmd;
+	u16 vr_id;
+} __packed __aligned(4);
+
+struct mvsw_msg_vr_ret {
+	struct mvsw_msg_ret ret;
+	u16 vr_id;
+} __packed __aligned(4);
+
+#define fw_check_resp(_response)	\
+({								\
+	int __er = 0;						\
+	typeof(_response) __r = (_response);			\
+	if (__r->ret.cmd.type != MVSW_MSG_TYPE_ACK)		\
+		__er = -EBADE;					\
+	else if (__r->ret.status != MVSW_MSG_ACK_OK)		\
+		__er = -EINVAL;					\
+	(__er);							\
+})
+
+#define __fw_send_req_resp(_switch, _type, _req, _req_size,	\
+_response, _wait)						\
+({								\
+	int __e;						\
+	typeof(_switch) __sw = (_switch);			\
+	typeof(_req) __req = (_req);				\
+	typeof(_response) __resp = (_response);			\
+	__req->cmd.type = (_type);				\
+	__e = __sw->dev->send_req(__sw->dev,			\
+		(u8 *)__req, _req_size,				\
+		(u8 *)__resp, sizeof(*__resp),			\
+		_wait);						\
+	if (!__e)						\
+		__e = fw_check_resp(__resp);			\
+	(__e);							\
+})
+
+#define fw_send_nreq_resp(_sw, _t, _req, _req_size, _resp)	\
+	__fw_send_req_resp(_sw, _t, _req, _req_size, _resp, 0)
+
+#define fw_send_req_resp(_sw, _t, _req, _resp)	\
+	__fw_send_req_resp(_sw, _t, _req, sizeof(*_req), _resp, 0)
+
+#define fw_send_req_resp_wait(_sw, _t, _req, _resp, _wait)	\
+	__fw_send_req_resp(_sw, _t, _req, sizeof(*_req), _resp, _wait)
+
+#define fw_send_req(_sw, _t, _req)	\
+({							\
+	struct mvsw_msg_common_response __re;		\
+	(fw_send_req_resp(_sw, _t, _req, &__re));	\
+})
+
+struct mvsw_fw_event_handler {
+	struct list_head list;
+	enum mvsw_pr_event_type type;
+	void (*func)(struct mvsw_pr_switch *sw, struct mvsw_pr_event *evt);
+};
+
+static int fw_parse_port_evt(u8 *msg, struct mvsw_pr_event *evt)
+{
+	struct mvsw_msg_event_port *hw_evt = (struct mvsw_msg_event_port *)msg;
+
+	evt->port_evt.port_id = hw_evt->port_id;
+
+	if (evt->id == MVSW_PORT_EVENT_STATE_CHANGED)
+		evt->port_evt.data.oper_state = hw_evt->param.oper_state;
+	else
+		return -EINVAL;
+
+	return 0;
+}
+
+static int fw_parse_fdb_evt(u8 *msg, struct mvsw_pr_event *evt)
+{
+	struct mvsw_msg_event_fdb *hw_evt = (struct mvsw_msg_event_fdb *)msg;
+
+	evt->fdb_evt.port_id	= hw_evt->port_id;
+	evt->fdb_evt.vid	= hw_evt->vid;
+
+	memcpy(&evt->fdb_evt.data, &hw_evt->param, sizeof(u8) * ETH_ALEN);
+
+	return 0;
+}
+
+static int fw_parse_log_evt(u8 *msg, struct mvsw_pr_event *evt)
+{
+	struct mvsw_msg_event_log *hw_evt = (struct mvsw_msg_event_log *)msg;
+
+	evt->fw_log_evt.log_len	= hw_evt->log_string_size;
+	evt->fw_log_evt.data	= hw_evt->log_string;
+
+	return 0;
+}
+
+struct mvsw_fw_evt_parser {
+	int (*func)(u8 *msg, struct mvsw_pr_event *evt);
+};
+
+static struct mvsw_fw_evt_parser fw_event_parsers[MVSW_EVENT_TYPE_MAX] = {
+	[MVSW_EVENT_TYPE_PORT] = {.func = fw_parse_port_evt},
+	[MVSW_EVENT_TYPE_FDB] = {.func = fw_parse_fdb_evt},
+	[MVSW_EVENT_TYPE_FW_LOG] = {.func = fw_parse_log_evt}
+};
+
+static struct mvsw_fw_event_handler *
+__find_event_handler(const struct mvsw_pr_switch *sw,
+		     enum mvsw_pr_event_type type)
+{
+	struct mvsw_fw_event_handler *eh;
+
+	list_for_each_entry_rcu(eh, &sw->event_handlers, list) {
+		if (eh->type == type)
+			return eh;
+	}
+
+	return NULL;
+}
+
+static int fw_event_recv(struct mvsw_pr_device *dev, u8 *buf, size_t size)
+{
+	void (*cb)(struct mvsw_pr_switch *sw, struct mvsw_pr_event *evt) = NULL;
+	struct mvsw_msg_event *msg = (struct mvsw_msg_event *)buf;
+	struct mvsw_pr_switch *sw = dev->priv;
+	struct mvsw_fw_event_handler *eh;
+	struct mvsw_pr_event evt;
+	int err;
+
+	if (msg->type >= MVSW_EVENT_TYPE_MAX)
+		return -EINVAL;
+
+	rcu_read_lock();
+	eh = __find_event_handler(sw, msg->type);
+	if (eh)
+		cb = eh->func;
+	rcu_read_unlock();
+
+	if (!cb || !fw_event_parsers[msg->type].func)
+		return 0;
+
+	evt.id = msg->id;
+
+	err = fw_event_parsers[msg->type].func(buf, &evt);
+	if (!err)
+		cb(sw, &evt);
+
+	return err;
+}
+
+static struct mvsw_msg_buff *mvsw_msg_buff_create(u16 head_size)
+{
+	struct mvsw_msg_buff *msg_buff;
+
+	msg_buff = kzalloc(sizeof(*msg_buff), GFP_KERNEL);
+	if (!msg_buff)
+		return NULL;
+
+	msg_buff->data = kzalloc(MVSW_PR_MSG_BUFF_CHUNK_SIZE + head_size,
+				 GFP_KERNEL);
+	if (!msg_buff->data) {
+		kfree(msg_buff);
+		return NULL;
+	}
+
+	msg_buff->total = MVSW_PR_MSG_BUFF_CHUNK_SIZE + head_size;
+	msg_buff->free = MVSW_PR_MSG_BUFF_CHUNK_SIZE;
+	msg_buff->used = head_size;
+
+	return msg_buff;
+}
+
+static void *mvsw_msg_buff_data(struct mvsw_msg_buff *msg_buff)
+{
+	return msg_buff->data;
+}
+
+static u32 mvsw_msg_buff_size(const struct mvsw_msg_buff *msg_buff)
+{
+	return msg_buff->used;
+}
+
+static int mvsw_msg_buff_resize(struct mvsw_msg_buff *msg_buff)
+{
+	void *data;
+
+	data = krealloc(msg_buff->data,
+			msg_buff->total + MVSW_PR_MSG_BUFF_CHUNK_SIZE,
+			GFP_KERNEL);
+	if (!data)
+		return -ENOMEM;
+
+	msg_buff->total += MVSW_PR_MSG_BUFF_CHUNK_SIZE;
+	msg_buff->free += MVSW_PR_MSG_BUFF_CHUNK_SIZE;
+	msg_buff->data = data;
+
+	return 0;
+}
+
+static int mvsw_msg_buff_put(struct mvsw_msg_buff *msg_buff,
+			     void *data, u16 size)
+{
+	void *data_ptr;
+	int err;
+
+	if (size > msg_buff->free) {
+		err = mvsw_msg_buff_resize(msg_buff);
+		if (err)
+			return err;
+	}
+	/* point to unused data */
+	data_ptr = msg_buff->data + msg_buff->used;
+
+	/* set the data */
+	memcpy(data_ptr, data, size);
+	msg_buff->used += size;
+	msg_buff->free -= size;
+
+	return 0;
+}
+
+static int mvsw_msg_buff_terminate(struct mvsw_msg_buff *msg_buff)
+{
+	u16 padding_size;
+	void *data_ptr;
+	int err;
+
+	/* the data should be aligned to 4 byte, so calculate
+	 * the padding leaving at least one byte for termination
+	 */
+	padding_size = ALIGN(msg_buff->used + sizeof(u8),
+			     sizeof(u32)) - msg_buff->used;
+	if (msg_buff->free < padding_size) {
+		err = mvsw_msg_buff_resize(msg_buff);
+		if (err)
+			return err;
+	}
+	/* point to unused data */
+	data_ptr = msg_buff->data + msg_buff->used;
+
+	/* terminate buffer by zero byte */
+	memset(data_ptr, 0, padding_size);
+	msg_buff->used += padding_size;
+	msg_buff->free -= padding_size;
+	data_ptr += padding_size;
+
+	return 0;
+}
+
+static void mvsw_msg_buff_destroy(struct mvsw_msg_buff *msg_buff)
+{
+	kfree(msg_buff->data);
+	kfree(msg_buff);
+}
+
+int mvsw_pr_hw_port_info_get(const struct mvsw_pr_port *port,
+			     u16 *fp_id, u32 *hw_id, u32 *dev_id)
+{
+	struct mvsw_msg_port_info_ret resp;
+	struct mvsw_msg_port_info_cmd req = {
+		.port = port->id
+	};
+	int err;
+
+	err = fw_send_req_resp(port->sw, MVSW_MSG_TYPE_PORT_INFO_GET,
+			       &req, &resp);
+	if (err)
+		return err;
+
+	*hw_id = resp.hw_id;
+	*dev_id = resp.dev_id;
+	*fp_id = resp.fp_id;
+
+	return 0;
+}
+
+int mvsw_pr_hw_switch_init(struct mvsw_pr_switch *sw)
+{
+	struct mvsw_msg_switch_init_ret resp;
+	struct mvsw_msg_common_request req;
+	int err = 0;
+
+	INIT_LIST_HEAD(&sw->event_handlers);
+
+	err = fw_send_req_resp_wait(sw, MVSW_MSG_TYPE_SWITCH_INIT, &req, &resp,
+				    MVSW_PR_INIT_TIMEOUT);
+	if (err)
+		return err;
+
+	sw->id = resp.switch_id;
+	sw->port_count = resp.port_count;
+	sw->mtu_min = MVSW_PR_MIN_MTU;
+	sw->mtu_max = resp.mtu_max;
+	sw->dev->recv_msg = fw_event_recv;
+	memcpy(sw->base_mac, resp.mac, ETH_ALEN);
+
+	return err;
+}
+
+int mvsw_pr_hw_switch_ageing_set(const struct mvsw_pr_switch *sw,
+				 u32 ageing_time)
+{
+	struct mvsw_msg_switch_attr_cmd req = {
+		.param = {.ageing_timeout = ageing_time}
+	};
+
+	return fw_send_req(sw, MVSW_MSG_TYPE_AGEING_TIMEOUT_SET, &req);
+}
+
+int mvsw_pr_hw_port_state_set(const struct mvsw_pr_port *port,
+			      bool admin_state)
+{
+	struct mvsw_msg_port_attr_cmd req = {
+		.attr = MVSW_MSG_PORT_ATTR_ADMIN_STATE,
+		.port = port->hw_id,
+		.dev = port->dev_id,
+		.param = {.admin_state = admin_state ? 1 : 0}
+	};
+
+	return fw_send_req(port->sw, MVSW_MSG_TYPE_PORT_ATTR_SET, &req);
+}
+
+int mvsw_pr_hw_port_state_get(const struct mvsw_pr_port *port,
+			      bool *admin_state, bool *oper_state)
+{
+	struct mvsw_msg_port_attr_ret resp;
+	struct mvsw_msg_port_attr_cmd req = {
+		.port = port->hw_id,
+		.dev = port->dev_id
+	};
+	int err;
+
+	if (admin_state) {
+		req.attr = MVSW_MSG_PORT_ATTR_ADMIN_STATE;
+		err = fw_send_req_resp(port->sw, MVSW_MSG_TYPE_PORT_ATTR_GET,
+				       &req, &resp);
+		if (err)
+			return err;
+		*admin_state = resp.param.admin_state != 0;
+	}
+
+	if (oper_state) {
+		req.attr = MVSW_MSG_PORT_ATTR_OPER_STATE;
+		err = fw_send_req_resp(port->sw, MVSW_MSG_TYPE_PORT_ATTR_GET,
+				       &req, &resp);
+		if (err)
+			return err;
+		*oper_state = resp.param.oper_state != 0;
+	}
+
+	return 0;
+}
+
+int mvsw_pr_hw_port_mtu_set(const struct mvsw_pr_port *port, u32 mtu)
+{
+	struct mvsw_msg_port_attr_cmd req = {
+		.attr = MVSW_MSG_PORT_ATTR_MTU,
+		.port = port->hw_id,
+		.dev = port->dev_id,
+		.param = {.mtu = mtu}
+	};
+
+	return fw_send_req(port->sw, MVSW_MSG_TYPE_PORT_ATTR_SET, &req);
+}
+
+int mvsw_pr_hw_port_mtu_get(const struct mvsw_pr_port *port, u32 *mtu)
+{
+	struct mvsw_msg_port_attr_ret resp;
+	struct mvsw_msg_port_attr_cmd req = {
+		.attr = MVSW_MSG_PORT_ATTR_MTU,
+		.port = port->hw_id,
+		.dev = port->dev_id
+	};
+	int err;
+
+	err = fw_send_req_resp(port->sw, MVSW_MSG_TYPE_PORT_ATTR_GET,
+			       &req, &resp);
+	if (err)
+		return err;
+
+	*mtu = resp.param.mtu;
+
+	return err;
+}
+
+int mvsw_pr_hw_port_mac_set(const struct mvsw_pr_port *port, char *mac)
+{
+	struct mvsw_msg_port_attr_cmd req = {
+		.attr = MVSW_MSG_PORT_ATTR_MAC,
+		.port = port->hw_id,
+		.dev = port->dev_id
+	};
+	memcpy(&req.param.mac, mac, sizeof(req.param.mac));
+
+	return fw_send_req(port->sw, MVSW_MSG_TYPE_PORT_ATTR_SET, &req);
+}
+
+int mvsw_pr_hw_port_mac_get(const struct mvsw_pr_port *port, char *mac)
+{
+	struct mvsw_msg_port_attr_ret resp;
+	struct mvsw_msg_port_attr_cmd req = {
+		.attr = MVSW_MSG_PORT_ATTR_MAC,
+		.port = port->hw_id,
+		.dev = port->dev_id
+	};
+	int err;
+
+	err = fw_send_req_resp(port->sw, MVSW_MSG_TYPE_PORT_ATTR_GET,
+			       &req, &resp);
+	if (err)
+		return err;
+
+	memcpy(mac, resp.param.mac, sizeof(resp.param.mac));
+
+	return err;
+}
+
+int mvsw_pr_hw_port_accept_frame_type_set(const struct mvsw_pr_port *port,
+					  enum mvsw_pr_accept_frame_type type)
+{
+	struct mvsw_msg_port_attr_cmd req = {
+		.attr = MVSW_MSG_PORT_ATTR_ACCEPT_FRAME_TYPE,
+		.port = port->hw_id,
+		.dev = port->dev_id,
+		.param = {.accept_frm_type = type}
+	};
+
+	return fw_send_req(port->sw, MVSW_MSG_TYPE_PORT_ATTR_SET, &req);
+}
+
+int mvsw_pr_hw_port_learning_set(const struct mvsw_pr_port *port, bool enable)
+{
+	struct mvsw_msg_port_attr_cmd req = {
+		.attr = MVSW_MSG_PORT_ATTR_LEARNING,
+		.port = port->hw_id,
+		.dev = port->dev_id,
+		.param = {.learning = enable ? 1 : 0}
+	};
+
+	return fw_send_req(port->sw, MVSW_MSG_TYPE_PORT_ATTR_SET, &req);
+}
+
+int mvsw_pr_hw_event_handler_register(struct mvsw_pr_switch *sw,
+				      enum mvsw_pr_event_type type,
+				      void (*cb)(struct mvsw_pr_switch *sw,
+						 struct mvsw_pr_event *evt))
+{
+	struct mvsw_fw_event_handler *eh;
+
+	eh = __find_event_handler(sw, type);
+	if (eh)
+		return -EEXIST;
+	eh = kmalloc(sizeof(*eh), GFP_KERNEL);
+	if (!eh)
+		return -ENOMEM;
+
+	eh->type = type;
+	eh->func = cb;
+
+	INIT_LIST_HEAD(&eh->list);
+
+	list_add_rcu(&eh->list, &sw->event_handlers);
+
+	return 0;
+}
+
+void mvsw_pr_hw_event_handler_unregister(struct mvsw_pr_switch *sw,
+					 enum mvsw_pr_event_type type,
+					 void (*cb)(struct mvsw_pr_switch *sw,
+						    struct mvsw_pr_event *evt))
+{
+	struct mvsw_fw_event_handler *eh;
+
+	eh = __find_event_handler(sw, type);
+	if (!eh)
+		return;
+
+	list_del_rcu(&eh->list);
+	synchronize_rcu();
+	kfree(eh);
+}
+
+int mvsw_pr_hw_vlan_create(const struct mvsw_pr_switch *sw, u16 vid)
+{
+	struct mvsw_msg_vlan_cmd req = {
+		.vid = vid,
+	};
+
+	return fw_send_req(sw, MVSW_MSG_TYPE_VLAN_CREATE, &req);
+}
+
+int mvsw_pr_hw_vlan_delete(const struct mvsw_pr_switch *sw, u16 vid)
+{
+	struct mvsw_msg_vlan_cmd req = {
+		.vid = vid,
+	};
+
+	return fw_send_req(sw, MVSW_MSG_TYPE_VLAN_DELETE, &req);
+}
+
+int mvsw_pr_hw_vlan_port_set(const struct mvsw_pr_port *port,
+			     u16 vid, bool is_member, bool untagged)
+{
+	struct mvsw_msg_vlan_cmd req = {
+		.port = port->hw_id,
+		.dev = port->dev_id,
+		.vid = vid,
+		.is_member = is_member ? 1 : 0,
+		.is_tagged = untagged ? 0 : 1
+	};
+
+	return fw_send_req(port->sw, MVSW_MSG_TYPE_VLAN_PORT_SET, &req);
+}
+
+int mvsw_pr_hw_vlan_port_vid_set(const struct mvsw_pr_port *port, u16 vid)
+{
+	struct mvsw_msg_vlan_cmd req = {
+		.port = port->hw_id,
+		.dev = port->dev_id,
+		.vid = vid
+	};
+
+	return fw_send_req(port->sw, MVSW_MSG_TYPE_VLAN_PVID_SET, &req);
+}
+
+int mvsw_pr_hw_port_speed_get(const struct mvsw_pr_port *port, u32 *speed)
+{
+	struct mvsw_msg_port_attr_ret resp;
+	struct mvsw_msg_port_attr_cmd req = {
+		.attr = MVSW_MSG_PORT_ATTR_SPEED,
+		.port = port->hw_id,
+		.dev = port->dev_id
+	};
+	int err;
+
+	err = fw_send_req_resp(port->sw, MVSW_MSG_TYPE_PORT_ATTR_GET,
+			       &req, &resp);
+	if (err)
+		return err;
+
+	*speed = resp.param.speed;
+
+	return err;
+}
+
+int mvsw_pr_hw_port_flood_set(const struct mvsw_pr_port *port, bool flood)
+{
+	struct mvsw_msg_port_attr_cmd req = {
+		.attr = MVSW_MSG_PORT_ATTR_FLOOD,
+		.port = port->hw_id,
+		.dev = port->dev_id,
+		.param = {.flood = flood ? 1 : 0}
+	};
+
+	return fw_send_req(port->sw, MVSW_MSG_TYPE_PORT_ATTR_SET, &req);
+}
+
+int mvsw_pr_hw_fdb_add(const struct mvsw_pr_port *port,
+		       const unsigned char *mac, u16 vid, bool dynamic)
+{
+	struct mvsw_msg_fdb_cmd req = {
+		.port = port->hw_id,
+		.dev = port->dev_id,
+		.vid = vid,
+		.dynamic = dynamic ? 1 : 0
+	};
+
+	memcpy(req.mac, mac, sizeof(req.mac));
+
+	return fw_send_req(port->sw, MVSW_MSG_TYPE_FDB_ADD, &req);
+}
+
+int mvsw_pr_hw_fdb_del(const struct mvsw_pr_port *port,
+		       const unsigned char *mac, u16 vid)
+{
+	struct mvsw_msg_fdb_cmd req = {
+		.port = port->hw_id,
+		.dev = port->dev_id,
+		.vid = vid
+	};
+
+	memcpy(req.mac, mac, sizeof(req.mac));
+
+	return fw_send_req(port->sw, MVSW_MSG_TYPE_FDB_DELETE, &req);
+}
+
+int mvsw_pr_hw_port_cap_get(const struct mvsw_pr_port *port,
+			    struct mvsw_pr_port_caps *caps)
+{
+	struct mvsw_msg_port_attr_ret resp;
+	struct mvsw_msg_port_attr_cmd req = {
+		.attr = MVSW_MSG_PORT_ATTR_CAPABILITY,
+		.port = port->hw_id,
+		.dev = port->dev_id
+	};
+	int err;
+
+	err = fw_send_req_resp(port->sw, MVSW_MSG_TYPE_PORT_ATTR_GET,
+			       &req, &resp);
+	if (err)
+		return err;
+
+	caps->supp_link_modes = resp.param.cap.link_mode;
+	caps->supp_fec = resp.param.cap.fec;
+	caps->type = resp.param.cap.type;
+	caps->transceiver = resp.param.cap.transceiver;
+
+	return err;
+}
+
+int mvsw_pr_hw_port_remote_cap_get(const struct mvsw_pr_port *port,
+				   u64 *link_mode_bitmap)
+{
+	struct mvsw_msg_port_attr_ret resp;
+	struct mvsw_msg_port_attr_cmd req = {
+		.attr = MVSW_MSG_PORT_ATTR_REMOTE_CAPABILITY,
+		.port = port->hw_id,
+		.dev = port->dev_id
+	};
+	int err;
+
+	err = fw_send_req_resp(port->sw, MVSW_MSG_TYPE_PORT_ATTR_GET,
+			       &req, &resp);
+	if (err)
+		return err;
+
+	*link_mode_bitmap = resp.param.cap.link_mode;
+
+	return err;
+}
+
+static u8 mvsw_mdix_to_eth(u8 mode)
+{
+	switch (mode) {
+	case MVSW_PORT_TP_MDI:
+		return ETH_TP_MDI;
+	case MVSW_PORT_TP_MDIX:
+		return ETH_TP_MDI_X;
+	case MVSW_PORT_TP_AUTO:
+		return ETH_TP_MDI_AUTO;
+	}
+
+	return ETH_TP_MDI_INVALID;
+}
+
+static u8 mvsw_mdix_from_eth(u8 mode)
+{
+	switch (mode) {
+	case ETH_TP_MDI:
+		return MVSW_PORT_TP_MDI;
+	case ETH_TP_MDI_X:
+		return MVSW_PORT_TP_MDIX;
+	case ETH_TP_MDI_AUTO:
+		return MVSW_PORT_TP_AUTO;
+	}
+
+	return MVSW_PORT_TP_NA;
+}
+
+int mvsw_pr_hw_port_mdix_get(const struct mvsw_pr_port *port, u8 *status,
+			     u8 *admin_mode)
+{
+	struct mvsw_msg_port_attr_ret resp;
+	struct mvsw_msg_port_attr_cmd req = {
+		.attr = MVSW_MSG_PORT_ATTR_MDIX,
+		.port = port->hw_id,
+		.dev = port->dev_id
+	};
+	int err;
+
+	err = fw_send_req_resp(port->sw, MVSW_MSG_TYPE_PORT_ATTR_GET,
+			       &req, &resp);
+	if (err)
+		return err;
+
+	*status = mvsw_mdix_to_eth(resp.param.mdix.status);
+	*admin_mode = mvsw_mdix_to_eth(resp.param.mdix.admin_mode);
+
+	return 0;
+}
+
+int mvsw_pr_hw_port_mdix_set(const struct mvsw_pr_port *port, u8 mode)
+{
+	struct mvsw_msg_port_attr_cmd req = {
+		.attr = MVSW_MSG_PORT_ATTR_MDIX,
+		.port = port->hw_id,
+		.dev = port->dev_id
+	};
+
+	req.param.mdix.admin_mode = mvsw_mdix_from_eth(mode);
+
+	return fw_send_req(port->sw, MVSW_MSG_TYPE_PORT_ATTR_SET, &req);
+}
+
+int mvsw_pr_hw_port_type_get(const struct mvsw_pr_port *port, u8 *type)
+{
+	struct mvsw_msg_port_attr_ret resp;
+	struct mvsw_msg_port_attr_cmd req = {
+		.attr = MVSW_MSG_PORT_ATTR_TYPE,
+		.port = port->hw_id,
+		.dev = port->dev_id
+	};
+	int err;
+
+	err = fw_send_req_resp(port->sw, MVSW_MSG_TYPE_PORT_ATTR_GET,
+			       &req, &resp);
+	if (err)
+		return err;
+
+	*type = resp.param.type;
+
+	return err;
+}
+
+int mvsw_pr_hw_port_fec_get(const struct mvsw_pr_port *port, u8 *fec)
+{
+	struct mvsw_msg_port_attr_ret resp;
+	struct mvsw_msg_port_attr_cmd req = {
+		.attr = MVSW_MSG_PORT_ATTR_FEC,
+		.port = port->hw_id,
+		.dev = port->dev_id
+	};
+	int err;
+
+	err = fw_send_req_resp(port->sw, MVSW_MSG_TYPE_PORT_ATTR_GET,
+			       &req, &resp);
+	if (err)
+		return err;
+
+	*fec = resp.param.fec;
+
+	return err;
+}
+
+int mvsw_pr_hw_port_fec_set(const struct mvsw_pr_port *port, u8 fec)
+{
+	struct mvsw_msg_port_attr_cmd req = {
+		.attr = MVSW_MSG_PORT_ATTR_FEC,
+		.port = port->hw_id,
+		.dev = port->dev_id,
+		.param = {.fec = fec}
+	};
+
+	return fw_send_req(port->sw, MVSW_MSG_TYPE_PORT_ATTR_SET, &req);
+}
+
+int mvsw_pr_hw_fw_log_level_set(const struct mvsw_pr_switch *sw,
+				u32 lib_name, u32 log_level)
+{
+	struct mvsw_msg_log_cmd req = {
+		.lib = lib_name,
+		.level = log_level
+	};
+
+	return fw_send_req(sw, MVSW_MSG_TYPE_LOG_LEVEL_SET, &req);
+}
+
+int mvsw_pr_hw_port_autoneg_set(const struct mvsw_pr_port *port,
+				bool autoneg, u64 link_modes, u8 fec)
+{
+	struct mvsw_msg_port_attr_cmd req = {
+		.attr = MVSW_MSG_PORT_ATTR_AUTONEG,
+		.port = port->hw_id,
+		.dev = port->dev_id,
+		.param = {.autoneg = {.link_mode = link_modes,
+				      .enable = autoneg ? 1 : 0,
+				      .fec = fec}
+		}
+	};
+
+	return fw_send_req(port->sw, MVSW_MSG_TYPE_PORT_ATTR_SET, &req);
+}
+
+int mvsw_pr_hw_port_duplex_get(const struct mvsw_pr_port *port, u8 *duplex)
+{
+	struct mvsw_msg_port_attr_ret resp;
+	struct mvsw_msg_port_attr_cmd req = {
+		.attr = MVSW_MSG_PORT_ATTR_DUPLEX,
+		.port = port->hw_id,
+		.dev = port->dev_id
+	};
+	int err;
+
+	err = fw_send_req_resp(port->sw, MVSW_MSG_TYPE_PORT_ATTR_GET,
+			       &req, &resp);
+	if (err)
+		return err;
+
+	*duplex = resp.param.duplex;
+
+	return err;
+}
+
+int mvsw_pr_hw_port_stats_get(const struct mvsw_pr_port *port,
+			      struct mvsw_pr_port_stats *stats)
+{
+	struct mvsw_msg_port_stats_ret resp;
+	struct mvsw_msg_port_attr_cmd req = {
+		.attr = MVSW_MSG_PORT_ATTR_STATS,
+		.port = port->hw_id,
+		.dev = port->dev_id
+	};
+	u64 *hw_val = resp.stats;
+	int err;
+
+	err = fw_send_req_resp(port->sw, MVSW_MSG_TYPE_PORT_ATTR_GET,
+			       &req, &resp);
+	if (err)
+		return err;
+
+	stats->good_octets_received = hw_val[MVSW_PORT_GOOD_OCTETS_RCV_CNT];
+	stats->bad_octets_received = hw_val[MVSW_PORT_BAD_OCTETS_RCV_CNT];
+	stats->mac_trans_error = hw_val[MVSW_PORT_MAC_TRANSMIT_ERR_CNT];
+	stats->broadcast_frames_received = hw_val[MVSW_PORT_BRDC_PKTS_RCV_CNT];
+	stats->multicast_frames_received = hw_val[MVSW_PORT_MC_PKTS_RCV_CNT];
+	stats->frames_64_octets = hw_val[MVSW_PORT_PKTS_64_OCTETS_CNT];
+	stats->frames_65_to_127_octets =
+		hw_val[MVSW_PORT_PKTS_65TO127_OCTETS_CNT];
+	stats->frames_128_to_255_octets =
+		hw_val[MVSW_PORT_PKTS_128TO255_OCTETS_CNT];
+	stats->frames_256_to_511_octets =
+		hw_val[MVSW_PORT_PKTS_256TO511_OCTETS_CNT];
+	stats->frames_512_to_1023_octets =
+		hw_val[MVSW_PORT_PKTS_512TO1023_OCTETS_CNT];
+	stats->frames_1024_to_max_octets =
+		hw_val[MVSW_PORT_PKTS_1024TOMAX_OCTETS_CNT];
+	stats->excessive_collision = hw_val[MVSW_PORT_EXCESSIVE_COLLISIONS_CNT];
+	stats->multicast_frames_sent = hw_val[MVSW_PORT_MC_PKTS_SENT_CNT];
+	stats->broadcast_frames_sent = hw_val[MVSW_PORT_BRDC_PKTS_SENT_CNT];
+	stats->fc_sent = hw_val[MVSW_PORT_FC_SENT_CNT];
+	stats->fc_received = hw_val[MVSW_PORT_GOOD_FC_RCV_CNT];
+	stats->buffer_overrun = hw_val[MVSW_PORT_DROP_EVENTS_CNT];
+	stats->undersize = hw_val[MVSW_PORT_UNDERSIZE_PKTS_CNT];
+	stats->fragments = hw_val[MVSW_PORT_FRAGMENTS_PKTS_CNT];
+	stats->oversize = hw_val[MVSW_PORT_OVERSIZE_PKTS_CNT];
+	stats->jabber = hw_val[MVSW_PORT_JABBER_PKTS_CNT];
+	stats->rx_error_frame_received = hw_val[MVSW_PORT_MAC_RCV_ERROR_CNT];
+	stats->bad_crc = hw_val[MVSW_PORT_BAD_CRC_CNT];
+	stats->collisions = hw_val[MVSW_PORT_COLLISIONS_CNT];
+	stats->late_collision = hw_val[MVSW_PORT_LATE_COLLISIONS_CNT];
+	stats->unicast_frames_received = hw_val[MVSW_PORT_GOOD_UC_PKTS_RCV_CNT];
+	stats->unicast_frames_sent = hw_val[MVSW_PORT_GOOD_UC_PKTS_SENT_CNT];
+	stats->sent_multiple = hw_val[MVSW_PORT_MULTIPLE_PKTS_SENT_CNT];
+	stats->sent_deferred = hw_val[MVSW_PORT_DEFERRED_PKTS_SENT_CNT];
+	stats->good_octets_sent = hw_val[MVSW_PORT_GOOD_OCTETS_SENT_CNT];
+
+	return 0;
+}
+
+int mvsw_pr_hw_bridge_create(const struct mvsw_pr_switch *sw, u16 *bridge_id)
+{
+	struct mvsw_msg_bridge_cmd req;
+	struct mvsw_msg_bridge_ret resp;
+	int err;
+
+	err = fw_send_req_resp(sw, MVSW_MSG_TYPE_BRIDGE_CREATE, &req, &resp);
+	if (err)
+		return err;
+
+	*bridge_id = resp.bridge;
+	return err;
+}
+
+int mvsw_pr_hw_bridge_delete(const struct mvsw_pr_switch *sw, u16 bridge_id)
+{
+	struct mvsw_msg_bridge_cmd req = {
+		.bridge = bridge_id
+	};
+
+	return fw_send_req(sw, MVSW_MSG_TYPE_BRIDGE_DELETE, &req);
+}
+
+int mvsw_pr_hw_bridge_port_add(const struct mvsw_pr_port *port, u16 bridge_id)
+{
+	struct mvsw_msg_bridge_cmd req = {
+		.bridge = bridge_id,
+		.port = port->hw_id,
+		.dev = port->dev_id
+	};
+
+	return fw_send_req(port->sw, MVSW_MSG_TYPE_BRIDGE_PORT_ADD, &req);
+}
+
+int mvsw_pr_hw_bridge_port_delete(const struct mvsw_pr_port *port,
+				  u16 bridge_id)
+{
+	struct mvsw_msg_bridge_cmd req = {
+		.bridge = bridge_id,
+		.port = port->hw_id,
+		.dev = port->dev_id
+	};
+
+	return fw_send_req(port->sw, MVSW_MSG_TYPE_BRIDGE_PORT_DELETE, &req);
+}
+
+int mvsw_pr_hw_fdb_flush_port(const struct mvsw_pr_port *port, u32 mode)
+{
+	struct mvsw_msg_fdb_cmd req = {
+		.port = port->hw_id,
+		.dev = port->dev_id,
+		.flush_mode = mode,
+	};
+
+	return fw_send_req(port->sw, MVSW_MSG_TYPE_FDB_FLUSH_PORT, &req);
+}
+
+int mvsw_pr_hw_fdb_flush_vlan(const struct mvsw_pr_switch *sw, u16 vid,
+			      u32 mode)
+{
+	struct mvsw_msg_fdb_cmd req = {
+		.vid = vid,
+		.flush_mode = mode,
+	};
+
+	return fw_send_req(sw, MVSW_MSG_TYPE_FDB_FLUSH_VLAN, &req);
+}
+
+int mvsw_pr_hw_fdb_flush_port_vlan(const struct mvsw_pr_port *port, u16 vid,
+				   u32 mode)
+{
+	struct mvsw_msg_fdb_cmd req = {
+		.port = port->hw_id,
+		.dev = port->dev_id,
+		.vid = vid,
+		.flush_mode = mode,
+	};
+
+	return fw_send_req(port->sw, MVSW_MSG_TYPE_FDB_FLUSH_PORT_VLAN, &req);
+}
+
+int mvsw_pr_hw_port_link_mode_get(const struct mvsw_pr_port *port,
+				  u32 *mode)
+{
+	struct mvsw_msg_port_attr_ret resp;
+	struct mvsw_msg_port_attr_cmd req = {
+		.attr = MVSW_MSG_PORT_ATTR_LINK_MODE,
+		.port = port->hw_id,
+		.dev = port->dev_id
+	};
+	int err;
+
+	err = fw_send_req_resp(port->sw, MVSW_MSG_TYPE_PORT_ATTR_GET,
+			       &req, &resp);
+	if (err)
+		return err;
+
+	*mode = resp.param.link_mode;
+
+	return err;
+}
+
+int mvsw_pr_hw_port_link_mode_set(const struct mvsw_pr_port *port,
+				  u32 mode)
+{
+	struct mvsw_msg_port_attr_cmd req = {
+		.attr = MVSW_MSG_PORT_ATTR_LINK_MODE,
+		.port = port->hw_id,
+		.dev = port->dev_id,
+		.param = {.link_mode = mode}
+	};
+
+	return fw_send_req(port->sw, MVSW_MSG_TYPE_PORT_ATTR_SET, &req);
+}
+
+int mvsw_pr_hw_rif_port_create(const struct mvsw_pr_port *port,
+			       u16 vr_id, u8 *mac, u16 *rif_id)
+{
+	struct mvsw_msg_rif_ret resp;
+	struct mvsw_msg_rif_cmd req = {
+		.vr_id = vr_id,
+		.port = port->hw_id,
+		.dev = port->dev_id,
+		.mtu = port->net_dev->mtu,
+	};
+	int err;
+
+	memcpy(req.mac, port->net_dev->dev_addr, ETH_ALEN);
+
+	err = fw_send_req_resp(port->sw, MVSW_MSG_TYPE_RIF_PORT_CREATE,
+			       &req, &resp);
+	if (err)
+		return err;
+
+	*rif_id = resp.rif_id;
+	return err;
+}
+
+int mvsw_pr_hw_rif_vlan_create(const struct mvsw_pr_switch *sw,
+			       u16 vr_id, u8 *mac, u16 vid, u16 *rif_id)
+{
+	struct mvsw_msg_rif_ret resp;
+	struct mvsw_msg_rif_cmd req = {
+		.vr_id = vr_id,
+		.bridge = vid,
+	};
+	int err;
+
+	memcpy(req.mac, mac, ETH_ALEN);
+
+	err = fw_send_req_resp(sw, MVSW_MSG_TYPE_RIF_VLAN_CREATE,
+			       &req, &resp);
+	if (err)
+		return err;
+
+	*rif_id = resp.rif_id;
+	return err;
+}
+
+int mvsw_pr_hw_rif_bridge_create(const struct mvsw_pr_switch *sw,
+				 u16 vr_id,
+				 u8 *mac, u16 bridge_id, u16 *rif_id)
+{
+	struct mvsw_msg_rif_ret resp;
+	struct mvsw_msg_rif_cmd req = {
+		.vr_id = vr_id,
+		.bridge = bridge_id,
+	};
+	int err;
+
+	memcpy(req.mac, mac, ETH_ALEN);
+
+	err = fw_send_req_resp(sw, MVSW_MSG_TYPE_RIF_BRIDGE_CREATE,
+			       &req, &resp);
+	if (err)
+		return err;
+
+	*rif_id = resp.rif_id;
+	return err;
+}
+
+int mvsw_pr_hw_rif_delete(const struct mvsw_pr_switch *sw, u16 vr_id,
+			  u16 rif_id, struct mvsw_pr_port *port)
+{
+	struct mvsw_msg_rif_cmd req = {
+		.rif_id = rif_id,
+		.vr_id = vr_id,
+	};
+
+	if (port) {
+		req.port = port->hw_id;
+		req.dev = port->dev_id;
+	}
+
+	return fw_send_req(sw, MVSW_MSG_TYPE_RIF_DELETE, &req);
+}
+
+int mvsw_pr_hw_rif_set(const struct mvsw_pr_switch *sw,
+		       u16 *rif_id, u32 mtu, u8 *mac)
+{
+	struct mvsw_msg_rif_ret resp;
+	struct mvsw_msg_rif_cmd req = {
+		.rif_id = *rif_id,
+		.mtu = mtu,
+	};
+	int err;
+
+	if (mac)
+		memcpy(req.mac, mac, ETH_ALEN);
+
+	err = fw_send_req_resp(sw, MVSW_MSG_TYPE_RIF_SET, &req, &resp);
+	if (err)
+		return err;
+
+	*rif_id = resp.rif_id;
+	return err;
+}
+
+int mvsw_pr_hw_vr_create(const struct mvsw_pr_switch *sw, u16 *vr_id)
+{
+	int err;
+	struct mvsw_msg_vr_ret resp;
+	struct mvsw_msg_vr_cmd req;
+
+	err = fw_send_req_resp(sw, MVSW_MSG_TYPE_VR_CREATE, &req, &resp);
+	if (err)
+		return err;
+
+	*vr_id = resp.vr_id;
+	return err;
+}
+
+int mvsw_pr_hw_vr_delete(const struct mvsw_pr_switch *sw, u16 vr_id)
+{
+	struct mvsw_msg_vr_cmd req = {
+		.vr_id = vr_id,
+	};
+
+	return fw_send_req(sw, MVSW_MSG_TYPE_VR_DELETE, &req);
+}
+
+int mvsw_pr_hw_lpm_update(const struct mvsw_pr_switch *sw, u16 vr_id,
+			  __be32 dst, u32 dst_len, u16 vid)
+{
+	struct mvsw_msg_lpm_cmd req = {
+		.dst = dst,
+		.dst_len = dst_len,
+		.vid = vid,
+		.vr_id = vr_id
+	};
+
+	return fw_send_req(sw, MVSW_MSG_TYPE_LPM_UPDATE, &req);
+}
+
+int mvsw_pr_hw_lpm_del(const struct mvsw_pr_switch *sw, u16 vr_id, __be32 dst,
+		       u32 dst_len)
+{
+	struct mvsw_msg_lpm_cmd req = {
+		.dst = dst,
+		.dst_len = dst_len,
+		.vr_id = vr_id
+	};
+
+	return fw_send_req(sw, MVSW_MSG_TYPE_LPM_DELETE, &req);
+}
+
+int mvsw_pr_hw_nh_entry_add(const struct mvsw_pr_switch *sw, u16 vr_id, u16 vid,
+			    __be32 dst, u8 *mac, u32 *hw_id)
+{
+	struct mvsw_msg_nh_ret resp;
+	struct mvsw_msg_nh_cmd req = {
+		.dst = dst,
+		.vid = vid,
+		.vr_id = vr_id,
+	};
+	int err;
+
+	memcpy(req.mac, mac, ETH_ALEN);
+
+	err = fw_send_req_resp(sw, MVSW_MSG_TYPE_NH_ADD, &req, &resp);
+
+	if (err)
+		return err;
+
+	*hw_id = resp.hw_id;
+	return err;
+}
+
+int mvsw_pr_hw_nh_entry_del(const struct mvsw_pr_switch *sw, u16 vr_id,
+			    __be32 dst, u32 dst_len, u8 *mac)
+{
+	struct mvsw_msg_nh_cmd req = {
+		.dst = dst,
+		.dst_len = dst_len,
+		.vr_id = vr_id
+	};
+
+	memcpy(req.mac, mac, ETH_ALEN);
+
+	return fw_send_req(sw, MVSW_MSG_TYPE_NH_DELETE, &req);
+}
+
+int mvsw_pr_hw_nh_entry_set(const struct mvsw_pr_port *port, u16 vr_id, u16 vid,
+			    __be32 dst, u32 dst_len, u8 *mac, u32 *hw_id)
+{
+	struct mvsw_msg_nh_ret resp;
+	struct mvsw_msg_nh_cmd req = {
+		.port = port->hw_id,
+		.dev = port->dev_id,
+		.dst = dst,
+		.dst_len = dst_len,
+		.vr_id = vr_id,
+		.vid = vid
+	};
+	int err;
+
+	memcpy(req.mac, mac, ETH_ALEN);
+
+	err = fw_send_req_resp(port->sw, MVSW_MSG_TYPE_NH_SET, &req, &resp);
+	if (err)
+		return err;
+
+	*hw_id = resp.hw_id;
+	return err;
+}
+
+int mvsw_pr_hw_nh_get(const struct mvsw_pr_switch *sw, u32 hw_id, u8 *is_active)
+{
+	struct mvsw_msg_nh_cmd req = {
+		.hw_id = hw_id,
+	};
+	struct mvsw_msg_nh_ret resp;
+	int err;
+
+	err = fw_send_req_resp(sw, MVSW_MSG_TYPE_NH_GET, &req, &resp);
+	if (err)
+		return err;
+
+	*is_active = resp.is_active;
+	return err;
+}
+
+int mvsw_pr_hw_rxtx_init(const struct mvsw_pr_switch *sw, bool use_sdma,
+			 u32 *map_addr)
+{
+	struct mvsw_msg_rxtx_ret resp;
+	struct mvsw_msg_rxtx_cmd req;
+	int err;
+
+	req.use_sdma = use_sdma;
+
+	err = fw_send_req_resp(sw, MVSW_MSG_TYPE_RXTX_INIT, &req, &resp);
+	if (err)
+		return err;
+
+	if (map_addr)
+		*map_addr = resp.map_addr;
+
+	return 0;
+}
+
+int mvsw_pr_hw_port_autoneg_restart(struct mvsw_pr_port *port)
+{
+	struct mvsw_msg_port_attr_cmd req = {
+		.attr = MVSW_MSG_PORT_ATTR_AUTONEG_RESTART,
+		.port = port->hw_id,
+		.dev = port->dev_id,
+	};
+
+	return fw_send_req(port->sw, MVSW_MSG_TYPE_PORT_ATTR_SET, &req);
+}
+
+int mvsw_pr_hw_port_remote_fc_get(const struct mvsw_pr_port *port,
+				  bool *pause, bool *asym_pause)
+{
+	struct mvsw_msg_port_attr_ret resp;
+	struct mvsw_msg_port_attr_cmd req = {
+		.attr = MVSW_MSG_PORT_ATTR_REMOTE_FC,
+		.port = port->hw_id,
+		.dev = port->dev_id
+	};
+	int err;
+
+	err = fw_send_req_resp(port->sw, MVSW_MSG_TYPE_PORT_ATTR_GET,
+			       &req, &resp);
+	if (err)
+		return err;
+
+	switch (resp.param.fc) {
+	case MVSW_FC_SYMMETRIC:
+		*pause = true;
+		*asym_pause = false;
+		break;
+	case MVSW_FC_ASYMMETRIC:
+		*pause = false;
+		*asym_pause = true;
+		break;
+	case MVSW_FC_SYMM_ASYMM:
+		*pause = true;
+		*asym_pause = true;
+		break;
+	default:
+		*pause = false;
+		*asym_pause = false;
+	};
+
+	return err;
+}
+
+/* ACL API */
+int mvsw_pr_hw_acl_ruleset_create(const struct mvsw_pr_switch *sw,
+				  u16 *ruleset_id)
+{
+	int err;
+	struct mvsw_msg_acl_ruleset_ret resp;
+	struct mvsw_msg_acl_ruleset_cmd req;
+
+	err = fw_send_req_resp(sw, MVSW_MSG_TYPE_ACL_RULESET_CREATE,
+			       &req, &resp);
+	if (err)
+		return err;
+
+	*ruleset_id = resp.id;
+	return 0;
+}
+
+int mvsw_pr_hw_acl_ruleset_del(const struct mvsw_pr_switch *sw,
+			       u16 ruleset_id)
+{
+	struct mvsw_msg_acl_ruleset_cmd req = {
+		.id = ruleset_id,
+	};
+
+	return fw_send_req(sw, MVSW_MSG_TYPE_ACL_RULESET_DELETE, &req);
+}
+
+int mvsw_pr_hw_acl_rule_add(const struct mvsw_pr_switch *sw,
+			    struct mvsw_pr_acl_rule *rule,
+			    u32 *rule_id)
+{
+	int err;
+	struct mvsw_msg_acl_rule_ret resp;
+	struct mvsw_pr_acl_rule_match_entry *m_entry;
+	struct list_head *m_list = mvsw_pr_acl_rule_match_list_get(rule);
+	u32 priority = mvsw_pr_acl_rule_priority_get(rule);
+	u8 actions = mvsw_pr_acl_rule_actions_get(rule);
+	u16 ruleset_id = mvsw_pr_acl_rule_ruleset_id_get(rule);
+	struct mvsw_msg_acl_rule_cmd *req;
+	struct mvsw_msg_buff *msg;
+
+	msg = mvsw_msg_buff_create(sizeof(*req));
+	if (!msg)
+		return -ENOMEM;
+
+	/* put actions/priority first */
+	err = mvsw_msg_buff_put(msg, &actions, sizeof(actions));
+	err |= mvsw_msg_buff_put(msg, &priority, sizeof(priority));
+	if (err)
+		goto free_msg;
+
+	/* put match list */
+	list_for_each_entry(m_entry, m_list, list) {
+		err = mvsw_msg_buff_put(msg, (u8 *)&m_entry->type, sizeof(u8));
+		if (err)
+			goto free_msg;
+
+		switch (m_entry->type) {
+		case MVSW_ACL_RULE_MATCH_ENTRY_TYPE_ETH_TYPE:
+		case MVSW_ACL_RULE_MATCH_ENTRY_TYPE_L4_PORT_SRC:
+		case MVSW_ACL_RULE_MATCH_ENTRY_TYPE_L4_PORT_DST:
+		case MVSW_ACL_RULE_MATCH_ENTRY_TYPE_VLAN_ID:
+		case MVSW_ACL_RULE_MATCH_ENTRY_TYPE_VLAN_TPID:
+			err = mvsw_msg_buff_put
+				(msg, &m_entry->keymask.u16.key,
+				 sizeof(m_entry->keymask.u16.key));
+			err |= mvsw_msg_buff_put
+				(msg, &m_entry->keymask.u16.mask,
+				 sizeof(m_entry->keymask.u16.mask));
+			break;
+		case MVSW_ACL_RULE_MATCH_ENTRY_TYPE_IP_PROTO:
+			err = mvsw_msg_buff_put
+				(msg, &m_entry->keymask.u8.key,
+				 sizeof(m_entry->keymask.u8.key));
+			err |= mvsw_msg_buff_put
+				(msg, &m_entry->keymask.u8.mask,
+				 sizeof(m_entry->keymask.u8.mask));
+			break;
+		case MVSW_ACL_RULE_MATCH_ENTRY_TYPE_ETH_SMAC:
+		case MVSW_ACL_RULE_MATCH_ENTRY_TYPE_ETH_DMAC:
+			err = mvsw_msg_buff_put
+				(msg, &m_entry->keymask.mac.key,
+				 sizeof(m_entry->keymask.mac.key));
+			err |= mvsw_msg_buff_put
+				(msg, &m_entry->keymask.mac.mask,
+				 sizeof(m_entry->keymask.mac.mask));
+			break;
+		case MVSW_ACL_RULE_MATCH_ENTRY_TYPE_IP_SRC:
+		case MVSW_ACL_RULE_MATCH_ENTRY_TYPE_IP_DST:
+		case MVSW_ACL_RULE_MATCH_ENTRY_TYPE_L4_PORT_RANGE_SRC:
+		case MVSW_ACL_RULE_MATCH_ENTRY_TYPE_L4_PORT_RANGE_DST:
+			err = mvsw_msg_buff_put
+				(msg, &m_entry->keymask.u32.key,
+				 sizeof(m_entry->keymask.u32.key));
+			err |= mvsw_msg_buff_put
+				(msg, &m_entry->keymask.u32.mask,
+				 sizeof(m_entry->keymask.u32.mask));
+			break;
+		case MVSW_ACL_RULE_MATCH_ENTRY_TYPE_PORT:
+			err = mvsw_msg_buff_put
+				(msg, &m_entry->keymask.u64.key,
+				 sizeof(m_entry->keymask.u64.key));
+			err |= mvsw_msg_buff_put
+				(msg, &m_entry->keymask.u64.mask,
+				 sizeof(m_entry->keymask.u64.mask));
+			break;
+		default:
+			err = -EINVAL;
+		}
+		if (err)
+			goto free_msg;
+	}
+
+	err = mvsw_msg_buff_terminate(msg);
+	if (err)
+		goto free_msg;
+
+	req = (struct mvsw_msg_acl_rule_cmd *)mvsw_msg_buff_data(msg);
+
+	req->ruleset_id = ruleset_id;
+
+	err = fw_send_nreq_resp(sw, MVSW_MSG_TYPE_ACL_RULE_ADD, req,
+				mvsw_msg_buff_size(msg), &resp);
+	if (err)
+		goto free_msg;
+
+	*rule_id = resp.id;
+free_msg:
+	mvsw_msg_buff_destroy(msg);
+	return err;
+}
+
+int mvsw_pr_hw_acl_rule_del(const struct mvsw_pr_switch *sw, u32 rule_id)
+{
+	struct mvsw_msg_acl_rule_cmd req = {
+		.id = rule_id
+	};
+
+	return fw_send_req(sw, MVSW_MSG_TYPE_ACL_RULE_DELETE, &req);
+}
+
+int mvsw_pr_hw_acl_rule_stats_get(const struct mvsw_pr_switch *sw, u32 rule_id,
+				  u64 *packets, u64 *bytes)
+{
+	int err;
+	struct mvsw_msg_acl_rule_stats_ret resp;
+	struct mvsw_msg_acl_rule_cmd req = {
+		.id = rule_id
+	};
+
+	err = fw_send_req_resp(sw, MVSW_MSG_TYPE_ACL_RULE_STATS_GET,
+			       &req, &resp);
+	if (err)
+		return err;
+
+	*packets = resp.packets;
+	*bytes = resp.bytes;
+	return 0;
+}
+
+int mvsw_pr_hw_acl_port_bind(const struct mvsw_pr_port *port, u16 ruleset_id)
+{
+	struct mvsw_msg_acl_ruleset_bind_cmd req = {
+		.port = port->hw_id,
+		.dev = port->dev_id,
+		.ruleset_id = ruleset_id,
+	};
+
+	return fw_send_req(port->sw, MVSW_MSG_TYPE_ACL_PORT_BIND, &req);
+}
+
+int mvsw_pr_hw_acl_port_unbind(const struct mvsw_pr_port *port, u16 ruleset_id)
+{
+	struct mvsw_msg_acl_ruleset_bind_cmd req = {
+		.port = port->hw_id,
+		.dev = port->dev_id,
+		.ruleset_id = ruleset_id,
+	};
+
+	return fw_send_req(port->sw, MVSW_MSG_TYPE_ACL_PORT_UNBIND, &req);
+}
diff --git a/drivers/net/ethernet/marvell/prestera_sw/prestera_hw.h b/drivers/net/ethernet/marvell/prestera_sw/prestera_hw.h
new file mode 100644
index 0000000..5300c21
--- /dev/null
+++ b/drivers/net/ethernet/marvell/prestera_sw/prestera_hw.h
@@ -0,0 +1,218 @@
+/* SPDX-License-Identifier: BSD-3-Clause OR GPL-2.0
+ *
+ * Copyright (c) 2019-2020 Marvell International Ltd. All rights reserved.
+ *
+ */
+
+#ifndef _MVSW_PRESTERA_HW_H_
+#define _MVSW_PRESTERA_HW_H_
+
+#include <linux/types.h>
+
+enum mvsw_pr_accept_frame_type {
+	MVSW_ACCEPT_FRAME_TYPE_TAGGED,
+	MVSW_ACCEPT_FRAME_TYPE_UNTAGGED,
+	MVSW_ACCEPT_FRAME_TYPE_ALL
+};
+
+enum {
+	MVSW_LINK_MODE_10baseT_Half_BIT,
+	MVSW_LINK_MODE_10baseT_Full_BIT,
+	MVSW_LINK_MODE_100baseT_Half_BIT,
+	MVSW_LINK_MODE_100baseT_Full_BIT,
+	MVSW_LINK_MODE_1000baseT_Half_BIT,
+	MVSW_LINK_MODE_1000baseT_Full_BIT,
+	MVSW_LINK_MODE_1000baseX_Full_BIT,
+	MVSW_LINK_MODE_1000baseKX_Full_BIT,
+	MVSW_LINK_MODE_2500baseX_Full_BIT,
+	MVSW_LINK_MODE_10GbaseKR_Full_BIT,
+	MVSW_LINK_MODE_10GbaseSR_Full_BIT,
+	MVSW_LINK_MODE_10GbaseLR_Full_BIT,
+	MVSW_LINK_MODE_20GbaseKR2_Full_BIT,
+	MVSW_LINK_MODE_25GbaseCR_Full_BIT,
+	MVSW_LINK_MODE_25GbaseKR_Full_BIT,
+	MVSW_LINK_MODE_25GbaseSR_Full_BIT,
+	MVSW_LINK_MODE_40GbaseKR4_Full_BIT,
+	MVSW_LINK_MODE_40GbaseCR4_Full_BIT,
+	MVSW_LINK_MODE_40GbaseSR4_Full_BIT,
+	MVSW_LINK_MODE_50GbaseCR2_Full_BIT,
+	MVSW_LINK_MODE_50GbaseKR2_Full_BIT,
+	MVSW_LINK_MODE_50GbaseSR2_Full_BIT,
+	MVSW_LINK_MODE_100GbaseKR4_Full_BIT,
+	MVSW_LINK_MODE_100GbaseSR4_Full_BIT,
+	MVSW_LINK_MODE_100GbaseCR4_Full_BIT,
+	MVSW_LINK_MODE_MAX,
+};
+
+enum {
+	MVSW_PORT_TYPE_NONE,
+	MVSW_PORT_TYPE_TP,
+	MVSW_PORT_TYPE_AUI,
+	MVSW_PORT_TYPE_MII,
+	MVSW_PORT_TYPE_FIBRE,
+	MVSW_PORT_TYPE_BNC,
+	MVSW_PORT_TYPE_DA,
+	MVSW_PORT_TYPE_OTHER,
+	MVSW_PORT_TYPE_MAX,
+};
+
+enum {
+	MVSW_PORT_TRANSCEIVER_COPPER,
+	MVSW_PORT_TRANSCEIVER_SFP,
+	MVSW_PORT_TRANSCEIVER_MAX,
+};
+
+enum {
+	MVSW_PORT_FEC_OFF_BIT,
+	MVSW_PORT_FEC_BASER_BIT,
+	MVSW_PORT_FEC_RS_BIT,
+	MVSW_PORT_FEC_MAX,
+};
+
+enum {
+	MVSW_PORT_DUPLEX_HALF,
+	MVSW_PORT_DUPLEX_FULL
+};
+
+struct mvsw_pr_switch;
+struct mvsw_pr_port;
+struct mvsw_pr_port_stats;
+struct mvsw_pr_port_caps;
+struct mvsw_pr_acl_rule;
+
+enum mvsw_pr_event_type;
+struct mvsw_pr_event;
+
+/* Switch API */
+int mvsw_pr_hw_switch_init(struct mvsw_pr_switch *sw);
+int mvsw_pr_hw_switch_ageing_set(const struct mvsw_pr_switch *sw,
+				 u32 ageing_time);
+
+/* Port API */
+int mvsw_pr_hw_port_info_get(const struct mvsw_pr_port *port,
+			     u16 *fp_id, u32 *hw_id, u32 *dev_id);
+int mvsw_pr_hw_port_state_set(const struct mvsw_pr_port *port,
+			      bool admin_state);
+int mvsw_pr_hw_port_state_get(const struct mvsw_pr_port *port,
+			      bool *admin_state, bool *oper_state);
+int mvsw_pr_hw_port_mtu_set(const struct mvsw_pr_port *port, u32 mtu);
+int mvsw_pr_hw_port_mtu_get(const struct mvsw_pr_port *port, u32 *mtu);
+int mvsw_pr_hw_port_mac_set(const struct mvsw_pr_port *port, char *mac);
+int mvsw_pr_hw_port_mac_get(const struct mvsw_pr_port *port, char *mac);
+int mvsw_pr_hw_port_accept_frame_type_set(const struct mvsw_pr_port *port,
+					  enum mvsw_pr_accept_frame_type type);
+int mvsw_pr_hw_port_learning_set(const struct mvsw_pr_port *port, bool enable);
+int mvsw_pr_hw_port_speed_get(const struct mvsw_pr_port *port, u32 *speed);
+int mvsw_pr_hw_port_flood_set(const struct mvsw_pr_port *port, bool flood);
+int mvsw_pr_hw_port_cap_get(const struct mvsw_pr_port *port,
+			    struct mvsw_pr_port_caps *caps);
+int mvsw_pr_hw_port_remote_cap_get(const struct mvsw_pr_port *port,
+				   u64 *link_mode_bitmap);
+int mvsw_pr_hw_port_remote_fc_get(const struct mvsw_pr_port *port,
+				  bool *pause, bool *asym_pause);
+int mvsw_pr_hw_port_type_get(const struct mvsw_pr_port *port, u8 *type);
+int mvsw_pr_hw_port_fec_get(const struct mvsw_pr_port *port, u8 *fec);
+int mvsw_pr_hw_port_fec_set(const struct mvsw_pr_port *port, u8 fec);
+int mvsw_pr_hw_port_autoneg_set(const struct mvsw_pr_port *port,
+				bool autoneg, u64 link_modes, u8 fec);
+int mvsw_pr_hw_port_duplex_get(const struct mvsw_pr_port *port, u8 *duplex);
+int mvsw_pr_hw_port_stats_get(const struct mvsw_pr_port *port,
+			      struct mvsw_pr_port_stats *stats);
+int mvsw_pr_hw_port_link_mode_get(const struct mvsw_pr_port *port,
+				  u32 *mode);
+int mvsw_pr_hw_port_link_mode_set(const struct mvsw_pr_port *port,
+				  u32 mode);
+int mvsw_pr_hw_port_mdix_get(const struct mvsw_pr_port *port, u8 *status,
+			     u8 *admin_mode);
+int mvsw_pr_hw_port_mdix_set(const struct mvsw_pr_port *port, u8 mode);
+int mvsw_pr_hw_port_autoneg_restart(struct mvsw_pr_port *port);
+
+/* Vlan API */
+int mvsw_pr_hw_vlan_create(const struct mvsw_pr_switch *sw, u16 vid);
+int mvsw_pr_hw_vlan_delete(const struct mvsw_pr_switch *sw, u16 vid);
+int mvsw_pr_hw_vlan_port_set(const struct mvsw_pr_port *port,
+			     u16 vid, bool is_member, bool untagged);
+int mvsw_pr_hw_vlan_port_vid_set(const struct mvsw_pr_port *port, u16 vid);
+
+/* FDB API */
+int mvsw_pr_hw_fdb_add(const struct mvsw_pr_port *port,
+		       const unsigned char *mac, u16 vid, bool dynamic);
+int mvsw_pr_hw_fdb_del(const struct mvsw_pr_port *port,
+		       const unsigned char *mac, u16 vid);
+int mvsw_pr_hw_fdb_flush_port(const struct mvsw_pr_port *port, u32 mode);
+int mvsw_pr_hw_fdb_flush_vlan(const struct mvsw_pr_switch *sw, u16 vid,
+			      u32 mode);
+int mvsw_pr_hw_fdb_flush_port_vlan(const struct mvsw_pr_port *port, u16 vid,
+				   u32 mode);
+
+/* Bridge API */
+int mvsw_pr_hw_bridge_create(const struct mvsw_pr_switch *sw, u16 *bridge_id);
+int mvsw_pr_hw_bridge_delete(const struct mvsw_pr_switch *sw, u16 bridge_id);
+int mvsw_pr_hw_bridge_port_add(const struct mvsw_pr_port *port, u16 bridge_id);
+int mvsw_pr_hw_bridge_port_delete(const struct mvsw_pr_port *port,
+				  u16 bridge_id);
+
+/* ACL API */
+int mvsw_pr_hw_acl_ruleset_create(const struct mvsw_pr_switch *sw,
+				  u16 *ruleset_id);
+int mvsw_pr_hw_acl_ruleset_del(const struct mvsw_pr_switch *sw,
+			       u16 ruleset_id);
+int mvsw_pr_hw_acl_rule_add(const struct mvsw_pr_switch *sw,
+			    struct mvsw_pr_acl_rule *rule,
+			    u32 *rule_id);
+int mvsw_pr_hw_acl_rule_del(const struct mvsw_pr_switch *sw, u32 rule_id);
+int mvsw_pr_hw_acl_rule_stats_get(const struct mvsw_pr_switch *sw, u32 rule_id,
+				  u64 *packets, u64 *bytes);
+int mvsw_pr_hw_acl_port_bind(const struct mvsw_pr_port *port, u16 ruleset_id);
+int mvsw_pr_hw_acl_port_unbind(const struct mvsw_pr_port *port, u16 ruleset_id);
+
+/* Router API */
+int mvsw_pr_hw_rif_port_create(const struct mvsw_pr_port *port,
+			       u16 vr_id, u8 *mac, u16 *rif_id);
+int mvsw_pr_hw_rif_vlan_create(const struct mvsw_pr_switch *sw,
+			       u16 vr_id, u8 *mac, u16 vid, u16 *rif_id);
+int mvsw_pr_hw_rif_bridge_create(const struct mvsw_pr_switch *sw,
+				 u16 vr_id,
+				 u8 *mac, u16 bridge_id, u16 *rif_id);
+int mvsw_pr_hw_rif_delete(const struct mvsw_pr_switch *sw, u16 vr_id,
+			  u16 rif_id, struct mvsw_pr_port *port);
+int mvsw_pr_hw_rif_set(const struct mvsw_pr_switch *sw,
+		       u16 *rif_id, u32 mtu, u8 *mac);
+
+/* Virtual Router API */
+int mvsw_pr_hw_vr_create(const struct mvsw_pr_switch *sw, u16 *vr_id);
+int mvsw_pr_hw_vr_delete(const struct mvsw_pr_switch *sw, u16 vr_id);
+
+/* LPM API */
+int mvsw_pr_hw_lpm_update(const struct mvsw_pr_switch *sw, u16 vr_id,
+			  __be32 dst, u32 dst_len, u16 vid);
+int mvsw_pr_hw_lpm_del(const struct mvsw_pr_switch *sw, u16 vr_id, __be32 dst,
+		       u32 dst_len);
+
+/* NH API */
+int mvsw_pr_hw_nh_entry_add(const struct mvsw_pr_switch *sw, u16 vr_id, u16 vid,
+			    __be32 dst, u8 *mac, u32 *hw_id);
+int mvsw_pr_hw_nh_entry_del(const struct mvsw_pr_switch *sw, u16 vr_id,
+			    __be32 dst, u32 dst_len, u8 *mac);
+int mvsw_pr_hw_nh_entry_set(const struct mvsw_pr_port *port, u16 vr_id, u16 vid,
+			    __be32 dst, u32 dst_len, u8 *mac, u32 *hw_id);
+int mvsw_pr_hw_nh_get(const struct mvsw_pr_switch *sw, u32 hw_id,
+		      u8 *is_valid);
+
+/* Event handlers */
+int mvsw_pr_hw_event_handler_register(struct mvsw_pr_switch *sw,
+				      enum mvsw_pr_event_type type,
+				      void (*cb)(struct mvsw_pr_switch *sw,
+						 struct mvsw_pr_event *evt));
+void mvsw_pr_hw_event_handler_unregister(struct mvsw_pr_switch *sw,
+					 enum mvsw_pr_event_type type,
+					 void (*cb)(struct mvsw_pr_switch *sw,
+						    struct mvsw_pr_event *evt));
+/* SW Dev Log API */
+int mvsw_pr_hw_fw_log_level_set(const struct mvsw_pr_switch *sw,
+				u32 lib_name, u32 log_level);
+
+int mvsw_pr_hw_rxtx_init(const struct mvsw_pr_switch *sw, bool use_sdma,
+			 u32 *map_addr);
+
+#endif /* _MVSW_PRESTERA_HW_H_ */
diff --git a/drivers/net/ethernet/marvell/prestera_sw/prestera_log.c b/drivers/net/ethernet/marvell/prestera_sw/prestera_log.c
new file mode 100644
index 0000000..bf2fb60
--- /dev/null
+++ b/drivers/net/ethernet/marvell/prestera_sw/prestera_log.c
@@ -0,0 +1,203 @@
+/* SPDX-License-Identifier: BSD-3-Clause OR GPL-2.0
+ *
+ * Copyright (c) 2019-2020 Marvell International Ltd. All rights reserved.
+ *
+ */
+#include "prestera_log.h"
+
+static const char unknown[] = "UNKNOWN";
+
+DEF_ENUM_MAP(netdev_cmd) = {
+	[NETDEV_UP] = "NETDEV_UP",
+	[NETDEV_DOWN] = "NETDEV_DOWN",
+	[NETDEV_REBOOT] = "NETDEV_REBOOT",
+	[NETDEV_CHANGE] = "NETDEV_CHANGE",
+	[NETDEV_REGISTER] = "NETDEV_REGISTER",
+	[NETDEV_UNREGISTER] = "NETDEV_UNREGISTER",
+	[NETDEV_CHANGEMTU] = "NETDEV_CHANGEMTU",
+	[NETDEV_CHANGEADDR] = "NETDEV_CHANGEADDR",
+	[NETDEV_PRE_CHANGEADDR] = "NETDEV_PRE_CHANGEADDR",
+	[NETDEV_GOING_DOWN] = "NETDEV_GOING_DOWN",
+	[NETDEV_CHANGENAME] = "NETDEV_CHANGENAME",
+	[NETDEV_FEAT_CHANGE] = "NETDEV_FEAT_CHANGE",
+	[NETDEV_BONDING_FAILOVER] = "NETDEV_BONDING_FAILOVER",
+	[NETDEV_PRE_UP] = "NETDEV_PRE_UP",
+	[NETDEV_PRE_TYPE_CHANGE] = "NETDEV_PRE_TYPE_CHANGE",
+	[NETDEV_POST_TYPE_CHANGE] = "NETDEV_POST_TYPE_CHANGE",
+	[NETDEV_POST_INIT] = "NETDEV_POST_INIT",
+	[NETDEV_RELEASE] = "NETDEV_RELEASE",
+	[NETDEV_NOTIFY_PEERS] = "NETDEV_NOTIFY_PEERS",
+	[NETDEV_JOIN] = "NETDEV_JOIN",
+	[NETDEV_CHANGEUPPER] = "NETDEV_CHANGEUPPER",
+	[NETDEV_RESEND_IGMP] = "NETDEV_RESEND_IGMP",
+	[NETDEV_PRECHANGEMTU] = "NETDEV_PRECHANGEMTU",
+	[NETDEV_CHANGEINFODATA] = "NETDEV_CHANGEINFODATA",
+	[NETDEV_BONDING_INFO] = "NETDEV_BONDING_INFO",
+	[NETDEV_PRECHANGEUPPER] = "NETDEV_PRECHANGEUPPER",
+	[NETDEV_CHANGELOWERSTATE] = "NETDEV_CHANGELOWERSTATE",
+	[NETDEV_UDP_TUNNEL_PUSH_INFO] = "NETDEV_UDP_TUNNEL_PUSH_INFO",
+	[NETDEV_UDP_TUNNEL_DROP_INFO] = "NETDEV_UDP_TUNNEL_DROP_INFO",
+	[NETDEV_CHANGE_TX_QUEUE_LEN] = "NETDEV_CHANGE_TX_QUEUE_LEN",
+	[NETDEV_CVLAN_FILTER_PUSH_INFO] = "NETDEV_CVLAN_FILTER_PUSH_INFO",
+	[NETDEV_CVLAN_FILTER_DROP_INFO] = "NETDEV_CVLAN_FILTER_DROP_INFO",
+	[NETDEV_SVLAN_FILTER_PUSH_INFO] = "NETDEV_SVLAN_FILTER_PUSH_INFO",
+	[NETDEV_SVLAN_FILTER_DROP_INFO] = "NETDEV_SVLAN_FILTER_DROP_INFO"
+};
+
+DEF_ENUM_MAP(switchdev_notifier_type) = {
+	[SWITCHDEV_FDB_ADD_TO_BRIDGE] = "SWITCHDEV_FDB_ADD_TO_BRIDGE",
+	[SWITCHDEV_FDB_DEL_TO_BRIDGE] = "SWITCHDEV_FDB_DEL_TO_BRIDGE",
+	[SWITCHDEV_FDB_ADD_TO_DEVICE] = "SWITCHDEV_FDB_ADD_TO_DEVICE",
+	[SWITCHDEV_FDB_DEL_TO_DEVICE] = "SWITCHDEV_FDB_DEL_TO_DEVICE",
+	[SWITCHDEV_FDB_OFFLOADED] = "SWITCHDEV_FDB_OFFLOADED",
+	[SWITCHDEV_PORT_OBJ_ADD] = "SWITCHDEV_PORT_OBJ_ADD",
+	[SWITCHDEV_PORT_OBJ_DEL] = "SWITCHDEV_PORT_OBJ_DEL",
+	[SWITCHDEV_PORT_ATTR_SET] = "SWITCHDEV_PORT_ATTR_SET",
+	[SWITCHDEV_VXLAN_FDB_ADD_TO_BRIDGE] =
+		"SWITCHDEV_VXLAN_FDB_ADD_TO_BRIDGE",
+	[SWITCHDEV_VXLAN_FDB_DEL_TO_BRIDGE] =
+		"SWITCHDEV_VXLAN_FDB_DEL_TO_BRIDGE",
+	[SWITCHDEV_VXLAN_FDB_ADD_TO_DEVICE] =
+		"SWITCHDEV_VXLAN_FDB_ADD_TO_DEVICE",
+	[SWITCHDEV_VXLAN_FDB_DEL_TO_DEVICE] =
+		"SWITCHDEV_VXLAN_FDB_DEL_TO_DEVICE",
+	[SWITCHDEV_VXLAN_FDB_OFFLOADED] = "SWITCHDEV_VXLAN_FDB_OFFLOADED"
+};
+
+DEF_ENUM_MAP(switchdev_attr_id) = {
+	[SWITCHDEV_ATTR_ID_UNDEFINED] =
+		"SWITCHDEV_ATTR_ID_UNDEFINED",
+	[SWITCHDEV_ATTR_ID_PORT_STP_STATE] =
+		"SWITCHDEV_ATTR_ID_PORT_STP_STATE",
+	[SWITCHDEV_ATTR_ID_PORT_BRIDGE_FLAGS] =
+		"SWITCHDEV_ATTR_ID_PORT_BRIDGE_FLAGS",
+	[SWITCHDEV_ATTR_ID_PORT_PRE_BRIDGE_FLAGS] =
+		"SWITCHDEV_ATTR_ID_PORT_PRE_BRIDGE_FLAGS",
+	[SWITCHDEV_ATTR_ID_PORT_MROUTER] =
+		"SWITCHDEV_ATTR_ID_PORT_MROUTER",
+	[SWITCHDEV_ATTR_ID_BRIDGE_AGEING_TIME] =
+		"SWITCHDEV_ATTR_ID_BRIDGE_AGEING_TIME",
+	[SWITCHDEV_ATTR_ID_BRIDGE_VLAN_FILTERING] =
+		"SWITCHDEV_ATTR_ID_BRIDGE_VLAN_FILTERING",
+	[SWITCHDEV_ATTR_ID_BRIDGE_MC_DISABLED] =
+		"SWITCHDEV_ATTR_ID_BRIDGE_MC_DISABLED",
+	[SWITCHDEV_ATTR_ID_BRIDGE_MROUTER] =
+		"SWITCHDEV_ATTR_ID_BRIDGE_MROUTER"
+};
+
+DEF_ENUM_MAP(switchdev_obj_id) = {
+	[SWITCHDEV_OBJ_ID_UNDEFINED] = "SWITCHDEV_OBJ_ID_UNDEFINED",
+	[SWITCHDEV_OBJ_ID_PORT_VLAN] = "SWITCHDEV_OBJ_ID_PORT_VLAN",
+	[SWITCHDEV_OBJ_ID_PORT_MDB] = "SWITCHDEV_OBJ_ID_PORT_MDB",
+	[SWITCHDEV_OBJ_ID_HOST_MDB] = "SWITCHDEV_OBJ_ID_HOST_MDB",
+};
+
+DEF_ENUM_MAP(fib_event_type) = {
+	[FIB_EVENT_ENTRY_REPLACE] = "FIB_EVENT_ENTRY_REPLACE",
+	[FIB_EVENT_ENTRY_APPEND] = "FIB_EVENT_ENTRY_APPEND",
+	[FIB_EVENT_ENTRY_ADD] = "FIB_EVENT_ENTRY_ADD",
+	[FIB_EVENT_ENTRY_DEL] = "FIB_EVENT_ENTRY_DEL",
+	[FIB_EVENT_RULE_ADD] = "FIB_EVENT_RULE_ADD",
+	[FIB_EVENT_RULE_DEL] = "FIB_EVENT_RULE_DEL",
+	[FIB_EVENT_NH_ADD] = "FIB_EVENT_NH_ADD",
+	[FIB_EVENT_NH_DEL] = "FIB_EVENT_NH_DEL",
+	[FIB_EVENT_VIF_ADD] = "FIB_EVENT_VIF_ADD",
+	[FIB_EVENT_VIF_DEL] = "FIB_EVENT_VIF_DEL",
+};
+
+DEF_ENUM_MAP(netevent_notif_type) = {
+	[NETEVENT_NEIGH_UPDATE] = "NETEVENT_NEIGH_UPDATE",
+	[NETEVENT_REDIRECT] = "NETEVENT_REDIRECT",
+	[NETEVENT_DELAY_PROBE_TIME_UPDATE] =
+		"NETEVENT_DELAY_PROBE_TIME_UPDATE",
+	[NETEVENT_IPV4_MPATH_HASH_UPDATE] =
+		"NETEVENT_IPV4_MPATH_HASH_UPDATE",
+	[NETEVENT_IPV6_MPATH_HASH_UPDATE] =
+		"NETEVENT_IPV6_MPATH_HASH_UPDATE",
+	[NETEVENT_IPV4_FWD_UPDATE_PRIORITY_UPDATE] =
+		"NETEVENT_IPV4_FWD_UPDATE_PRIORITY_UPDATE",
+};
+
+DEF_ENUM_MAP(tc_setup_type) = {
+	[TC_SETUP_QDISC_MQPRIO] = "TC_SETUP_QDISC_MQPRIO",
+	[TC_SETUP_CLSU32] = "TC_SETUP_CLSU32",
+	[TC_SETUP_CLSFLOWER] = "TC_SETUP_CLSFLOWER",
+	[TC_SETUP_CLSMATCHALL] = "TC_SETUP_CLSMATCHALL",
+	[TC_SETUP_CLSBPF] = "TC_SETUP_CLSBPF",
+	[TC_SETUP_BLOCK] = "TC_SETUP_BLOCK",
+	[TC_SETUP_QDISC_CBS] = "TC_SETUP_QDISC_CBS",
+	[TC_SETUP_QDISC_RED] = "TC_SETUP_QDISC_RED",
+	[TC_SETUP_QDISC_PRIO] = "TC_SETUP_QDISC_PRIO",
+	[TC_SETUP_QDISC_MQ] = "TC_SETUP_QDISC_MQ",
+	[TC_SETUP_QDISC_ETF] = "TC_SETUP_QDISC_ETF",
+	[TC_SETUP_ROOT_QDISC] = "TC_SETUP_ROOT_QDISC",
+	[TC_SETUP_QDISC_GRED] = "TC_SETUP_QDISC_GRED",
+};
+
+DEF_ENUM_MAP(flow_block_binder_type) = {
+	[FLOW_BLOCK_BINDER_TYPE_UNSPEC] =
+		"FLOW_BLOCK_BINDER_TYPE_UNSPEC",
+	[FLOW_BLOCK_BINDER_TYPE_CLSACT_INGRESS] =
+		"FLOW_BLOCK_BINDER_TYPE_CLSACT_INGRESS",
+	[FLOW_BLOCK_BINDER_TYPE_CLSACT_EGRESS] =
+		"FLOW_BLOCK_BINDER_TYPE_CLSACT_EGRESS",
+};
+
+DEF_ENUM_MAP(tc_matchall_command) = {
+	[TC_CLSMATCHALL_REPLACE] = "TC_CLSMATCHALL_REPLACE",
+	[TC_CLSMATCHALL_DESTROY] = "TC_CLSMATCHALL_DESTROY",
+	[TC_CLSMATCHALL_STATS] = "TC_CLSMATCHALL_STATS",
+};
+
+DEF_ENUM_MAP(flow_cls_command) = {
+	[FLOW_CLS_REPLACE] = "FLOW_CLS_REPLACE",
+	[FLOW_CLS_DESTROY] = "FLOW_CLS_DESTROY",
+	[FLOW_CLS_STATS] = "FLOW_CLS_STATS",
+	[FLOW_CLS_TMPLT_CREATE] = "FLOW_CLS_TMPLT_CREATE",
+	[FLOW_CLS_TMPLT_DESTROY] = "FLOW_CLS_TMPLT_DESTROY",
+};
+
+DEF_ENUM_MAP(flow_action_id) = {
+	[FLOW_ACTION_ACCEPT] = "FLOW_ACTION_ACCEPT",
+	[FLOW_ACTION_DROP] = "FLOW_ACTION_DROP",
+	[FLOW_ACTION_TRAP] = "FLOW_ACTION_TRAP",
+	[FLOW_ACTION_GOTO] = "FLOW_ACTION_GOTO",
+	[FLOW_ACTION_REDIRECT] = "FLOW_ACTION_REDIRECT",
+	[FLOW_ACTION_MIRRED] = "FLOW_ACTION_MIRRED",
+	[FLOW_ACTION_VLAN_PUSH] = "FLOW_ACTION_VLAN_PUSH",
+	[FLOW_ACTION_VLAN_POP] = "FLOW_ACTION_VLAN_POP",
+	[FLOW_ACTION_VLAN_MANGLE] = "FLOW_ACTION_VLAN_MANGLE",
+	[FLOW_ACTION_TUNNEL_ENCAP] = "FLOW_ACTION_TUNNEL_ENCAP",
+	[FLOW_ACTION_TUNNEL_DECAP] = "FLOW_ACTION_TUNNEL_DECAP",
+	[FLOW_ACTION_MANGLE] = "FLOW_ACTION_MANGLE",
+	[FLOW_ACTION_ADD] = "FLOW_ACTION_ADD",
+	[FLOW_ACTION_CSUM] = "FLOW_ACTION_CSUM",
+	[FLOW_ACTION_MARK] = "FLOW_ACTION_MARK",
+	[FLOW_ACTION_WAKE] = "FLOW_ACTION_WAKE",
+	[FLOW_ACTION_QUEUE] = "FLOW_ACTION_QUEUE",
+	[FLOW_ACTION_SAMPLE] = "FLOW_ACTION_SAMPLE",
+	[FLOW_ACTION_POLICE] = "FLOW_ACTION_POLICE",
+	[FLOW_ACTION_CT] = "FLOW_ACTION_CT",
+};
+
+DEF_ENUM_FUNC(netdev_cmd, NETDEV_UP, NETDEV_SVLAN_FILTER_DROP_INFO)
+
+DEF_ENUM_FUNC(switchdev_notifier_type, SWITCHDEV_FDB_ADD_TO_BRIDGE,
+	      SWITCHDEV_VXLAN_FDB_OFFLOADED)
+DEF_ENUM_FUNC(switchdev_attr_id, SWITCHDEV_ATTR_ID_UNDEFINED,
+	      SWITCHDEV_ATTR_ID_BRIDGE_MROUTER)
+DEF_ENUM_FUNC(switchdev_obj_id, SWITCHDEV_OBJ_ID_UNDEFINED,
+	      SWITCHDEV_OBJ_ID_HOST_MDB)
+
+DEF_ENUM_FUNC(fib_event_type, FIB_EVENT_ENTRY_REPLACE, FIB_EVENT_VIF_DEL)
+
+DEF_ENUM_FUNC(netevent_notif_type, NETEVENT_NEIGH_UPDATE,
+	      NETEVENT_IPV4_FWD_UPDATE_PRIORITY_UPDATE)
+
+/* TC traffic control */
+DEF_ENUM_FUNC(tc_setup_type, TC_SETUP_QDISC_MQPRIO, TC_SETUP_QDISC_GRED)
+DEF_ENUM_FUNC(flow_block_binder_type, FLOW_BLOCK_BINDER_TYPE_UNSPEC,
+	      FLOW_BLOCK_BINDER_TYPE_CLSACT_EGRESS)
+DEF_ENUM_FUNC(tc_matchall_command, TC_CLSMATCHALL_REPLACE, TC_CLSMATCHALL_STATS)
+DEF_ENUM_FUNC(flow_cls_command, FLOW_CLS_REPLACE, FLOW_CLS_TMPLT_DESTROY)
+DEF_ENUM_FUNC(flow_action_id, FLOW_ACTION_ACCEPT, FLOW_ACTION_CT)
diff --git a/drivers/net/ethernet/marvell/prestera_sw/prestera_log.h b/drivers/net/ethernet/marvell/prestera_sw/prestera_log.h
new file mode 100644
index 0000000..aacb296
--- /dev/null
+++ b/drivers/net/ethernet/marvell/prestera_sw/prestera_log.h
@@ -0,0 +1,58 @@
+/* SPDX-License-Identifier: BSD-3-Clause OR GPL-2.0
+ *
+ * Copyright (c) 2019-2020 Marvell International Ltd. All rights reserved.
+ *
+ */
+
+#ifndef _MVSW_PRESTERA_LOG_H_
+#define _MVSW_PRESTERA_LOG_H_
+
+#ifdef CONFIG_MRVL_PRESTERA_DEBUG
+
+#include <linux/netdevice.h>
+#include <linux/version.h>
+#include <net/switchdev.h>
+#include <net/fib_notifier.h>
+#include <net/netevent.h>
+#include <net/pkt_cls.h>
+
+#define DEF_ENUM_MAP(enum_name) \
+static const char *enum_name##_map[]
+
+#define DEF_ENUM_FUNC(enum_name, enum_min, enum_max) \
+const char *enum_name##_to_name(enum enum_name val) \
+{ \
+	if (val < enum_min || val > enum_max) \
+		return unknown; \
+	return enum_name##_map[val]; \
+}
+
+#define DEC_ENUM_FUNC(enum_name) \
+const char *enum_name##_to_name(enum enum_name)
+
+#define ENUM_TO_NAME(enum_name, val) enum_name##_to_name(val)
+
+#define MVSW_LOG_INFO(fmt, ...) \
+	pr_info("%s:%d: " fmt "\n", __func__, __LINE__, ##__VA_ARGS__)
+
+#define MVSW_LOG_ERROR(fmt, ...) \
+	pr_err("%s:%d: " fmt "\n", __func__, __LINE__, ##__VA_ARGS__)
+
+DEC_ENUM_FUNC(netdev_cmd);
+DEC_ENUM_FUNC(switchdev_notifier_type);
+DEC_ENUM_FUNC(switchdev_attr_id);
+DEC_ENUM_FUNC(switchdev_obj_id);
+DEC_ENUM_FUNC(fib_event_type);
+DEC_ENUM_FUNC(netevent_notif_type);
+DEC_ENUM_FUNC(tc_setup_type);
+DEC_ENUM_FUNC(flow_block_binder_type);
+DEC_ENUM_FUNC(tc_matchall_command);
+DEC_ENUM_FUNC(flow_cls_command);
+DEC_ENUM_FUNC(flow_action_id);
+
+#else /* CONFIG_MRVL_PRESTERA_DEBUG */
+#define MVSW_LOG_INFO(...)
+#define MVSW_LOG_ERROR(...)
+#endif /* CONFIG_MRVL_PRESTERA_DEBUG */
+
+#endif /* _MVSW_PRESTERA_LOG_H_ */
diff --git a/drivers/net/ethernet/marvell/prestera_sw/prestera_pci.c b/drivers/net/ethernet/marvell/prestera_sw/prestera_pci.c
new file mode 100644
index 0000000..305f124
--- /dev/null
+++ b/drivers/net/ethernet/marvell/prestera_sw/prestera_pci.c
@@ -0,0 +1,862 @@
+/* SPDX-License-Identifier: BSD-3-Clause OR GPL-2.0
+ *
+ * Copyright (c) 2019-2020 Marvell International Ltd. All rights reserved.
+ *
+ */
+
+#include <linux/module.h>
+#include <linux/kernel.h>
+#include <linux/device.h>
+#include <linux/pci.h>
+#include <linux/circ_buf.h>
+#include <linux/firmware.h>
+
+#include "prestera.h"
+
+#define MVSW_FW_FILENAME	"marvell/mvsw_prestera_fw.img"
+
+#define MVSW_SUPP_FW_MAJ_VER 2
+#define MVSW_SUPP_FW_MIN_VER 0
+#define MVSW_SUPP_FW_PATCH_VER 0
+
+#define mvsw_wait_timeout(cond, waitms) \
+({ \
+	unsigned long __wait_end = jiffies + msecs_to_jiffies(waitms); \
+	bool __wait_ret = false; \
+	do { \
+		if (cond) { \
+			__wait_ret = true; \
+			break; \
+		} \
+		cond_resched(); \
+	} while (time_before(jiffies, __wait_end)); \
+	__wait_ret; \
+})
+
+#define MVSW_FW_HDR_MAGIC 0x351D9D06
+#define MVSW_FW_DL_TIMEOUT 50000
+#define MVSW_FW_BLK_SZ 1024
+
+#define FW_VER_MAJ_MUL 1000000
+#define FW_VER_MIN_MUL 1000
+
+#define FW_VER_MAJ(v)	((v) / FW_VER_MAJ_MUL)
+
+#define FW_VER_MIN(v) \
+	(((v) - (FW_VER_MAJ(v) * FW_VER_MAJ_MUL)) / FW_VER_MIN_MUL)
+
+#define FW_VER_PATCH(v) \
+	(v - (FW_VER_MAJ(v) * FW_VER_MAJ_MUL) - (FW_VER_MIN(v) * FW_VER_MIN_MUL))
+
+struct mvsw_pr_fw_header {
+	__be32 magic_number;
+	__be32 version_value;
+	u8 reserved[8];
+} __packed;
+
+struct mvsw_pr_ldr_regs {
+	u32 ldr_ready;
+	u32 pad1;
+
+	u32 ldr_img_size;
+	u32 ldr_ctl_flags;
+
+	u32 ldr_buf_offs;
+	u32 ldr_buf_size;
+
+	u32 ldr_buf_rd;
+	u32 pad2;
+	u32 ldr_buf_wr;
+
+	u32 ldr_status;
+} __packed __aligned(4);
+
+#define MVSW_LDR_REG_OFFSET(f)	offsetof(struct mvsw_pr_ldr_regs, f)
+
+#define MVSW_LDR_READY_MAGIC	0xf00dfeed
+
+#define MVSW_LDR_STATUS_IMG_DL		BIT(0)
+#define MVSW_LDR_STATUS_START_FW	BIT(1)
+#define MVSW_LDR_STATUS_INVALID_IMG	BIT(2)
+#define MVSW_LDR_STATUS_NOMEM		BIT(3)
+
+#define mvsw_ldr_write(fw, reg, val) \
+	writel(val, (fw)->ldr_regs + (reg))
+#define mvsw_ldr_read(fw, reg)	\
+	readl((fw)->ldr_regs + (reg))
+
+/* fw loader registers */
+#define MVSW_LDR_READY_REG	MVSW_LDR_REG_OFFSET(ldr_ready)
+#define MVSW_LDR_IMG_SIZE_REG	MVSW_LDR_REG_OFFSET(ldr_img_size)
+#define MVSW_LDR_CTL_REG	MVSW_LDR_REG_OFFSET(ldr_ctl_flags)
+#define MVSW_LDR_BUF_SIZE_REG	MVSW_LDR_REG_OFFSET(ldr_buf_size)
+#define MVSW_LDR_BUF_OFFS_REG	MVSW_LDR_REG_OFFSET(ldr_buf_offs)
+#define MVSW_LDR_BUF_RD_REG	MVSW_LDR_REG_OFFSET(ldr_buf_rd)
+#define MVSW_LDR_BUF_WR_REG	MVSW_LDR_REG_OFFSET(ldr_buf_wr)
+#define MVSW_LDR_STATUS_REG	MVSW_LDR_REG_OFFSET(ldr_status)
+
+#define MVSW_LDR_CTL_DL_START	BIT(0)
+
+#define MVSW_LDR_WR_IDX_MOVE(fw, n) \
+do { \
+	typeof(fw) __fw = (fw); \
+	(__fw)->ldr_wr_idx = ((__fw)->ldr_wr_idx + (n)) & \
+				((__fw)->ldr_buf_len - 1); \
+} while (0)
+
+#define MVSW_LDR_WR_IDX_COMMIT(fw) \
+({ \
+	typeof(fw) __fw = (fw); \
+	mvsw_ldr_write((__fw), MVSW_LDR_BUF_WR_REG, \
+		       (__fw)->ldr_wr_idx); \
+})
+
+#define MVSW_LDR_WR_PTR(fw) \
+({ \
+	typeof(fw) __fw = (fw); \
+	((__fw)->ldr_ring_buf + (__fw)->ldr_wr_idx); \
+})
+
+#define MVSW_EVT_QNUM_MAX	4
+
+struct mvsw_pr_fw_evtq_regs {
+	u32 rd_idx;
+	u32 pad1;
+	u32 wr_idx;
+	u32 pad2;
+	u32 offs;
+	u32 len;
+};
+
+struct mvsw_pr_fw_regs {
+	u32 fw_ready;
+	u32 pad;
+	u32 cmd_offs;
+	u32 cmd_len;
+	u32 evt_offs;
+	u32 evt_qnum;
+
+	u32 cmd_req_ctl;
+	u32 cmd_req_len;
+	u32 cmd_rcv_ctl;
+	u32 cmd_rcv_len;
+
+	u32 fw_status;
+
+	struct mvsw_pr_fw_evtq_regs evtq_list[MVSW_EVT_QNUM_MAX];
+};
+
+#define MVSW_FW_REG_OFFSET(f)	offsetof(struct mvsw_pr_fw_regs, f)
+
+#define MVSW_FW_READY_MAGIC	0xcafebabe
+
+/* fw registers */
+#define MVSW_FW_READY_REG		MVSW_FW_REG_OFFSET(fw_ready)
+
+#define MVSW_CMD_BUF_OFFS_REG		MVSW_FW_REG_OFFSET(cmd_offs)
+#define MVSW_CMD_BUF_LEN_REG		MVSW_FW_REG_OFFSET(cmd_len)
+#define MVSW_EVT_BUF_OFFS_REG		MVSW_FW_REG_OFFSET(evt_offs)
+#define MVSW_EVT_QNUM_REG		MVSW_FW_REG_OFFSET(evt_qnum)
+
+#define MVSW_CMD_REQ_CTL_REG		MVSW_FW_REG_OFFSET(cmd_req_ctl)
+#define MVSW_CMD_REQ_LEN_REG		MVSW_FW_REG_OFFSET(cmd_req_len)
+
+#define MVSW_CMD_RCV_CTL_REG		MVSW_FW_REG_OFFSET(cmd_rcv_ctl)
+#define MVSW_CMD_RCV_LEN_REG		MVSW_FW_REG_OFFSET(cmd_rcv_len)
+#define MVSW_FW_STATUS_REG		MVSW_FW_REG_OFFSET(fw_status)
+
+/* MVSW_CMD_REQ_CTL_REG flags */
+#define MVSW_CMD_F_REQ_SENT		BIT(0)
+#define MVSW_CMD_F_REPL_RCVD		BIT(1)
+
+/* MVSW_CMD_RCV_CTL_REG flags */
+#define MVSW_CMD_F_REPL_SENT		BIT(0)
+
+#define MVSW_EVTQ_REG_OFFSET(q, f)			\
+	(MVSW_FW_REG_OFFSET(evtq_list) +		\
+	 (q) * sizeof(struct mvsw_pr_fw_evtq_regs) +	\
+	 offsetof(struct mvsw_pr_fw_evtq_regs, f))
+
+#define MVSW_EVTQ_RD_IDX_REG(q)		MVSW_EVTQ_REG_OFFSET(q, rd_idx)
+#define MVSW_EVTQ_WR_IDX_REG(q)		MVSW_EVTQ_REG_OFFSET(q, wr_idx)
+#define MVSW_EVTQ_OFFS_REG(q)		MVSW_EVTQ_REG_OFFSET(q, offs)
+#define MVSW_EVTQ_LEN_REG(q)		MVSW_EVTQ_REG_OFFSET(q, len)
+
+#define mvsw_fw_write(fw, reg, val)	writel(val, (fw)->hw_regs + (reg))
+#define mvsw_fw_read(fw, reg)		readl((fw)->hw_regs + (reg))
+
+struct mvsw_pr_fw_evtq {
+	u8 __iomem *addr;
+	size_t len;
+};
+
+struct mvsw_pr_fw {
+	struct workqueue_struct *wq;
+	struct mvsw_pr_device dev;
+	struct pci_dev *pci_dev;
+	u8 __iomem *mem_addr;
+
+	u8 __iomem *ldr_regs;
+	u8 __iomem *hw_regs;
+
+	u8 __iomem *ldr_ring_buf;
+	u32 ldr_buf_len;
+	u32 ldr_wr_idx;
+
+	/* serialize access to dev->send_req */
+	struct mutex cmd_mtx;
+	size_t cmd_mbox_len;
+	u8 __iomem *cmd_mbox;
+	struct mvsw_pr_fw_evtq evt_queue[MVSW_EVT_QNUM_MAX];
+	u8 evt_qnum;
+	struct work_struct evt_work;
+	u8 __iomem *evt_buf;
+	u8 *evt_msg;
+};
+
+#define mvsw_fw_dev(fw)	((fw)->dev.dev)
+
+#define PRESTERA_DEVICE(id) PCI_VDEVICE(MARVELL, (id))
+
+static struct mvsw_pr_pci_match {
+	struct pci_driver driver;
+	const struct pci_device_id id;
+	bool registered;
+} mvsw_pci_devices[] = {
+	{
+		.driver = { .name = "AC3x B2B", },
+		.id = { PRESTERA_DEVICE(0xC804), 0 },
+	},
+	{
+		.driver = { .name = "Aldrin2", },
+		.id = { PRESTERA_DEVICE(0xCC1E), 0 },
+	},
+	{{ }, { },}
+};
+
+static int mvsw_pr_fw_load(struct mvsw_pr_fw *fw);
+
+static u32 mvsw_pr_fw_evtq_len(struct mvsw_pr_fw *fw, u8 qid)
+{
+	return fw->evt_queue[qid].len;
+}
+
+static u32 mvsw_pr_fw_evtq_avail(struct mvsw_pr_fw *fw, u8 qid)
+{
+	u32 wr_idx = mvsw_fw_read(fw, MVSW_EVTQ_WR_IDX_REG(qid));
+	u32 rd_idx = mvsw_fw_read(fw, MVSW_EVTQ_RD_IDX_REG(qid));
+
+	return CIRC_CNT(wr_idx, rd_idx, mvsw_pr_fw_evtq_len(fw, qid));
+}
+
+static void mvsw_pr_fw_evtq_rd_set(struct mvsw_pr_fw *fw,
+				   u8 qid, u32 idx)
+{
+	u32 rd_idx = idx & (mvsw_pr_fw_evtq_len(fw, qid) - 1);
+
+	mvsw_fw_write(fw, MVSW_EVTQ_RD_IDX_REG(qid), rd_idx);
+}
+
+static u8 __iomem *mvsw_pr_fw_evtq_buf(struct mvsw_pr_fw *fw,
+				       u8 qid)
+{
+	return fw->evt_queue[qid].addr;
+}
+
+static u32 mvsw_pr_fw_evtq_read32(struct mvsw_pr_fw *fw, u8 qid)
+{
+	u32 rd_idx = mvsw_fw_read(fw, MVSW_EVTQ_RD_IDX_REG(qid));
+	u32 val;
+
+	val = readl(mvsw_pr_fw_evtq_buf(fw, qid) + rd_idx);
+	mvsw_pr_fw_evtq_rd_set(fw, qid, rd_idx + 4);
+	return val;
+}
+
+static ssize_t mvsw_pr_fw_evtq_read_buf(struct mvsw_pr_fw *fw,
+					u8 qid, u8 *buf, size_t len)
+{
+	u32 idx = mvsw_fw_read(fw, MVSW_EVTQ_RD_IDX_REG(qid));
+	u8 __iomem *evtq_addr = mvsw_pr_fw_evtq_buf(fw, qid);
+	u32 *buf32 = (u32 *)buf;
+	int i;
+
+	for (i = 0; i < len / 4; buf32++, i++) {
+		*buf32 = readl_relaxed(evtq_addr + idx);
+		idx = (idx + 4) & (mvsw_pr_fw_evtq_len(fw, qid) - 1);
+	}
+
+	mvsw_pr_fw_evtq_rd_set(fw, qid, idx);
+
+	return i;
+}
+
+static u8 mvsw_pr_fw_evtq_pick(struct mvsw_pr_fw *fw)
+{
+	int qid;
+
+	for (qid = 0; qid < fw->evt_qnum; qid++) {
+		if (mvsw_pr_fw_evtq_avail(fw, qid) >= 4)
+			return qid;
+	}
+
+	return MVSW_EVT_QNUM_MAX;
+}
+
+static void mvsw_pr_fw_evt_work_fn(struct work_struct *work)
+{
+	struct mvsw_pr_fw *fw;
+	u8 *msg;
+	u8 qid;
+
+	fw = container_of(work, struct mvsw_pr_fw, evt_work);
+	msg = fw->evt_msg;
+
+	while ((qid = mvsw_pr_fw_evtq_pick(fw)) < MVSW_EVT_QNUM_MAX) {
+		u32 idx;
+		u32 len;
+
+		len = mvsw_pr_fw_evtq_read32(fw, qid);
+		idx = mvsw_fw_read(fw, MVSW_EVTQ_RD_IDX_REG(qid));
+
+		WARN_ON(mvsw_pr_fw_evtq_avail(fw, qid) < len);
+
+		if (WARN_ON(len > MVSW_MSG_MAX_SIZE)) {
+			mvsw_pr_fw_evtq_rd_set(fw, qid, idx + len);
+			continue;
+		}
+
+		mvsw_pr_fw_evtq_read_buf(fw, qid, msg, len);
+
+		if (fw->dev.recv_msg)
+			fw->dev.recv_msg(&fw->dev, msg, len);
+	}
+}
+
+static int mvsw_pr_fw_wait_reg32(struct mvsw_pr_fw *fw,
+				 u32 reg, u32 val, unsigned int wait)
+{
+	if (mvsw_wait_timeout(mvsw_fw_read(fw, reg) == val, wait))
+		return 0;
+
+	return -EBUSY;
+}
+
+static void mvsw_pci_copy_to(u8 __iomem *dst, u8 *src, size_t len)
+{
+	u32 __iomem *dst32 = (u32 __iomem *)dst;
+	u32 *src32 = (u32 *)src;
+	int i;
+
+	for (i = 0; i < (len / 4); dst32++, src32++, i++)
+		writel_relaxed(*src32, dst32);
+}
+
+static void mvsw_pci_copy_from(u8 *dst, u8 __iomem *src, size_t len)
+{
+	u32 *dst32 = (u32 *)dst;
+	u32 __iomem *src32 = (u32 __iomem *)src;
+	int i;
+
+	for (i = 0; i < (len / 4); dst32++, src32++, i++)
+		*dst32 = readl_relaxed(src32);
+}
+
+static int mvsw_pr_fw_cmd_send(struct mvsw_pr_fw *fw,
+			       u8 *in_msg, size_t in_size,
+			       u8 *out_msg, size_t out_size,
+			       unsigned int wait)
+{
+	u32 ret_size = 0;
+	int err = 0;
+
+	if (!wait)
+		wait = 30000;
+
+	if (ALIGN(in_size, 4) > fw->cmd_mbox_len)
+		return -EMSGSIZE;
+
+	/* wait for finish previous reply from FW */
+	err = mvsw_pr_fw_wait_reg32(fw, MVSW_CMD_RCV_CTL_REG, 0, 30);
+	if (err) {
+		dev_err(mvsw_fw_dev(fw), "finish reply from FW is timed out\n");
+		return err;
+	}
+
+	mvsw_fw_write(fw, MVSW_CMD_REQ_LEN_REG, in_size);
+	mvsw_pci_copy_to(fw->cmd_mbox, in_msg, in_size);
+
+	mvsw_fw_write(fw, MVSW_CMD_REQ_CTL_REG, MVSW_CMD_F_REQ_SENT);
+
+	/* wait for reply from FW */
+	err = mvsw_pr_fw_wait_reg32(fw, MVSW_CMD_RCV_CTL_REG, MVSW_CMD_F_REPL_SENT,
+				    wait);
+	if (err) {
+		dev_err(mvsw_fw_dev(fw), "reply from FW is timed out\n");
+		goto cmd_exit;
+	}
+
+	ret_size = mvsw_fw_read(fw, MVSW_CMD_RCV_LEN_REG);
+	if (ret_size > out_size) {
+		dev_err(mvsw_fw_dev(fw), "ret_size (%u) > out_len(%zu)\n",
+			ret_size, out_size);
+		err = -EMSGSIZE;
+		goto cmd_exit;
+	}
+
+	mvsw_pci_copy_from(out_msg, fw->cmd_mbox + in_size, ret_size);
+
+cmd_exit:
+	mvsw_fw_write(fw, MVSW_CMD_REQ_CTL_REG, MVSW_CMD_F_REPL_RCVD);
+	return err;
+}
+
+static int mvsw_pr_fw_send_req(struct mvsw_pr_device *dev,
+			       u8 *in_msg, size_t in_size, u8 *out_msg,
+			       size_t out_size, unsigned int wait)
+{
+	struct mvsw_pr_fw *fw;
+	ssize_t ret;
+
+	fw = container_of(dev, struct mvsw_pr_fw, dev);
+
+	mutex_lock(&fw->cmd_mtx);
+	ret = mvsw_pr_fw_cmd_send(fw, in_msg, in_size, out_msg, out_size, wait);
+	mutex_unlock(&fw->cmd_mtx);
+
+	return ret;
+}
+
+static int mvsw_pr_fw_init(struct mvsw_pr_fw *fw)
+{
+	u8 __iomem *base;
+	int err;
+	u8 qid;
+
+	err = mvsw_pr_fw_load(fw);
+	if (err && err != -ETIMEDOUT)
+		return err;
+
+	err = mvsw_pr_fw_wait_reg32(fw, MVSW_FW_READY_REG,
+				    MVSW_FW_READY_MAGIC, 20000);
+	if (err) {
+		dev_err(mvsw_fw_dev(fw), "FW is failed to start\n");
+		return err;
+	}
+
+	base = fw->mem_addr;
+
+	fw->cmd_mbox = base + mvsw_fw_read(fw, MVSW_CMD_BUF_OFFS_REG);
+	fw->cmd_mbox_len = mvsw_fw_read(fw, MVSW_CMD_BUF_LEN_REG);
+	mutex_init(&fw->cmd_mtx);
+
+	fw->evt_buf = base + mvsw_fw_read(fw, MVSW_EVT_BUF_OFFS_REG);
+	fw->evt_qnum = mvsw_fw_read(fw, MVSW_EVT_QNUM_REG);
+	fw->evt_msg = kmalloc(MVSW_MSG_MAX_SIZE, GFP_KERNEL);
+	if (!fw->evt_msg)
+		return -ENOMEM;
+
+	for (qid = 0; qid < fw->evt_qnum; qid++) {
+		u32 offs = mvsw_fw_read(fw, MVSW_EVTQ_OFFS_REG(qid));
+		struct mvsw_pr_fw_evtq *evtq = &fw->evt_queue[qid];
+
+		evtq->len = mvsw_fw_read(fw, MVSW_EVTQ_LEN_REG(qid));
+		evtq->addr = fw->evt_buf + offs;
+	}
+
+	return 0;
+}
+
+static void mvsw_pr_fw_uninit(struct mvsw_pr_fw *fw)
+{
+	kfree(fw->evt_msg);
+}
+
+static irqreturn_t mvsw_pci_irq_handler(int irq, void *dev_id)
+{
+	struct mvsw_pr_fw *fw = dev_id;
+
+	queue_work(fw->wq, &fw->evt_work);
+
+	return IRQ_HANDLED;
+}
+
+static int mvsw_pr_ldr_wait_reg32(struct mvsw_pr_fw *fw,
+				  u32 reg, u32 val, unsigned int wait)
+{
+	if (mvsw_wait_timeout(mvsw_ldr_read(fw, reg) == val, wait))
+		return 0;
+
+	return -EBUSY;
+}
+
+static u32 mvsw_pr_ldr_buf_avail(struct mvsw_pr_fw *fw)
+{
+	u32 rd_idx = mvsw_ldr_read(fw, MVSW_LDR_BUF_RD_REG);
+
+	return CIRC_SPACE(fw->ldr_wr_idx, rd_idx, fw->ldr_buf_len);
+}
+
+static int mvsw_pr_ldr_send_buf(struct mvsw_pr_fw *fw, const u8 *buf,
+				size_t len)
+{
+	int i;
+
+	if (!mvsw_wait_timeout(mvsw_pr_ldr_buf_avail(fw) >= len, 100)) {
+		dev_err(mvsw_fw_dev(fw), "failed wait for sending firmware\n");
+		return -EBUSY;
+	}
+
+	for (i = 0; i < len; i += 4) {
+		writel_relaxed(*(u32 *)(buf + i), MVSW_LDR_WR_PTR(fw));
+		MVSW_LDR_WR_IDX_MOVE(fw, 4);
+	}
+
+	MVSW_LDR_WR_IDX_COMMIT(fw);
+	return 0;
+}
+
+static int mvsw_pr_ldr_send(struct mvsw_pr_fw *fw,
+			    const char *img, u32 fw_size)
+{
+	unsigned long mask;
+	u32 status;
+	u32 pos;
+	int err;
+
+	if (mvsw_pr_ldr_wait_reg32(fw, MVSW_LDR_STATUS_REG,
+				   MVSW_LDR_STATUS_IMG_DL, 1000)) {
+		dev_err(mvsw_fw_dev(fw), "Loader is not ready to load image\n");
+		return -EBUSY;
+	}
+
+	for (pos = 0; pos < fw_size; pos += MVSW_FW_BLK_SZ) {
+		if (pos + MVSW_FW_BLK_SZ > fw_size)
+			break;
+
+		err = mvsw_pr_ldr_send_buf(fw, img + pos, MVSW_FW_BLK_SZ);
+		if (err)
+			return err;
+	}
+
+	if (pos < fw_size) {
+		err = mvsw_pr_ldr_send_buf(fw, img + pos, fw_size - pos);
+		if (err)
+			return err;
+	}
+
+	/* Waiting for status IMG_DOWNLOADING to change to something else */
+	mask = ~(MVSW_LDR_STATUS_IMG_DL);
+
+	if (!mvsw_wait_timeout(mvsw_ldr_read(fw, MVSW_LDR_STATUS_REG) & mask,
+			       MVSW_FW_DL_TIMEOUT)) {
+		dev_err(mvsw_fw_dev(fw), "Timeout to load FW img [state=%d]",
+			mvsw_ldr_read(fw, MVSW_LDR_STATUS_REG));
+		return -ETIMEDOUT;
+	}
+
+	status = mvsw_ldr_read(fw, MVSW_LDR_STATUS_REG);
+	if (status != MVSW_LDR_STATUS_START_FW) {
+		switch (status) {
+		case MVSW_LDR_STATUS_INVALID_IMG:
+			dev_err(mvsw_fw_dev(fw), "FW img has bad crc\n");
+			return -EINVAL;
+		case MVSW_LDR_STATUS_NOMEM:
+			dev_err(mvsw_fw_dev(fw), "Loader has no enough mem\n");
+			return -ENOMEM;
+		default:
+			break;
+		}
+	}
+
+	return 0;
+}
+
+static bool mvsw_pr_ldr_is_ready(struct mvsw_pr_fw *fw)
+{
+	return mvsw_ldr_read(fw, MVSW_LDR_READY_REG) == MVSW_LDR_READY_MAGIC;
+}
+
+static void mvsw_pr_fw_rev_parse(const struct mvsw_pr_fw_header *hdr,
+				 struct mvsw_fw_rev *rev)
+{
+	u32 version = be32_to_cpu(hdr->version_value);
+
+	rev->maj = FW_VER_MAJ(version);
+	rev->min = FW_VER_MIN(version);
+	rev->sub = FW_VER_PATCH(version);
+}
+
+static int mvsw_pr_fw_rev_check(struct mvsw_pr_fw *fw)
+{
+	struct mvsw_fw_rev *rev = &fw->dev.fw_rev;
+
+	if (rev->maj == MVSW_SUPP_FW_MAJ_VER &&
+	    rev->min >= MVSW_SUPP_FW_MIN_VER) {
+		return 0;
+	}
+
+	dev_err(mvsw_fw_dev(fw), "Driver supports FW version only '%u.%u.%u'",
+		MVSW_SUPP_FW_MAJ_VER,
+		MVSW_SUPP_FW_MIN_VER,
+		MVSW_SUPP_FW_PATCH_VER);
+
+	return -EINVAL;
+}
+
+static int mvsw_pr_fw_hdr_parse(struct mvsw_pr_fw *fw,
+				const struct firmware *img)
+{
+	struct mvsw_pr_fw_header *hdr = (struct mvsw_pr_fw_header *)img->data;
+	struct mvsw_fw_rev *rev = &fw->dev.fw_rev;
+	u32 magic;
+
+	magic = be32_to_cpu(hdr->magic_number);
+	if (magic != MVSW_FW_HDR_MAGIC) {
+		dev_err(mvsw_fw_dev(fw), "FW img type is invalid");
+		return -EINVAL;
+	}
+
+	mvsw_pr_fw_rev_parse(hdr, rev);
+
+	dev_info(mvsw_fw_dev(fw), "FW version '%u.%u.%u'\n",
+		 rev->maj, rev->min, rev->sub);
+
+	return mvsw_pr_fw_rev_check(fw);
+}
+
+static int mvsw_pr_fw_load(struct mvsw_pr_fw *fw)
+{
+	size_t hlen = sizeof(struct mvsw_pr_fw_header);
+	const struct firmware *f;
+	bool has_ldr;
+	int err;
+
+	has_ldr = mvsw_wait_timeout(mvsw_pr_ldr_is_ready(fw), 1000);
+	if (!has_ldr) {
+		dev_err(mvsw_fw_dev(fw), "waiting for FW loader is timed out");
+		return -ETIMEDOUT;
+	}
+
+	fw->ldr_ring_buf = fw->ldr_regs +
+		mvsw_ldr_read(fw, MVSW_LDR_BUF_OFFS_REG);
+
+	fw->ldr_buf_len =
+		mvsw_ldr_read(fw, MVSW_LDR_BUF_SIZE_REG);
+
+	fw->ldr_wr_idx = 0;
+
+	err = request_firmware_direct(&f, MVSW_FW_FILENAME, &fw->pci_dev->dev);
+	if (err) {
+		dev_err(mvsw_fw_dev(fw), "failed to request firmware file\n");
+		return err;
+	}
+
+	if (!IS_ALIGNED(f->size, 4)) {
+		dev_err(mvsw_fw_dev(fw), "FW image file is not aligned");
+		release_firmware(f);
+		return -EINVAL;
+	}
+
+	err = mvsw_pr_fw_hdr_parse(fw, f);
+	if (err) {
+		dev_err(mvsw_fw_dev(fw), "FW image header is invalid\n");
+		release_firmware(f);
+		return err;
+	}
+
+	mvsw_ldr_write(fw, MVSW_LDR_IMG_SIZE_REG, f->size - hlen);
+	mvsw_ldr_write(fw, MVSW_LDR_CTL_REG, MVSW_LDR_CTL_DL_START);
+
+	dev_info(mvsw_fw_dev(fw), "Loading prestera FW image ...");
+
+	err = mvsw_pr_ldr_send(fw, f->data + hlen, f->size - hlen);
+
+	release_firmware(f);
+	return err;
+}
+
+static int mvsw_pr_pci_probe(struct pci_dev *pdev,
+			     const struct pci_device_id *id)
+{
+	const char *driver_name = pdev->driver->name;
+	u8 __iomem *mem_addr, *pp_addr;
+	struct mvsw_pr_fw *fw;
+	int err;
+
+	err = pci_enable_device(pdev);
+	if (err) {
+		dev_err(&pdev->dev, "pci_enable_device failed\n");
+		goto err_pci_enable_device;
+	}
+
+	err = pci_request_regions(pdev, driver_name);
+	if (err) {
+		dev_err(&pdev->dev, "pci_request_regions failed\n");
+		goto err_pci_request_regions;
+	}
+
+	if (dma_set_mask_and_coherent(&pdev->dev, DMA_BIT_MASK(30))) {
+		dev_err(&pdev->dev, "fail to set DMA mask\n");
+		goto err_dma_mask;
+	}
+
+	mem_addr = pci_ioremap_bar(pdev, 2);
+	if (!mem_addr) {
+		dev_err(&pdev->dev, "pci mem ioremap failed\n");
+		err = -EIO;
+		goto err_mem_ioremap;
+	}
+
+	pp_addr = ioremap(pci_resource_start(pdev, 4),
+			  pci_resource_len(pdev, 4));
+	if (!pp_addr) {
+		dev_err(&pdev->dev, "pp regs ioremap failed\n");
+		err = -EIO;
+		goto err_pp_ioremap;
+	}
+
+	pci_set_master(pdev);
+
+	fw = kzalloc(sizeof(*fw), GFP_KERNEL);
+	if (!fw) {
+		err = -ENOMEM;
+		goto err_pci_dev_alloc;
+	}
+
+	fw->pci_dev = pdev;
+	fw->dev.dev = &pdev->dev;
+	fw->dev.send_req = mvsw_pr_fw_send_req;
+	fw->dev.pp_regs = pp_addr;
+	fw->mem_addr = mem_addr;
+	fw->ldr_regs = mem_addr;
+	fw->hw_regs = mem_addr;
+
+	fw->wq = alloc_workqueue("mvsw_fw_wq", WQ_HIGHPRI, 1);
+	if (!fw->wq)
+		goto err_wq_alloc;
+
+	INIT_WORK(&fw->evt_work, mvsw_pr_fw_evt_work_fn);
+
+	err = pci_alloc_irq_vectors(pdev, 1, 1, PCI_IRQ_MSI);
+	if (err < 0) {
+		dev_err(&pdev->dev, "MSI IRQ init failed\n");
+		goto err_irq_alloc;
+	}
+
+	err = request_irq(pci_irq_vector(pdev, 0), mvsw_pci_irq_handler,
+			  0, driver_name, fw);
+	if (err) {
+		dev_err(&pdev->dev, "fail to request IRQ\n");
+		goto err_request_irq;
+	}
+
+	pci_set_drvdata(pdev, fw);
+
+	err = mvsw_pr_fw_init(fw);
+	if (err)
+		goto err_mvsw_fw_init;
+
+	dev_info(mvsw_fw_dev(fw), "Prestera Switch FW is ready\n");
+
+	err = mvsw_pr_device_register(&fw->dev);
+	if (err)
+		goto err_mvsw_dev_register;
+
+	return 0;
+
+err_mvsw_dev_register:
+	mvsw_pr_fw_uninit(fw);
+err_mvsw_fw_init:
+	free_irq(pci_irq_vector(pdev, 0), fw);
+err_request_irq:
+	pci_free_irq_vectors(pdev);
+err_irq_alloc:
+	destroy_workqueue(fw->wq);
+err_wq_alloc:
+	kfree(fw);
+err_pci_dev_alloc:
+	iounmap(pp_addr);
+err_pp_ioremap:
+	iounmap(mem_addr);
+err_mem_ioremap:
+err_dma_mask:
+	pci_release_regions(pdev);
+err_pci_request_regions:
+	pci_disable_device(pdev);
+err_pci_enable_device:
+	return err;
+}
+
+static void mvsw_pr_pci_remove(struct pci_dev *pdev)
+{
+	struct mvsw_pr_fw *fw = pci_get_drvdata(pdev);
+
+	free_irq(pci_irq_vector(pdev, 0), fw);
+	pci_free_irq_vectors(pdev);
+	mvsw_pr_device_unregister(&fw->dev);
+	flush_workqueue(fw->wq);
+	destroy_workqueue(fw->wq);
+	mvsw_pr_fw_uninit(fw);
+	iounmap(fw->dev.pp_regs);
+	iounmap(fw->mem_addr);
+	pci_release_regions(pdev);
+	pci_disable_device(pdev);
+	kfree(fw);
+}
+
+static int __init mvsw_pr_pci_init(void)
+{
+	struct mvsw_pr_pci_match *match;
+	int err;
+
+	for (match = mvsw_pci_devices; match->driver.name; match++) {
+		match->driver.probe = mvsw_pr_pci_probe;
+		match->driver.remove = mvsw_pr_pci_remove;
+		match->driver.id_table = &match->id;
+
+		err = pci_register_driver(&match->driver);
+		if (err) {
+			pr_err("prestera_pci: failed to register %s\n",
+			       match->driver.name);
+			break;
+		}
+
+		match->registered = true;
+	}
+
+	if (err) {
+		for (match = mvsw_pci_devices; match->driver.name; match++) {
+			if (!match->registered)
+				break;
+
+			pci_unregister_driver(&match->driver);
+		}
+
+		return err;
+	}
+
+	pr_info("prestera_pci: Registered Marvell Prestera PCI driver\n");
+	return 0;
+}
+
+static void __exit mvsw_pr_pci_exit(void)
+{
+	struct mvsw_pr_pci_match *match;
+
+	for (match = mvsw_pci_devices; match->driver.name; match++) {
+		if (!match->registered)
+			break;
+
+		pci_unregister_driver(&match->driver);
+	}
+
+	pr_info("prestera_pci: Unregistered Marvell Prestera PCI driver\n");
+}
+
+module_init(mvsw_pr_pci_init);
+module_exit(mvsw_pr_pci_exit);
+
+MODULE_AUTHOR("Marvell Semi.");
+MODULE_LICENSE("GPL");
+MODULE_DESCRIPTION("Marvell Prestera switch PCI interface");
diff --git a/drivers/net/ethernet/marvell/prestera_sw/prestera_router.c b/drivers/net/ethernet/marvell/prestera_sw/prestera_router.c
new file mode 100644
index 0000000..574e11b
--- /dev/null
+++ b/drivers/net/ethernet/marvell/prestera_sw/prestera_router.c
@@ -0,0 +1,1956 @@
+// SPDX-License-Identifier: BSD-3-Clause OR GPL-2.0
+/*
+ * Copyright (c) 2019-2020 Marvell International Ltd. All rights reserved.
+ *
+ */
+
+#include <linux/kernel.h>
+#include <linux/types.h>
+#include <linux/notifier.h>
+#include <linux/inetdevice.h>
+#include <linux/netdevice.h>
+#include <linux/if_bridge.h>
+#include <net/netevent.h>
+#include <net/neighbour.h>
+#include <net/addrconf.h>
+#include <net/fib_notifier.h>
+#include <net/switchdev.h>
+#include <net/arp.h>
+#include <net/nexthop.h>
+
+#include "prestera.h"
+#include "prestera_hw.h"
+#include "prestera_log.h"
+
+#define MVSW_PR_UNRESOLVED_NH_PROBE_INTERVAL 2000 /* ms */
+
+static const char mvsw_driver_name[] = "mrvl_switchdev";
+
+struct mvsw_pr_router {
+	struct mvsw_pr_switch *sw;
+	struct list_head rif_list;	/* list of mvsw_pr_rif */
+	struct list_head vr_list;	/* list of mvsw_pr_vr */
+	struct list_head nexthop_list;
+	struct list_head nexthop_neighs_list;
+	struct {
+		struct delayed_work dw;
+		unsigned int interval;	/* ms */
+	} neighs_update;
+	struct notifier_block netevent_nb;
+	struct notifier_block inetaddr_nb;
+	struct notifier_block fib_nb;
+	bool aborted;
+};
+
+struct mvsw_pr_rif {
+	struct net_device *dev;
+	struct list_head router_node;
+	enum mvsw_pr_rif_type type;
+	unsigned char addr[ETH_ALEN];
+	unsigned int mtu;
+	u16 rif_id;
+	u16 vr_id;
+	struct mvsw_pr_switch *sw;
+
+};
+
+struct mvsw_pr_rif_params {
+	struct net_device *dev;
+	u16 vid;
+};
+
+struct mvsw_pr_fib_node {
+	struct mvsw_pr_fib4_entry *fib4_entry;
+	struct list_head fib_node;
+	struct mvsw_pr_vr *vr;
+	u32 dst;
+	int dst_len;
+};
+
+struct mvsw_pr_fib4_entry {
+	struct mvsw_pr_fib_node *fib_node;
+	struct net_device *nh_dev;
+	struct fib_info *fi;
+	u32 tb_id;
+	u8 type;
+};
+
+struct mvsw_pr_vr {
+	u16 id;				/* virtual router ID */
+	u32 tb_id;			/* kernel fib table id */
+	struct list_head router_node;
+	struct list_head fib_list;	/* list of mvsw_pr_fib_node */
+	unsigned int ref_cnt;
+};
+
+struct mvsw_pr_nh {
+	struct list_head nh_node;
+	struct neighbour *n;
+	bool connected, has_gw;
+	u8 ha[ETH_ALEN];
+	__be32 gw_ip, n_ip;
+	u32 dst_len, hw_id;
+	u16 vr_id;
+	int ref_cnt;
+};
+
+static struct workqueue_struct *mvsw_r_wq;
+static struct workqueue_struct *mvsw_r_owq;
+
+static const unsigned char mvsw_pr_mac_mask[ETH_ALEN] = {
+	0xff, 0xff, 0xff, 0xff, 0xfc, 0x00
+};
+
+static struct mvsw_pr_vr *mvsw_pr_vr_get(struct mvsw_pr_switch *sw, u32 tb_id,
+					 struct netlink_ext_ack *extack);
+static u16 mvsw_pr_nh_dev_to_vr_id(struct mvsw_pr_switch *sw,
+				   struct net_device *dev);
+static u32 mvsw_pr_fix_tb_id(u32 tb_id);
+static void mvsw_pr_vr_put(struct mvsw_pr_switch *sw, struct mvsw_pr_vr *vr);
+static struct mvsw_pr_vr *mvsw_pr_vr_find(struct mvsw_pr_switch *sw, u32 tb_id);
+static struct mvsw_pr_vr *mvsw_pr_vr_find_by_id(struct mvsw_pr_switch *sw,
+						u16 vr_id);
+static void mvsw_pr_router_fib_abort(struct mvsw_pr_switch *sw);
+static struct mvsw_pr_rif *mvsw_pr_rif_create(struct mvsw_pr_switch *sw,
+					      const struct mvsw_pr_rif_params
+					      *params,
+					      struct netlink_ext_ack *extack);
+static void mvsw_pr_rif_destroy(struct mvsw_pr_rif *rif);
+static int mvsw_pr_rif_edit(struct mvsw_pr_switch *sw, u16 *rif_id, char *mac,
+			    int mtu);
+
+static enum mvsw_pr_rif_type mvsw_pr_dev_rif_type(const struct net_device *dev)
+{
+	enum mvsw_pr_rif_type type;
+
+	if (is_vlan_dev(dev) && netif_is_bridge_master(vlan_dev_real_dev(dev)))
+		type = MVSW_PR_RIF_TYPE_VLAN;
+	else if (netif_is_bridge_master(dev) && br_vlan_enabled(dev))
+		type = MVSW_PR_RIF_TYPE_VLAN;
+	else if (netif_is_bridge_master(dev))
+		type = MVSW_PR_RIF_TYPE_BRIDGE;
+	else
+		type = MVSW_PR_RIF_TYPE_PORT;
+
+	return type;
+}
+
+static struct mvsw_pr_nh*
+mvsw_pr_nh_neigh_find(const struct mvsw_pr_switch *sw,
+		      __be32 addr)
+{
+	struct mvsw_pr_nh *neigh;
+
+	list_for_each_entry(neigh, &sw->router->nexthop_list, nh_node) {
+		if (neigh->has_gw && neigh->gw_ip == addr)
+			return neigh;
+	}
+
+	return NULL;
+}
+
+static struct mvsw_pr_nh *mvsw_pr_nh_find(const struct mvsw_pr_switch *sw,
+					  const struct neighbour *n)
+{
+	struct mvsw_pr_nh *neigh;
+
+	list_for_each_entry(neigh, &sw->router->nexthop_list, nh_node) {
+		if (neigh->n == n && !neigh->has_gw)
+			return neigh;
+	}
+
+	return NULL;
+}
+
+static struct mvsw_pr_fib_node *
+mvsw_pr_fib_node_create(struct mvsw_pr_vr *vr, u32 dst,
+			size_t dst_len)
+{
+	struct mvsw_pr_fib_node *fib_node;
+
+	fib_node = kzalloc(sizeof(*fib_node), GFP_KERNEL);
+	if (!fib_node)
+		return NULL;
+
+	fib_node->vr = vr;
+	fib_node->dst = dst;
+	fib_node->dst_len = dst_len;
+
+	vr->ref_cnt++;
+	list_add(&fib_node->fib_node, &vr->fib_list);
+
+	return fib_node;
+}
+
+static void mvsw_pr_fib_node_destroy(struct mvsw_pr_fib_node *fib_node)
+{
+	list_del(&fib_node->fib_node);
+	kfree(fib_node);
+}
+
+static void mvsw_pr_fib_node_put(struct mvsw_pr_switch *sw,
+				 struct mvsw_pr_fib_node *fib_node)
+{
+	struct mvsw_pr_vr *vr = fib_node->vr;
+
+	if (fib_node->fib4_entry)
+		return;
+
+	mvsw_pr_fib_node_destroy(fib_node);
+	mvsw_pr_vr_put(sw, vr);
+}
+
+static struct mvsw_pr_fib_node *
+mvsw_pr_fib_node_lookup(struct mvsw_pr_vr *vr, u32 dst, size_t dst_len)
+{
+	struct mvsw_pr_fib_node *fib_node;
+
+	list_for_each_entry(fib_node, &vr->fib_list, fib_node) {
+		if (fib_node->dst_len == dst_len &&
+		    fib_node->dst == dst)
+			return fib_node;
+	}
+
+	return NULL;
+}
+
+static struct mvsw_pr_fib_node *
+mvsw_pr_fib_node_get(struct mvsw_pr_switch *sw, u32 tb_id, u32 dst,
+		     size_t dst_len)
+{
+	struct mvsw_pr_fib_node *fib_node;
+	struct mvsw_pr_vr *vr;
+	int err;
+
+	vr = mvsw_pr_vr_get(sw, tb_id ? : RT_TABLE_MAIN, NULL);
+	if (IS_ERR(vr))
+		return ERR_CAST(vr);
+
+	fib_node = mvsw_pr_fib_node_lookup(vr, dst, dst_len);
+	if (fib_node)
+		return fib_node;
+
+	fib_node = mvsw_pr_fib_node_create(vr, dst, dst_len);
+	if (!fib_node) {
+		err = -ENOMEM;
+		goto err_fib_node_create;
+	}
+
+	return fib_node;
+
+err_fib_node_create:
+	mvsw_pr_vr_put(sw, vr);
+	return ERR_PTR(err);
+}
+
+static struct mvsw_pr_nh
+*mvsw_pr_nh_neigh_alloc(struct mvsw_pr_switch *sw,
+				 struct neighbour *n,
+				 __be32 dst,
+				 u32 dst_len)
+{
+	struct mvsw_pr_nh *nh;
+
+	nh = kzalloc(sizeof(*nh), GFP_KERNEL);
+	if (!nh)
+		return NULL;
+
+	nh->has_gw = true;
+	nh->n = n;
+	nh->gw_ip = dst;
+	nh->dst_len = dst_len;
+	nh->vr_id = mvsw_pr_nh_dev_to_vr_id(sw, n->dev);
+	nh->ref_cnt = 1;
+
+	list_add_tail(&nh->nh_node, &sw->router->nexthop_list);
+	return nh;
+}
+
+static struct mvsw_pr_nh *mvsw_pr_nh_alloc(struct mvsw_pr_switch *sw,
+					   struct neighbour *n)
+{
+	struct mvsw_pr_nh *nh;
+
+	nh = kzalloc(sizeof(*nh), GFP_KERNEL);
+	if (!nh)
+		return NULL;
+
+	nh->n = n;
+	nh->dst_len = 32;
+
+	list_add(&nh->nh_node, &sw->router->nexthop_list);
+	return nh;
+}
+
+static struct mvsw_pr_nh *mvsw_pr_nh_get(struct mvsw_pr_switch *sw,
+					 struct neighbour *n)
+{
+	struct mvsw_pr_nh *nh;
+
+	nh = mvsw_pr_nh_find(sw, n);
+	if (nh)
+		return nh;
+
+	return mvsw_pr_nh_alloc(sw, n);
+}
+
+static void mvsw_pr_nh_release(struct mvsw_pr_switch *sw,
+			       struct mvsw_pr_nh *nh)
+{
+	__be32 addr = nh->has_gw ? nh->gw_ip : nh->n_ip;
+
+	mvsw_pr_nh_entry_delete(sw,
+				nh->vr_id, addr,
+				nh->dst_len, nh->ha);
+}
+
+static void mvsw_pr_nh_put(struct mvsw_pr_nh *nh)
+{
+	if (!nh->ref_cnt) {
+		list_del(&nh->nh_node);
+		kfree(nh);
+	}
+}
+
+static void mvsw_pr_nh_destroy(struct mvsw_pr_nh *nh)
+{
+	nh->ref_cnt--;
+	mvsw_pr_nh_put(nh);
+}
+
+struct mvsw_net_event_work {
+	struct work_struct work;
+	struct mvsw_pr_router *router;
+	struct neighbour *n;
+	u16 vr_id;
+};
+
+static void mvsw_pr_router_update_nh(struct mvsw_pr_router *router)
+{
+	struct mvsw_pr_nh *nh;
+	u8 is_active;
+	int err;
+
+	rtnl_lock();
+	list_for_each_entry(nh, &router->nexthop_list, nh_node) {
+		/* Keep linux aware of an active neighbours */
+		if (nh->n_ip) {
+			err = mvsw_pr_hw_nh_get(router->sw, nh->hw_id,
+						&is_active);
+			if (err) {
+				pr_err("Failed to get neighbour entry state");
+				continue;
+			}
+
+			if (is_active)
+				neigh_event_send(nh->n, NULL);
+		}
+	}
+	rtnl_unlock();
+}
+
+static u16 mvsw_pr_nh_dev_to_vr_id(struct mvsw_pr_switch *sw,
+				   struct net_device *dev)
+{
+	struct mvsw_pr_vr *vr;
+	u16 tb_id, vr_id = 0;
+
+	tb_id = l3mdev_fib_table(dev);
+	tb_id = mvsw_pr_fix_tb_id(tb_id ? : RT_TABLE_MAIN);
+
+	vr = mvsw_pr_vr_find(sw, tb_id);
+	if (vr)
+		vr_id = vr->id;
+
+	return vr_id;
+}
+
+static u16 mvsw_pr_nh_dev_to_vid(struct mvsw_pr_switch *sw,
+				 struct net_device *dev)
+{
+	u16 vid = 0;
+
+	if (is_vlan_dev(dev) && netif_is_bridge_master(vlan_dev_real_dev(dev)))
+		vid = vlan_dev_vlan_id(dev);
+	else if (netif_is_bridge_master(dev) && br_vlan_enabled(dev))
+		br_vlan_get_pvid(dev, &vid);
+	else if (netif_is_bridge_master(dev))
+		vid = mvsw_pr_vlan_dev_vlan_id(sw->bridge, dev);
+
+	return vid;
+}
+
+static struct mvsw_pr_port *
+mvsw_pr_nh_dev_to_port(struct mvsw_pr_switch *sw, struct net_device *dev,
+		       u8 *ha)
+{
+	struct net_device *bridge_dev, *port_dev = dev;
+	u16 vid = mvsw_pr_nh_dev_to_vid(sw, dev);
+
+	if (is_vlan_dev(dev) &&
+	    netif_is_bridge_master(vlan_dev_real_dev(dev))) {
+		bridge_dev = vlan_dev_priv(dev)->real_dev;
+		port_dev = br_fdb_find_port(bridge_dev, ha,
+					    vid);
+	} else if (netif_is_bridge_master(dev) && br_vlan_enabled(dev)) {
+		port_dev = br_fdb_find_port(dev, ha, vid);
+	} else if (netif_is_bridge_master(dev)) {
+		/* vid in .1d bridge is 0 */
+		port_dev = br_fdb_find_port(dev, ha, 0);
+	}
+
+	if (!port_dev)
+		return NULL;
+
+	return netdev_priv(port_dev);
+}
+
+static int mvsw_pr_router_gw_nh_refresh(struct mvsw_pr_switch *sw,
+					struct mvsw_pr_nh *nh)
+{
+	struct mvsw_pr_port *port;
+	struct neighbour *n = nh->n;
+	int hw_id, err = 0;
+	u8 mac[ETH_ALEN];
+	u16 vid;
+
+	read_lock_bh(&n->lock);
+	memcpy(mac, n->ha, ETH_ALEN);
+	read_unlock_bh(&n->lock);
+
+	port = mvsw_pr_nh_dev_to_port(sw, nh->n->dev, nh->ha);
+	vid = mvsw_pr_nh_dev_to_vid(sw, n->dev);
+
+	err = mvsw_pr_nh_entry_set(port, nh->vr_id, vid,
+				   nh->gw_ip, nh->dst_len,
+				   mac, &hw_id);
+	if (err)
+		return err;
+
+	nh->hw_id = hw_id;
+	return err;
+}
+
+static int mvsw_pr_router_nhs_refresh(struct mvsw_pr_router *router,
+				      struct mvsw_pr_port *port,
+				      struct neighbour *n)
+{
+	struct mvsw_pr_nh *nh;
+	u8 mac[ETH_ALEN];
+	__be32 addr;
+	u16 vid;
+	int hw_id, err = 0;
+
+	read_lock_bh(&n->lock);
+	memcpy(mac, n->ha, ETH_ALEN);
+	read_unlock_bh(&n->lock);
+
+	list_for_each_entry(nh, &router->nexthop_list, nh_node) {
+		if (nh->n == n) {
+			addr = nh->has_gw ? nh->gw_ip : nh->n_ip;
+			vid = mvsw_pr_nh_dev_to_vid(router->sw, n->dev);
+
+			err = mvsw_pr_nh_entry_set(port, nh->vr_id, vid,
+						   addr, nh->dst_len,
+						   mac, &hw_id);
+			if (err)
+				return err;
+
+			nh->hw_id = hw_id;
+		}
+	}
+
+	n->flags |= NTF_OFFLOADED;
+	return err;
+}
+
+static void
+mvsw_pr_router_nhs_release(struct mvsw_pr_router *router,
+			   struct neighbour *n)
+{
+	struct mvsw_pr_nh *nh, *tmp;
+
+	list_for_each_entry_safe(nh, tmp, &router->nexthop_list, nh_node) {
+		if (nh->n == n) {
+			mvsw_pr_nh_release(router->sw, nh);
+			if (!nh->has_gw)
+				mvsw_pr_nh_destroy(nh);
+		}
+	}
+	n->flags &= ~NTF_OFFLOADED;
+}
+
+static bool
+mvsw_pr_router_is_defalt_gw_prefix(__be32 dst, u32 dst_len)
+{
+	/* default route has zero dst/dst_len to match any route */
+	return !dst && !dst_len;
+}
+
+static bool
+mvsw_pr_router_is_nh_default_gw(struct mvsw_pr_nh *nh)
+{
+	return mvsw_pr_router_is_defalt_gw_prefix(nh->gw_ip, nh->dst_len);
+}
+
+static bool
+mvsw_pr_router_nh_match_default_gw(struct mvsw_pr_nh *nh, u16 vr_id)
+{
+	return nh->has_gw &&
+		mvsw_pr_router_is_nh_default_gw(nh) &&
+		(nh->vr_id == vr_id);
+}
+
+static void
+mvsw_pr_router_release_default_gw(struct mvsw_pr_switch *sw, u16 vr_id)
+{
+	struct mvsw_pr_nh *nh, *tmp;
+
+	list_for_each_entry_safe(nh, tmp, &sw->router->nexthop_list, nh_node) {
+		if (mvsw_pr_router_nh_match_default_gw(nh, vr_id)) {
+			mvsw_pr_nh_entry_delete(sw, nh->vr_id,
+						nh->gw_ip, nh->dst_len, nh->ha);
+			mvsw_pr_nh_destroy(nh);
+			break;
+		}
+	}
+}
+
+static void
+mvsw_pr_router_update_resolved_nh(struct mvsw_pr_router *router,
+				  struct mvsw_pr_nh *nh, bool is_connected)
+{
+	struct mvsw_pr_port *port;
+	int err;
+
+	if (nh && !is_connected) {
+		mvsw_pr_router_nhs_release(router, nh->n);
+	} else if (nh) {
+		read_lock_bh(&nh->n->lock);
+		memcpy(nh->ha, nh->n->ha, ETH_ALEN);
+		read_unlock_bh(&nh->n->lock);
+
+		port = mvsw_pr_nh_dev_to_port(router->sw, nh->n->dev, nh->ha);
+		if (!port)
+			return;
+
+		err = mvsw_pr_router_nhs_refresh(router, port, nh->n);
+		if (err)
+			pr_err("Failed to update neighbours");
+
+		nh->connected = is_connected;
+	}
+}
+
+static struct mvsw_pr_nh*
+mvsw_pr_nh_offload(struct mvsw_pr_switch *sw, struct neighbour *n)
+{
+	struct mvsw_pr_nh *nh;
+	bool nh_connected;
+	u8 ha[ETH_ALEN];
+	int hw_id, err;
+	__be32 ip_addr;
+	u16 vr_id;
+
+	read_lock_bh(&n->lock);
+	memcpy(ha, n->ha, ETH_ALEN);
+	nh_connected = (n->nud_state & NUD_VALID && !n->dead);
+	ip_addr = *(__be32 *)n->primary_key;
+	read_unlock_bh(&n->lock);
+
+	nh = mvsw_pr_nh_get(sw, n);
+	if (!nh) {
+		MVSW_LOG_ERROR("Failed to allocate new_nh");
+		err = -ENOMEM;
+		goto err_nh_alloc;
+	}
+
+	vr_id = mvsw_pr_nh_dev_to_vr_id(sw, n->dev);
+	err = mvsw_pr_hw_nh_entry_add(sw, vr_id,
+				      mvsw_pr_nh_dev_to_vid(sw, n->dev),
+				      ip_addr, ha, &hw_id);
+	if (err) {
+		MVSW_LOG_INFO
+			("failed to offload neigh entry (err %d)\n", err);
+		goto err_nh_offload;
+	}
+
+	memcpy(nh->ha, ha, ETH_ALEN);
+	nh->connected = nh_connected;
+	nh->n_ip = ip_addr;
+	nh->hw_id = hw_id;
+	nh->vr_id = vr_id;
+	nh->ref_cnt++;
+
+	return nh;
+err_nh_offload:
+	list_del(&nh->nh_node);
+	kfree(nh);
+err_nh_alloc:
+	return ERR_PTR(err);
+}
+
+static struct mvsw_pr_nh*
+mvsw_pr_enque_nh_resolving(struct mvsw_pr_switch *sw, struct neighbour *n)
+{
+	struct mvsw_pr_nh *nh;
+
+	nh = mvsw_pr_nh_find(sw, n);
+	if (nh)
+		return nh;
+
+	return mvsw_pr_nh_offload(sw, n);
+}
+
+static void mvsw_pr_router_neigh_event_work(struct work_struct *work)
+{
+	struct mvsw_net_event_work *net_work =
+		container_of(work, struct mvsw_net_event_work, work);
+	struct mvsw_pr_switch *sw = net_work->router->sw;
+	struct neighbour *n = net_work->n;
+	struct mvsw_pr_nh *nh;
+	bool nh_connected;
+
+	rtnl_lock();
+	read_lock_bh(&n->lock);
+	nh_connected = (n->nud_state & NUD_VALID && !n->dead);
+	read_unlock_bh(&n->lock);
+
+	nh = mvsw_pr_nh_find(sw, n);
+	if (!nh  && !nh_connected)
+		goto out;
+
+	if (!nh) {
+		if (!mvsw_pr_port_dev_lower_find(n->dev))
+			goto out;
+
+		nh = mvsw_pr_nh_offload(sw, n);
+		if (IS_ERR(nh)) {
+			pr_err("Failed to queue nh for offloading (%lu)",
+			       PTR_ERR(nh));
+			goto out;
+		}
+	}
+
+	mvsw_pr_router_update_resolved_nh(sw->router, nh, nh_connected);
+
+out:
+	neigh_release(n);
+	rtnl_unlock();
+	kfree(net_work);
+}
+
+static int mvsw_pr_router_netevent_event(struct notifier_block *nb,
+					 unsigned long event, void *ptr)
+{
+	struct mvsw_net_event_work *net_work;
+	struct mvsw_pr_router *router;
+	struct neighbour *n = ptr;
+
+	router = container_of(nb, struct mvsw_pr_router, netevent_nb);
+	if (router->aborted)
+		return NOTIFY_DONE;
+
+	MVSW_LOG_INFO("Processing fib netevent event [event=%s]",
+		      ENUM_TO_NAME(netevent_notif_type, event));
+
+	switch (event) {
+	case NETEVENT_NEIGH_UPDATE:
+		if (n->tbl != &arp_tbl)
+			return NOTIFY_DONE;
+
+		net_work = kzalloc(sizeof(*net_work), GFP_ATOMIC);
+		if (WARN_ON(!net_work))
+			return NOTIFY_BAD;
+
+		neigh_clone(n);
+		net_work->n = n;
+		net_work->router = router;
+		INIT_WORK(&net_work->work, mvsw_pr_router_neigh_event_work);
+		queue_work(mvsw_r_owq, &net_work->work);
+
+		break;
+	case NETEVENT_DELAY_PROBE_TIME_UPDATE:
+	case NETEVENT_IPV4_MPATH_HASH_UPDATE:
+	case NETEVENT_IPV6_MPATH_HASH_UPDATE:
+	case NETEVENT_IPV4_FWD_UPDATE_PRIORITY_UPDATE:
+		break;
+	}
+
+	return NOTIFY_DONE;
+}
+
+static void
+mvsw_pr_router_neighs_update_interval_init(struct mvsw_pr_router *router)
+{
+	unsigned long interval;
+
+	interval = NEIGH_VAR(&arp_tbl.parms, DELAY_PROBE_TIME);
+	router->neighs_update.interval = jiffies_to_msecs(interval);
+}
+
+static void mvsw_pr_router_update_nh_work(struct work_struct *work)
+{
+	struct mvsw_pr_router *router;
+
+	router = container_of(work, struct mvsw_pr_router,
+			      neighs_update.dw.work);
+	mvsw_pr_router_update_nh(router);
+
+	mvsw_pr_router_neighs_update_interval_init(router);
+	queue_delayed_work(mvsw_r_wq, &router->neighs_update.dw,
+			   msecs_to_jiffies(router->neighs_update.interval));
+}
+
+static int mvsw_pr_neigh_init(struct mvsw_pr_switch *sw)
+{
+	mvsw_pr_router_neighs_update_interval_init(sw->router);
+
+	INIT_DELAYED_WORK(&sw->router->neighs_update.dw,
+			  mvsw_pr_router_update_nh_work);
+	queue_delayed_work(mvsw_r_wq, &sw->router->neighs_update.dw, 0);
+	return 0;
+}
+
+static void mvsw_pr_neigh_fini(struct mvsw_pr_switch *sw)
+{
+	cancel_delayed_work_sync(&sw->router->neighs_update.dw);
+}
+
+static struct mvsw_pr_rif *mvsw_pr_rif_find_by_dev(const struct mvsw_pr_switch
+						   *sw,
+						   const struct net_device *dev)
+{
+	struct mvsw_pr_rif *rif;
+
+	list_for_each_entry(rif, &sw->router->rif_list, router_node) {
+		if (rif->dev == dev)
+			return rif;
+	}
+
+	return NULL;
+}
+
+static int
+mvsw_pr_port_vlan_router_join(struct mvsw_pr_port_vlan *mvsw_pr_port_vlan,
+			      struct net_device *dev,
+			      struct netlink_ext_ack *extack)
+{
+	struct mvsw_pr_port *mvsw_pr_port = mvsw_pr_port_vlan->mvsw_pr_port;
+	struct mvsw_pr_switch *sw = mvsw_pr_port->sw;
+
+	struct mvsw_pr_rif_params params = {
+		.dev = dev,
+	};
+	struct mvsw_pr_rif *rif;
+
+	MVSW_LOG_ERROR("NOT IMPLEMENTED!!!");
+
+	rif = mvsw_pr_rif_find_by_dev(sw, dev);
+	if (!rif)
+		rif = mvsw_pr_rif_create(sw, &params, extack);
+
+	if (IS_ERR(rif))
+		return PTR_ERR(rif);
+
+	/* TODO:
+	 * - vid learning set (false)
+	 * - stp state set (FORWARDING)
+	 */
+
+	return 0;
+}
+
+static void
+mvsw_pr_port_vlan_router_leave(struct mvsw_pr_port_vlan *mvsw_pr_port_vlan)
+{
+	MVSW_LOG_ERROR("NOT IMPLEMENTED!!!");
+
+	/* TODO:
+	 * - stp state set (BLOCKING)
+	 * - vid learning set (true)
+	 */
+}
+
+static int
+mvsw_pr_port_router_join(struct mvsw_pr_port *mvsw_pr_port,
+			 struct net_device *dev,
+			 struct netlink_ext_ack *extack)
+{
+	struct mvsw_pr_switch *sw = mvsw_pr_port->sw;
+
+	struct mvsw_pr_rif_params params = {
+		.dev = dev,
+	};
+	struct mvsw_pr_rif *rif;
+
+	rif = mvsw_pr_rif_find_by_dev(sw, dev);
+	if (!rif)
+		rif = mvsw_pr_rif_create(sw, &params, extack);
+
+	if (IS_ERR(rif))
+		return PTR_ERR(rif);
+
+	/* TODO:
+	 * - vid learning set (false)
+	 * - stp state set (FORWARDING)
+	 */
+
+	return 0;
+}
+
+void mvsw_pr_port_router_leave(struct mvsw_pr_port *mvsw_pr_port)
+{
+	struct mvsw_pr_rif *rif;
+
+	/* TODO:
+	 * - stp state set (BLOCKING)
+	 * - vid learning set (true)
+	 */
+
+	rif = mvsw_pr_rif_find_by_dev(mvsw_pr_port->sw, mvsw_pr_port->net_dev);
+	if (rif)
+		mvsw_pr_rif_destroy(rif);
+}
+
+static int mvsw_pr_router_port_check_rif_addr(struct mvsw_pr_switch *sw,
+					      struct net_device *dev,
+					      const unsigned char *dev_addr,
+					      struct netlink_ext_ack *extack)
+{
+	struct mvsw_pr_rif *rif;
+
+	list_for_each_entry(rif, &sw->router->rif_list, router_node) {
+		if (rif->dev && rif->dev != dev &&
+		    !ether_addr_equal_masked(rif->dev->dev_addr, dev_addr,
+					     mvsw_pr_mac_mask)) {
+			NL_SET_ERR_MSG_MOD(extack,
+					   "RIF MAC must have the same prefix");
+			return -EINVAL;
+		}
+	}
+
+	return 0;
+}
+
+static int mvsw_pr_inetaddr_port_event(struct net_device *port_dev,
+				       unsigned long event,
+				       struct netlink_ext_ack *extack)
+{
+	struct mvsw_pr_port *mvsw_pr_port = netdev_priv(port_dev);
+
+	MVSW_LOG_ERROR("dev=%s", port_dev->name);
+
+	if (netif_is_bridge_port(port_dev) ||
+	    netif_is_lag_port(port_dev) || netif_is_ovs_port(port_dev))
+		return 0;
+
+	switch (event) {
+	case NETDEV_UP:
+		return mvsw_pr_port_router_join(mvsw_pr_port, port_dev, extack);
+	case NETDEV_DOWN:
+		mvsw_pr_port_router_leave(mvsw_pr_port);
+		break;
+	}
+
+	return 0;
+}
+
+static int mvsw_pr_inetaddr_bridge_event(struct mvsw_pr_switch *sw,
+					 struct net_device *dev,
+					 unsigned long event,
+					 struct netlink_ext_ack *extack)
+{
+	struct mvsw_pr_rif_params params = {
+		.dev = dev,
+	};
+	struct mvsw_pr_rif *rif;
+
+	switch (event) {
+	case NETDEV_UP:
+		rif = mvsw_pr_rif_create(sw, &params, extack);
+		if (IS_ERR(rif))
+			return PTR_ERR(rif);
+		break;
+	case NETDEV_DOWN:
+		rif = mvsw_pr_rif_find_by_dev(sw, dev);
+		mvsw_pr_rif_destroy(rif);
+		break;
+	}
+
+	return 0;
+}
+
+static int mvsw_pr_inetaddr_port_vlan_event(struct net_device *l3_dev,
+					    struct net_device *port_dev,
+					    unsigned long event, u16 vid,
+					    struct netlink_ext_ack *extack)
+{
+	struct mvsw_pr_port *mvsw_pr_port = netdev_priv(port_dev);
+	struct mvsw_pr_port_vlan *mvsw_pr_port_vlan;
+
+	mvsw_pr_port_vlan = mvsw_pr_port_vlan_find_by_vid(mvsw_pr_port, vid);
+	if (WARN_ON(!mvsw_pr_port_vlan))
+		return -EINVAL;
+
+	switch (event) {
+	case NETDEV_UP:
+		return mvsw_pr_port_vlan_router_join(mvsw_pr_port_vlan,
+						     l3_dev, extack);
+	case NETDEV_DOWN:
+		mvsw_pr_port_vlan_router_leave(mvsw_pr_port_vlan);
+		break;
+	}
+
+	return 0;
+}
+
+static int mvsw_pr_inetaddr_vlan_event(struct mvsw_pr_switch *sw,
+				       struct net_device *vlan_dev,
+				       unsigned long event,
+				       struct netlink_ext_ack *extack)
+{
+	struct net_device *real_dev = vlan_dev_real_dev(vlan_dev);
+	u16 vid = vlan_dev_vlan_id(vlan_dev);
+
+	MVSW_LOG_ERROR("vlan_dev=%s, real_dev=%s", vlan_dev->name,
+		       real_dev->name);
+	if (netif_is_bridge_port(vlan_dev))
+		return 0;
+
+	if (mvsw_pr_netdev_check(real_dev))
+		return mvsw_pr_inetaddr_port_vlan_event(vlan_dev, real_dev,
+							event, vid, extack);
+	else if (netif_is_bridge_master(real_dev) && br_vlan_enabled(real_dev))
+		return mvsw_pr_inetaddr_bridge_event(sw, vlan_dev, event,
+						     extack);
+
+	return 0;
+}
+
+static int __mvsw_pr_inetaddr_event(struct mvsw_pr_switch *sw,
+				    struct net_device *dev,
+				    unsigned long event,
+				    struct netlink_ext_ack *extack)
+{
+	if (mvsw_pr_netdev_check(dev))
+		return mvsw_pr_inetaddr_port_event(dev, event, extack);
+	else if (is_vlan_dev(dev))
+		return mvsw_pr_inetaddr_vlan_event(sw, dev, event, extack);
+	else if (netif_is_bridge_master(dev))
+		return mvsw_pr_inetaddr_bridge_event(sw, dev, event, extack);
+
+	return 0;
+}
+
+static bool
+mvsw_pr_rif_should_config(struct mvsw_pr_rif *rif, struct net_device *dev,
+			  unsigned long event)
+{
+	bool addr_list_empty = true;
+	struct in_device *idev;
+
+	switch (event) {
+	case NETDEV_UP:
+		return !rif;
+	case NETDEV_DOWN:
+		idev = __in_dev_get_rtnl(dev);
+		if (idev && idev->ifa_list)
+			addr_list_empty = false;
+
+		if (netif_is_macvlan(dev) && addr_list_empty)
+			return true;
+
+		if (rif && addr_list_empty)
+			return true;
+
+		return false;
+	}
+
+	return false;
+}
+
+struct mvsw_pr_rif_cleanup_work {
+	struct work_struct work;
+	struct mvsw_pr_router *router;
+	struct in_ifaddr ifa;
+};
+
+static bool
+mvsw_pr_nh_match_ifa(struct mvsw_pr_nh *nh, struct in_ifaddr *ifa)
+{
+	return inet_ifa_match(nh->n_ip, ifa) || inet_ifa_match(nh->gw_ip, ifa);
+}
+
+static void
+mvsw_pr_rif_neigh_clean(struct mvsw_pr_router *router, struct in_ifaddr *ifa)
+{
+	struct net_device *dev = ifa->ifa_dev->dev;
+	struct mvsw_pr_nh *nh, *tmp;
+
+	/* Linux doesn`t explicitly remove neighbours found on the secondary */
+	/* ifa even though they are not rechable, therefore we need to remove */
+	/* them from hardware mannually to prevent the traffic forwarding */
+	list_for_each_entry_safe(nh, tmp, &router->nexthop_list, nh_node) {
+		if (nh->n->dev == dev && mvsw_pr_nh_match_ifa(nh, ifa)) {
+			mvsw_pr_nh_release(router->sw, nh);
+			mvsw_pr_nh_destroy(nh);
+		}
+	}
+}
+
+static int mvsw_pr_inetaddr_event(struct notifier_block *nb,
+				  unsigned long event, void *ptr)
+{
+	struct in_ifaddr *ifa = (struct in_ifaddr *)ptr;
+	struct net_device *dev = ifa->ifa_dev->dev;
+	struct in_device *idev = __in_dev_get_rtnl(dev);
+	struct mvsw_pr_router *router;
+	struct mvsw_pr_rif *rif;
+	int err = 0;
+
+	/* NETDEV_UP event is handled by mvsw_pr_inetaddr_valid_event */
+	if (event == NETDEV_UP)
+		goto out;
+
+	MVSW_LOG_ERROR("dev=%s", dev->name);
+	router = container_of(nb, struct mvsw_pr_router, inetaddr_nb);
+
+	if (idev && idev->ifa_list)
+		mvsw_pr_rif_neigh_clean(router, ifa);
+
+	rif = mvsw_pr_rif_find_by_dev(router->sw, dev);
+	if (!mvsw_pr_rif_should_config(rif, dev, event))
+		goto out;
+
+	err = __mvsw_pr_inetaddr_event(router->sw, dev, event, NULL);
+out:
+	return notifier_from_errno(err);
+}
+
+int mvsw_pr_inetaddr_valid_event(struct notifier_block *unused,
+				 unsigned long event, void *ptr)
+{
+	struct in_validator_info *ivi = (struct in_validator_info *)ptr;
+	struct net_device *dev = ivi->ivi_dev->dev;
+	struct mvsw_pr_switch *sw;
+	struct mvsw_pr_rif *rif;
+	int err = 0;
+
+	sw = mvsw_pr_switch_get(dev);
+	if (!sw)
+		goto out;
+
+	rif = mvsw_pr_rif_find_by_dev(sw, dev);
+	if (!mvsw_pr_rif_should_config(rif, dev, event))
+		goto out;
+
+	err = mvsw_pr_router_port_check_rif_addr(sw, dev, dev->dev_addr,
+						 ivi->extack);
+	if (err)
+		goto out;
+
+	err = __mvsw_pr_inetaddr_event(sw, dev, event, ivi->extack);
+out:
+	return notifier_from_errno(err);
+}
+
+struct mvsw_pr_fib_event_work {
+	struct work_struct work;
+	struct mvsw_pr_switch *sw;
+	union {
+		struct fib_entry_notifier_info fen_info;
+		struct fib_rule_notifier_info fr_info;
+	};
+	unsigned long event;
+};
+
+static struct mvsw_pr_nh *
+mvsw_pr_enque_nh_neigh_resolving(struct mvsw_pr_switch *sw,
+				 struct neighbour *n,
+				 struct fib_entry_notifier_info *fen_info)
+{
+	struct mvsw_pr_nh *nh_neigh;
+	struct mvsw_pr_vr *vr;
+
+	vr = mvsw_pr_vr_find(sw, fen_info->tb_id);
+	if (!vr)
+		return NULL;
+
+	/* reset old default gw before adding new */
+	if (mvsw_pr_router_is_defalt_gw_prefix
+		(htonl(fen_info->dst), fen_info->dst_len))
+		mvsw_pr_router_release_default_gw(sw, vr->id);
+
+	nh_neigh = mvsw_pr_nh_neigh_alloc(sw, n, htonl(fen_info->dst),
+					  fen_info->dst_len);
+	return nh_neigh;
+}
+
+static int mvsw_pr_router_resolve_nh(struct mvsw_pr_switch *sw,
+				     struct fib_entry_notifier_info *fen_info)
+{
+	struct mvsw_pr_nh *nh, *nh_neigh;
+	struct net_device *nh_dev;
+	struct neighbour *n;
+	struct fib_nh *fib_nh;
+	__be32 nh_ip;
+	int err, is_connected;
+
+	fib_nh = fib_info_nh(fen_info->fi, 0);
+	nh_dev = fib_nh->fib_nh_dev;
+	nh_ip = fib_nh->fib_nh_gw4;
+
+	n = neigh_lookup(&arp_tbl, &nh_ip, nh_dev);
+	if (!n) {
+		n = neigh_create(&arp_tbl, &nh_ip, nh_dev);
+		if (IS_ERR(n))
+			return PTR_ERR(n);
+	}
+
+	nh = mvsw_pr_enque_nh_resolving(sw, n);
+	if (IS_ERR(nh)) {
+		pr_err("Failed to queue nh for offloading (%lu)",
+		       PTR_ERR(nh));
+		goto out;
+	}
+
+	is_connected = nh->connected;
+	nh_neigh = mvsw_pr_enque_nh_neigh_resolving(sw, n, fen_info);
+	if (!nh_neigh) {
+		MVSW_LOG_ERROR("Failed to queue nh_neigh for offloading");
+		err = -ENOMEM;
+		goto out;
+	}
+
+	/* Update default route if next-hop is already resolved */
+	if (is_connected)
+		mvsw_pr_router_gw_nh_refresh(sw, nh);
+
+	n->flags |= NTF_OFFLOADED;
+out:
+	neigh_release(n);
+	return err;
+}
+
+static bool mvsw_pr_fib4_allow_replace(struct mvsw_pr_fib4_entry *fib4_entry)
+{
+	struct mvsw_pr_fib_node *fib_node = fib4_entry->fib_node;
+	struct mvsw_pr_fib4_entry *fib4_replaced = fib_node->fib4_entry;
+
+	if (!fib4_replaced)
+		return true;
+
+	if (fib4_entry->tb_id == RT_TABLE_MAIN &&
+	    fib4_replaced->tb_id == RT_TABLE_LOCAL)
+		return false;
+
+	return true;
+}
+
+static struct mvsw_pr_fib4_entry *
+mvsw_pr_fib4_entry_create(struct mvsw_pr_switch *sw,
+			  struct mvsw_pr_fib_node *fib_node,
+			  struct fib_entry_notifier_info *fen_info)
+{
+	struct mvsw_pr_fib4_entry *fib4_entry;
+	struct fib_nh *nh;
+
+	fib4_entry = kzalloc(sizeof(*fib4_entry), GFP_KERNEL);
+	if (!fib4_entry)
+		return ERR_PTR(-ENOMEM);
+
+	nh  = fib_info_nh(fen_info->fi, 0);
+	if (nh->fib_nh_gw4)
+		mvsw_pr_router_resolve_nh(sw, fen_info);
+
+	fib4_entry->nh_dev = nh->fib_nh_dev;
+	fib4_entry->tb_id = fen_info->tb_id;
+	fib4_entry->type = fen_info->type;
+	fib4_entry->fi = fen_info->fi;
+
+	fib4_entry->fib_node = fib_node;
+	fib_info_hold(fib4_entry->fi);
+
+	return fib4_entry;
+}
+
+static void mvsw_pr_fib4_entry_destroy(struct mvsw_pr_fib4_entry *fib4_entry)
+{
+	fib_info_put(fib4_entry->fi);
+	kfree(fib4_entry);
+}
+
+static int mvsw_pr_fib_node_entry_offload(struct mvsw_pr_switch *sw,
+					  struct mvsw_pr_fib4_entry *fib4_entry)
+{
+	struct mvsw_pr_fib_node *fib_node = fib4_entry->fib_node;
+	u16 vid = mvsw_pr_nh_dev_to_vid(sw, fib4_entry->nh_dev);
+	struct fib_nh *nh;
+
+	fib_node->fib4_entry = fib4_entry;
+	nh = fib_info_nh(fib4_entry->fi, 0);
+	nh->fib_nh_flags |= RTNH_F_OFFLOAD;
+
+	return mvsw_pr_lpm_update(sw, fib_node->vr->id,
+				  fib_node->dst,
+				  fib_node->dst_len,
+				  vid);
+}
+
+static void
+mvsw_pr_fib_node_entry_release(struct mvsw_pr_switch *sw,
+			       struct mvsw_pr_fib_node *fib_node)
+{
+	struct fib_nh *nh;
+
+	if (mvsw_pr_router_is_defalt_gw_prefix
+		(htonl(fib_node->dst), fib_node->dst_len))
+		mvsw_pr_router_release_default_gw(sw, fib_node->vr->id);
+
+	mvsw_pr_lpm_del(sw, fib_node->vr->id,
+			fib_node->dst, fib_node->dst_len);
+	nh = fib_info_nh(fib_node->fib4_entry->fi, 0);
+	nh->fib_nh_flags &= ~RTNH_F_OFFLOAD;
+	fib_node->fib4_entry = NULL;
+}
+
+static void mvsw_pr_fib4_replace(struct mvsw_pr_switch *sw,
+				 struct mvsw_pr_fib_node *fib_node,
+				 struct mvsw_pr_fib4_entry *fib4_entry)
+{
+	struct mvsw_pr_fib4_entry *fib4_replaced = fib_node->fib4_entry;
+	struct mvsw_pr_nh *nh_neigh;
+	struct fib_nh *nh;
+
+	fib_node->fib4_entry = fib4_entry;
+	if (!fib4_replaced)
+		return;
+
+	nh_neigh = mvsw_pr_nh_neigh_find(sw, htonl(fib_node->dst));
+	if (nh_neigh)
+		mvsw_pr_nh_destroy(nh_neigh);
+
+	nh = fib_info_nh(fib4_replaced->fi, 0);
+	nh->fib_nh_flags &= ~RTNH_F_OFFLOAD;
+
+	mvsw_pr_fib4_entry_destroy(fib4_replaced);
+}
+
+static int
+mvsw_pr_router_fib4_replace(struct mvsw_pr_switch *sw,
+			    struct fib_entry_notifier_info *fen_info)
+{
+	struct mvsw_pr_fib4_entry *fib4_entry;
+	struct mvsw_pr_fib_node *fib_node;
+	int err;
+
+	if (sw->router->aborted)
+		return 0;
+
+	fib_node = mvsw_pr_fib_node_get(sw, fen_info->tb_id,
+					fen_info->dst, fen_info->dst_len);
+	if (IS_ERR(fib_node)) {
+		MVSW_LOG_ERROR("Failed to get FIB node\n");
+		return PTR_ERR(fib_node);
+	}
+
+	fib4_entry = mvsw_pr_fib4_entry_create(sw, fib_node, fen_info);
+	if (IS_ERR(fib4_entry)) {
+		MVSW_LOG_ERROR("Failed to create FIB entry\n");
+		err = PTR_ERR(fib4_entry);
+		goto err_fib4_entry_create;
+	}
+
+	if (!mvsw_pr_fib4_allow_replace(fib4_entry)) {
+		mvsw_pr_fib4_entry_destroy(fib4_entry);
+		mvsw_pr_fib_node_put(sw, fib_node);
+		return 0;
+	}
+
+	mvsw_pr_fib4_replace(sw, fib_node, fib4_entry);
+	err = mvsw_pr_fib_node_entry_offload(sw, fib4_entry);
+	if (err) {
+		MVSW_LOG_ERROR("Failed to offload FIB entry\n");
+		goto offload_err;
+	}
+
+	return 0;
+
+offload_err:
+err_fib4_entry_create:
+	mvsw_pr_fib_node_put(sw, fib_node);
+	return err;
+}
+
+static struct mvsw_pr_fib4_entry *
+mvsw_pr_fib4_entry_lookup(struct mvsw_pr_switch *sw,
+			  const struct fib_entry_notifier_info *fen_info)
+{
+	struct mvsw_pr_fib4_entry *fib4_entry;
+	struct mvsw_pr_fib_node *fib_node;
+	struct mvsw_pr_vr *vr;
+
+	vr = mvsw_pr_vr_find(sw, fen_info->tb_id);
+	if (!vr)
+		return NULL;
+
+	fib_node = mvsw_pr_fib_node_lookup(vr, fen_info->dst,
+					   fen_info->dst_len);
+	if (!fib_node)
+		return NULL;
+
+	fib4_entry = fib_node->fib4_entry;
+	if (fib4_entry->tb_id == fen_info->tb_id &&
+	    fib4_entry->type == fen_info->type &&
+	    fib4_entry->fi == fen_info->fi)
+		return fib4_entry;
+
+	return NULL;
+}
+
+static void mvsw_pr_router_fib4_del(struct mvsw_pr_switch *sw,
+				    struct fib_entry_notifier_info *fen_info)
+{
+	struct mvsw_pr_fib4_entry *fib4_entry;
+	struct mvsw_pr_fib_node *fib_node;
+	struct mvsw_pr_nh *nh_neigh;
+
+	if (sw->router->aborted)
+		return;
+
+	fib4_entry = mvsw_pr_fib4_entry_lookup(sw, fen_info);
+	if (!fib4_entry)
+		return;
+	fib_node = fib4_entry->fib_node;
+
+	nh_neigh = mvsw_pr_nh_neigh_find(sw, htonl(fen_info->dst));
+	if (nh_neigh)
+		mvsw_pr_nh_destroy(nh_neigh);
+
+	mvsw_pr_fib_node_entry_release(sw, fib_node);
+	mvsw_pr_fib4_entry_destroy(fib4_entry);
+	mvsw_pr_fib_node_put(sw, fib_node);
+}
+
+static void mvsw_pr_router_fib4_event_work(struct work_struct *work)
+{
+	struct mvsw_pr_fib_event_work *fib_work =
+			container_of(work, struct mvsw_pr_fib_event_work, work);
+	struct mvsw_pr_switch *sw = fib_work->sw;
+	int err;
+
+	rtnl_lock();
+
+	switch (fib_work->event) {
+	case FIB_EVENT_ENTRY_ADD:
+	case FIB_EVENT_ENTRY_REPLACE:
+	case FIB_EVENT_ENTRY_APPEND:
+		err = mvsw_pr_router_fib4_replace(sw,
+						  &fib_work->fen_info);
+		if (err)
+			mvsw_pr_router_fib_abort(sw);
+		fib_info_put(fib_work->fen_info.fi);
+		break;
+	case FIB_EVENT_ENTRY_DEL:
+		mvsw_pr_router_fib4_del(sw, &fib_work->fen_info);
+		fib_info_put(fib_work->fen_info.fi);
+		break;
+	}
+
+	rtnl_unlock();
+	kfree(fib_work);
+}
+
+static void mvsw_pr_router_fib4_event(struct mvsw_pr_fib_event_work *fib_work,
+				      struct fib_notifier_info *info)
+{
+	struct fib_entry_notifier_info *fen_info;
+
+	switch (fib_work->event) {
+	case FIB_EVENT_ENTRY_REPLACE: /* fall through */
+	case FIB_EVENT_ENTRY_DEL:
+		fen_info = container_of(info, struct fib_entry_notifier_info,
+					info);
+		fib_work->fen_info = *fen_info;
+		/* Take reference on fib_info to prevent it from being
+		 * freed while work is queued. Release it afterwards.
+		 */
+		fib_info_hold(fib_work->fen_info.fi);
+		break;
+	}
+}
+
+static bool
+mvsw_pr_fen_info_verify(struct fib_entry_notifier_info *fen_info)
+{
+	struct fib_nh *nh;
+
+	nh = fib_info_nh(fen_info->fi, 0);
+	if (!nh->fib_nh_dev || !mvsw_pr_port_dev_lower_find(nh->fib_nh_dev))
+		return false;
+
+	return true;
+}
+
+/* Called with rcu_read_lock() */
+static int mvsw_pr_router_fib_event(struct notifier_block *nb,
+				    unsigned long event, void *ptr)
+{
+	struct fib_entry_notifier_info *fen_info = ptr;
+	struct fib_notifier_info *info = ptr;
+	struct mvsw_pr_fib_event_work *fib_work;
+	struct mvsw_pr_router *router;
+
+	if (info->family != AF_INET)
+		return NOTIFY_DONE;
+
+	router = container_of(nb, struct mvsw_pr_router, fib_nb);
+	if (router->aborted)
+		return NOTIFY_DONE;
+
+	switch (event) {
+	case FIB_EVENT_RULE_ADD:
+	case FIB_EVENT_RULE_DEL:
+		/* TODO: */
+		return notifier_from_errno(0);
+	case FIB_EVENT_ENTRY_ADD:
+	case FIB_EVENT_ENTRY_REPLACE:
+	case FIB_EVENT_ENTRY_APPEND:
+
+		if (!fen_info->fi)
+			return NOTIFY_DONE;
+
+		if (fen_info->fi->nh)
+			return notifier_from_errno(-EINVAL);
+
+		if (fen_info->fi->fib_nh_is_v6)
+			return notifier_from_errno(-EINVAL);
+
+		if (!mvsw_pr_fen_info_verify(fen_info))
+			return NOTIFY_DONE;
+	}
+
+	fib_work = kzalloc(sizeof(*fib_work), GFP_ATOMIC);
+	if (WARN_ON(!fib_work))
+		return NOTIFY_BAD;
+
+	INIT_WORK(&fib_work->work, mvsw_pr_router_fib4_event_work);
+	fib_work->event = event;
+	fib_work->sw = router->sw;
+
+	mvsw_pr_router_fib4_event(fib_work, info);
+	queue_work(mvsw_r_owq, &fib_work->work);
+
+	return NOTIFY_DONE;
+}
+
+static void mvsw_pr_router_nh_flush(struct mvsw_pr_router *router)
+{
+	struct mvsw_pr_nh *nh, *tmp;
+
+	list_for_each_entry_safe(nh, tmp, &router->nexthop_list, nh_node) {
+		if (!nh->gw_ip) {
+			mvsw_pr_nh_entry_delete(router->sw, nh->vr_id,
+						nh->n_ip, nh->dst_len, nh->ha);
+			list_del(&nh->nh_node);
+		}
+
+		nh->n->flags &= ~NTF_OFFLOADED;
+	}
+}
+
+static void mvsw_pr_router_lpm_flush(struct mvsw_pr_router *router)
+{
+	struct mvsw_pr_fib_node *node, *tmp;
+	struct mvsw_pr_fib4_entry *fib4_entry;
+	struct mvsw_pr_vr *vr;
+
+	list_for_each_entry(vr, &router->vr_list, router_node) {
+		list_for_each_entry_safe(node, tmp, &vr->fib_list, fib_node) {
+			fib4_entry = node->fib4_entry;
+			mvsw_pr_fib_node_entry_release(router->sw, node);
+			mvsw_pr_fib4_entry_destroy(fib4_entry);
+			mvsw_pr_fib_node_put(router->sw, node);
+		}
+	}
+}
+
+static void __mvsw_pr_router_fib_flush(struct mvsw_pr_router *router)
+{
+	/* Flush pending FIB notifications and then flush the device's
+	 * table before requesting another dump. The FIB notification
+	 * block is unregistered, so no need to take RTNL.
+	 */
+	mvsw_pr_router_nh_flush(router);
+	mvsw_pr_router_lpm_flush(router);
+}
+
+static void mvsw_pr_router_fib_dump_flush(struct notifier_block *nb)
+{
+	struct mvsw_pr_router *router;
+
+	router = container_of(nb, struct mvsw_pr_router, fib_nb);
+	__mvsw_pr_router_fib_flush(router);
+}
+
+static void mvsw_pr_router_fib_abort(struct mvsw_pr_switch *sw)
+{
+	struct mvsw_pr_router *router = sw->router;
+
+	if (router->aborted)
+		return;
+
+	router->aborted = true;
+	__mvsw_pr_router_fib_flush(router);
+	dev_err(sw->dev->dev, "Abort. HW routing offloading disabled");
+}
+
+static int
+mvsw_pr_router_port_change(struct mvsw_pr_switch *sw, struct mvsw_pr_rif *rif)
+{
+	struct net_device *dev = rif->dev;
+	int err;
+
+	err = mvsw_pr_rif_edit(sw, &rif->rif_id, dev->dev_addr, dev->mtu);
+	if (err)
+		return err;
+
+	ether_addr_copy(rif->addr, dev->dev_addr);
+	rif->mtu = dev->mtu;
+
+	netdev_dbg(dev, "Updated RIF=%d\n", rif->rif_id);
+
+	return 0;
+}
+
+static int
+mvsw_pr_router_port_pre_change(struct mvsw_pr_rif *rif,
+			       struct netdev_notifier_pre_changeaddr_info *info)
+{
+	struct netlink_ext_ack *extack;
+
+	extack = netdev_notifier_info_to_extack(&info->info);
+	return mvsw_pr_router_port_check_rif_addr(rif->sw, rif->dev,
+						  info->dev_addr, extack);
+}
+
+int mvsw_pr_netdevice_router_port_event(struct net_device *dev,
+					unsigned long event, void *ptr)
+{
+	struct mvsw_pr_switch *sw;
+	struct mvsw_pr_rif *rif;
+
+	sw = mvsw_pr_switch_get(dev);
+	if (!sw)
+		return 0;
+
+	rif = mvsw_pr_rif_find_by_dev(sw, dev);
+	if (!rif)
+		return 0;
+
+	switch (event) {
+	case NETDEV_CHANGEADDR:
+		return mvsw_pr_router_port_change(sw, rif);
+	case NETDEV_PRE_CHANGEADDR:
+		return mvsw_pr_router_port_pre_change(rif, ptr);
+	}
+
+	return 0;
+}
+
+static int mvsw_pr_port_vrf_join(struct mvsw_pr_switch *sw,
+				 struct net_device *dev,
+				 struct netlink_ext_ack *extack)
+{
+	struct mvsw_pr_rif *rif;
+
+	/* If netdev is already associated with a RIF, then we need to
+	 * destroy it and create a new one with the new virtual router ID.
+	 */
+	rif = mvsw_pr_rif_find_by_dev(sw, dev);
+	if (rif)
+		__mvsw_pr_inetaddr_event(sw, dev, NETDEV_DOWN, extack);
+
+	return __mvsw_pr_inetaddr_event(sw, dev, NETDEV_UP, extack);
+}
+
+static void mvsw_pr_port_vrf_leave(struct mvsw_pr_switch *sw,
+				   struct net_device *dev)
+{
+	struct mvsw_pr_rif *rif;
+
+	rif = mvsw_pr_rif_find_by_dev(sw, dev);
+	if (!rif)
+		return;
+
+	__mvsw_pr_inetaddr_event(sw, dev, NETDEV_DOWN, NULL);
+}
+
+int mvsw_pr_netdevice_vrf_event(struct net_device *dev, unsigned long event,
+				struct netdev_notifier_changeupper_info *info)
+{
+	struct mvsw_pr_switch *sw = mvsw_pr_switch_get(dev);
+	int err = 0;
+
+	if (!sw || netif_is_macvlan(dev))
+		return 0;
+
+	switch (event) {
+	case NETDEV_PRECHANGEUPPER:
+		return 0;
+	case NETDEV_CHANGEUPPER:
+		if (info->linking) {
+			struct netlink_ext_ack *extack;
+
+			extack = netdev_notifier_info_to_extack(&info->info);
+			err = mvsw_pr_port_vrf_join(sw, dev, extack);
+		} else {
+			mvsw_pr_port_vrf_leave(sw, dev);
+		}
+		break;
+	}
+
+	return err;
+}
+
+static struct notifier_block mvsw_pr_inetaddr_valid_nb __read_mostly = {
+	.notifier_call = mvsw_pr_inetaddr_valid_event,
+};
+
+int mvsw_pr_router_init(struct mvsw_pr_switch *sw)
+{
+	struct mvsw_pr_router *router;
+	int err;
+
+	router = kzalloc(sizeof(*sw->router), GFP_KERNEL);
+	if (!router)
+		return -ENOMEM;
+	sw->router = router;
+	router->sw = sw;
+
+	err = register_inetaddr_validator_notifier(&mvsw_pr_inetaddr_valid_nb);
+	if (err)
+		goto err_register_inetaddr_validator_notifier;
+
+	router->inetaddr_nb.notifier_call = mvsw_pr_inetaddr_event;
+	err = register_inetaddr_notifier(&router->inetaddr_nb);
+	if (err)
+		goto err_register_inetaddr_notifier;
+
+	INIT_LIST_HEAD(&sw->router->rif_list);
+	INIT_LIST_HEAD(&sw->router->vr_list);
+	INIT_LIST_HEAD(&sw->router->nexthop_neighs_list);
+	INIT_LIST_HEAD(&sw->router->nexthop_list);
+
+	mvsw_r_wq = alloc_workqueue(mvsw_driver_name, 0, 0);
+	if (!mvsw_r_wq) {
+		err = -ENOMEM;
+		goto err_alloc_workqueue;
+	}
+
+	mvsw_r_owq = alloc_ordered_workqueue("%s_ordered", 0, "mvsw_prestera");
+	if (!mvsw_r_owq) {
+		err = -ENOMEM;
+		goto err_alloc_oworkqueue;
+	}
+
+	err = mvsw_pr_neigh_init(sw);
+	if (err)
+		goto err_neigh_init;
+
+	sw->router->netevent_nb.notifier_call = mvsw_pr_router_netevent_event;
+	err = register_netevent_notifier(&sw->router->netevent_nb);
+	if (err)
+		goto err_register_netevent_notifier;
+
+	sw->router->fib_nb.notifier_call = mvsw_pr_router_fib_event;
+	err = register_fib_notifier(&init_net, &sw->router->fib_nb,
+				    mvsw_pr_router_fib_dump_flush, NULL);
+	if (err)
+		goto err_register_fib_notifier;
+
+	return 0;
+
+err_register_fib_notifier:
+	unregister_netevent_notifier(&sw->router->netevent_nb);
+err_register_netevent_notifier:
+	mvsw_pr_neigh_fini(sw);
+err_neigh_init:
+	destroy_workqueue(mvsw_r_owq);
+err_alloc_oworkqueue:
+	destroy_workqueue(mvsw_r_wq);
+err_alloc_workqueue:
+	unregister_inetaddr_notifier(&router->inetaddr_nb);
+err_register_inetaddr_notifier:
+	unregister_inetaddr_validator_notifier(&mvsw_pr_inetaddr_valid_nb);
+err_register_inetaddr_validator_notifier:
+	kfree(sw->router);
+	return err;
+}
+
+void mvsw_pr_router_fini(struct mvsw_pr_switch *sw)
+{
+	unregister_fib_notifier(&init_net, &sw->router->fib_nb);
+	unregister_netevent_notifier(&sw->router->netevent_nb);
+	mvsw_pr_neigh_fini(sw);
+	unregister_inetaddr_notifier(&sw->router->inetaddr_nb);
+	unregister_inetaddr_validator_notifier(&mvsw_pr_inetaddr_valid_nb);
+
+	flush_workqueue(mvsw_r_wq);
+	flush_workqueue(mvsw_r_owq);
+	destroy_workqueue(mvsw_r_wq);
+	destroy_workqueue(mvsw_r_owq);
+
+	WARN_ON(!list_empty(&sw->router->rif_list));
+
+	kfree(sw->router);
+	sw->router = NULL;
+}
+
+static u32 mvsw_pr_fix_tb_id(u32 tb_id)
+{
+	if (tb_id == RT_TABLE_LOCAL || tb_id == RT_TABLE_DEFAULT)
+		return tb_id = RT_TABLE_MAIN;
+	return tb_id;
+}
+
+static struct mvsw_pr_vr *mvsw_pr_vr_find(struct mvsw_pr_switch *sw, u32 tb_id)
+{
+	struct mvsw_pr_vr *vr;
+
+	tb_id = mvsw_pr_fix_tb_id(tb_id);
+
+	list_for_each_entry(vr, &sw->router->vr_list, router_node) {
+		if (vr->tb_id == tb_id)
+			return vr;
+	}
+
+	return NULL;
+}
+
+static struct mvsw_pr_vr *mvsw_pr_vr_find_by_id(struct mvsw_pr_switch *sw,
+						u16 vr_id)
+{
+	struct mvsw_pr_vr *vr;
+
+	list_for_each_entry(vr, &sw->router->vr_list, router_node) {
+		if (vr->id == vr_id)
+			return vr;
+	}
+
+	return NULL;
+}
+
+static struct mvsw_pr_vr *mvsw_pr_vr_create(struct mvsw_pr_switch *sw,
+					    u32 tb_id,
+					    struct netlink_ext_ack *extack)
+{
+	struct mvsw_pr_vr *vr;
+	u16 vr_id;
+	int err;
+
+	err = mvsw_pr_hw_vr_create(sw, &vr_id);
+	if (err)
+		return ERR_PTR(-ENOMEM);
+
+	vr = kzalloc(sizeof(*vr), GFP_KERNEL);
+	if (!vr)
+		return ERR_PTR(-ENOMEM);
+
+	vr->tb_id = tb_id;
+	vr->id = vr_id;
+
+	list_add(&vr->router_node, &sw->router->vr_list);
+	INIT_LIST_HEAD(&vr->fib_list);
+
+	return vr;
+}
+
+static void mvsw_pr_vr_destroy(struct mvsw_pr_switch *sw, struct mvsw_pr_vr *vr)
+{
+	mvsw_pr_hw_vr_delete(sw, vr->id);
+	list_del(&vr->router_node);
+	kfree(vr);
+}
+
+static struct mvsw_pr_vr *mvsw_pr_vr_get(struct mvsw_pr_switch *sw, u32 tb_id,
+					 struct netlink_ext_ack *extack)
+{
+	struct mvsw_pr_vr *vr;
+
+	tb_id = mvsw_pr_fix_tb_id(tb_id);
+	vr = mvsw_pr_vr_find(sw, tb_id);
+	if (!vr)
+		vr = mvsw_pr_vr_create(sw, tb_id, extack);
+	if (IS_ERR(vr))
+		return ERR_CAST(vr);
+
+	return vr;
+}
+
+static void mvsw_pr_vr_put(struct mvsw_pr_switch *sw, struct mvsw_pr_vr *vr)
+{
+	if (!vr->ref_cnt)
+		mvsw_pr_vr_destroy(sw, vr);
+}
+
+static struct mvsw_pr_rif*
+mvsw_pr_rif_alloc(struct mvsw_pr_switch *sw,
+		  struct mvsw_pr_vr *vr,
+		  const struct mvsw_pr_rif_params *params)
+{
+	enum mvsw_pr_rif_type type;
+	struct mvsw_pr_rif *rif;
+	int err;
+
+	type = mvsw_pr_dev_rif_type(params->dev);
+	rif = kzalloc(sizeof(*rif), GFP_KERNEL);
+	if (!rif) {
+		err = -ENOMEM;
+		goto err_rif_alloc;
+	}
+
+	if (params->dev) {
+		ether_addr_copy(rif->addr, params->dev->dev_addr);
+		rif->mtu = params->dev->mtu;
+		rif->dev = params->dev;
+	}
+	rif->type = type;
+	rif->vr_id = vr->id;
+	rif->sw = sw;
+
+	return rif;
+
+err_rif_alloc:
+	return ERR_PTR(err);
+}
+
+static int mvsw_pr_rif_offload(struct mvsw_pr_switch *sw,
+			       struct mvsw_pr_rif *rif)
+{
+	struct mvsw_pr_port *mvsw_pr_port;
+	u16 rif_id;
+	u16 vid;
+	int err = 0;
+
+	switch (rif->type) {
+	case MVSW_PR_RIF_TYPE_PORT:
+		mvsw_pr_port = netdev_priv(rif->dev);
+		err = mvsw_pr_hw_rif_port_create(mvsw_pr_port,
+						 rif->vr_id,
+						 rif->addr,
+						 &rif_id);
+		if (err)
+			return err;
+
+		break;
+	case MVSW_PR_RIF_TYPE_VLAN:
+		vid = mvsw_pr_nh_dev_to_vid(sw, rif->dev);
+		err = mvsw_pr_hw_rif_vlan_create(sw, rif->vr_id,
+						 rif->dev->dev_addr,
+						 vid, &rif_id);
+		if (err)
+			return err;
+
+		break;
+	case MVSW_PR_RIF_TYPE_BRIDGE:
+		vid = mvsw_pr_vlan_dev_vlan_id(sw->bridge,
+					       rif->dev);
+		err = mvsw_pr_hw_rif_bridge_create(sw, rif->vr_id,
+						   rif->dev->dev_addr,
+						   vid, &rif_id);
+		if (err)
+			return err;
+
+		break;
+	default:
+		WARN_ON(1);
+		return -EINVAL;
+	}
+
+	rif->rif_id = rif_id;
+	return err;
+}
+
+static struct mvsw_pr_rif *mvsw_pr_rif_create(struct mvsw_pr_switch *sw,
+					      const struct mvsw_pr_rif_params
+					      *params,
+					      struct netlink_ext_ack *extack)
+{
+	u32 tb_id = l3mdev_fib_table(params->dev);
+	struct mvsw_pr_rif *rif;
+	struct mvsw_pr_vr *vr;
+	int err;
+
+	vr = mvsw_pr_vr_get(sw, tb_id ? : RT_TABLE_MAIN, extack);
+	if (IS_ERR(vr))
+		return ERR_CAST(vr);
+
+	rif = mvsw_pr_rif_alloc(sw, vr, params);
+	if (IS_ERR(rif)) {
+		mvsw_pr_vr_put(sw, vr);
+		return rif;
+	}
+
+	err = mvsw_pr_rif_offload(sw, rif);
+	if (err)  {
+		NL_SET_ERR_MSG_MOD(extack,
+				   "Exceeded number of supported rifs");
+		goto err_rif_offload;
+	}
+
+	vr->ref_cnt++;
+	// dev_hold(rif->dev);
+	list_add(&rif->router_node, &sw->router->rif_list);
+
+	/* TODO: rif->configure() ??? */
+
+	return rif;
+
+err_rif_offload:
+	kfree(rif);
+	return ERR_PTR(err);
+}
+
+static void mvsw_pr_rif_destroy(struct mvsw_pr_rif *rif)
+{
+	struct mvsw_pr_vr *vr;
+
+	MVSW_LOG_ERROR("dev=%s", rif->dev->name);
+
+	vr = mvsw_pr_vr_find_by_id(rif->sw, rif->vr_id);
+	/* TODO: rif->deconfigure() ??? */
+	mvsw_pr_hw_rif_delete(rif->sw, rif->vr_id, rif->rif_id,
+			      mvsw_pr_port_dev_lower_find(rif->dev));
+	list_del(&rif->router_node);
+	//dev_put(rif->dev);
+	kfree(rif);
+	vr->ref_cnt--;
+	mvsw_pr_vr_put(rif->sw, vr);
+}
+
+void mvsw_pr_rif_disable(struct mvsw_pr_switch *sw, struct net_device *dev)
+{
+	struct mvsw_pr_rif *rif;
+
+	rif = mvsw_pr_rif_find_by_dev(sw, dev);
+	if (rif)
+		mvsw_pr_hw_rif_delete(sw, rif->vr_id, rif->rif_id,
+				      mvsw_pr_port_dev_lower_find(rif->dev));
+}
+
+void mvsw_pr_rif_enable(struct mvsw_pr_switch *sw, struct net_device *dev)
+{
+	struct mvsw_pr_rif *rif;
+
+	rif = mvsw_pr_rif_find_by_dev(sw, dev);
+	if (rif)
+		mvsw_pr_rif_offload(rif->sw, rif);
+}
+
+static int mvsw_pr_rif_edit(struct mvsw_pr_switch *sw, u16 *rif_id,
+			    char *mac, int mtu)
+{
+	return mvsw_pr_hw_rif_set(sw, rif_id, mtu, mac);
+}
diff --git a/drivers/net/ethernet/marvell/prestera_sw/prestera_rxtx.c b/drivers/net/ethernet/marvell/prestera_sw/prestera_rxtx.c
new file mode 100644
index 0000000..acab6d4
--- /dev/null
+++ b/drivers/net/ethernet/marvell/prestera_sw/prestera_rxtx.c
@@ -0,0 +1,255 @@
+// SPDX-License-Identifier: BSD-3-Clause OR GPL-2.0
+/*
+ * Copyright (c) 2019-2020 Marvell International Ltd. All rights reserved.
+ *
+ */
+#include "prestera.h"
+#include "prestera_rxtx_priv.h"
+#include "prestera_dsa.h"
+
+#include <linux/if_vlan.h>
+#include <net/ip.h>
+
+struct mvsw_pr_rxtx;
+
+enum mvsw_pr_rxtx_type {
+	MVSW_PR_RXTX_MVPP,
+	MVSW_PR_RXTX_ETH,
+	MVSW_PR_RXTX_SDMA,
+};
+
+static struct mvsw_pr_rxtx *rxtx_registered;
+
+netdev_tx_t mvsw_pr_rxtx_xmit(struct sk_buff *skb,
+			      struct mvsw_pr_rxtx_info *info)
+{
+	struct mvsw_pr_dsa dsa;
+	struct mvsw_pr_dsa_from_cpu *from_cpu;
+	struct net_device *dev = skb->dev;
+	struct mvsw_pr_port *port = netdev_priv(dev);
+	size_t dsa_resize_len = MVSW_PR_DSA_HLEN;
+
+	if (!rxtx_registered)
+		return NET_XMIT_DROP;
+
+	/* common DSA tag fill-up */
+	memset(&dsa, 0, sizeof(dsa));
+	dsa.dsa_cmd = MVSW_NET_DSA_CMD_FROM_CPU_E;
+
+	from_cpu = &dsa.dsa_info.from_cpu;
+	from_cpu->egr_filter_en = false;
+	from_cpu->egr_filter_registered = false;
+	from_cpu->dst_eport = port->hw_id;
+
+	from_cpu->dst_iface.dev_port.port_num = port->hw_id;
+	from_cpu->dst_iface.dev_port.hw_dev_num = port->dev_id;
+	from_cpu->dst_iface.type = MVSW_IF_PORT_E;
+
+	/* epmorary removing due to issue with vlan sub interface
+	 * on 1.Q bridge
+	 */
+	/* If (skb->protocol == htons(ETH_P_8021Q)) { */
+		/* 802.1q packet tag size is 4 bytes, so DSA len would
+		 * need only allocation of MVSW_PR_DSA_HLEN - size of
+		 * 802.1q tag
+		 */
+		/*dsa.common_params.vpt = skb_vlan_tag_get_prio(skb);
+		 * dsa.common_params.cfi_bit = skb_vlan_tag_get_cfi(skb);
+		 * dsa.common_params.vid = skb_vlan_tag_get_id(skb);
+		 * dsa_resize_len -= VLAN_HLEN;
+		 */
+	/* } */
+
+
+	if (skb_cow_head(skb, dsa_resize_len) < 0)
+		return NET_XMIT_DROP;
+
+	/* expects skb->data at mac header */
+	skb_push(skb, dsa_resize_len);
+	memmove(skb->data, skb->data + dsa_resize_len, 2 * ETH_ALEN);
+
+	if (mvsw_pr_dsa_build(&dsa, skb->data + 2 * ETH_ALEN) != 0)
+		return NET_XMIT_DROP;
+
+	return rxtx_registered->ops->rxtx_xmit(rxtx_registered, skb);
+}
+
+int mvsw_pr_rxtx_recv_skb(struct mvsw_pr_rxtx *rxtx, struct sk_buff *skb)
+{
+	const struct mvsw_pr_port *port;
+	struct mvsw_pr_dsa dsa;
+	u32 hw_port, hw_id;
+	int err;
+
+	skb_pull(skb, ETH_HLEN);
+
+	/* parse/process DSA tag
+	 * ethertype field is part of the dsa header
+	 */
+	err = mvsw_pr_dsa_parse(skb->data - ETH_TLEN, &dsa);
+	if (err)
+		return err;
+
+	/* get switch port */
+	hw_port = dsa.dsa_info.to_cpu.iface.port_num;
+	hw_id = dsa.dsa_info.to_cpu.hw_dev_num;
+	port = mvsw_pr_port_find(hw_id, hw_port);
+	if (unlikely(!port)) {
+		pr_warn_ratelimited("prestera: received pkt for non-existent port(%u, %u)\n",
+				    hw_id, hw_port);
+		return -EEXIST;
+	}
+
+	if (unlikely(!pskb_may_pull(skb, MVSW_PR_DSA_HLEN)))
+		return -EINVAL;
+
+	/* remove DSA tag and update checksum */
+	skb_pull_rcsum(skb, MVSW_PR_DSA_HLEN);
+
+	memmove(skb->data - ETH_HLEN, skb->data - ETH_HLEN - MVSW_PR_DSA_HLEN,
+		ETH_ALEN * 2);
+
+	skb_push(skb, ETH_HLEN);
+
+	skb->protocol = eth_type_trans(skb, port->net_dev);
+
+	if (dsa.dsa_info.to_cpu.is_tagged) {
+		u16 tci = dsa.common_params.vid & VLAN_VID_MASK;
+
+		tci |= dsa.common_params.vpt << VLAN_PRIO_SHIFT;
+		if (dsa.common_params.cfi_bit)
+			tci |= VLAN_CFI_MASK;
+
+		__vlan_hwaccel_put_tag(skb, htons(ETH_P_8021Q), tci);
+	}
+
+	return 0;
+}
+
+static struct mvsw_pr_rxtx_ops rxtx_driver_ops[] = {
+	[MVSW_PR_RXTX_MVPP] = {
+		.rxtx_init = mvsw_pr_rxtx_mvpp_init,
+		.rxtx_fini = mvsw_pr_rxtx_mvpp_fini,
+		.rxtx_xmit = mvsw_pr_rxtx_mvpp_xmit,
+	},
+	[MVSW_PR_RXTX_ETH] = {
+		.rxtx_init = mvsw_pr_rxtx_eth_init,
+		.rxtx_fini = mvsw_pr_rxtx_eth_fini,
+		.rxtx_switch_init = mvsw_pr_rxtx_eth_switch_init,
+		.rxtx_switch_fini = mvsw_pr_rxtx_eth_switch_fini,
+		.rxtx_xmit = mvsw_pr_rxtx_eth_xmit,
+	},
+	[MVSW_PR_RXTX_SDMA] = {
+		.rxtx_init = mvsw_pr_rxtx_sdma_init,
+		.rxtx_fini = mvsw_pr_rxtx_sdma_fini,
+		.rxtx_switch_init = mvsw_pr_rxtx_sdma_switch_init,
+		.rxtx_switch_fini = mvsw_pr_rxtx_sdma_switch_fini,
+		.rxtx_xmit = mvsw_pr_rxtx_sdma_xmit,
+	},
+};
+
+static int mvsw_pr_rxtx_probe(struct platform_device *pdev)
+{
+	struct device *dev = &pdev->dev;
+	enum mvsw_pr_rxtx_type rxtx_type;
+	struct mvsw_pr_rxtx *rxtx;
+	int err;
+
+	rxtx_type = (enum mvsw_pr_rxtx_type)of_device_get_match_data(dev);
+
+	rxtx = devm_kzalloc(dev, sizeof(*rxtx), GFP_KERNEL);
+	if (!rxtx)
+		return -ENOMEM;
+
+	rxtx->ops = &rxtx_driver_ops[rxtx_type];
+	rxtx->pdev = pdev;
+	rxtx->dev = dev;
+
+	platform_set_drvdata(pdev, rxtx);
+
+	if (rxtx->ops->rxtx_init) {
+		err = rxtx->ops->rxtx_init(rxtx);
+		if (err)
+			return err;
+	}
+
+	rxtx_registered = rxtx;
+
+	pr_info("Registered mvsw prestera rxtx driver\n");
+
+	return 0;
+}
+
+static int mvsw_pr_rxtx_remove(struct platform_device *pdev)
+{
+	struct mvsw_pr_rxtx *rxtx = platform_get_drvdata(pdev);
+	int err = 0;
+
+	if (rxtx->ops->rxtx_fini)
+		err = rxtx->ops->rxtx_fini(rxtx);
+
+	rxtx_registered = NULL;
+	return err;
+}
+
+static const struct of_device_id mvsw_pr_rxtx_match[] = {
+	{
+		.compatible = "marvell,prestera-switch-rxtx-mvpp",
+		.data = (void *)MVSW_PR_RXTX_MVPP,
+	},
+	{
+		.compatible = "marvell,prestera-switch-rxtx-eth",
+		.data = (void *)MVSW_PR_RXTX_ETH,
+	},
+	{
+		.compatible = "marvell,prestera-switch-rxtx-sdma",
+		.data = (void *)MVSW_PR_RXTX_SDMA,
+	},
+	{ }
+};
+
+static struct platform_driver mvsw_pr_rxtx_driver = {
+	.probe = mvsw_pr_rxtx_probe,
+	.remove = mvsw_pr_rxtx_remove,
+	.driver = {
+		.name = "mvsw_pr_rxtx",
+		.of_match_table = mvsw_pr_rxtx_match,
+	},
+};
+
+int mvsw_pr_rxtx_init(void)
+{
+	return platform_driver_register(&mvsw_pr_rxtx_driver);
+}
+
+void mvsw_pr_rxtx_fini(void)
+{
+	platform_driver_unregister(&mvsw_pr_rxtx_driver);
+}
+
+int mvsw_pr_rxtx_switch_init(const struct mvsw_pr_switch *sw)
+{
+	int err;
+
+	if (!rxtx_registered) {
+		pr_info("No RxTx driver registered");
+		return 0;
+	}
+
+	if (!rxtx_registered->ops->rxtx_switch_init)
+		return 0;
+
+	err = rxtx_registered->ops->rxtx_switch_init(rxtx_registered, sw);
+	if (err)
+		return err;
+
+	return 0;
+}
+
+void mvsw_pr_rxtx_switch_fini(const struct mvsw_pr_switch *sw)
+{
+	if (!rxtx_registered || !rxtx_registered->ops->rxtx_switch_init)
+		return;
+
+	return rxtx_registered->ops->rxtx_switch_fini(rxtx_registered, sw);
+}
diff --git a/drivers/net/ethernet/marvell/prestera_sw/prestera_rxtx.h b/drivers/net/ethernet/marvell/prestera_sw/prestera_rxtx.h
new file mode 100644
index 0000000..0566bc1
--- /dev/null
+++ b/drivers/net/ethernet/marvell/prestera_sw/prestera_rxtx.h
@@ -0,0 +1,28 @@
+/* SPDX-License-Identifier: BSD-3-Clause OR GPL-2.0
+ *
+ * Copyright (c) 2019-2020 Marvell International Ltd. All rights reserved.
+ *
+ */
+
+#ifndef _MVSW_PRESTERA_RXTX_H_
+#define _MVSW_PRESTERA_RXTX_H_
+
+#include <linux/netdevice.h>
+
+struct mvsw_pr_switch;
+
+struct mvsw_pr_rxtx_info {
+	u32 port_id;
+	u32 dev_id;
+};
+
+int mvsw_pr_rxtx_init(void);
+void mvsw_pr_rxtx_fini(void);
+
+int mvsw_pr_rxtx_switch_init(const struct mvsw_pr_switch *sw);
+void mvsw_pr_rxtx_switch_fini(const struct mvsw_pr_switch *sw);
+
+netdev_tx_t mvsw_pr_rxtx_xmit(struct sk_buff *skb,
+			      struct mvsw_pr_rxtx_info *info);
+
+#endif /* _MVSW_PRESTERA_RXTX_H_ */
diff --git a/drivers/net/ethernet/marvell/prestera_sw/prestera_rxtx_eth.c b/drivers/net/ethernet/marvell/prestera_sw/prestera_rxtx_eth.c
new file mode 100644
index 0000000..2983276
--- /dev/null
+++ b/drivers/net/ethernet/marvell/prestera_sw/prestera_rxtx_eth.c
@@ -0,0 +1,120 @@
+// SPDX-License-Identifier: BSD-3-Clause OR GPL-2.0
+/*
+ * Copyright (c) 2019-2020 Marvell International Ltd. All rights reserved.
+ *
+ */
+
+#include <linux/rtnetlink.h>
+#include <linux/of_net.h>
+
+#include "prestera_hw.h"
+#include "prestera_rxtx_priv.h"
+
+struct mvsw_pr_rxtx_eth {
+	struct mvsw_pr_rxtx *rxtx;
+	struct net_device *dev;
+};
+
+static rx_handler_result_t mvsw_pr_rxtx_eth_rx(struct sk_buff **pskb)
+{
+	struct mvsw_pr_rxtx_eth *rxtx_eth;
+	struct sk_buff *skb = *pskb;
+
+	skb_push(skb, ETH_HLEN);
+
+	rxtx_eth = rcu_dereference(skb->dev->rx_handler_data);
+	if (likely(!mvsw_pr_rxtx_recv_skb(rxtx_eth->rxtx, skb))) {
+		skb_reset_mac_header(skb);
+		skb_set_mac_header(skb, -ETH_HLEN);
+		skb_reset_network_header(skb);
+		skb_reset_mac_len(skb);
+		skb_reset_transport_header(skb);
+
+		/* update packet type */
+		if (skb->pkt_type != PACKET_BROADCAST &&
+		    skb->pkt_type != PACKET_MULTICAST)
+			skb->pkt_type = PACKET_HOST;
+
+		return RX_HANDLER_ANOTHER;
+	}
+
+	skb_pull(skb, ETH_HLEN);
+	return RX_HANDLER_PASS;
+}
+
+int mvsw_pr_rxtx_eth_init(struct mvsw_pr_rxtx *rxtx)
+{
+	struct device_node *rxtx_dn = rxtx->dev->of_node;
+	struct device_node *eth_dn = of_parse_phandle(rxtx_dn, "ethernet", 0);
+	unsigned int rxtx_flags = IFF_NOARP | IFF_PROMISC | IFF_UP;
+	struct mvsw_pr_rxtx_eth *rxtx_eth;
+	struct net_device *dev;
+	int err;
+
+	if (!eth_dn)
+		return -EINVAL;
+
+	dev = of_find_net_device_by_node(eth_dn);
+	if (!dev)
+		return -EPROBE_DEFER;
+
+	rxtx_eth = devm_kzalloc(rxtx->dev, sizeof(*rxtx_eth), GFP_KERNEL);
+	if (!rxtx_eth)
+		return -ENOMEM;
+
+	rxtx->priv = rxtx_eth;
+	rxtx_eth->rxtx = rxtx;
+	rxtx_eth->dev = dev;
+
+	rtnl_lock();
+
+	dev_change_flags(dev, dev->flags | rxtx_flags, NULL);
+
+	err = netdev_rx_handler_register(dev, mvsw_pr_rxtx_eth_rx, rxtx_eth);
+	if (err) {
+		pr_err("failed to register rx handler for the cpu interface\n");
+		rtnl_unlock();
+		return err;
+	}
+
+	rtnl_unlock();
+
+	return 0;
+}
+
+int mvsw_pr_rxtx_eth_fini(struct mvsw_pr_rxtx *rxtx)
+{
+	struct mvsw_pr_rxtx_eth *rxtx_eth = rxtx->priv;
+
+	rtnl_lock();
+	/* Can be called only while holding rtnl lock */
+	netdev_rx_handler_unregister(rxtx_eth->dev);
+	rtnl_unlock();
+
+	return 0;
+}
+
+int mvsw_pr_rxtx_eth_switch_init(struct mvsw_pr_rxtx *rxtx,
+				 const struct mvsw_pr_switch *sw)
+{
+	int err;
+
+	err = mvsw_pr_hw_rxtx_init(sw, false, NULL);
+	if (err)
+		return err;
+
+	return 0;
+}
+
+void mvsw_pr_rxtx_eth_switch_fini(struct mvsw_pr_rxtx *rxtx,
+				  const struct mvsw_pr_switch *sw)
+{
+}
+
+netdev_tx_t mvsw_pr_rxtx_eth_xmit(struct mvsw_pr_rxtx *rxtx,
+				  struct sk_buff *skb)
+{
+	struct mvsw_pr_rxtx_eth *rxtx_eth = rxtx->priv;
+
+	return rxtx_eth->dev->netdev_ops->ndo_start_xmit(skb, rxtx_eth->dev);
+}
diff --git a/drivers/net/ethernet/marvell/prestera_sw/prestera_rxtx_mvpp.c b/drivers/net/ethernet/marvell/prestera_sw/prestera_rxtx_mvpp.c
new file mode 100644
index 0000000..5f598db
--- /dev/null
+++ b/drivers/net/ethernet/marvell/prestera_sw/prestera_rxtx_mvpp.c
@@ -0,0 +1,129 @@
+// SPDX-License-Identifier: BSD-3-Clause OR GPL-2.0
+/*
+ * Copyright (c) 2019-2020 Marvell International Ltd. All rights reserved.
+ *
+ */
+#include <linux/kernel.h>
+#include <linux/netdevice.h>
+#include <linux/rtnetlink.h>
+#include <linux/platform_device.h>
+#include <linux/of.h>
+#include <linux/of_net.h>
+#include <linux/of_device.h>
+#ifdef CONFIG_MRVL_PRESTERA_SW_RXTX_MVPP2
+#include <linux/platform_data/mvpp2.h>
+#endif
+
+#include "prestera_rxtx_priv.h"
+
+#ifdef CONFIG_MRVL_PRESTERA_SW_RXTX_MVPP2
+struct mvsw_pr_rxtx_mvpp {
+	struct mvsw_pr_rxtx *rxtx;
+	struct device_node *mvpp_dn;
+	void *port_priv;
+};
+
+static int mvsw_pr_rxtx_mvpp_rx_hook(u8 port_id, struct sk_buff *skb, void *arg)
+{
+	struct mvsw_pr_rxtx_mvpp *rxtx_mvpp = arg;
+
+	return mvsw_pr_rxtx_recv_skb(rxtx_mvpp->rxtx, skb);
+}
+
+int mvsw_pr_rxtx_mvpp_init(struct mvsw_pr_rxtx *rxtx)
+{
+	const struct fwnode_handle *fwnode = rxtx->pdev->dev.fwnode;
+	struct mvsw_pr_rxtx_mvpp *rxtx_mvpp;
+	struct mvpp2_port_platform_data pdata;
+	struct fwnode_handle *port_fwnode;
+	struct platform_device *mvpp_pd;
+	struct device_node *mvpp_dn;
+	struct device_node *port_dn;
+	int err;
+
+	rxtx_mvpp = devm_kzalloc(rxtx->dev, sizeof(*rxtx_mvpp), GFP_KERNEL);
+	if (!rxtx_mvpp)
+		return -ENOMEM;
+
+	rxtx->priv = rxtx_mvpp;
+
+	mvpp_dn = of_find_compatible_node(NULL, NULL, "marvell,armada-7k-pp22");
+	if (!mvpp_dn) {
+		pr_err("failed to find mvpp2 node\n");
+		return -EEXIST;
+	}
+
+	mvpp_pd = of_find_device_by_node(mvpp_dn);
+	if (!mvpp_pd) {
+		pr_err("failed to find mvpp2 platform device\n");
+		err = -EINVAL;
+		goto put_node;
+	}
+
+	pdata.rx_hook.fn = mvsw_pr_rxtx_mvpp_rx_hook;
+	pdata.rx_hook.arg = rxtx_mvpp;
+
+	port_fwnode = fwnode_get_named_child_node(fwnode, "cpu_port");
+	if (!port_fwnode) {
+		pr_err("failed to find cpu port node\n");
+		err = -EINVAL;
+		goto put_node;
+	}
+
+	/* re-assign parent as mvpp2 node for irq lookup */
+	port_dn = to_of_node(port_fwnode);
+	if (port_dn)
+		port_dn->parent = mvpp_dn;
+
+	err = mvpp2_port_probe(mvpp_pd, port_fwnode, &pdata);
+	if (err) {
+		pr_err("mvsw prestera rxtx: mvpp2 probe failed\n");
+		goto err_port_probe;
+	}
+
+	rxtx_mvpp->port_priv = pdata.priv;
+	rxtx_mvpp->mvpp_dn = mvpp_dn;
+
+	pr_info("mvsw prestera rxtx: mvpp2 successfully probed\n");
+
+	return 0;
+
+err_port_probe:
+put_node:
+	of_node_put(mvpp_dn);
+	return err;
+}
+
+netdev_tx_t mvsw_pr_rxtx_mvpp_xmit(struct mvsw_pr_rxtx *rxtx,
+				   struct sk_buff *skb)
+{
+	return -EINVAL;
+}
+
+int mvsw_pr_rxtx_mvpp_fini(struct mvsw_pr_rxtx *rxtx)
+{
+	struct mvsw_pr_rxtx_mvpp *rxtx_mvpp = rxtx->priv;
+
+	of_node_put(rxtx_mvpp->mvpp_dn);
+
+	mvpp2_port_remove(rxtx_mvpp->port_priv);
+
+	return 0;
+}
+#else
+int mvsw_pr_rxtx_mvpp_init(struct mvsw_pr_rxtx *rxtx)
+{
+	return -EINVAL;
+}
+
+int mvsw_pr_rxtx_mvpp_fini(struct mvsw_pr_rxtx *rxtx)
+{
+	return -EINVAL;
+}
+
+netdev_tx_t mvsw_pr_rxtx_mvpp_xmit(struct mvsw_pr_rxtx *rxtx,
+				   struct sk_buff *skb)
+{
+	return -EINVAL;
+}
+#endif /* CONFIG_MRVL_PRESTERA_SW_RXTX_MVPP2 */
diff --git a/drivers/net/ethernet/marvell/prestera_sw/prestera_rxtx_priv.h b/drivers/net/ethernet/marvell/prestera_sw/prestera_rxtx_priv.h
new file mode 100644
index 0000000..66a5267
--- /dev/null
+++ b/drivers/net/ethernet/marvell/prestera_sw/prestera_rxtx_priv.h
@@ -0,0 +1,61 @@
+/* SPDX-License-Identifier: BSD-3-Clause OR GPL-2.0
+ *
+ * Copyright (c) 2019-2020 Marvell International Ltd. All rights reserved.
+ *
+ */
+#include <linux/kernel.h>
+#include <linux/netdevice.h>
+#include <linux/rtnetlink.h>
+#include <linux/platform_device.h>
+#include <linux/of.h>
+#include <linux/of_device.h>
+
+#include "prestera_rxtx.h"
+
+struct mvsw_pr_rxtx;
+
+struct mvsw_pr_rxtx_ops {
+	int (*rxtx_init)(struct mvsw_pr_rxtx *rxtx);
+	int (*rxtx_fini)(struct mvsw_pr_rxtx *rxtx);
+
+	int (*rxtx_switch_init)(struct mvsw_pr_rxtx *rxtx,
+				const struct mvsw_pr_switch *sw);
+	void (*rxtx_switch_fini)(struct mvsw_pr_rxtx *rxtx,
+				 const struct mvsw_pr_switch *sw);
+
+	netdev_tx_t (*rxtx_xmit)(struct mvsw_pr_rxtx *rxtx,
+				 struct sk_buff *skb);
+};
+
+struct mvsw_pr_rxtx {
+	struct platform_device *pdev;
+	struct device *dev;
+
+	const struct mvsw_pr_rxtx_ops *ops;
+	void *priv;
+};
+
+int mvsw_pr_rxtx_recv_skb(struct mvsw_pr_rxtx *rxtx, struct sk_buff *skb);
+
+int mvsw_pr_rxtx_eth_init(struct mvsw_pr_rxtx *rxtx);
+int mvsw_pr_rxtx_eth_fini(struct mvsw_pr_rxtx *rxtx);
+netdev_tx_t mvsw_pr_rxtx_eth_xmit(struct mvsw_pr_rxtx *rxtx,
+				  struct sk_buff *skb);
+
+int mvsw_pr_rxtx_mvpp_init(struct mvsw_pr_rxtx *rxtx);
+int mvsw_pr_rxtx_mvpp_fini(struct mvsw_pr_rxtx *rxtx);
+int mvsw_pr_rxtx_eth_switch_init(struct mvsw_pr_rxtx *rxtx,
+				 const struct mvsw_pr_switch *sw);
+void mvsw_pr_rxtx_eth_switch_fini(struct mvsw_pr_rxtx *rxtx,
+				  const struct mvsw_pr_switch *sw);
+netdev_tx_t mvsw_pr_rxtx_mvpp_xmit(struct mvsw_pr_rxtx *rxtx,
+				   struct sk_buff *skb);
+
+int mvsw_pr_rxtx_sdma_init(struct mvsw_pr_rxtx *rxtx);
+int mvsw_pr_rxtx_sdma_fini(struct mvsw_pr_rxtx *rxtx);
+int mvsw_pr_rxtx_sdma_switch_init(struct mvsw_pr_rxtx *rxtx,
+				  const struct mvsw_pr_switch *sw);
+void mvsw_pr_rxtx_sdma_switch_fini(struct mvsw_pr_rxtx *rxtx,
+				   const struct mvsw_pr_switch *sw);
+netdev_tx_t mvsw_pr_rxtx_sdma_xmit(struct mvsw_pr_rxtx *rxtx,
+				   struct sk_buff *skb);
diff --git a/drivers/net/ethernet/marvell/prestera_sw/prestera_rxtx_sdma.c b/drivers/net/ethernet/marvell/prestera_sw/prestera_rxtx_sdma.c
new file mode 100644
index 0000000..1fb8ee3
--- /dev/null
+++ b/drivers/net/ethernet/marvell/prestera_sw/prestera_rxtx_sdma.c
@@ -0,0 +1,649 @@
+// SPDX-License-Identifier: BSD-3-Clause OR GPL-2.0
+/*
+ * Copyright (c) 2019-2020 Marvell International Ltd. All rights reserved.
+ *
+ */
+
+#include <linux/platform_device.h>
+#include <linux/of.h>
+#include <linux/of_address.h>
+#include <linux/of_device.h>
+#include <linux/dmapool.h>
+
+#include "prestera.h"
+#include "prestera_hw.h"
+#include "prestera_rxtx_priv.h"
+
+struct mvsw_sdma_desc {
+	__le32 word1;
+	__le32 word2;
+	__le32 buff;
+	__le32 next;
+} __packed __aligned(16);
+
+#define SDMA_BUFF_SIZE_MAX	1544
+
+#define SDMA_RX_DESC_PKT_LEN(desc) \
+	((le32_to_cpu((desc)->word2) >> 16) & 0x3FFF)
+
+#define SDMA_RX_DESC_OWNER(desc) \
+	(le32_to_cpu(((desc)->word1)) >> 31)
+
+#define SDMA_RX_DESC_CPU_OWN	0
+#define SDMA_RX_DESC_DMA_OWN	1
+
+#define SDMA_RX_QUEUE_NUM	8
+
+#define SDMA_RX_DESC_PER_Q	1000
+
+#define SDMA_TX_DESC_NUM	1000
+
+#define SDMA_TX_DESC_OWNER(desc)	(le32_to_cpu((desc)->word1) >> 31)
+
+#define SDMA_TX_DESC_CPU_OWN	0
+#define SDMA_TX_DESC_DMA_OWN	1
+
+#define SDMA_TX_DESC_LAST	BIT(20)
+#define SDMA_TX_DESC_FIRST	BIT(21)
+#define SDMA_TX_DESC_SINGLE	(SDMA_TX_DESC_FIRST | SDMA_TX_DESC_LAST)
+#define SDMA_TX_DESC_CALC_CRC	BIT(12)
+
+#define mvsw_reg_write(sw, reg, val) \
+	writel(val, (sw)->dev->pp_regs + (reg))
+#define mvsw_reg_read(sw, reg) \
+	readl((sw)->dev->pp_regs + (reg))
+
+#define SDMA_RX_QUEUE_STATUS_REG	0x2680
+#define SDMA_RX_QUEUE_DESC_REG(n)	(0x260C + (n) * 16)
+
+#define SDMA_TX_QUEUE_DESC_REG		0x26C0
+#define SDMA_TX_QUEUE_START_REG		0x2868
+
+struct mvsw_sdma_buf {
+	struct mvsw_sdma_desc *desc;
+	struct list_head list;
+	dma_addr_t desc_dma;
+	struct sk_buff *skb;
+	dma_addr_t buf_dma;
+	bool is_used;
+};
+
+struct mvsw_sdma_rx_ring {
+	struct mvsw_sdma_buf *bufs;
+};
+
+struct mvsw_sdma_tx_ring {
+	struct mvsw_sdma_buf *bufs;
+	int next_tx;
+};
+
+struct mvsw_pr_rxtx_sdma {
+	struct mvsw_sdma_rx_ring rx_ring[SDMA_RX_QUEUE_NUM];
+	struct mvsw_sdma_tx_ring tx_ring;
+	const struct mvsw_pr_switch *sw;
+	struct dma_pool *desc_pool;
+	struct mvsw_pr_rxtx *rxtx;
+	struct workqueue_struct *rxwq;
+	struct work_struct rx_work;
+	struct work_struct tx_work;
+	bool finish_rx;
+	u32 map_addr;
+	u64 dma_mask;
+};
+
+static int mvsw_sdma_buf_desc_alloc(struct mvsw_pr_rxtx_sdma *sdma,
+				    struct mvsw_sdma_buf *buf)
+{
+	struct device *dma_dev = sdma->sw->dev->dev;
+	struct mvsw_sdma_desc *desc;
+	dma_addr_t dma;
+
+	desc = dma_pool_alloc(sdma->desc_pool, GFP_DMA | GFP_KERNEL, &dma);
+	if (!desc)
+		return -ENOMEM;
+
+	if (dma + sizeof(struct mvsw_sdma_desc) > sdma->dma_mask) {
+		dev_err(dma_dev, "failed to alloc desc\n");
+		dma_pool_free(sdma->desc_pool, desc, dma);
+		return -ENOMEM;
+	}
+
+	buf->desc_dma = dma;
+	buf->desc = desc;
+
+	return 0;
+}
+
+static u32 mvsw_sdma_addr_phy(struct mvsw_pr_rxtx_sdma *sdma, dma_addr_t pa)
+{
+	return sdma->map_addr + pa;
+}
+
+static void mvsw_sdma_rx_desc_set_len(struct mvsw_sdma_desc *desc, size_t val)
+{
+	u32 word = le32_to_cpu(desc->word2);
+
+	word = (word & ~GENMASK(15, 0)) | val;
+	desc->word2 = cpu_to_le32(word);
+}
+
+static void mvsw_sdma_rx_desc_init(struct mvsw_pr_rxtx_sdma *sdma,
+				   struct mvsw_sdma_desc *desc,
+				   dma_addr_t buf)
+{
+	mvsw_sdma_rx_desc_set_len(desc, SDMA_BUFF_SIZE_MAX);
+	desc->buff = cpu_to_le32(mvsw_sdma_addr_phy(sdma, buf));
+	/* make sure buffer is set before reset the descriptor */
+	wmb();
+	desc->word1 = cpu_to_le32(0xA0000000);
+}
+
+static void mvsw_sdma_rx_desc_set_next(struct mvsw_pr_rxtx_sdma *sdma,
+				       struct mvsw_sdma_desc *desc,
+				       dma_addr_t next)
+{
+	desc->next = cpu_to_le32(mvsw_sdma_addr_phy(sdma, next));
+}
+
+static int mvsw_sdma_rx_dma_alloc(struct mvsw_pr_rxtx_sdma *sdma,
+				  struct mvsw_sdma_buf *buf)
+{
+	struct device *dev = sdma->sw->dev->dev;
+
+	buf->skb = __netdev_alloc_skb_ip_align(NULL, SDMA_BUFF_SIZE_MAX,
+					       GFP_DMA | GFP_KERNEL);
+	if (!buf->skb)
+		return -ENOMEM;
+
+	buf->buf_dma = dma_map_single(dev, buf->skb->data, buf->skb->len,
+				      DMA_FROM_DEVICE);
+
+	if (dma_mapping_error(dev, buf->buf_dma))
+		goto err_dma_map;
+	if (buf->buf_dma + buf->skb->len > sdma->dma_mask)
+		goto err_dma_range;
+
+	return 0;
+
+err_dma_range:
+	dma_unmap_single(dev, buf->buf_dma, buf->skb->len, DMA_FROM_DEVICE);
+	buf->buf_dma = DMA_MAPPING_ERROR;
+err_dma_map:
+	kfree_skb(buf->skb);
+	buf->skb = NULL;
+
+	return -ENOMEM;
+}
+
+static struct sk_buff *mvsw_sdma_rx_buf_get(struct mvsw_pr_rxtx_sdma *sdma,
+					    struct mvsw_sdma_buf *buf)
+{
+	struct sk_buff *skb_orig = buf->skb;
+	dma_addr_t buf_dma = buf->buf_dma;
+	u32 len = skb_orig->len;
+	int err;
+
+	err = mvsw_sdma_rx_dma_alloc(sdma, buf);
+	if (err) {
+		struct sk_buff *skb;
+
+		buf->buf_dma = buf_dma;
+		buf->skb = skb_orig;
+
+		skb = __netdev_alloc_skb_ip_align(NULL, SDMA_BUFF_SIZE_MAX,
+						  GFP_KERNEL);
+		if (!skb)
+			return NULL;
+
+		skb_copy_from_linear_data(buf->skb, skb_put(skb, len), len);
+		return skb;
+	}
+
+	return skb_orig;
+}
+
+static void mvsw_sdma_rx_work_fn(struct work_struct *work)
+{
+	struct mvsw_pr_rxtx_sdma *sdma;
+	int q, b;
+
+	sdma = container_of(work, struct mvsw_pr_rxtx_sdma, rx_work);
+
+	for (b = 0; b < SDMA_RX_DESC_PER_Q; b++) {
+		for (q = 0; q < SDMA_RX_QUEUE_NUM; q++) {
+			struct mvsw_sdma_rx_ring *ring = &sdma->rx_ring[q];
+			struct mvsw_sdma_buf *buf = &ring->bufs[b];
+			struct mvsw_sdma_desc *desc = buf->desc;
+			struct sk_buff *skb;
+
+			if (sdma->finish_rx)
+				return;
+
+			if (SDMA_RX_DESC_OWNER(desc) != SDMA_RX_DESC_CPU_OWN)
+				continue;
+
+			__skb_trim(buf->skb, SDMA_RX_DESC_PKT_LEN(desc));
+
+			skb = mvsw_sdma_rx_buf_get(sdma, buf);
+			if (!skb)
+				goto rx_reset_buf;
+
+			if (unlikely(mvsw_pr_rxtx_recv_skb(sdma->rxtx, skb)))
+				goto rx_reset_buf;
+
+			netif_rx_ni(skb);
+rx_reset_buf:
+			mvsw_sdma_rx_desc_init(sdma, buf->desc, buf->buf_dma);
+		}
+	}
+
+	/* TODO: remove it when rx interrupt will be supported */
+	if (!sdma->finish_rx)
+		queue_work(sdma->rxwq, work);
+}
+
+static void mvsw_sdma_rx_fini(struct mvsw_pr_rxtx_sdma *sdma)
+{
+	int q, b;
+
+	/* disable all rx queues */
+	mvsw_reg_write(sdma->sw, SDMA_RX_QUEUE_STATUS_REG, 0xff00);
+
+	sdma->finish_rx = true;
+
+	flush_workqueue(sdma->rxwq);
+	destroy_workqueue(sdma->rxwq);
+
+	for (q = 0; q < SDMA_RX_QUEUE_NUM; q++) {
+		struct mvsw_sdma_rx_ring *ring = &sdma->rx_ring[q];
+
+		if (!ring->bufs)
+			break;
+
+		for (b = 0; b < SDMA_RX_DESC_PER_Q; b++) {
+			struct mvsw_sdma_buf *buf = &ring->bufs[b];
+
+			if (buf->desc_dma)
+				dma_pool_free(sdma->desc_pool, buf->desc,
+					      buf->desc_dma);
+
+			if (!buf->skb)
+				continue;
+
+			if (buf->buf_dma != DMA_MAPPING_ERROR)
+				dma_unmap_single(sdma->sw->dev->dev,
+						 buf->buf_dma, buf->skb->len,
+						 DMA_FROM_DEVICE);
+			kfree_skb(buf->skb);
+		}
+	}
+}
+
+static int mvsw_sdma_rx_init(struct mvsw_pr_rxtx_sdma *sdma)
+{
+	int q, b;
+	int err;
+
+	sdma->rxwq = alloc_workqueue("prestera_sdma_wq", WQ_HIGHPRI, 1);
+	if (!sdma->rxwq)
+		return -ENOMEM;
+
+	INIT_WORK(&sdma->rx_work, mvsw_sdma_rx_work_fn);
+
+	/* disable all rx queues */
+	mvsw_reg_write(sdma->sw, SDMA_RX_QUEUE_STATUS_REG, 0xff00);
+
+	for (q = 0; q < SDMA_RX_QUEUE_NUM; q++) {
+		struct mvsw_sdma_rx_ring *ring = &sdma->rx_ring[q];
+		struct mvsw_sdma_buf *head;
+
+		ring->bufs = kmalloc_array(SDMA_RX_DESC_PER_Q, sizeof(*head),
+					   GFP_KERNEL);
+		if (!ring->bufs)
+			return -ENOMEM;
+
+		head = &ring->bufs[0];
+
+		for (b = 0; b < SDMA_RX_DESC_PER_Q; b++) {
+			struct mvsw_sdma_buf *buf = &ring->bufs[b];
+
+			err = mvsw_sdma_buf_desc_alloc(sdma, buf);
+			if (err)
+				return err;
+
+			err = mvsw_sdma_rx_dma_alloc(sdma, buf);
+			if (err)
+				return err;
+
+			mvsw_sdma_rx_desc_init(sdma, buf->desc, buf->buf_dma);
+
+			if (b == 0)
+				continue;
+
+			mvsw_sdma_rx_desc_set_next(sdma, ring->bufs[b - 1].desc,
+						   buf->desc_dma);
+
+			if (b == SDMA_RX_DESC_PER_Q - 1)
+				mvsw_sdma_rx_desc_set_next(sdma, buf->desc,
+							   head->desc_dma);
+		}
+
+		mvsw_reg_write(sdma->sw, SDMA_RX_QUEUE_DESC_REG(q),
+			       mvsw_sdma_addr_phy(sdma, head->desc_dma));
+	}
+
+	/* make sure all rx descs are filled before enabling all rx queues */
+	wmb();
+	mvsw_reg_write(sdma->sw, SDMA_RX_QUEUE_STATUS_REG, 0xff);
+
+	/* TODO: remove it when rx interrupt will be supported */
+	queue_work(sdma->rxwq, &sdma->rx_work);
+	return 0;
+}
+
+static void mvsw_sdma_tx_desc_init(struct mvsw_pr_rxtx_sdma *sdma,
+				   struct mvsw_sdma_desc *desc)
+{
+	desc->word1 = cpu_to_le32(SDMA_TX_DESC_SINGLE | SDMA_TX_DESC_CALC_CRC);
+	desc->word2 = 0;
+}
+
+static void mvsw_sdma_tx_desc_set_next(struct mvsw_pr_rxtx_sdma *sdma,
+				       struct mvsw_sdma_desc *desc,
+				       dma_addr_t next)
+{
+	desc->next = cpu_to_le32(mvsw_sdma_addr_phy(sdma, next));
+}
+
+static void mvsw_sdma_tx_desc_set_buf(struct mvsw_pr_rxtx_sdma *sdma,
+				      struct mvsw_sdma_desc *desc,
+				      dma_addr_t buf, size_t len)
+{
+	u32 word = le32_to_cpu(desc->word2);
+
+	word = (word & ~GENMASK(30, 16)) | ((len + 4) << 16);
+
+	desc->buff = cpu_to_le32(mvsw_sdma_addr_phy(sdma, buf));
+	desc->word2 = cpu_to_le32(word);
+}
+
+static void mvsw_sdma_tx_desc_xmit(struct mvsw_sdma_desc *desc)
+{
+	u32 word = le32_to_cpu(desc->word1);
+
+	word |= (SDMA_TX_DESC_DMA_OWN << 31);
+
+	/* make sure everything is written before enable xmit */
+	wmb();
+	desc->word1 = cpu_to_le32(word);
+}
+
+static void mvsw_sdma_tx_recycle_work_fn(struct work_struct *work)
+{
+	struct mvsw_pr_rxtx_sdma *sdma;
+	struct device *dma_dev;
+	int b;
+
+	sdma = container_of(work, struct mvsw_pr_rxtx_sdma, tx_work);
+
+	dma_dev = sdma->sw->dev->dev;
+
+	for (b = 0; b < SDMA_TX_DESC_NUM; b++) {
+		struct mvsw_sdma_buf *buf = &sdma->tx_ring.bufs[b];
+
+		if (!buf->skb)
+			continue;
+		if (!buf->is_used)
+			continue;
+		if (SDMA_TX_DESC_OWNER(buf->desc) == SDMA_TX_DESC_DMA_OWN)
+			continue;
+
+		dma_unmap_single(dma_dev, buf->buf_dma, buf->skb->len,
+				 DMA_TO_DEVICE);
+		dev_consume_skb_any(buf->skb);
+		buf->skb = NULL;
+		/* make sure everything is cleaned up */
+		wmb();
+		buf->is_used = false;
+	}
+}
+
+static int mvsw_sdma_tx_init(struct mvsw_pr_rxtx_sdma *sdma)
+{
+	struct mvsw_sdma_tx_ring *tx_ring = &sdma->tx_ring;
+	struct mvsw_sdma_buf *head;
+	int err;
+	int b;
+
+	INIT_WORK(&sdma->tx_work, mvsw_sdma_tx_recycle_work_fn);
+
+	tx_ring->bufs = kmalloc_array(SDMA_TX_DESC_NUM, sizeof(*head),
+				      GFP_KERNEL);
+	if (!tx_ring->bufs)
+		return -ENOMEM;
+
+	head = &tx_ring->bufs[0];
+	tx_ring->next_tx = 0;
+
+	for (b = 0; b < SDMA_TX_DESC_NUM; b++) {
+		struct mvsw_sdma_buf *buf = &tx_ring->bufs[b];
+
+		err = mvsw_sdma_buf_desc_alloc(sdma, buf);
+		if (err)
+			return err;
+
+		mvsw_sdma_tx_desc_init(sdma, buf->desc);
+
+		buf->is_used = false;
+
+		if (b == 0)
+			continue;
+
+		mvsw_sdma_tx_desc_set_next(sdma, tx_ring->bufs[b - 1].desc,
+					   buf->desc_dma);
+
+		if (b == SDMA_TX_DESC_NUM - 1)
+			mvsw_sdma_tx_desc_set_next(sdma, buf->desc,
+						   head->desc_dma);
+	}
+
+	/* make sure descriptors are written */
+	wmb();
+	mvsw_reg_write(sdma->sw, SDMA_TX_QUEUE_DESC_REG,
+		       mvsw_sdma_addr_phy(sdma, head->desc_dma));
+
+	return 0;
+}
+
+static void mvsw_sdma_tx_fini(struct mvsw_pr_rxtx_sdma *sdma)
+{
+	struct mvsw_sdma_tx_ring *ring = &sdma->tx_ring;
+	int b;
+
+	cancel_work_sync(&sdma->tx_work);
+
+	if (!ring->bufs)
+		return;
+
+	for (b = 0; b < SDMA_TX_DESC_NUM; b++) {
+		struct mvsw_sdma_buf *buf = &ring->bufs[b];
+
+		if (buf->desc)
+			dma_pool_free(sdma->desc_pool, buf->desc,
+				      buf->desc_dma);
+
+		if (!buf->skb)
+			continue;
+
+		dma_unmap_single(sdma->sw->dev->dev, buf->buf_dma,
+				 buf->skb->len, DMA_TO_DEVICE);
+
+		dev_consume_skb_any(buf->skb);
+	}
+}
+
+int mvsw_pr_rxtx_sdma_init(struct mvsw_pr_rxtx *rxtx)
+{
+	struct mvsw_pr_rxtx_sdma *sdma;
+
+	sdma = kzalloc(sizeof(*sdma), GFP_KERNEL);
+	if (!sdma)
+		return -ENOMEM;
+
+	rxtx->priv = sdma;
+	sdma->rxtx = rxtx;
+
+	return 0;
+}
+
+int mvsw_pr_rxtx_sdma_fini(struct mvsw_pr_rxtx *rxtx)
+{
+	kfree(rxtx->priv);
+	return 0;
+}
+
+int mvsw_pr_rxtx_sdma_switch_init(struct mvsw_pr_rxtx *rxtx,
+				  const struct mvsw_pr_switch *sw)
+{
+	struct mvsw_pr_rxtx_sdma *sdma = rxtx->priv;
+	int err;
+
+	err = mvsw_pr_hw_rxtx_init(sw, true, &sdma->map_addr);
+	if (err) {
+		dev_err(sw->dev->dev, "failed to init rxtx by hw\n");
+		return err;
+	}
+
+	sdma->dma_mask = dma_get_mask(sw->dev->dev);
+	sdma->sw = sw;
+
+	sdma->desc_pool = dma_pool_create("desc_pool", sdma->sw->dev->dev,
+					  sizeof(struct mvsw_sdma_desc), 16, 0);
+	if (!sdma->desc_pool)
+		return -ENOMEM;
+
+	err = mvsw_sdma_rx_init(sdma);
+	if (err) {
+		dev_err(sw->dev->dev, "failed to init rx ring\n");
+		goto err_rx_init;
+	}
+
+	err = mvsw_sdma_tx_init(sdma);
+	if (err) {
+		dev_err(sw->dev->dev, "failed to init tx ring\n");
+		goto err_tx_init;
+	}
+
+	return 0;
+
+err_tx_init:
+	mvsw_sdma_tx_fini(sdma);
+err_rx_init:
+	mvsw_sdma_rx_fini(sdma);
+
+	dma_pool_destroy(sdma->desc_pool);
+	return err;
+}
+
+void mvsw_pr_rxtx_sdma_switch_fini(struct mvsw_pr_rxtx *rxtx,
+				   const struct mvsw_pr_switch *sw)
+{
+	struct mvsw_pr_rxtx_sdma *sdma = rxtx->priv;
+
+	mvsw_sdma_rx_fini(sdma);
+	mvsw_sdma_tx_fini(sdma);
+	dma_pool_destroy(sdma->desc_pool);
+}
+
+static int mvsw_sdma_tx_buf_map(struct mvsw_pr_rxtx_sdma *sdma,
+				struct mvsw_sdma_buf *buf,
+				struct sk_buff *skb)
+{
+	struct device *dma_dev = sdma->sw->dev->dev;
+	struct sk_buff *new_skb;
+	size_t len = skb->len;
+	dma_addr_t dma;
+
+	dma = dma_map_single(dma_dev, skb->data, len, DMA_TO_DEVICE);
+	if (!dma_mapping_error(dma_dev, dma) && dma + len <= sdma->dma_mask) {
+		buf->buf_dma = dma;
+		buf->skb = skb;
+		return 0;
+	}
+
+	if (!dma_mapping_error(dma_dev, dma))
+		dma_unmap_single(dma_dev, dma, len, DMA_TO_DEVICE);
+
+	new_skb = alloc_skb(len, GFP_ATOMIC | GFP_DMA);
+	if (!new_skb)
+		goto err_alloc_skb;
+
+	dma = dma_map_single(dma_dev, new_skb->data, len, DMA_TO_DEVICE);
+	if (dma_mapping_error(dma_dev, dma))
+		goto err_dma_map;
+	if (dma + len > sdma->dma_mask)
+		goto err_dma_range;
+
+	skb_copy_from_linear_data(skb, skb_put(new_skb, len), len);
+
+	dev_consume_skb_any(skb);
+
+	buf->skb = new_skb;
+	buf->buf_dma = dma;
+
+	return 0;
+
+err_dma_range:
+	dma_unmap_single(dma_dev, dma, len, DMA_TO_DEVICE);
+err_dma_map:
+	dev_kfree_skb(new_skb);
+err_alloc_skb:
+	dev_kfree_skb(skb);
+
+	return -ENOMEM;
+}
+
+netdev_tx_t mvsw_pr_rxtx_sdma_xmit(struct mvsw_pr_rxtx *rxtx,
+				   struct sk_buff *skb)
+{
+	struct mvsw_pr_rxtx_sdma *sdma = rxtx->priv;
+	struct device *dma_dev = sdma->sw->dev->dev;
+	struct net_device *dev = skb->dev;
+	struct mvsw_sdma_buf *buf;
+	unsigned int len;
+	int err;
+
+	buf = &sdma->tx_ring.bufs[sdma->tx_ring.next_tx];
+	if (buf->is_used) {
+		schedule_work(&sdma->tx_work);
+		goto drop_skb;
+	}
+
+	if (skb_pad(skb, ETH_ZLEN))
+		goto drop_skb_nofree;
+
+	err = mvsw_sdma_tx_buf_map(sdma, buf, skb);
+	if (err)
+		goto drop_skb;
+
+	len = max_t(unsigned int, skb->len, ETH_ZLEN);
+
+	mvsw_sdma_tx_desc_set_buf(sdma, buf->desc, buf->buf_dma, len);
+
+	dma_sync_single_for_device(dma_dev, buf->buf_dma, len, DMA_TO_DEVICE);
+
+	mvsw_sdma_tx_desc_xmit(buf->desc);
+
+	sdma->tx_ring.next_tx = (sdma->tx_ring.next_tx + 1) % SDMA_TX_DESC_NUM;
+	buf->is_used = true;
+
+	mvsw_reg_write(sdma->sw, SDMA_TX_QUEUE_START_REG, 1);
+	schedule_work(&sdma->tx_work);
+	return NETDEV_TX_OK;
+
+drop_skb:
+	dev_consume_skb_any(skb);
+drop_skb_nofree:
+	dev->stats.tx_dropped++;
+	return NETDEV_TX_OK;
+}
diff --git a/drivers/net/ethernet/marvell/prestera_sw/prestera_switchdev.c b/drivers/net/ethernet/marvell/prestera_sw/prestera_switchdev.c
new file mode 100644
index 0000000..427409c
--- /dev/null
+++ b/drivers/net/ethernet/marvell/prestera_sw/prestera_switchdev.c
@@ -0,0 +1,1290 @@
+/* SPDX-License-Identifier: BSD-3-Clause OR GPL-2.0
+ *
+ * Copyright (c) 2019-2020 Marvell International Ltd. All rights reserved.
+ *
+ */
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/if_vlan.h>
+#include <linux/if_bridge.h>
+#include <linux/notifier.h>
+#include <net/switchdev.h>
+#include <net/netevent.h>
+#include <net/vxlan.h>
+
+#include "prestera.h"
+
+struct mvsw_pr_bridge {
+	struct mvsw_pr_switch *sw;
+	u32 ageing_time;
+	struct list_head bridge_list;
+	bool bridge_8021q_exists;
+};
+
+struct mvsw_pr_bridge_device {
+	struct net_device *dev;
+	struct list_head bridge_node;
+	struct list_head port_list;
+	u16 bridge_id;
+	u8 vlan_enabled:1, multicast_enabled:1, mrouter:1;
+};
+
+struct mvsw_pr_bridge_port {
+	struct net_device *dev;
+	struct mvsw_pr_bridge_device *bridge_device;
+	struct list_head bridge_device_node;
+	struct list_head vlan_list;
+	unsigned int ref_count;
+	u8 stp_state;
+	unsigned long flags;
+};
+
+struct mvsw_pr_bridge_vlan {
+	struct list_head bridge_port_node;
+	struct list_head port_vlan_list;
+	u16 vid;
+};
+
+struct mvsw_pr_event_work {
+	struct work_struct work;
+	struct switchdev_notifier_fdb_info fdb_info;
+	struct net_device *dev;
+	unsigned long event;
+};
+
+static struct workqueue_struct *mvsw_owq;
+
+static struct mvsw_pr_bridge_port *
+mvsw_pr_bridge_port_get(struct mvsw_pr_bridge *bridge,
+			struct net_device *brport_dev);
+
+static void mvsw_pr_bridge_port_put(struct mvsw_pr_bridge *bridge,
+				    struct mvsw_pr_bridge_port *br_port);
+
+struct mvsw_pr_bridge_device *
+mvsw_pr_bridge_device_find(const struct mvsw_pr_bridge *bridge,
+			   const struct net_device *br_dev)
+{
+	struct mvsw_pr_bridge_device *bridge_device;
+
+	list_for_each_entry(bridge_device, &bridge->bridge_list,
+			    bridge_node)
+		if (bridge_device->dev == br_dev)
+			return bridge_device;
+
+	return NULL;
+}
+
+static bool
+mvsw_pr_bridge_device_is_offloaded(const struct mvsw_pr_switch *sw,
+				   const struct net_device *br_dev)
+{
+	return !!mvsw_pr_bridge_device_find(sw->bridge, br_dev);
+}
+
+static struct mvsw_pr_bridge_port *
+__mvsw_pr_bridge_port_find(const struct mvsw_pr_bridge_device *bridge_device,
+			   const struct net_device *brport_dev)
+{
+	struct mvsw_pr_bridge_port *br_port;
+
+	list_for_each_entry(br_port, &bridge_device->port_list,
+			    bridge_device_node) {
+		if (br_port->dev == brport_dev)
+			return br_port;
+	}
+
+	return NULL;
+}
+
+static struct mvsw_pr_bridge_port *
+mvsw_pr_bridge_port_find(struct mvsw_pr_bridge *bridge,
+			 struct net_device *brport_dev)
+{
+	struct net_device *br_dev = netdev_master_upper_dev_get(brport_dev);
+	struct mvsw_pr_bridge_device *bridge_device;
+
+	if (!br_dev)
+		return NULL;
+
+	bridge_device = mvsw_pr_bridge_device_find(bridge, br_dev);
+	if (!bridge_device)
+		return NULL;
+
+	return __mvsw_pr_bridge_port_find(bridge_device, brport_dev);
+}
+
+static struct mvsw_pr_bridge_vlan *
+mvsw_pr_bridge_vlan_find(const struct mvsw_pr_bridge_port *br_port, u16 vid)
+{
+	struct mvsw_pr_bridge_vlan *br_vlan;
+
+	list_for_each_entry(br_vlan, &br_port->vlan_list, bridge_port_node) {
+		if (br_vlan->vid == vid)
+			return br_vlan;
+	}
+
+	return NULL;
+}
+
+u16 mvsw_pr_vlan_dev_vlan_id(struct mvsw_pr_bridge *bridge,
+			     struct net_device *dev)
+{
+	struct mvsw_pr_bridge_device *bridge_dev;
+
+	bridge_dev = mvsw_pr_bridge_device_find(bridge, dev);
+
+	return bridge_dev->bridge_id;
+}
+
+static struct mvsw_pr_bridge_vlan *
+mvsw_pr_bridge_vlan_create(struct mvsw_pr_bridge_port *br_port, u16 vid)
+{
+	struct mvsw_pr_bridge_vlan *br_vlan;
+
+	br_vlan = kzalloc(sizeof(*br_vlan), GFP_KERNEL);
+	if (!br_vlan)
+		return NULL;
+
+	INIT_LIST_HEAD(&br_vlan->port_vlan_list);
+	br_vlan->vid = vid;
+	list_add(&br_vlan->bridge_port_node, &br_port->vlan_list);
+
+	return br_vlan;
+}
+
+static void
+mvsw_pr_bridge_vlan_destroy(struct mvsw_pr_bridge_vlan *br_vlan)
+{
+	list_del(&br_vlan->bridge_port_node);
+	WARN_ON(!list_empty(&br_vlan->port_vlan_list));
+	kfree(br_vlan);
+}
+
+static struct mvsw_pr_bridge_vlan *
+mvsw_pr_bridge_vlan_get(struct mvsw_pr_bridge_port *br_port, u16 vid)
+{
+	struct mvsw_pr_bridge_vlan *br_vlan;
+
+	br_vlan = mvsw_pr_bridge_vlan_find(br_port, vid);
+	if (br_vlan)
+		return br_vlan;
+
+	return mvsw_pr_bridge_vlan_create(br_port, vid);
+}
+
+static void mvsw_pr_bridge_vlan_put(struct mvsw_pr_bridge_vlan *br_vlan)
+{
+	if (list_empty(&br_vlan->port_vlan_list))
+		mvsw_pr_bridge_vlan_destroy(br_vlan);
+}
+
+static int
+mvsw_pr_port_vlan_bridge_join(struct mvsw_pr_port_vlan *port_vlan,
+			      struct mvsw_pr_bridge_port *br_port,
+			      struct netlink_ext_ack *extack)
+{
+	struct mvsw_pr_port *port = port_vlan->mvsw_pr_port;
+	struct mvsw_pr_bridge_vlan *br_vlan;
+	u16 vid = port_vlan->vid;
+	int err;
+
+	if (port_vlan->bridge_port)
+		return 0;
+
+	err = mvsw_pr_port_flood_set(port, br_port->flags & BR_FLOOD);
+	if (err)
+		return err;
+
+	err = mvsw_pr_port_learning_set(port, br_port->flags & BR_LEARNING);
+	if (err)
+		goto err_port_learning_set;
+
+	br_vlan = mvsw_pr_bridge_vlan_get(br_port, vid);
+	if (!br_vlan) {
+		err = -ENOMEM;
+		goto err_bridge_vlan_get;
+	}
+
+	list_add(&port_vlan->bridge_vlan_node, &br_vlan->port_vlan_list);
+
+	mvsw_pr_bridge_port_get(port->sw->bridge, br_port->dev);
+	port_vlan->bridge_port = br_port;
+
+	return 0;
+
+err_bridge_vlan_get:
+	mvsw_pr_port_learning_set(port, false);
+err_port_learning_set:
+	return err;
+}
+
+static int
+mvsw_pr_bridge_vlan_port_count_get(struct mvsw_pr_bridge_device *bridge_device,
+				   u16 vid)
+{
+	int count = 0;
+	struct mvsw_pr_bridge_port *br_port;
+	struct mvsw_pr_bridge_vlan *br_vlan;
+
+	list_for_each_entry(br_port, &bridge_device->port_list,
+			    bridge_device_node) {
+		list_for_each_entry(br_vlan, &br_port->vlan_list,
+				    bridge_port_node) {
+			if (br_vlan->vid == vid) {
+				count += 1;
+				break;
+			}
+		}
+	}
+
+	return count;
+}
+
+void
+mvsw_pr_port_vlan_bridge_leave(struct mvsw_pr_port_vlan *port_vlan)
+{
+	struct mvsw_pr_port *port = port_vlan->mvsw_pr_port;
+	struct mvsw_pr_bridge_vlan *br_vlan;
+	struct mvsw_pr_bridge_port *br_port;
+	int port_count;
+	u16 vid = port_vlan->vid;
+	bool last_port, last_vlan;
+
+	br_port = port_vlan->bridge_port;
+	last_vlan = list_is_singular(&br_port->vlan_list);
+	port_count =
+	    mvsw_pr_bridge_vlan_port_count_get(br_port->bridge_device, vid);
+	br_vlan = mvsw_pr_bridge_vlan_find(br_port, vid);
+	last_port = port_count == 1;
+	if (last_vlan) {
+		mvsw_pr_fdb_flush_port(port, MVSW_PR_FDB_FLUSH_MODE_DYNAMIC);
+	} else if (last_port) {
+		mvsw_pr_fdb_flush_vlan(port->sw, vid,
+				       MVSW_PR_FDB_FLUSH_MODE_DYNAMIC);
+	} else {
+		mvsw_pr_fdb_flush_port_vlan(port, vid,
+					    MVSW_PR_FDB_FLUSH_MODE_DYNAMIC);
+	}
+
+	list_del(&port_vlan->bridge_vlan_node);
+	mvsw_pr_bridge_vlan_put(br_vlan);
+	mvsw_pr_bridge_port_put(port->sw->bridge, br_port);
+	port_vlan->bridge_port = NULL;
+}
+
+static int
+mvsw_pr_bridge_port_vlan_add(struct mvsw_pr_port *port,
+			     struct mvsw_pr_bridge_port *br_port,
+			     u16 vid, bool is_untagged, bool is_pvid,
+			     struct netlink_ext_ack *extack)
+{
+	u16 pvid;
+	struct mvsw_pr_port_vlan *port_vlan;
+	u16 old_pvid = port->pvid;
+	int err;
+
+	if (is_pvid)
+		pvid = vid;
+	else
+		pvid = port->pvid == vid ? 0 : port->pvid;
+
+	port_vlan = mvsw_pr_port_vlan_find_by_vid(port, vid);
+	if (port_vlan && port_vlan->bridge_port != br_port)
+		return -EEXIST;
+
+	if (!port_vlan) {
+		port_vlan = mvsw_pr_port_vlan_create(port, vid, is_untagged);
+		if (IS_ERR(port_vlan))
+			return PTR_ERR(port_vlan);
+	} else {
+		err = mvsw_pr_port_vlan_set(port, vid, true, is_untagged);
+		if (err)
+			goto err_port_vlan_set;
+	}
+
+	err = mvsw_pr_port_pvid_set(port, pvid);
+	if (err)
+		goto err_port_pvid_set;
+
+	err = mvsw_pr_port_vlan_bridge_join(port_vlan, br_port, extack);
+	if (err)
+		goto err_port_vlan_bridge_join;
+
+	return 0;
+
+err_port_vlan_bridge_join:
+	mvsw_pr_port_pvid_set(port, old_pvid);
+err_port_pvid_set:
+	mvsw_pr_port_vlan_set(port, vid, false, false);
+err_port_vlan_set:
+	mvsw_pr_port_vlan_destroy(port_vlan);
+
+	return err;
+}
+
+static int mvsw_pr_port_vlans_add(struct mvsw_pr_port *port,
+				  const struct switchdev_obj_port_vlan *vlan,
+				  struct switchdev_trans *trans,
+				  struct netlink_ext_ack *extack)
+{
+	bool flag_untagged = vlan->flags & BRIDGE_VLAN_INFO_UNTAGGED;
+	bool flag_pvid = vlan->flags & BRIDGE_VLAN_INFO_PVID;
+	struct net_device *orig_dev = vlan->obj.orig_dev;
+	struct mvsw_pr_bridge_port *br_port;
+	struct mvsw_pr_bridge_device *bridge_device;
+	struct mvsw_pr_switch *sw = port->sw;
+	u16 vid;
+
+	if (netif_is_bridge_master(orig_dev))
+		return 0;
+
+	if (switchdev_trans_ph_commit(trans))
+		return 0;
+
+	br_port = mvsw_pr_bridge_port_find(sw->bridge, orig_dev);
+	if (WARN_ON(!br_port))
+		return -EINVAL;
+
+	bridge_device = br_port->bridge_device;
+	if (!bridge_device->vlan_enabled)
+		return 0;
+
+	for (vid = vlan->vid_begin; vid <= vlan->vid_end; vid++) {
+		int err;
+
+		err = mvsw_pr_bridge_port_vlan_add(port, br_port,
+						   vid, flag_untagged,
+						   flag_pvid, extack);
+		if (err)
+			return err;
+	}
+
+	if (list_is_singular(&bridge_device->port_list))
+		mvsw_pr_rif_enable(port->sw, bridge_device->dev);
+
+	return 0;
+}
+
+static int mvsw_pr_port_obj_add(struct net_device *dev,
+				const struct switchdev_obj *obj,
+				struct switchdev_trans *trans,
+				struct netlink_ext_ack *extack)
+{
+	int err = 0;
+	struct mvsw_pr_port *port = netdev_priv(dev);
+	const struct switchdev_obj_port_vlan *vlan;
+
+	switch (obj->id) {
+	case SWITCHDEV_OBJ_ID_PORT_VLAN:
+		vlan = SWITCHDEV_OBJ_PORT_VLAN(obj);
+		err = mvsw_pr_port_vlans_add(port, vlan, trans, extack);
+		break;
+	default:
+		err = -EOPNOTSUPP;
+	}
+
+	return err;
+}
+
+static void
+mvsw_pr_bridge_port_vlan_del(struct mvsw_pr_port *port,
+			     struct mvsw_pr_bridge_port *br_port, u16 vid)
+{
+	u16 pvid = port->pvid == vid ? 0 : port->pvid;
+	struct mvsw_pr_port_vlan *port_vlan;
+
+	port_vlan = mvsw_pr_port_vlan_find_by_vid(port, vid);
+	if (WARN_ON(!port_vlan))
+		return;
+
+	mvsw_pr_port_vlan_bridge_leave(port_vlan);
+	mvsw_pr_port_pvid_set(port, pvid);
+	mvsw_pr_port_vlan_destroy(port_vlan);
+}
+
+static int mvsw_pr_port_vlans_del(struct mvsw_pr_port *port,
+				  const struct switchdev_obj_port_vlan *vlan)
+{
+	struct mvsw_pr_switch *sw = port->sw;
+	struct net_device *orig_dev = vlan->obj.orig_dev;
+	struct mvsw_pr_bridge_port *br_port;
+	u16 vid;
+
+	if (netif_is_bridge_master(orig_dev))
+		return -EOPNOTSUPP;
+
+	br_port = mvsw_pr_bridge_port_find(sw->bridge, orig_dev);
+	if (WARN_ON(!br_port))
+		return -EINVAL;
+
+	if (!br_port->bridge_device->vlan_enabled)
+		return 0;
+
+	for (vid = vlan->vid_begin; vid <= vlan->vid_end; vid++)
+		mvsw_pr_bridge_port_vlan_del(port, br_port, vid);
+
+	return 0;
+}
+
+static int mvsw_pr_port_obj_del(struct net_device *dev,
+				const struct switchdev_obj *obj)
+{
+	int err = 0;
+	struct mvsw_pr_port *port = netdev_priv(dev);
+
+	switch (obj->id) {
+	case SWITCHDEV_OBJ_ID_PORT_VLAN:
+		err = mvsw_pr_port_vlans_del(port,
+					     SWITCHDEV_OBJ_PORT_VLAN(obj));
+		break;
+	default:
+		err = -EOPNOTSUPP;
+		break;
+	}
+
+	return err;
+}
+
+static int mvsw_pr_port_attr_br_vlan_set(struct mvsw_pr_port *port,
+					 struct switchdev_trans *trans,
+					 struct net_device *orig_dev,
+					 bool vlan_enabled)
+{
+	struct mvsw_pr_switch *sw = port->sw;
+	struct mvsw_pr_bridge_device *bridge_device;
+
+	if (!switchdev_trans_ph_prepare(trans))
+		return 0;
+
+	bridge_device = mvsw_pr_bridge_device_find(sw->bridge, orig_dev);
+	if (WARN_ON(!bridge_device))
+		return -EINVAL;
+
+	if (bridge_device->vlan_enabled == vlan_enabled)
+		return 0;
+
+	netdev_err(bridge_device->dev,
+		   "VLAN filtering can't be changed for existing bridge\n");
+	return -EINVAL;
+}
+
+static int mvsw_pr_port_attr_br_flags_set(struct mvsw_pr_port *port,
+					  struct switchdev_trans *trans,
+					  struct net_device *orig_dev,
+					  unsigned long flags)
+{
+	struct mvsw_pr_bridge_port *br_port;
+	int err;
+
+	if (switchdev_trans_ph_prepare(trans))
+		return 0;
+
+	br_port = mvsw_pr_bridge_port_find(port->sw->bridge, orig_dev);
+	if (!br_port)
+		return 0;
+
+	err = mvsw_pr_port_flood_set(port, flags & BR_FLOOD);
+	if (err)
+		return err;
+
+	err = mvsw_pr_port_learning_set(port, flags & BR_LEARNING);
+	if (err)
+		return err;
+
+	memcpy(&br_port->flags, &flags, sizeof(flags));
+	return 0;
+}
+
+static int mvsw_pr_port_attr_br_ageing_set(struct mvsw_pr_port *port,
+					   struct switchdev_trans *trans,
+					   unsigned long ageing_clock_t)
+{
+	int err;
+	struct mvsw_pr_switch *sw = port->sw;
+	unsigned long ageing_jiffies = clock_t_to_jiffies(ageing_clock_t);
+	u32 ageing_time = jiffies_to_msecs(ageing_jiffies) / 1000;
+
+	if (switchdev_trans_ph_prepare(trans)) {
+		if (ageing_time < MVSW_PR_MIN_AGEING_TIME ||
+		    ageing_time > MVSW_PR_MAX_AGEING_TIME)
+			return -ERANGE;
+		else
+			return 0;
+	}
+
+	err = mvsw_pr_switch_ageing_set(sw, ageing_time);
+	if (!err)
+		sw->bridge->ageing_time = ageing_time;
+
+	return err;
+}
+
+static int mvsw_pr_port_obj_attr_set(struct net_device *dev,
+				     const struct switchdev_attr *attr,
+				     struct switchdev_trans *trans)
+{
+	int err = 0;
+	struct mvsw_pr_port *port = netdev_priv(dev);
+
+	switch (attr->id) {
+	case SWITCHDEV_ATTR_ID_PORT_STP_STATE:
+		err = -EOPNOTSUPP;
+		break;
+	case SWITCHDEV_ATTR_ID_PORT_PRE_BRIDGE_FLAGS:
+		if (attr->u.brport_flags &
+		    ~(BR_LEARNING | BR_FLOOD | BR_MCAST_FLOOD))
+			err = -EINVAL;
+		break;
+	case SWITCHDEV_ATTR_ID_PORT_BRIDGE_FLAGS:
+		err = mvsw_pr_port_attr_br_flags_set(port, trans,
+						     attr->orig_dev,
+						     attr->u.brport_flags);
+		break;
+	case SWITCHDEV_ATTR_ID_BRIDGE_AGEING_TIME:
+		err = mvsw_pr_port_attr_br_ageing_set(port, trans,
+						      attr->u.ageing_time);
+		break;
+	case SWITCHDEV_ATTR_ID_BRIDGE_VLAN_FILTERING:
+		err = mvsw_pr_port_attr_br_vlan_set(port, trans,
+						    attr->orig_dev,
+						    attr->u.vlan_filtering);
+		break;
+	default:
+		err = -EOPNOTSUPP;
+	}
+
+	return err;
+}
+
+static void mvsw_fdb_offload_notify(struct mvsw_pr_port *port,
+				    struct switchdev_notifier_fdb_info *info)
+{
+	struct switchdev_notifier_fdb_info send_info;
+
+	send_info.addr = info->addr;
+	send_info.vid = info->vid;
+	send_info.offloaded = true;
+	call_switchdev_notifiers(SWITCHDEV_FDB_OFFLOADED,
+				 port->net_dev, &send_info.info, NULL);
+}
+
+static int
+mvsw_pr_port_fdb_set(struct mvsw_pr_port *port,
+		     struct switchdev_notifier_fdb_info *fdb_info, bool adding)
+{
+	struct mvsw_pr_switch *sw = port->sw;
+	struct mvsw_pr_bridge_port *br_port;
+	struct mvsw_pr_bridge_device *bridge_device;
+	struct net_device *orig_dev = fdb_info->info.dev;
+	int err;
+	u16 vid;
+
+	br_port = mvsw_pr_bridge_port_find(sw->bridge, orig_dev);
+	if (!br_port)
+		return -EINVAL;
+
+	bridge_device = br_port->bridge_device;
+
+	if (bridge_device->vlan_enabled)
+		vid = fdb_info->vid;
+	else
+		vid = bridge_device->bridge_id;
+
+	if (adding)
+		err = mvsw_pr_fdb_add(port, fdb_info->addr, vid, false);
+	else
+		err = mvsw_pr_fdb_del(port, fdb_info->addr, vid);
+
+	return err;
+}
+
+static void mvsw_pr_bridge_fdb_event_work(struct work_struct *work)
+{
+	int err = 0;
+	struct mvsw_pr_event_work *switchdev_work =
+	    container_of(work, struct mvsw_pr_event_work, work);
+	struct net_device *dev = switchdev_work->dev;
+	struct switchdev_notifier_fdb_info *fdb_info;
+	struct mvsw_pr_port *port;
+
+	rtnl_lock();
+	if (netif_is_vxlan(dev))
+		goto out;
+
+	port = mvsw_pr_port_dev_lower_find(dev);
+	if (!port)
+		goto out;
+
+	switch (switchdev_work->event) {
+	case SWITCHDEV_FDB_ADD_TO_DEVICE:
+		fdb_info = &switchdev_work->fdb_info;
+		if (!fdb_info->added_by_user)
+			break;
+		err = mvsw_pr_port_fdb_set(port, fdb_info, true);
+		if (err)
+			break;
+		mvsw_fdb_offload_notify(port, fdb_info);
+		break;
+	case SWITCHDEV_FDB_DEL_TO_DEVICE:
+		fdb_info = &switchdev_work->fdb_info;
+		mvsw_pr_port_fdb_set(port, fdb_info, false);
+		break;
+	case SWITCHDEV_FDB_ADD_TO_BRIDGE:
+	case SWITCHDEV_FDB_DEL_TO_BRIDGE:
+		break;
+	}
+
+out:
+	rtnl_unlock();
+	kfree(switchdev_work->fdb_info.addr);
+	kfree(switchdev_work);
+	dev_put(dev);
+}
+
+static int mvsw_pr_switchdev_event(struct notifier_block *unused,
+				   unsigned long event, void *ptr)
+{
+	int err = 0;
+	struct net_device *net_dev = switchdev_notifier_info_to_dev(ptr);
+	struct mvsw_pr_event_work *switchdev_work;
+	struct switchdev_notifier_fdb_info *fdb_info;
+	struct switchdev_notifier_info *info = ptr;
+	struct net_device *upper_br;
+
+	if (event == SWITCHDEV_PORT_ATTR_SET) {
+		err = switchdev_handle_port_attr_set(net_dev, ptr,
+						     mvsw_pr_netdev_check,
+						     mvsw_pr_port_obj_attr_set);
+		return notifier_from_errno(err);
+	}
+
+	upper_br = netdev_master_upper_dev_get_rcu(net_dev);
+	if (!upper_br)
+		return NOTIFY_DONE;
+
+	if (!netif_is_bridge_master(upper_br))
+		return NOTIFY_DONE;
+
+	switchdev_work = kzalloc(sizeof(*switchdev_work), GFP_ATOMIC);
+	if (!switchdev_work)
+		return NOTIFY_BAD;
+
+	switchdev_work->dev = net_dev;
+	switchdev_work->event = event;
+
+	switch (event) {
+	case SWITCHDEV_FDB_ADD_TO_DEVICE:
+	case SWITCHDEV_FDB_DEL_TO_DEVICE:
+	case SWITCHDEV_FDB_ADD_TO_BRIDGE:
+	case SWITCHDEV_FDB_DEL_TO_BRIDGE:
+		fdb_info = container_of(info,
+					struct switchdev_notifier_fdb_info,
+					info);
+
+		INIT_WORK(&switchdev_work->work, mvsw_pr_bridge_fdb_event_work);
+		memcpy(&switchdev_work->fdb_info, ptr,
+		       sizeof(switchdev_work->fdb_info));
+		switchdev_work->fdb_info.addr = kzalloc(ETH_ALEN, GFP_ATOMIC);
+		if (!switchdev_work->fdb_info.addr)
+			goto out;
+		ether_addr_copy((u8 *)switchdev_work->fdb_info.addr,
+				fdb_info->addr);
+		dev_hold(net_dev);
+
+		break;
+	case SWITCHDEV_VXLAN_FDB_ADD_TO_DEVICE:
+	case SWITCHDEV_VXLAN_FDB_DEL_TO_DEVICE:
+	default:
+		kfree(switchdev_work);
+		return NOTIFY_DONE;
+	}
+
+	queue_work(mvsw_owq, &switchdev_work->work);
+	return NOTIFY_DONE;
+out:
+	kfree(switchdev_work);
+	return NOTIFY_BAD;
+}
+
+static int mvsw_pr_switchdev_blocking_event(struct notifier_block *unused,
+					    unsigned long event, void *ptr)
+{
+	int err = 0;
+	struct net_device *net_dev = switchdev_notifier_info_to_dev(ptr);
+
+	switch (event) {
+	case SWITCHDEV_PORT_OBJ_ADD:
+		if (netif_is_vxlan(net_dev)) {
+			err = -EOPNOTSUPP;
+		} else {
+			err = switchdev_handle_port_obj_add
+			    (net_dev, ptr, mvsw_pr_netdev_check,
+			     mvsw_pr_port_obj_add);
+		}
+		break;
+	case SWITCHDEV_PORT_OBJ_DEL:
+		if (netif_is_vxlan(net_dev)) {
+			err = -EOPNOTSUPP;
+		} else {
+			err = switchdev_handle_port_obj_del
+			    (net_dev, ptr, mvsw_pr_netdev_check,
+			     mvsw_pr_port_obj_del);
+		}
+		break;
+	case SWITCHDEV_PORT_ATTR_SET:
+		err = switchdev_handle_port_attr_set
+		    (net_dev, ptr, mvsw_pr_netdev_check,
+		    mvsw_pr_port_obj_attr_set);
+		break;
+	default:
+		err = -EOPNOTSUPP;
+	}
+
+	return notifier_from_errno(err);
+}
+
+static struct mvsw_pr_bridge_device *
+mvsw_pr_bridge_device_create(struct mvsw_pr_bridge *bridge,
+			     struct net_device *br_dev)
+{
+	struct mvsw_pr_bridge_device *bridge_device;
+	bool vlan_enabled = br_vlan_enabled(br_dev);
+	u16 bridge_id;
+	int err;
+
+	if (vlan_enabled && bridge->bridge_8021q_exists) {
+		netdev_err(br_dev, "Only one VLAN-aware bridge is supported\n");
+		return ERR_PTR(-EINVAL);
+	}
+
+	bridge_device = kzalloc(sizeof(*bridge_device), GFP_KERNEL);
+	if (!bridge_device)
+		return ERR_PTR(-ENOMEM);
+
+	if (vlan_enabled) {
+		bridge->bridge_8021q_exists = true;
+	} else {
+		err = mvsw_pr_8021d_bridge_create(bridge->sw, &bridge_id);
+		if (err) {
+			kfree(bridge_device);
+			return ERR_PTR(err);
+		}
+
+		bridge_device->bridge_id = bridge_id;
+	}
+
+	bridge_device->dev = br_dev;
+	bridge_device->vlan_enabled = vlan_enabled;
+	bridge_device->multicast_enabled = br_multicast_enabled(br_dev);
+	bridge_device->mrouter = br_multicast_router(br_dev);
+	INIT_LIST_HEAD(&bridge_device->port_list);
+
+	list_add(&bridge_device->bridge_node, &bridge->bridge_list);
+
+	return bridge_device;
+}
+
+static void
+mvsw_pr_bridge_device_destroy(struct mvsw_pr_bridge *bridge,
+			      struct mvsw_pr_bridge_device *bridge_device)
+{
+	list_del(&bridge_device->bridge_node);
+	if (bridge_device->vlan_enabled)
+		bridge->bridge_8021q_exists = false;
+	else
+		mvsw_pr_8021d_bridge_delete(bridge->sw,
+					    bridge_device->bridge_id);
+
+	WARN_ON(!list_empty(&bridge_device->port_list));
+	kfree(bridge_device);
+}
+
+static struct mvsw_pr_bridge_device *
+mvsw_pr_bridge_device_get(struct mvsw_pr_bridge *bridge,
+			  struct net_device *br_dev)
+{
+	struct mvsw_pr_bridge_device *bridge_device;
+
+	bridge_device = mvsw_pr_bridge_device_find(bridge, br_dev);
+	if (bridge_device)
+		return bridge_device;
+
+	return mvsw_pr_bridge_device_create(bridge, br_dev);
+}
+
+static void
+mvsw_pr_bridge_device_put(struct mvsw_pr_bridge *bridge,
+			  struct mvsw_pr_bridge_device *bridge_device)
+{
+	if (list_empty(&bridge_device->port_list))
+		mvsw_pr_bridge_device_destroy(bridge, bridge_device);
+}
+
+static struct mvsw_pr_bridge_port *
+mvsw_pr_bridge_port_create(struct mvsw_pr_bridge_device *bridge_device,
+			   struct net_device *brport_dev)
+{
+	struct mvsw_pr_bridge_port *br_port;
+	struct mvsw_pr_port *port;
+
+	br_port = kzalloc(sizeof(*br_port), GFP_KERNEL);
+	if (!br_port)
+		return NULL;
+
+	port = mvsw_pr_port_dev_lower_find(brport_dev);
+
+	br_port->dev = brport_dev;
+	br_port->bridge_device = bridge_device;
+	br_port->stp_state = BR_STATE_DISABLED;
+	br_port->flags = BR_LEARNING | BR_FLOOD | BR_LEARNING_SYNC |
+				BR_MCAST_FLOOD;
+	INIT_LIST_HEAD(&br_port->vlan_list);
+	list_add(&br_port->bridge_device_node, &bridge_device->port_list);
+	br_port->ref_count = 1;
+
+	return br_port;
+}
+
+static void
+mvsw_pr_bridge_port_destroy(struct mvsw_pr_bridge_port *br_port)
+{
+	list_del(&br_port->bridge_device_node);
+	WARN_ON(!list_empty(&br_port->vlan_list));
+	kfree(br_port);
+}
+
+static struct mvsw_pr_bridge_port *
+mvsw_pr_bridge_port_get(struct mvsw_pr_bridge *bridge,
+			struct net_device *brport_dev)
+{
+	struct net_device *br_dev = netdev_master_upper_dev_get(brport_dev);
+	struct mvsw_pr_bridge_device *bridge_device;
+	struct mvsw_pr_bridge_port *br_port;
+	int err;
+
+	br_port = mvsw_pr_bridge_port_find(bridge, brport_dev);
+	if (br_port) {
+		br_port->ref_count++;
+		return br_port;
+	}
+
+	bridge_device = mvsw_pr_bridge_device_get(bridge, br_dev);
+	if (IS_ERR(bridge_device))
+		return ERR_CAST(bridge_device);
+
+	br_port = mvsw_pr_bridge_port_create(bridge_device, brport_dev);
+	if (!br_port) {
+		err = -ENOMEM;
+		goto err_brport_create;
+	}
+
+	return br_port;
+
+err_brport_create:
+	mvsw_pr_bridge_device_put(bridge, bridge_device);
+	return ERR_PTR(err);
+}
+
+static void mvsw_pr_bridge_port_put(struct mvsw_pr_bridge *bridge,
+				    struct mvsw_pr_bridge_port *br_port)
+{
+	struct mvsw_pr_bridge_device *bridge_device;
+
+	if (--br_port->ref_count != 0)
+		return;
+	bridge_device = br_port->bridge_device;
+	mvsw_pr_bridge_port_destroy(br_port);
+	if (list_empty(&bridge_device->port_list))
+		mvsw_pr_rif_disable(bridge->sw, bridge_device->dev);
+	mvsw_pr_bridge_device_put(bridge, bridge_device);
+}
+
+static int
+mvsw_pr_bridge_8021q_port_join(struct mvsw_pr_bridge_device *bridge_device,
+			       struct mvsw_pr_bridge_port *br_port,
+			       struct mvsw_pr_port *port,
+			       struct netlink_ext_ack *extack)
+{
+	if (is_vlan_dev(br_port->dev)) {
+		NL_SET_ERR_MSG_MOD(extack,
+				   "Can not enslave a VLAN device to a VLAN-aware bridge");
+		return -EINVAL;
+	}
+
+	return 0;
+}
+
+static int
+mvsw_pr_bridge_8021d_port_join(struct mvsw_pr_bridge_device *bridge_device,
+			       struct mvsw_pr_bridge_port *br_port,
+			       struct mvsw_pr_port *port,
+			       struct netlink_ext_ack *extack)
+{
+	int err;
+
+	if (is_vlan_dev(br_port->dev)) {
+		NL_SET_ERR_MSG_MOD(extack,
+				   "Enslaving of a VLAN device is not supported");
+		return -ENOTSUPP;
+	}
+	err = mvsw_pr_8021d_bridge_port_add(port, bridge_device->bridge_id);
+	if (err)
+		return err;
+
+	err = mvsw_pr_port_flood_set(port, br_port->flags & BR_FLOOD);
+	if (err)
+		goto err_port_flood_set;
+
+	err = mvsw_pr_port_learning_set(port, br_port->flags & BR_LEARNING);
+	if (err)
+		goto err_port_learning_set;
+
+	if (list_is_singular(&bridge_device->port_list))
+		mvsw_pr_rif_enable(port->sw, bridge_device->dev);
+
+	return err;
+
+err_port_learning_set:
+	mvsw_pr_port_flood_set(port, false);
+err_port_flood_set:
+	mvsw_pr_8021d_bridge_port_delete(port, bridge_device->bridge_id);
+	return err;
+}
+
+static int mvsw_pr_port_bridge_join(struct mvsw_pr_port *port,
+				    struct net_device *brport_dev,
+				    struct net_device *br_dev,
+				    struct netlink_ext_ack *extack)
+{
+	struct mvsw_pr_bridge_device *bridge_device;
+	struct mvsw_pr_switch *sw = port->sw;
+	struct mvsw_pr_bridge_port *br_port;
+	int err;
+
+	br_port = mvsw_pr_bridge_port_get(sw->bridge, brport_dev);
+	if (IS_ERR(br_port))
+		return PTR_ERR(br_port);
+
+	bridge_device = br_port->bridge_device;
+
+	if (bridge_device->vlan_enabled) {
+		err = mvsw_pr_bridge_8021q_port_join(bridge_device, br_port,
+						     port, extack);
+	} else {
+		err = mvsw_pr_bridge_8021d_port_join(bridge_device, br_port,
+						     port, extack);
+	}
+
+	if (err)
+		goto err_port_join;
+
+	return 0;
+
+err_port_join:
+	mvsw_pr_bridge_port_put(sw->bridge, br_port);
+	return err;
+}
+
+static void
+mvsw_pr_bridge_8021d_port_leave(struct mvsw_pr_bridge_device *bridge_device,
+				struct mvsw_pr_bridge_port *br_port,
+				struct mvsw_pr_port *port)
+{
+	mvsw_pr_fdb_flush_port(port, MVSW_PR_FDB_FLUSH_MODE_ALL);
+	mvsw_pr_8021d_bridge_port_delete(port, bridge_device->bridge_id);
+}
+
+static void
+mvsw_pr_bridge_8021q_port_leave(struct mvsw_pr_bridge_device *bridge_device,
+				struct mvsw_pr_bridge_port *br_port,
+				struct mvsw_pr_port *port)
+{
+	mvsw_pr_fdb_flush_port(port, MVSW_PR_FDB_FLUSH_MODE_ALL);
+	mvsw_pr_port_pvid_set(port, MVSW_PR_DEFAULT_VID);
+}
+
+static void mvsw_pr_port_bridge_leave(struct mvsw_pr_port *port,
+				      struct net_device *brport_dev,
+				      struct net_device *br_dev)
+{
+	struct mvsw_pr_switch *sw = port->sw;
+	struct mvsw_pr_bridge_device *bridge_device;
+	struct mvsw_pr_bridge_port *br_port;
+
+	bridge_device = mvsw_pr_bridge_device_find(sw->bridge, br_dev);
+	if (!bridge_device)
+		return;
+	br_port = __mvsw_pr_bridge_port_find(bridge_device, brport_dev);
+	if (!br_port)
+		return;
+
+	if (bridge_device->vlan_enabled)
+		mvsw_pr_bridge_8021q_port_leave(bridge_device, br_port, port);
+	else
+		mvsw_pr_bridge_8021d_port_leave(bridge_device, br_port, port);
+
+	mvsw_pr_port_learning_set(port, false);
+	mvsw_pr_port_flood_set(port, false);
+	mvsw_pr_bridge_port_put(sw->bridge, br_port);
+}
+
+static int mvsw_pr_netdevice_port_upper_event(struct net_device *lower_dev,
+					      struct net_device *dev,
+					      unsigned long event, void *ptr)
+{
+	struct netdev_notifier_changeupper_info *info;
+	struct mvsw_pr_port *port;
+	struct netlink_ext_ack *extack;
+	struct net_device *upper_dev;
+	struct mvsw_pr_switch *sw;
+	int err = 0;
+
+	port = netdev_priv(dev);
+	sw = port->sw;
+	info = ptr;
+	extack = netdev_notifier_info_to_extack(&info->info);
+
+	switch (event) {
+	case NETDEV_PRECHANGEUPPER:
+		upper_dev = info->upper_dev;
+		if (!netif_is_bridge_master(upper_dev)) {
+			NL_SET_ERR_MSG_MOD(extack, "Unknown upper device type");
+			return -EINVAL;
+		}
+		if (!info->linking)
+			break;
+		if (netdev_has_any_upper_dev(upper_dev) &&
+		    (!netif_is_bridge_master(upper_dev) ||
+		     !mvsw_pr_bridge_device_is_offloaded(sw, upper_dev))) {
+			NL_SET_ERR_MSG_MOD(extack,
+					   "Enslaving a port to a device that already has an upper device is not supported");
+			return -EINVAL;
+		}
+		break;
+	case NETDEV_CHANGEUPPER:
+		upper_dev = info->upper_dev;
+		if (netif_is_bridge_master(upper_dev)) {
+			if (info->linking)
+				err = mvsw_pr_port_bridge_join(port,
+							       lower_dev,
+							       upper_dev,
+							       extack);
+			else
+				mvsw_pr_port_bridge_leave(port,
+							  lower_dev,
+							  upper_dev);
+		}
+		break;
+	}
+
+	return err;
+}
+
+static int mvsw_pr_netdevice_port_event(struct net_device *lower_dev,
+					struct net_device *port_dev,
+					unsigned long event, void *ptr)
+{
+	switch (event) {
+	case NETDEV_PRECHANGEUPPER:
+	case NETDEV_CHANGEUPPER:
+		return mvsw_pr_netdevice_port_upper_event(lower_dev, port_dev,
+							  event, ptr);
+	case NETDEV_CHANGELOWERSTATE:
+		break;
+	}
+
+	return 0;
+}
+
+static int mvsw_pr_netdevice_bridge_event(struct net_device *br_dev,
+					  unsigned long event, void *ptr)
+{
+	struct mvsw_pr_switch *sw = mvsw_pr_switch_get(br_dev);
+	struct netdev_notifier_changeupper_info *info = ptr;
+	struct netlink_ext_ack *extack;
+
+	if (!sw)
+		return 0;
+
+	extack = netdev_notifier_info_to_extack(&info->info);
+
+	switch (event) {
+	case NETDEV_PRECHANGEUPPER:
+		/* TODO:  */
+		break;
+	case NETDEV_CHANGEUPPER:
+		/* TODO:  */
+		break;
+	}
+
+	return 0;
+}
+
+static bool mvsw_pr_is_vrf_event(unsigned long event, void *ptr)
+{
+	struct netdev_notifier_changeupper_info *info = ptr;
+
+	if (event != NETDEV_PRECHANGEUPPER && event != NETDEV_CHANGEUPPER)
+		return false;
+
+	return netif_is_l3_master(info->upper_dev);
+}
+
+static int mvsw_pr_netdevice_event(struct notifier_block *nb,
+				   unsigned long event, void *ptr)
+{
+	struct net_device *dev = netdev_notifier_info_to_dev(ptr);
+	struct mvsw_pr_switch *sw;
+	int err = 0;
+
+	sw = container_of(nb, struct mvsw_pr_switch, netdevice_nb);
+
+	if (event == NETDEV_PRE_CHANGEADDR ||
+	    event == NETDEV_CHANGEADDR)
+		err = mvsw_pr_netdevice_router_port_event(dev, event, ptr);
+	else if (mvsw_pr_is_vrf_event(event, ptr))
+		err = mvsw_pr_netdevice_vrf_event(dev, event, ptr);
+	else if (mvsw_pr_netdev_check(dev))
+		err = mvsw_pr_netdevice_port_event(dev, dev, event, ptr);
+	else if (netif_is_bridge_master(dev))
+		err = mvsw_pr_netdevice_bridge_event(dev, event, ptr);
+
+	return notifier_from_errno(err);
+}
+
+static int mvsw_pr_fdb_init(struct mvsw_pr_switch *sw)
+{
+	int err;
+
+	err = mvsw_pr_switch_ageing_set(sw, MVSW_PR_DEFAULT_AGEING_TIME);
+	if (err)
+		return err;
+
+	return 0;
+}
+
+static int mvsw_pr_switchdev_init(struct mvsw_pr_switch *sw)
+{
+	int err = 0;
+	struct mvsw_pr_switchdev *swdev;
+	struct mvsw_pr_bridge *bridge;
+
+	if (sw->switchdev)
+		return -EPERM;
+
+	bridge = kzalloc(sizeof(*sw->bridge), GFP_KERNEL);
+	if (!bridge)
+		return -ENOMEM;
+
+	swdev = kzalloc(sizeof(*sw->switchdev), GFP_KERNEL);
+	if (!swdev) {
+		kfree(bridge);
+		return -ENOMEM;
+	}
+
+	sw->bridge = bridge;
+	bridge->sw = sw;
+	sw->switchdev = swdev;
+	swdev->sw = sw;
+
+	INIT_LIST_HEAD(&sw->bridge->bridge_list);
+
+	mvsw_owq = alloc_ordered_workqueue("%s_ordered", 0, "prestera_sw");
+	if (!mvsw_owq) {
+		err = -ENOMEM;
+		goto err_alloc_workqueue;
+	}
+
+	swdev->swdev_n.notifier_call = mvsw_pr_switchdev_event;
+	err = register_switchdev_notifier(&swdev->swdev_n);
+	if (err)
+		goto err_register_switchdev_notifier;
+
+	swdev->swdev_blocking_n.notifier_call =
+			mvsw_pr_switchdev_blocking_event;
+	err = register_switchdev_blocking_notifier(&swdev->swdev_blocking_n);
+	if (err)
+		goto err_register_block_switchdev_notifier;
+
+	mvsw_pr_fdb_init(sw);
+
+	return 0;
+
+err_register_block_switchdev_notifier:
+	unregister_switchdev_notifier(&swdev->swdev_n);
+err_register_switchdev_notifier:
+	destroy_workqueue(mvsw_owq);
+err_alloc_workqueue:
+	kfree(swdev);
+	kfree(bridge);
+	return err;
+}
+
+static void mvsw_pr_switchdev_fini(struct mvsw_pr_switch *sw)
+{
+	if (!sw->switchdev)
+		return;
+
+	unregister_switchdev_notifier(&sw->switchdev->swdev_n);
+	unregister_switchdev_blocking_notifier
+	    (&sw->switchdev->swdev_blocking_n);
+	flush_workqueue(mvsw_owq);
+	destroy_workqueue(mvsw_owq);
+	kfree(sw->switchdev);
+	sw->switchdev = NULL;
+	kfree(sw->bridge);
+}
+
+static int mvsw_pr_netdev_init(struct mvsw_pr_switch *sw)
+{
+	int err = 0;
+
+	if (sw->netdevice_nb.notifier_call)
+		return -EPERM;
+
+	sw->netdevice_nb.notifier_call = mvsw_pr_netdevice_event;
+	err = register_netdevice_notifier(&sw->netdevice_nb);
+	return err;
+}
+
+static void mvsw_pr_netdev_fini(struct mvsw_pr_switch *sw)
+{
+	if (sw->netdevice_nb.notifier_call)
+		unregister_netdevice_notifier(&sw->netdevice_nb);
+}
+
+int mvsw_pr_switchdev_register(struct mvsw_pr_switch *sw)
+{
+	int err;
+
+	err = mvsw_pr_switchdev_init(sw);
+	if (err)
+		return err;
+
+	err = mvsw_pr_router_init(sw);
+
+	if (err) {
+		pr_err("Failed to initialize fib notifier\n");
+		goto err_fib_notifier;
+	}
+
+	err = mvsw_pr_netdev_init(sw);
+	if (err)
+		goto err_netdevice_notifier;
+
+	return 0;
+
+err_netdevice_notifier:
+	mvsw_pr_router_fini(sw);
+err_fib_notifier:
+	mvsw_pr_switchdev_fini(sw);
+	return err;
+}
+
+void mvsw_pr_switchdev_unregister(struct mvsw_pr_switch *sw)
+{
+	mvsw_pr_netdev_fini(sw);
+	mvsw_pr_router_fini(sw);
+	mvsw_pr_switchdev_fini(sw);
+}
diff --git a/drivers/net/ethernet/marvell/prestera_sw/tests/prestera_ipc_tests.c b/drivers/net/ethernet/marvell/prestera_sw/tests/prestera_ipc_tests.c
new file mode 100644
index 0000000..e389ef5
--- /dev/null
+++ b/drivers/net/ethernet/marvell/prestera_sw/tests/prestera_ipc_tests.c
@@ -0,0 +1,529 @@
+// SPDX-License-Identifier: BSD-3-Clause OR GPL-2.0
+/*
+ * Copyright (c) 2019-2020 Marvell International Ltd. All rights reserved.
+ *
+ */
+#include <linux/cpu.h>
+#include <linux/slab.h>
+#include <linux/random.h>
+#include <linux/atomic.h>
+#include <linux/completion.h>
+#include <linux/kthread.h>
+
+#include "prestera_tests.h"
+
+struct test_send_req_cmd {
+	struct mvsw_pr_test_cmd cmd;
+	u32 data_sum;
+	u32 data_num;
+	u32 data[0];
+} __packed __aligned(4);
+
+struct test_evt_gen_cmd {
+	struct mvsw_pr_test_cmd cmd;
+	u32 loop_num;
+	u32 data_num;
+	u32 evt_type;
+	u32 delay;
+	u32 flags;
+} __packed __aligned(4);
+
+struct test_ipc_evt {
+	u16 type;
+	u16 id;
+	u32 len;
+} __packed __aligned(4);
+
+struct test_ipc_evt_sum {
+	struct test_ipc_evt evt;
+	u32 data_sum;
+	u32 data_num;
+	u32 data[0];
+} __packed __aligned(4);
+
+#define MVSW_PR_TEST_SUM_NUM	512
+
+int test_ipc_send_req_init(struct mvsw_pr_test *test)
+{
+	struct test_send_req_cmd *cmd;
+	size_t cmd_size;
+
+	cmd_size = sizeof(struct test_send_req_cmd) + MVSW_PR_TEST_SUM_NUM * 4;
+
+	test->priv = kmalloc(cmd_size, GFP_KERNEL);
+	if (!test->priv)
+		return -ENOMEM;
+
+	cmd = test->priv;
+
+	cmd->cmd.type = MVSW_PR_TEST_CMD_RUN;
+	cmd->cmd.id = MVSW_PR_TEST_ID_SUM;
+	cmd->cmd.len = cmd_size;
+
+	return 0;
+}
+
+int test_ipc_send_req_fini(struct mvsw_pr_test *test)
+{
+	kfree(test->priv);
+	return 0;
+}
+
+static int mvsw_pr_test_calc_sum(struct mvsw_pr_test *test,
+				 struct test_send_req_cmd *cmd)
+{
+	int i;
+
+	cmd->data_sum = 0;
+	cmd->data_num = 0;
+
+	while (!cmd->data_num) {
+		cmd->data_num = prandom_u32();
+		cmd->data_num = cmd->data_num % MVSW_PR_TEST_SUM_NUM;
+	}
+
+	for (i = 0; i < cmd->data_num; i++) {
+		cmd->data[i] = prandom_u32();
+		cmd->data_sum += cmd->data[i];
+	}
+
+	return test_send_cmd(&cmd->cmd);
+}
+
+int test_ipc_send_req_run(struct mvsw_pr_test *test)
+{
+	struct test_send_req_cmd *sum_cmd = test->priv;
+	int err;
+	int i;
+
+	for (i = 0; i < 1000; i++) {
+		err = mvsw_pr_test_calc_sum(test, sum_cmd);
+		if (err)
+			return err;
+	}
+
+	return 0;
+}
+
+struct mvsw_pr_test_ipc_async {
+	struct test_send_req_cmd **data;
+	struct task_struct **tasks;
+	struct completion tasks_done;
+	atomic_t tasks_pending;
+	int num_tasks;
+};
+
+struct test_ipc_async_task_ctx {
+	struct mvsw_pr_test *test;
+	int task_id;
+};
+
+static int test_ipc_send_req_async_task(void *arg)
+{
+	struct test_ipc_async_task_ctx *ctx = arg;
+	struct mvsw_pr_test_ipc_async *test_priv = ctx->test->priv;
+	struct test_send_req_cmd *sum_cmd = test_priv->data[ctx->task_id];
+	int err = 0;
+	int i;
+
+	for (i = 0; i < 1000; i++) {
+		err = mvsw_pr_test_calc_sum(ctx->test, sum_cmd);
+		if (err)
+			goto done;
+	}
+
+done:
+	if (atomic_dec_return_relaxed(&test_priv->tasks_pending) == 0)
+		complete(&test_priv->tasks_done);
+
+	for (;;) {
+		set_current_state(TASK_INTERRUPTIBLE);
+		if (kthread_should_park())
+			break;
+
+		schedule();
+	}
+
+	kthread_parkme();
+	return err;
+}
+
+int test_ipc_send_req_async_init(struct mvsw_pr_test *test)
+{
+	struct mvsw_pr_test_ipc_async *test_priv;
+	int num_cpus = num_online_cpus();
+	struct task_struct *task;
+	size_t cmd_size;
+	int cpu;
+	int err;
+
+	test_priv = kmalloc(sizeof(*test_priv), GFP_KERNEL);
+	if (!test_priv)
+		return -ENOMEM;
+
+	test_priv->num_tasks = 0;
+
+	test_priv->data = kmalloc_array(num_cpus, sizeof(*test_priv->data),
+					GFP_KERNEL);
+	if (!test_priv->data) {
+		err = -ENOMEM;
+		goto err_malloc_array;
+	}
+
+	memset(test_priv->data, 0, sizeof(*test_priv->data) * num_cpus);
+
+	cmd_size = sizeof(struct test_send_req_cmd) + MVSW_PR_TEST_SUM_NUM * 4;
+
+	for (cpu = 0; cpu < num_cpus; cpu++) {
+		test_priv->data[cpu] = kmalloc(cmd_size, GFP_KERNEL);
+		if (!test_priv->data[cpu]) {
+			err = -ENOMEM;
+			goto err_malloc_data;
+		}
+
+		test_priv->data[cpu]->cmd.type = MVSW_PR_TEST_CMD_RUN;
+		test_priv->data[cpu]->cmd.id = MVSW_PR_TEST_ID_SUM;
+		test_priv->data[cpu]->cmd.len = cmd_size;
+	}
+
+	test_priv->tasks = kmalloc_array(num_cpus, sizeof(*test_priv->tasks),
+					 GFP_KERNEL);
+	if (!test_priv->tasks) {
+		err = -ENOMEM;
+		goto err_create_task;
+	}
+
+	for_each_online_cpu(cpu) {
+		struct test_ipc_async_task_ctx *ctx;
+
+		ctx = kmalloc(sizeof(*ctx), GFP_KERNEL);
+
+		ctx->task_id = test_priv->num_tasks;
+		ctx->test = test;
+
+		task = kthread_create(test_ipc_send_req_async_task, ctx,
+				      "test_ipc_send_req_async_task%d",
+				      cpu);
+
+		if (IS_ERR(task)) {
+			pr_err("failed to create test thread\n");
+			kfree(ctx);
+		} else {
+			test_priv->tasks[test_priv->num_tasks++] = task;
+			kthread_bind(task, cpu);
+		}
+	}
+
+	atomic_set(&test_priv->tasks_pending, test_priv->num_tasks);
+	init_completion(&test_priv->tasks_done);
+	test->priv = test_priv;
+	return 0;
+
+err_create_task:
+err_malloc_data:
+	for (cpu = 0; cpu < num_cpus; cpu++)
+		kfree(test_priv->data[cpu]);
+	kfree(test_priv->data);
+err_malloc_array:
+	kfree(test_priv);
+	test->priv = NULL;
+	return err;
+}
+
+int test_ipc_send_req_async_fini(struct mvsw_pr_test *test)
+{
+	struct mvsw_pr_test_ipc_async *test_priv = test->priv;
+	int i;
+
+	if (!test_priv)
+		return 0;
+
+	for (i = 0; i < test_priv->num_tasks; i++)
+		kfree(test_priv->data[i]);
+
+	kfree(test_priv->tasks);
+	kfree(test_priv->data);
+	kfree(test_priv);
+	return 0;
+}
+
+int test_ipc_send_req_async_run(struct mvsw_pr_test *test)
+{
+	struct mvsw_pr_test_ipc_async *test_priv = test->priv;
+	int err = 0;
+	int i;
+
+	for (i = 0; i < test_priv->num_tasks; i++)
+		wake_up_process(test_priv->tasks[i]);
+
+	wait_for_completion(&test_priv->tasks_done);
+
+	for (i = 0; i < test_priv->num_tasks; i++) {
+		err |= kthread_park(test_priv->tasks[i]);
+		err |= kthread_stop(test_priv->tasks[i]);
+	}
+
+	return err;
+}
+
+struct test_ipc_evt_recv_ctx {
+	u32 evts_count;
+	u32 evts_recvd;
+	u32 bytes_recvd;
+	u32 evt_type;
+	u32 data_num;
+	u32 err;
+};
+
+int test_ipc_evt_recv_init(struct mvsw_pr_test *test)
+{
+	struct test_ipc_evt_recv_ctx *ctx;
+
+	ctx = kmalloc(sizeof(*ctx), GFP_KERNEL);
+	if (!ctx)
+		return -ENOMEM;
+
+	memset(ctx, 0, sizeof(*ctx));
+	test->priv = ctx;
+	return 0;
+}
+
+int test_ipc_evt_recv_fini(struct mvsw_pr_test *test)
+{
+	kfree(test->priv);
+	return 0;
+}
+
+static void test_ipc_evt_recv_fn(u8 *evt, size_t len, void *arg)
+{
+	struct test_ipc_evt_sum *evt_sum = (struct test_ipc_evt_sum *)evt;
+	struct mvsw_pr_test *test = arg;
+	struct test_ipc_evt_recv_ctx *ctx = test->priv;
+	u32 sum = 0;
+	u32 i;
+
+	if (evt_sum->evt.type != ctx->evt_type) {
+		pr_err("evt(%u): err: type: exp(%u), act(%u)\n",
+		       ctx->evts_recvd, ctx->evt_type, evt_sum->evt.type);
+		ctx->err++;
+		goto done;
+	}
+	if (evt_sum->evt.len != len) {
+		pr_err("evt(%u): err: len: exp(%u), act(%u)\n",
+		       ctx->evts_recvd, len, evt_sum->evt.len);
+		ctx->err++;
+		goto done;
+	}
+	if (evt_sum->data_num != ctx->data_num) {
+		pr_err("evt(%u): err: count: exp(%u), act(%u)\n",
+		       ctx->evts_recvd, ctx->data_num, evt_sum->data_num);
+		ctx->err++;
+		goto done;
+	}
+
+	for (i = 0; i < evt_sum->data_num; i++)
+		sum += evt_sum->data[i];
+
+	if (sum != evt_sum->data_sum) {
+		pr_err("evt(%u): err: sum: exp(%u), act(%u)\n",
+		       ctx->evts_recvd, evt_sum->data_sum, sum);
+		ctx->err++;
+	}
+
+done:
+	ctx->bytes_recvd += len;
+	ctx->evts_recvd++;
+}
+
+static bool test_ipc_evt_recv_check(struct mvsw_pr_test *test)
+{
+	struct test_ipc_evt_recv_ctx *ctx = test->priv;
+
+	return ctx->err ||
+		(ctx->evts_recvd == ctx->evts_count);
+}
+
+static int test_ipc_evt_recv_run(struct mvsw_pr_test *test, u32 num)
+{
+	struct test_ipc_evt_recv_ctx *ctx = test->priv;
+	struct test_evt_gen_cmd evt_cmd;
+	int err = 0;
+
+	evt_cmd.cmd.type = MVSW_PR_TEST_CMD_RUN;
+	evt_cmd.cmd.id = MVSW_PR_TEST_ID_EVT;
+	evt_cmd.cmd.len = sizeof(evt_cmd);
+
+	evt_cmd.loop_num = num;
+	evt_cmd.data_num = 128;
+	evt_cmd.evt_type = 1;
+	evt_cmd.delay = 0;
+	evt_cmd.flags = 0;
+
+	ctx->evts_count = evt_cmd.loop_num;
+	ctx->evt_type = evt_cmd.evt_type;
+	ctx->data_num = evt_cmd.data_num;
+
+	test_evt_handler_set(test_ipc_evt_recv_fn, (void *)test);
+
+	err = test_send_cmd(&evt_cmd.cmd);
+	if (err)
+		goto done;
+
+	test_wait_timeout(test_ipc_evt_recv_check(test), 60 * 1000);
+
+	if (ctx->err)
+		err = -EINVAL;
+	if (ctx->evts_recvd != ctx->evts_count || !ctx->evts_recvd)
+		err = -EINVAL;
+
+done:
+	test_evt_handler_set(NULL, NULL);
+	return err;
+}
+
+int test_ipc_evt_recv_one_run(struct mvsw_pr_test *test)
+{
+	return test_ipc_evt_recv_run(test, 1);
+}
+
+int test_ipc_evt_recv_overflow_run(struct mvsw_pr_test *test)
+{
+	return test_ipc_evt_recv_run(test, 100000);
+}
+
+#define TEST_IPC_EVT_TYPE_MAX	4
+
+struct test_ipc_evt_type_recv_ctx {
+	struct test_ipc_evt_recv_ctx evts[TEST_IPC_EVT_TYPE_MAX];
+	int count;
+	int err;
+};
+
+int test_ipc_evt_type_recv_async_init(struct mvsw_pr_test *test)
+{
+	struct test_ipc_evt_type_recv_ctx *ctx;
+
+	ctx = kmalloc(sizeof(*ctx), GFP_KERNEL);
+	if (!ctx)
+		return -ENOMEM;
+
+	memset(ctx, 0, sizeof(*ctx));
+	test->priv = ctx;
+	return 0;
+}
+
+int test_ipc_evt_type_recv_async_fini(struct mvsw_pr_test *test)
+{
+	kfree(test->priv);
+	return 0;
+}
+
+static void test_ipc_evt_type_recv_fn(u8 *evt, size_t len, void *arg)
+{
+	struct test_ipc_evt_sum *evt_sum = (struct test_ipc_evt_sum *)evt;
+	struct mvsw_pr_test *test = arg;
+	struct test_ipc_evt_type_recv_ctx *ctx = test->priv;
+	struct test_ipc_evt_recv_ctx *evt_ctx;
+	u32 sum = 0;
+	u32 i;
+
+	if (evt_sum->evt.type >= ctx->count) {
+		pr_err("evt: err: type: greater than %u\n", ctx->count);
+		ctx->err++;
+		return;
+	}
+
+	evt_ctx = &ctx->evts[evt_sum->evt.type];
+
+	if (evt_sum->evt.len != len) {
+		pr_err("evt(%u): err: len: exp(%u), act(%u)\n",
+		       evt_ctx->evts_recvd, len, evt_sum->evt.len);
+		ctx->err++;
+		goto done;
+	}
+	if (evt_sum->data_num != evt_ctx->data_num) {
+		pr_err("evt(%u): err: count: exp(%u), act(%u)\n",
+		       evt_ctx->evts_recvd, evt_ctx->data_num,
+		       evt_sum->data_num);
+		ctx->err++;
+		goto done;
+	}
+
+	for (i = 0; i < evt_sum->data_num; i++)
+		sum += evt_sum->data[i];
+
+	if (sum != evt_sum->data_sum) {
+		pr_err("evt(%u): err: sum: exp(%u), act(%u)\n",
+		       evt_ctx->evts_recvd, evt_sum->data_sum, sum);
+		ctx->err++;
+	}
+
+done:
+	evt_ctx->bytes_recvd += len;
+	evt_ctx->evts_recvd++;
+}
+
+static bool test_ipc_evt_type_recv_check(struct mvsw_pr_test *test, u32 type)
+{
+	struct test_ipc_evt_type_recv_ctx *ctx = test->priv;
+	struct test_ipc_evt_recv_ctx *evt_ctx = &ctx->evts[type];
+
+	return ctx->err ||
+		(evt_ctx->evts_recvd == evt_ctx->evts_count);
+}
+
+int test_ipc_evt_type_recv_async_run(struct mvsw_pr_test *test)
+{
+	struct test_ipc_evt_type_recv_ctx *ctx = test->priv;
+	struct test_evt_gen_cmd evt_cmd;
+	int count = TEST_IPC_EVT_TYPE_MAX;
+	int err = 0;
+	int i;
+
+	evt_cmd.cmd.type = MVSW_PR_TEST_CMD_RUN;
+	evt_cmd.cmd.id = MVSW_PR_TEST_ID_EVT;
+	evt_cmd.cmd.len = sizeof(evt_cmd);
+
+	evt_cmd.loop_num = 1000;
+	evt_cmd.data_num = 128;
+	evt_cmd.delay = 0;
+	evt_cmd.flags = 0;
+
+	ctx->count = count;
+
+	for (i = 0; i < count; i++) {
+		struct test_ipc_evt_recv_ctx *evt_ctx = &ctx->evts[i];
+
+		evt_cmd.evt_type = i;
+
+		evt_ctx->evts_count = evt_cmd.loop_num;
+		evt_ctx->evt_type = evt_cmd.evt_type;
+		evt_ctx->data_num = evt_cmd.data_num;
+
+		test_evt_handler_set(test_ipc_evt_type_recv_fn, (void *)test);
+
+		err = test_send_cmd(&evt_cmd.cmd);
+		if (err)
+			goto done;
+	}
+
+	for (i = 0; i < count; i++) {
+		struct test_ipc_evt_recv_ctx *evt_ctx = &ctx->evts[i];
+
+		test_wait_timeout(test_ipc_evt_type_recv_check(test, i),
+				  60 * 1000);
+
+		if (evt_ctx->evts_recvd != evt_ctx->evts_count ||
+		    !evt_ctx->evts_recvd) {
+			pr_err("(%d)evts_recvd: exp(%u), act(%u)\n",
+			       i, evt_ctx->evts_count, evt_ctx->evts_recvd);
+			err = -EINVAL;
+		} else if (evt_ctx->err) {
+			err = -EINVAL;
+		}
+	}
+
+done:
+	test_evt_handler_set(NULL, NULL);
+	return err;
+}
diff --git a/drivers/net/ethernet/marvell/prestera_sw/tests/prestera_tests.c b/drivers/net/ethernet/marvell/prestera_sw/tests/prestera_tests.c
new file mode 100644
index 0000000..b5a5b6a
--- /dev/null
+++ b/drivers/net/ethernet/marvell/prestera_sw/tests/prestera_tests.c
@@ -0,0 +1,426 @@
+// SPDX-License-Identifier: BSD-3-Clause OR GPL-2.0
+/*
+ * Copyright (c) 2019-2020 Marvell International Ltd. All rights reserved.
+ *
+ */
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/debugfs.h>
+#include <linux/uaccess.h>
+
+#include "../prestera.h"
+#include "prestera_tests.h"
+
+#define MVSW_TESTS_DIR "mvsw_tests"
+
+static ssize_t mvsw_pr_test_status_fs_read(struct file *file,
+					   char __user *ubuf,
+					   size_t count, loff_t *ppos);
+static ssize_t mvsw_pr_test_list_fs_read(struct file *file,
+					 char __user *ubuf,
+					 size_t count, loff_t *ppos);
+static ssize_t mvsw_pr_test_select_fs_read(struct file *file,
+					   char __user *ubuf,
+					   size_t count, loff_t *ppos);
+static ssize_t mvsw_pr_test_select_fs_write(struct file *file,
+					    const char __user *ubuf,
+					    size_t count, loff_t *ppos);
+static ssize_t mvsw_pr_test_run_fs_write(struct file *file,
+					 const char __user *ubuf,
+					 size_t count, loff_t *ppos);
+
+static const struct file_operations fs_status_ops = {
+	.read = mvsw_pr_test_status_fs_read,
+	.open = simple_open,
+	.llseek = default_llseek,
+};
+
+static const struct file_operations fs_select_ops = {
+	.read = mvsw_pr_test_select_fs_read,
+	.write = mvsw_pr_test_select_fs_write,
+	.open = simple_open,
+	.llseek = default_llseek,
+};
+
+static const struct file_operations fs_list_ops = {
+	.read = mvsw_pr_test_list_fs_read,
+	.open = simple_open,
+	.llseek = default_llseek,
+};
+
+static const struct file_operations fs_run_ops = {
+	.write = mvsw_pr_test_run_fs_write,
+	.open = simple_open,
+	.llseek = default_llseek,
+};
+
+static void (*evt_recv_func)(u8 *evt, size_t len, void *arg);
+static void *evt_recv_arg;
+static DEFINE_SPINLOCK(evt_recv_lock);
+
+static struct mvsw_pr_test tests[] = {
+	{
+		.name = "test_ipc_send_req",
+		.init = test_ipc_send_req_init,
+		.fini = test_ipc_send_req_fini,
+		.run = test_ipc_send_req_run,
+	},
+	{
+		.name = "test_ipc_send_req_async",
+		.init = test_ipc_send_req_async_init,
+		.fini = test_ipc_send_req_async_fini,
+		.run = test_ipc_send_req_async_run,
+	},
+	{
+		.name = "test_ipc_evt_one",
+		.init = test_ipc_evt_recv_init,
+		.fini = test_ipc_evt_recv_fini,
+		.run = test_ipc_evt_recv_one_run,
+	},
+	{
+		.name = "test_ipc_evt_overflow",
+		.init = test_ipc_evt_recv_init,
+		.fini = test_ipc_evt_recv_fini,
+		.run = test_ipc_evt_recv_overflow_run,
+	},
+	{
+		.name = "test_ipc_evt_type_async",
+		.init = test_ipc_evt_type_recv_async_init,
+		.fini = test_ipc_evt_type_recv_async_fini,
+		.run = test_ipc_evt_type_recv_async_run,
+	},
+	{},
+};
+
+#define mvsw_pr_for_each_test(t) \
+	for (t = &tests[0]; (t)->name; (t)++)
+
+#define mvsw_pr_for_each_selected_test(t) \
+	for (t = &tests[0]; (t)->name; (t)++) \
+		if ((t)->is_selected)
+
+static struct mvsw_pr_device *test_device;
+
+static struct dentry *fs_tests_dir;
+static struct dentry *fs_status;
+static struct dentry *fs_select;
+static struct dentry *fs_list;
+static struct dentry *fs_run;
+
+static int mvsw_pr_test_init(struct mvsw_pr_test *test)
+{
+	int err = 0;
+
+	test->err = 0;
+
+	if (test->init) {
+		err = test->init(test);
+		if (err)
+			pr_err("test: %s: failed init\n", test->name);
+	}
+
+	return err;
+}
+
+static int mvsw_pr_test_fini(struct mvsw_pr_test *test)
+{
+	int err = 0;
+
+	if (test->fini) {
+		err = test->fini(test);
+		if (err)
+			pr_err("test: %s: failed fini\n", test->name);
+	}
+
+	return err;
+}
+
+static int mvsw_pr_test_run(struct mvsw_pr_test *test)
+{
+	test->flags = MVSW_PR_TEST_F_RUNNING;
+	test->err = 0;
+
+	test->err = test->run(test);
+	if (test->err)
+		pr_err("test: %s: [FAILED]\n", test->name);
+	else
+		pr_info("test: %s: [PASSED]\n", test->name);
+
+	test->flags |= MVSW_PR_TEST_F_FINISHED;
+
+	return test->err;
+}
+
+static ssize_t mvsw_pr_test_status_fs_read(struct file *file, char __user *ubuf,
+					   size_t count, loff_t *ppos)
+{
+	struct mvsw_pr_test *t;
+	char buf[1024];
+
+	buf[0] = '\0';
+
+	mvsw_pr_for_each_selected_test(t) {
+		if (!(t->flags & MVSW_PR_TEST_F_FINISHED))
+			continue;
+
+		strcat(buf, t->name);
+		if (t->err)
+			strcat(buf, "    FAIL");
+		else
+			strcat(buf, "    OK");
+
+		strcat(buf, "\n");
+	}
+
+	return simple_read_from_buffer(ubuf, count, ppos, buf, strlen(buf));
+}
+
+static ssize_t mvsw_pr_test_select_fs_read(struct file *file, char __user *ubuf,
+					   size_t count, loff_t *ppos)
+{
+	struct mvsw_pr_test *test;
+	char buf[1024];
+
+	buf[0] = '\0';
+
+	mvsw_pr_for_each_selected_test(test) {
+		strcat(buf, test->name);
+		strcat(buf, "\n");
+	}
+
+	return simple_read_from_buffer(ubuf, count, ppos, buf, strlen(buf));
+}
+
+static ssize_t mvsw_pr_test_select_fs_write(struct file *file,
+					    const char __user *ubuf,
+					    size_t count, loff_t *ppos)
+{
+	size_t min_len = 1024;
+	size_t len = min(count, min_len);
+	struct mvsw_pr_test *t;
+	bool select_all;
+	char buf[1024];
+	int err;
+
+	err = copy_from_user(buf, ubuf, len);
+	if (err)
+		return err;
+
+	buf[len - 1] = '\0';
+
+	/* clear selected tests */
+	if (strcmp("reset", buf) == 0) {
+		mvsw_pr_for_each_selected_test(t) {
+			if (!t->is_selected)
+				continue;
+
+			t->is_selected = false;
+			t->flags = 0;
+		}
+
+		goto exit;
+	}
+
+	select_all = strcmp(buf, "all") == 0;
+
+	mvsw_pr_for_each_test(t) {
+		if (select_all || strcmp(buf, t->name) == 0) {
+			if (t->is_selected)
+				continue;
+
+			t->is_selected = true;
+			t->flags = 0;
+		}
+	}
+
+exit:
+	return len;
+}
+
+static ssize_t mvsw_pr_test_run_fs_write(struct file *file,
+					 const char __user *ubuf,
+					 size_t count, loff_t *ppos)
+{
+	size_t min_len = 2;
+	size_t len = min(count, min_len);
+	struct mvsw_pr_test *t;
+	char buf[2];
+	int err;
+
+	err = copy_from_user(buf, ubuf, len);
+	if (err)
+		return err;
+
+	if (buf[0] == '1') {
+		mvsw_pr_for_each_selected_test(t) {
+			pr_info("test: %s running ...\n", t->name);
+
+			err = mvsw_pr_test_init(t);
+			if (err)
+				continue;
+
+			mvsw_pr_test_run(t);
+			mvsw_pr_test_fini(t);
+		}
+	}
+
+	return len;
+}
+
+static ssize_t mvsw_pr_test_list_fs_read(struct file *file, char __user *ubuf,
+					 size_t count, loff_t *ppos)
+{
+	struct mvsw_pr_test *t;
+	char buf[1024];
+
+	buf[0] = '\0';
+
+	mvsw_pr_for_each_test(t) {
+		strcat(buf, t->name);
+		strcat(buf, "\n");
+	}
+
+	return simple_read_from_buffer(ubuf, count, ppos, buf, strlen(buf));
+}
+
+int test_send_cmd(struct mvsw_pr_test_cmd *cmd)
+{
+	struct mvsw_pr_test_ret ret;
+	int err;
+
+	err = test_device->send_req(test_device, (u8 *)cmd, cmd->len,
+				    (u8 *)&ret, sizeof(ret), 0);
+	if (err)
+		return err;
+	if (ret.status != MVSW_PR_TEST_RET_OK)
+		return -EINVAL;
+
+	return 0;
+}
+
+void test_evt_handler_set(void (*recv)(u8 *buf, size_t len, void *arg),
+			  void *arg)
+{
+	unsigned long flags;
+
+	spin_lock_irqsave(&evt_recv_lock, flags);
+	evt_recv_arg = arg;
+	evt_recv_func = recv;
+	spin_unlock_irqrestore(&evt_recv_lock, flags);
+}
+
+static int mvsw_pr_tests_setup(void)
+{
+	struct mvsw_pr_test_cmd cmd;
+	int err;
+
+	cmd.type = MVSW_PR_TEST_CMD_INIT;
+	cmd.len = sizeof(cmd);
+
+	err = test_send_cmd(&cmd);
+	if (err) {
+		pr_err("error while sending test init cmd\n");
+		return err;
+	}
+
+	fs_tests_dir = debugfs_create_dir("mvsw_pr_tests", NULL);
+	if (!fs_tests_dir) {
+		pr_err("failed to create tests dir\n");
+		return -EINVAL;
+	}
+
+	fs_status = debugfs_create_file("status", 0644,
+					fs_tests_dir, NULL,
+					&fs_status_ops);
+	if (!fs_status) {
+		debugfs_remove(fs_tests_dir);
+		return -EINVAL;
+	}
+
+	fs_list = debugfs_create_file("list", 0644,
+				      fs_tests_dir, NULL,
+				      &fs_list_ops);
+	if (!fs_list) {
+		debugfs_remove(fs_status);
+		debugfs_remove(fs_tests_dir);
+		return -EINVAL;
+	}
+
+	fs_select = debugfs_create_file("select", 0644,
+					fs_tests_dir, NULL,
+					&fs_select_ops);
+	if (!fs_select) {
+		debugfs_remove(fs_list);
+		debugfs_remove(fs_status);
+		debugfs_remove(fs_tests_dir);
+		return -EINVAL;
+	}
+
+	fs_run = debugfs_create_file("run", 0644,
+				     fs_tests_dir, NULL,
+				     &fs_run_ops);
+	if (!fs_run) {
+		debugfs_remove(fs_status);
+		debugfs_remove(fs_select);
+		debugfs_remove(fs_tests_dir);
+	}
+
+	return 0;
+}
+
+static int test_evt_recv_msg(struct mvsw_pr_device *dev, u8 *msg, size_t size)
+{
+	void (*func)(u8 *evt, size_t len, void *arg);
+	unsigned long flags;
+	void *arg;
+
+	spin_lock_irqsave(&evt_recv_lock, flags);
+	func = evt_recv_func;
+	arg = evt_recv_arg;
+	spin_unlock_irqrestore(&evt_recv_lock, flags);
+
+	if (func)
+		func(msg, size, arg);
+
+	return 0;
+}
+
+int mvsw_pr_device_register(struct mvsw_pr_device *dev)
+{
+	dev->recv_msg = test_evt_recv_msg;
+	test_device = dev;
+
+	return mvsw_pr_tests_setup();
+}
+EXPORT_SYMBOL(mvsw_pr_device_register);
+
+void mvsw_pr_device_unregister(struct mvsw_pr_device *dev)
+{
+	test_device = NULL;
+}
+EXPORT_SYMBOL(mvsw_pr_device_unregister);
+
+static int __init mvsw_pr_tests_init(void)
+{
+	pr_info("Loading Marvell Prestera testing module\n");
+
+	return 0;
+}
+
+static void __exit mvsw_pr_tests_exit(void)
+{
+	pr_info("Un-loading Marvell Prestera testing module\n");
+
+	debugfs_remove(fs_run);
+	debugfs_remove(fs_list);
+	debugfs_remove(fs_status);
+	debugfs_remove(fs_select);
+	debugfs_remove(fs_tests_dir);
+}
+
+module_init(mvsw_pr_tests_init);
+module_exit(mvsw_pr_tests_exit);
+
+MODULE_AUTHOR("Marvell Semi.");
+MODULE_LICENSE("GPL");
+MODULE_DESCRIPTION("Marvell Prestera switch driver");
diff --git a/drivers/net/ethernet/marvell/prestera_sw/tests/prestera_tests.h b/drivers/net/ethernet/marvell/prestera_sw/tests/prestera_tests.h
new file mode 100644
index 0000000..6489215
--- /dev/null
+++ b/drivers/net/ethernet/marvell/prestera_sw/tests/prestera_tests.h
@@ -0,0 +1,100 @@
+/* SPDX-License-Identifier: BSD-3-Clause OR GPL-2.0
+ *
+ * Copyright (c) 2019-2020 Marvell International Ltd. All rights reserved.
+ *
+ */
+
+#ifndef _MVSW_PRESTERA_TESTS_H_
+#define _MVSW_PRESTERA_TESTS_H_
+
+#include <linux/kernel.h>
+
+#define test_wait_timeout(cond, waitms) \
+({ \
+	unsigned long __wait_end = jiffies + msecs_to_jiffies(waitms); \
+	bool __wait_ret = false; \
+	do { \
+		if (cond) { \
+			__wait_ret = true; \
+			break; \
+		} \
+		cond_resched(); \
+	} while (time_before(jiffies, __wait_end)); \
+	__wait_ret; \
+})
+
+enum {
+	MVSW_PR_TEST_RUNNING,
+	MVSW_PR_TEST_SUCCESS,
+	MVSW_PR_TEST_FAILED,
+};
+
+enum {
+	MVSW_PR_TEST_CMD_INIT,
+	MVSW_PR_TEST_CMD_RUN,
+};
+
+enum {
+	MVSW_PR_TEST_ID_SUM,
+	MVSW_PR_TEST_ID_EVT,
+};
+
+enum {
+	MVSW_PR_TEST_RET_OK,
+	MVSW_PR_TEST_RET_FAIL,
+};
+
+enum {
+	MVSW_PR_TEST_F_FINISHED = BIT(0),
+	MVSW_PR_TEST_F_RUNNING	= BIT(1),
+};
+
+struct mvsw_pr_test_ret {
+	u32 status;
+	u32 value;
+} __packed __aligned(4);
+
+struct mvsw_pr_test_cmd {
+	u16 type;
+	u16 id;
+	u32 len;
+} __packed __aligned(4);
+
+struct mvsw_pr_test {
+	const char *name;
+	bool is_selected;
+	u32 flags;
+
+	int (*init)(struct mvsw_pr_test *test);
+	int (*fini)(struct mvsw_pr_test *test);
+	int (*run)(struct mvsw_pr_test *test);
+
+	void *priv;
+	int err;
+};
+
+int test_send_cmd(struct mvsw_pr_test_cmd *cmd);
+
+void test_evt_handler_set(void (*recv)(u8 *buf, size_t len, void *arg),
+			  void *arg);
+
+/* tests declaration */
+
+int test_ipc_send_req_init(struct mvsw_pr_test *test);
+int test_ipc_send_req_fini(struct mvsw_pr_test *test);
+int test_ipc_send_req_run(struct mvsw_pr_test *test);
+
+int test_ipc_send_req_async_init(struct mvsw_pr_test *test);
+int test_ipc_send_req_async_fini(struct mvsw_pr_test *test);
+int test_ipc_send_req_async_run(struct mvsw_pr_test *test);
+
+int test_ipc_evt_recv_init(struct mvsw_pr_test *test);
+int test_ipc_evt_recv_fini(struct mvsw_pr_test *test);
+int test_ipc_evt_recv_one_run(struct mvsw_pr_test *test);
+int test_ipc_evt_recv_overflow_run(struct mvsw_pr_test *test);
+
+int test_ipc_evt_type_recv_async_init(struct mvsw_pr_test *test);
+int test_ipc_evt_type_recv_async_fini(struct mvsw_pr_test *test);
+int test_ipc_evt_type_recv_async_run(struct mvsw_pr_test *test);
+
+#endif /* _MVSW_PRESTERA_TESTS_H_ */
